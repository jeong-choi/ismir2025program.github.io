


<!DOCTYPE html>
<html lang="en">
  <head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <link rel="icon" type="image/png" sizes="32x32" href="static/images/ismir_tab_icon.png">

    <link rel="stylesheet" href="static/css/main.css" />
    <link rel="stylesheet" href="static/css/custom.css" />
    <link rel="stylesheet" href="static/css/lazy_load.css" />
    <link rel="stylesheet" href="static/css/typeahead.css" />
    <link rel="stylesheet" href="static/css/poster.css" />
    <link rel="stylesheet" href="static/css/music.css" />
    <link rel="stylesheet" href="static/css/piece.css" />


    <!-- <link rel="stylesheet" href="https://gitcdn.xyz/repo/wingkwong/jquery-chameleon/master/src/chameleon.min.css"> -->

    <!-- External Javascript libs  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

    <script
      src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
      integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
      crossorigin="anonymous"
    ></script>


    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>
    <!-- <script src="static/js/chameleon.js"></script> -->


    <!-- Library libs -->
    <script src="static/js/typeahead.bundle.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
      href="static/css/Lato.css"
      rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet" />
    <link
      href="static/css/Cuprum.css"
      rel="stylesheet"
    />
    <link
      href="static/css/Ambiant.css"
      rel="stylesheet"
    />
    <!-- <link href="static/css/chameleon.css" rel="stylesheet"/> -->
    <title>ISMIR 2025: CultureMERT: Continual Pre-Training for Cross-Cultural Music Representation Learning</title>
    
<meta name="citation_title" content="CultureMERT: Continual Pre-Training for Cross-Cultural Music Representation Learning" />

<meta name="citation_author" content="Angelos-Nikolaos Kanatas" />

<meta name="citation_author" content="Charilaos Papaioannou" />

<meta name="citation_author" content="Alexandros Potamianos" />

<meta name="citation_publication_date" content="21-25 September 2025" />
<meta name="citation_conference_title" content="Ismir 2025 Hybrid Conference" />
<meta name="citation_inbook_title" content="Proceedings of the First MiniCon Conference" />
<meta name="citation_abstract" content="Recent advances in music foundation models have improved audio representation learning, yet their effectiveness across diverse musical traditions remains limited. We introduce CultureMERT-95M, a multi-culturally adapted foundation model developed to enhance cross-cultural music representation learning and understanding. To achieve this, we propose a two-stage continual pre-training strategy that integrates learning rate re-warming and re-decaying, enabling stable adaptation even with limited computational resources. Training on a 650-hour multi-cultural data mix, comprising Greek, Turkish, and Indian music traditions, results in an average improvement of 4.9% in ROC-AUC and AP across diverse non-Western music auto-tagging tasks, surpassing prior state-of-the-art, with minimal forgetting on Western-centric benchmarks. We further investigate task arithmetic, an alternative approach to multi-cultural adaptation that merges single-culture adapted models in the weight space. Task arithmetic performs on par with our multi-culturally trained model on non-Western auto-tagging tasks and shows no regression on Western datasets. Cross-cultural evaluation reveals that single-culture models transfer with varying effectiveness across musical traditions, whereas the multi-culturally adapted model achieves the best overall performance. To support research on world music representation learning, we publicly release CultureMERT-95M and CultureMERT-TA-95M, fostering the development of more culturally aware music foundation models.&lt;br&gt;&lt;br&gt; &lt;b&gt;&lt;p align=&#34;center&#34;&gt;[Direct link to video]()&lt;/b&gt;" />

<meta name="citation_keywords" content="Computational ethnomusicology" />

<meta name="citation_keywords" content="Machine learning/artificial intelligence for music" />

<meta name="citation_keywords" content="Automatic classification" />

<meta name="citation_keywords" content="MIR tasks" />

<meta name="citation_keywords" content="Metadata, tags, linked data, and semantic web" />

<meta name="citation_keywords" content="MIR fundamentals and methodology" />

<meta name="citation_keywords" content="Knowledge-driven approaches to MIR" />

<meta name="citation_pdf_url" content="" />
<meta id="yt-id" data-name= />
<meta id="bb-id" data-name= />


  </head>

  <body>
    <!-- NAV -->
    
    <!--

    ('tutorials.html', 'Tutorials'),
    ('index.html, 'Home'),
    ('special_meetings.html', 'Special Meetings'),
    -->

    <nav
      class="navbar sticky-top navbar-expand-lg navbar-light mr-auto"
      id="main-nav"
    >
      <div class="container">
        <a class="navbar-brand" href="https://ismir2025.ismir.net">
          <img
             class="logo" src="static/images/ismir_tabicon.png"
             height="auto"
             width="300px"
          />
        </a>
        
        <button
          class="navbar-toggler"
          type="button"
          data-toggle="collapse"
          data-target="#navbarNav"
          aria-controls="navbarNav"
          aria-expanded="false"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div
          class="collapse navbar-collapse text-right flex-grow-1"
          id="navbarNav"
        >
          <ul class="navbar-nav ml-auto">
            
            <li class="nav-item ">
              <a class="nav-link" href="calendar.html">Schedule</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="papers.html">Papers</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="music.html">Music</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="lbds.html">LBDs</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="industry.html?session=Platinum">Industry</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="jobs.html">Jobs</a>
            </li>
            
          </ul>
        </div>
      </div>
    </nav>
    

    
    <!-- User Overrides -->
     

    <div class="container">
      <!-- Tabs -->
      <div class="tabs">
         
      </div>
      <!-- Content -->
      <div class="content">
        

<!-- Title -->
<div class="pp-card m-3" style="">
  <div class="card-header">
    <h2 class="card-title main-title text-center" style="">
      P5-07: CultureMERT: Continual Pre-Training for Cross-Cultural Music Representation Learning
    </h2>
    <h3 class="card-subtitle mb-2 text-muted text-center">
      
      <a href="papers.html?filter=authors&search=Angelos-Nikolaos Kanatas" class="text-muted"
        >Angelos-Nikolaos Kanatas</a
      >,
      
      <a href="papers.html?filter=authors&search=Charilaos Papaioannou" class="text-muted"
        >Charilaos Papaioannou</a
      >,
      
      <a href="papers.html?filter=authors&search=Alexandros Potamianos" class="text-muted"
        >Alexandros Potamianos</a
      >
      
    </h3>
    <p class="card-text text-center">
      <span class="">Subjects:</span>
      
      <a
        href="papers.html?filter=keywords&search=Computational ethnomusicology"
        class="text-secondary text-decoration-none"
        >Computational ethnomusicology</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Machine learning/artificial intelligence for music"
        class="text-secondary text-decoration-none"
        >Machine learning/artificial intelligence for music</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Automatic classification"
        class="text-secondary text-decoration-none"
        >Automatic classification</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=MIR tasks"
        class="text-secondary text-decoration-none"
        >MIR tasks</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Metadata, tags, linked data, and semantic web"
        class="text-secondary text-decoration-none"
        >Metadata, tags, linked data, and semantic web</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=MIR fundamentals and methodology"
        class="text-secondary text-decoration-none"
        >MIR fundamentals and methodology</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Knowledge-driven approaches to MIR"
        class="text-secondary text-decoration-none"
        >Knowledge-driven approaches to MIR</a
      > 
      
    </p>
    <h4 class="text-muted text-center">
      Presented In-person, in Daejeon
    </h4>
    <h4 class="text-muted text-center">
      
        4-minute short-format presentation
      
    </h4>

  </div>
</div>
<div style="display: flex; justify-content: center;" class="btn-toolbar mt-4" role="toolbar">
  <div class="poster-buttons btn-group btn-group-toggle mb-3" data-toggle="buttons">
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#details">
      Abstract
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#paper">
      Paper
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#poster">
      Poster
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#video">
      Video
    </button>
    
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#meta-review">
        Meta
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review1">
        R1
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review2">
        R2
      </button>
      
        <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review3">
          R3
        </button>
      
    
    <!--  -->
  </div>
  
</div>
<div class="poster-content">
  <div id="details" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="abstractExample">
          <span class="font-weight-bold">Abstract:</span>
          <p>Recent advances in music foundation models have improved audio representation learning, yet their effectiveness across diverse musical traditions remains limited. We introduce CultureMERT-95M, a multi-culturally adapted foundation model developed to enhance cross-cultural music representation learning and understanding. To achieve this, we propose a two-stage continual pre-training strategy that integrates learning rate re-warming and re-decaying, enabling stable adaptation even with limited computational resources. Training on a 650-hour multi-cultural data mix, comprising Greek, Turkish, and Indian music traditions, results in an average improvement of 4.9% in ROC-AUC and AP across diverse non-Western music auto-tagging tasks, surpassing prior state-of-the-art, with minimal forgetting on Western-centric benchmarks. We further investigate task arithmetic, an alternative approach to multi-cultural adaptation that merges single-culture adapted models in the weight space. Task arithmetic performs on par with our multi-culturally trained model on non-Western auto-tagging tasks and shows no regression on Western datasets. Cross-cultural evaluation reveals that single-culture models transfer with varying effectiveness across musical traditions, whereas the multi-culturally adapted model achieves the best overall performance. To support research on world music representation learning, we publicly release CultureMERT-95M and CultureMERT-TA-95M, fostering the development of more culturally aware music foundation models.<br><br> <b><p align="center"><a href="">Direct link to video</a></b></p>
        </div>
      </div>
      <p></p>
    </div>
  </div>
  <div id="paper" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "https://drive.google.com/file/d/1jv2olgDNTX6u2OVUvIvF8PtwMELFAiWH/preview#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="poster" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="video" class="collapse">
    
      <div  style="display: flex; justify-content: center;">
      <iframe src="" width="960" height="540" allow="encrypted-media" allowfullscreen></iframe>
      </div>
    
  </div>
  
  <div id="meta-review" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="metaReviewExample">
          <span class="font-weight-bold">Meta Review:</span>
          <br>
          <p><strong>Q2 ( I am an expert on the topic of the paper.)</strong></p>
<p>Agree</p>
<p><strong>Q3 ( The title and abstract reflect the content of the paper.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q4 (The paper discusses, cites and compares with all relevant related work.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by “n” pages of references or ethical considerations, references are well formatted). If you selected “No”, please explain the issue in your comments.)</strong></p>
<p>Yes</p>
<p><strong>Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q9 (Scholarly/scientific quality: The content is scientifically correct.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view "novelty" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)</strong></p>
<p>Agree</p>
<p><strong>Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)</strong></p>
<p>Agree</p>
<p><strong>Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated “Strongly Agree” and “Agree” can be highlighted, but please do not penalize papers rated “Disagree” or “Strongly Disagree”. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)</strong></p>
<p>Disagree (Standard topic, task, or application)</p>
<p><strong>Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)</strong></p>
<p>Agree</p>
<p><strong>Q15 (Please explain your assessment of reusable insights in the paper.)</strong></p>
<p>The paper provides how continual pretraining can be effectively applied to adapt foundation models like MERT for cross-cultural music tasks, even with limited computational resources.</p>
<p><strong>Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)</strong></p>
<p>A two-stage training strategy allows foundation models to be adapted for cross-cultural music understanding with limited data.</p>
<p><strong>Q17 (This paper is of award-winning quality.)</strong></p>
<p>No</p>
<p><strong>Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)</strong></p>
<p>Agree</p>
<p><strong>Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)</strong></p>
<p>Strong accept</p>
<p><strong>Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)</strong></p>
<p>This paper presents a method that fine-tuning the MERT model so that it can become a multicultural music understanding model. The retrained MERT model shows improved performance across four multicultural music tasks and two Western music tasks, outperforming uni-cultural systems and other state-of-the-art baselines.</p>
<p>The main contribution is a two-stage pretraining strategy. In the first stage, the Transformer encoder is frozen and only a small subset of the data was used to train. The second stage performs full adaptation. The model is designed to predict correct EnCodec tokens to make sure the acoustic features are learned, as well as a CQT reconstruction loss.</p>
<p>Overall, the paper is readable and the results are promising, showing strong performance on several tasks.</p>
<p>This strong paper could be strengthened by addressing the following points.</p>
<ul>
<li>
<p>The authors chose to update the CNN part only while freezing the Transformer encoder, but the rationale for this decision is not clearly explained. Other strategies, such as gradually updating a few layers of the Transformer encoder, could also be considered. It would strengthen the paper to discuss these options and explain why the chosen method was selected.</p>
</li>
<li>
<p>The paper already covers four non-western music genres, however, to be more inclusive and general, it would be helpful to discuss how to handle new genres beyond the four. Section 5.1 touches on this but not general enough to support the claim of a multicultural model.</p>
</li>
</ul>
<p><strong>Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid “weak accepts” or “weak rejects” if possible.)</strong></p>
<p>Strong accept</p>
<p><strong>Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))</strong></p>
<p>This paper presents a method that fine-tuning the MERT model so that it can become a multicultural music understanding model. The retrained MERT model shows improved performance across four multicultural music tasks and two Western music tasks, outperforming uni-cultural systems and other state-of-the-art baselines.</p>
<p>The main contribution is a two-stage pretraining strategy. In the first stage, the Transformer encoder is frozen and only a small subset of the data was used to train. The second stage performs full adaptation. The model is designed to predict correct EnCodec tokens to make sure the acoustic features are learned, as well as a CQT reconstruction loss.</p>
<p>The paper is well-written, well-organized, and makes a meaningful contribution toward more inclusive, cross-cultural MIR. As all reviewers have noted, its strengths are substantial and its weaknesses are minor and addressable.</p>
<p>Thus, I recommend a strong accept.</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review1" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review1Example">
          <span class="font-weight-bold">Review 1:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Strong accept</li>
</ul>
<h4>Main Reviews</h4>
<p>In this paper, the authors present a framework to adapt the pre-trained MERT foundation model to more diverse music, i.e., from non-western traditions. Specifically, two different approaches are discussed. In the first, a single multi-cultural MERT model is fine-tuned on the entire merged dataset, while in the second, a culturally specialized MERT model is created for each cultural dataset with the outputs aggregated through task-arithmetic. Additionally, in both cases, a continual pre-training strategy is adopted to improve the stability of the fine-tuning and ensure knowledge retention. The evaluation is performed on a music-tagging task by adding a simple MLP on top of each MERT feature extractor. Results demonstrate that both strategies outperform the original MERT on non-western tasks without degrading performances on the original western music datasets.</p>
<p>The paper addresses a highly relevant topic and offers useful insights for the MIR community. The experimental setup is detailed explicitly, and results are clearly commented on. Overall, the paper is clear and well-written. However, I do have one minor comment for the authors. I believe it would be interesting to apply a similar evaluation to other MIR tasks which are less dependent on a music's cultural origin, such as music emotion recognition.</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review2" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review2Example">
          <span class="font-weight-bold">Review 2:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Strong accept</li>
</ul>
<h4>Main Reviews</h4>
<p>The paper is well written and structured. 
It addresses the Western-centric bias prevalent in current foundation models for music by exploring two adaptation strategies aimed at improving performance on underrepresented musical traditions. The authors demonstrate state-of-the-art results on a variety of downstream tasks (e.g., genre and instrument classification) across multiple non-Western benchmarks.</p>
<p>The proposed approach leverages a two-stage continual pretraining scheme to produce both single-culture and multi-cultural variants of the original MERT model. Futhermore single-culture models are aggregating using task-arithmetic. The results show that the multicultural model performs more robustly across non-Western benchmarks than models adapted to individual cultures.</p>
<p>Methodology: 
- In Section 3.2, it is unclear whether the single-culture adapted models also undergo the stabilization phase (Stage 1) using 20% of the Music4All data.</p>
<p>Questions:</p>
<ul>
<li>Can a track be in the training split for pre-training and then be in the test set for the probing? </li>
<li>Given the comparable size of pre-training data (1k hours vs 650) did you try training from scratch a culturally balanced?</li>
<li>If I understand Section 3.2 correctly, is the two-stage CPT strategy primarily motivated by memory or batch size constraints? In other words, if one were able to match the original model's batch size (1.5 hours of audio), would it be possible—and perhaps preferable—to train all layers directly on the multi-cultural data without the need for a stabilization phase?</li>
<li>In Table 2, could the performance gap between CultureMERT and single-culture MERT be attributed to the different amounts of data used during the CPT phase?</li>
</ul>
        </div>
      </div>
    </div> 
  </div>
  
    <div id="review3" class="pp-card m-3 collapse">
      <div class="card-body">
        <div class="card-text">
          <div id="review3Example">
            <span class="font-weight-bold">Review 3:</span>
            <br>
            <ul>
<li><strong>Overall Evaluation</strong>: Weak accept</li>
</ul>
<h4>Main Reviews</h4>
<p>The paper generally reads well, and the introduction is particularly well formed to ease into the topic. The paper clearly wants to explain concepts, methods and their goals to the reader, and generally constructs good connections to existing works and raises notable points, e.g. explanation of Eq. 5 simplifying to weight averaging under a certain condition.
Along this route, some redundancy is introduced across Chapters 1 to 3, which can occasionally create some confusion as necessary details for understanding are explained later, but that is not referred to in the earlier parts (i.e. “see details in Section 3.2”).
The method of Task Arithmetic is particularly interesting and may generate discourse due to its “intuitiveness” and capability of reducing computational cost. The results of CultureMERT-TA are slightly below CultureMERT, with both models exhibiting different strengths on individual music traditions, but still well above the baseline.
All in all, the foundation seems solid, and the figures help in supporting claims and gaining some insights into the architecture and model performances.</p>
<p>The training details could be clearer, as across the paper different numbers are scattered and only fall together in Section 4.3: “30-second segments” (146), “160 seconds per step” (251), “5-second audio segments” (318), and “batch size of 32 recordings (160 seconds)” (349). This information could be made clearer across the manuscript (e.g. simple references).</p>
<p>The improvement average of 4.43% in ROC-AUC only appears in the abstract and the introduction, but cannot be found for example in Table 2: the average here is only 3.2% between MERT-v1 and CultureMERT.</p>
<p>Section 2, 159: The last point (ii) is unclear. You want to generalize but ensure that the model was exposed to everything before evaluation?</p>
<p>Section 3.2: Solid approach to stabilize CPT. This section explains approaches, methods, and problems equally (e.g. stability gap). Instead of simply introducing the term “plasticity gradient” (234), you could make the connection clearer to the stability gap (257) from the get-go. Generally, the “Staged Adaptation” paragraph in this section repeats info, and introduces concepts in the beginning that are explained again at the end, but in a much clearer way (256-264). I suggest to merge the info here for better flow and reducing confusion.</p>
<p>241: Why is Music4All incorporated here for the Western “distribution shift safety”? Two other datasets were mentioned initially to represent Western music: MTAT and FMA-medium. What is the reasoning here, or the differences?</p>
<p>276: This may be a confusion in my understanding, but model (i) is not only trained on non-Western due to the inclusion of Western music in stage 1 for the stability gap, correct? Maybe it should say that it spans the four non-Western musical traditions AND the Western one, because preventing forgetting is one of the core goals?</p>
<p>343: “We apply a maximum duration cut as in [45].” It would be helpful to give a short detail here.</p>
<p>Table 2: How would you explain why the performance of LyraMERT on itself is worse than e.g. CarnaticMERT on Lyra? In 411-413 you give hints for makam vs Carnatic, that they share theoretical foundations etc. Do you think that extends to Lyra as well? According to Fig. 3, there are no striking similarities between Carnatic and Lyra as compared to others.</p>
<p>Figure 1, arrows pointing from the dataset to several parts of the architecture: not completely clear what they signify.</p>
<p>Table 1 caption: What does “Western replay” mean?</p>
<p>References are generally well formatted, but could be reduced with abbreviations (e.g. Proc. vs. Proceedings, removing editors where not strictly necessary, and writing the year only once, for example in [33], [34] and more...)</p>
          </div>
        </div>
      </div> 
    </div>
  
  
</div>


<script src="static/js/poster.js"></script>
<script src="static/js/video_selection.js"></script>

      </div>
    </div>
    
    

    <!-- Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "UA-");
    </script>

    <!-- Footer -->
    <footer class="footer bg-light p-4">
      <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2023 ISMIR Organization Committee</p>
      </div>
    </footer>

    <!-- Code for hash tags -->
    <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });
      });
    </script>
    <script src="static/js/lazy_load.js"></script>
    
  </body>
</html>