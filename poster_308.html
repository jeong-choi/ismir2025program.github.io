


<!DOCTYPE html>
<html lang="en">
  <head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <link rel="icon" type="image/png" sizes="32x32" href="static/images/ismir_tab_icon.png">

    <link rel="stylesheet" href="static/css/main.css" />
    <link rel="stylesheet" href="static/css/custom.css" />
    <link rel="stylesheet" href="static/css/lazy_load.css" />
    <link rel="stylesheet" href="static/css/typeahead.css" />
    <link rel="stylesheet" href="static/css/poster.css" />
    <link rel="stylesheet" href="static/css/music.css" />
    <link rel="stylesheet" href="static/css/piece.css" />


    <!-- <link rel="stylesheet" href="https://gitcdn.xyz/repo/wingkwong/jquery-chameleon/master/src/chameleon.min.css"> -->

    <!-- External Javascript libs  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

    <script
      src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
      integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
      crossorigin="anonymous"
    ></script>


    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>
    <!-- <script src="static/js/chameleon.js"></script> -->


    <!-- Library libs -->
    <script src="static/js/typeahead.bundle.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
      href="static/css/Lato.css"
      rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet" />
    <link
      href="static/css/Cuprum.css"
      rel="stylesheet"
    />
    <link
      href="static/css/Ambiant.css"
      rel="stylesheet"
    />
    <!-- <link href="static/css/chameleon.css" rel="stylesheet"/> -->
    <title>ISMIR 2025: Are you really listening? Boosting Perceptual Awareness in Music-QA Benchmarks</title>
    
<meta name="citation_title" content="Are you really listening? Boosting Perceptual Awareness in Music-QA Benchmarks" />

<meta name="citation_author" content="Yongyi Zang" />

<meta name="citation_author" content="Sean O&#39;Brien" />

<meta name="citation_author" content="Taylor Berg-Kirkpatrick" />

<meta name="citation_author" content="Julian McAuley" />

<meta name="citation_author" content="Zachary Novack" />

<meta name="citation_publication_date" content="21-25 September 2025" />
<meta name="citation_conference_title" content="Ismir 2025 Hybrid Conference" />
<meta name="citation_inbook_title" content="Proceedings of the First MiniCon Conference" />
<meta name="citation_abstract" content="Large Audio Language Models (LALMs), where pretrained text LLMs are finetuned with audio input, have made remarkable progress in music understanding. However, current evaluation methodologies exhibit critical limitations: on the leading Music Question Answering benchmark, MuchoMusic, text-only LLMs without audio perception capabilities achieve surprisingly high accuracy of up to 56.4%, much higher than chance. Furthermore, when presented with random Gaussian noise instead of actual audio, LALMs still perform significantly above chance. These findings suggest existing benchmarks predominantly assess reasoning abilities rather than audio perception. To overcome this challenge, we present RUListening, a framework that enhances perceptual evaluation in Music-QA benchmarks. We introduce the Perceptual Index (PI), a quantitative metric that measures a question&#39;s reliance on audio perception by analyzing log probability distributions from text-only language models. Using this metric, we generate synthetic, challenging distractors to create QA pairs that necessitate genuine audio perception. When applied to MuchoMusic, our filtered dataset successfully forces models to rely on perceptual information—text-only LLMs perform at chance levels, while LALMs similarly deteriorate when audio inputs are replaced with noise. These results validate our framework&#39;s effectiveness in creating benchmarks that more accurately evaluate audio perception capabilities.&lt;br&gt;&lt;br&gt; &lt;b&gt;&lt;p align=&#34;center&#34;&gt;[Direct link to video]()&lt;/b&gt;" />

<meta name="citation_keywords" content="Evaluation, datasets, and reproducibility" />

<meta name="citation_keywords" content="Evaluation methodology" />

<meta name="citation_keywords" content="Novel datasets and use cases" />

<meta name="citation_keywords" content="Evaluation metrics" />

<meta name="citation_pdf_url" content="" />
<meta id="yt-id" data-name= />
<meta id="bb-id" data-name= />


  </head>

  <body>
    <!-- NAV -->
    
    <!--

    ('tutorials.html', 'Tutorials'),
    ('index.html, 'Home'),
    ('special_meetings.html', 'Special Meetings'),
    -->

    <nav
      class="navbar sticky-top navbar-expand-lg navbar-light mr-auto"
      id="main-nav"
    >
      <div class="container">
        <a class="navbar-brand" href="https://ismir2025.ismir.net">
          <img
             class="logo" src="static/images/ismir_tabicon.png"
             height="auto"
             width="300px"
          />
        </a>
        
        <button
          class="navbar-toggler"
          type="button"
          data-toggle="collapse"
          data-target="#navbarNav"
          aria-controls="navbarNav"
          aria-expanded="false"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div
          class="collapse navbar-collapse text-right flex-grow-1"
          id="navbarNav"
        >
          <ul class="navbar-nav ml-auto">
            
            <li class="nav-item ">
              <a class="nav-link" href="calendar.html">Schedule</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="papers.html">Papers</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="music.html">Music</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="lbds.html">LBDs</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="industry.html?session=Platinum">Industry</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="jobs.html">Jobs</a>
            </li>
            
          </ul>
        </div>
      </div>
    </nav>
    

    
    <!-- User Overrides -->
     

    <div class="container">
      <!-- Tabs -->
      <div class="tabs">
         
      </div>
      <!-- Content -->
      <div class="content">
        

<!-- Title -->
<div class="pp-card m-3" style="">
  <div class="card-header">
    <h2 class="card-title main-title text-center" style="">
      P3-01: Are you really listening? Boosting Perceptual Awareness in Music-QA Benchmarks
    </h2>
    <h3 class="card-subtitle mb-2 text-muted text-center">
      
      <a href="papers.html?filter=authors&search=Yongyi Zang" class="text-muted"
        >Yongyi Zang</a
      >,
      
      <a href="papers.html?filter=authors&search=Sean O&#39;Brien" class="text-muted"
        >Sean O&#39;Brien</a
      >,
      
      <a href="papers.html?filter=authors&search=Taylor Berg-Kirkpatrick" class="text-muted"
        >Taylor Berg-Kirkpatrick</a
      >,
      
      <a href="papers.html?filter=authors&search=Julian McAuley" class="text-muted"
        >Julian McAuley</a
      >,
      
      <a href="papers.html?filter=authors&search=Zachary Novack" class="text-muted"
        >Zachary Novack</a
      >
      
    </h3>
    <p class="card-text text-center">
      <span class="">Subjects:</span>
      
      <a
        href="papers.html?filter=keywords&search=Evaluation, datasets, and reproducibility"
        class="text-secondary text-decoration-none"
        >Evaluation, datasets, and reproducibility</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Evaluation methodology"
        class="text-secondary text-decoration-none"
        >Evaluation methodology</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Novel datasets and use cases"
        class="text-secondary text-decoration-none"
        >Novel datasets and use cases</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Evaluation metrics"
        class="text-secondary text-decoration-none"
        >Evaluation metrics</a
      > 
      
    </p>
    <h4 class="text-muted text-center">
      Presented In-person, in Daejeon
    </h4>
    <h4 class="text-muted text-center">
      
        10-minute long-format presentation
      
    </h4>

  </div>
</div>
<div style="display: flex; justify-content: center;" class="btn-toolbar mt-4" role="toolbar">
  <div class="poster-buttons btn-group btn-group-toggle mb-3" data-toggle="buttons">
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#details">
      Abstract
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#paper">
      Paper
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#poster">
      Poster
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#video">
      Video
    </button>
    
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#meta-review">
        Meta
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review1">
        R1
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review2">
        R2
      </button>
      
        <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review3">
          R3
        </button>
      
    
    <!--  -->
  </div>
  
</div>
<div class="poster-content">
  <div id="details" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="abstractExample">
          <span class="font-weight-bold">Abstract:</span>
          <p>Large Audio Language Models (LALMs), where pretrained text LLMs are finetuned with audio input, have made remarkable progress in music understanding. However, current evaluation methodologies exhibit critical limitations: on the leading Music Question Answering benchmark, MuchoMusic, text-only LLMs without audio perception capabilities achieve surprisingly high accuracy of up to 56.4%, much higher than chance. Furthermore, when presented with random Gaussian noise instead of actual audio, LALMs still perform significantly above chance. These findings suggest existing benchmarks predominantly assess reasoning abilities rather than audio perception. To overcome this challenge, we present RUListening, a framework that enhances perceptual evaluation in Music-QA benchmarks. We introduce the Perceptual Index (PI), a quantitative metric that measures a question's reliance on audio perception by analyzing log probability distributions from text-only language models. Using this metric, we generate synthetic, challenging distractors to create QA pairs that necessitate genuine audio perception. When applied to MuchoMusic, our filtered dataset successfully forces models to rely on perceptual information—text-only LLMs perform at chance levels, while LALMs similarly deteriorate when audio inputs are replaced with noise. These results validate our framework's effectiveness in creating benchmarks that more accurately evaluate audio perception capabilities.<br><br> <b><p align="center"><a href="">Direct link to video</a></b></p>
        </div>
      </div>
      <p></p>
    </div>
  </div>
  <div id="paper" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "https://drive.google.com/file/d/1lFUDrQF9AioV1W5ktCpAM_RMerXMY7LY/preview#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="poster" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="video" class="collapse">
    
      <div  style="display: flex; justify-content: center;">
      <iframe src="" width="960" height="540" allow="encrypted-media" allowfullscreen></iframe>
      </div>
    
  </div>
  
  <div id="meta-review" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="metaReviewExample">
          <span class="font-weight-bold">Meta Review:</span>
          <br>
          <p><strong>Q2 ( I am an expert on the topic of the paper.)</strong></p>
<p>Agree</p>
<p><strong>Q3 ( The title and abstract reflect the content of the paper.)</strong></p>
<p>Agree</p>
<p><strong>Q4 (The paper discusses, cites and compares with all relevant related work.)</strong></p>
<p>Agree</p>
<p><strong>Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by “n” pages of references or ethical considerations, references are well formatted). If you selected “No”, please explain the issue in your comments.)</strong></p>
<p>Yes</p>
<p><strong>Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q9 (Scholarly/scientific quality: The content is scientifically correct.)</strong></p>
<p>Agree</p>
<p><strong>Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view "novelty" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated “Strongly Agree” and “Agree” can be highlighted, but please do not penalize papers rated “Disagree” or “Strongly Disagree”. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)</strong></p>
<p>Agree (Novel topic, task, or application)</p>
<p><strong>Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q15 (Please explain your assessment of reusable insights in the paper.)</strong></p>
<p>The paper highlights the limitations of current Q/A benchmark datasets used for evaluating large audio-language models and provides a methodology to improve them. Current datasets, such as MuChoMusic, are prone to allowing correct answers to be guessed based solely on the text input modality. The paper proposes a method for automatically creating distractor answers that are more challenging for text-only models.</p>
<p><strong>Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)</strong></p>
<p>A method to increase the difficulty of Q/A benchmarks for large audio-language models to avoid their overreliance on the text modality.</p>
<p><strong>Q17 (This paper is of award-winning quality.)</strong></p>
<p>Yes</p>
<p><strong>Q18 ( If yes, please explain why it should be awarded.)</strong></p>
<p>The paper presents a methodology and a new dataset that is capable of advancing the evaluation of music understanding by large audio-text models, which is a hot topic in MIR and audio AI research.</p>
<p><strong>Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)</strong></p>
<p>Strong accept</p>
<p><strong>Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)</strong></p>
<p>The paper presents a methodology and a new dataset that is capable of advancing the evaluation of music understanding by large audio-text models, which is a hot topic in MIR and audio AI research.</p>
<p>Large audio-text models are a hot topic, and evaluating their music understanding capabilities is of high importance. The paper continues recent efforts to create QA benchmark datasets, building on the MuChoMusic dataset proposed at ISMIR 2024. The authors highlight its key drawback, demonstrating how state-of-the-art (SOTA) text-only LLMs can achieve relatively high performance on this benchmark by exploiting their prior knowledge to effectively disregard part of the distractor answers.</p>
<p>The paper proposes a method to measure the difficulty of distractor answers in the benchmark in terms of the actual necessity of audio inputs (perceptual index). Using LLMs, the authors propose a method to generate synthetic distractor answers that are more challenging, eliminating the text bias present in the original MuChoMusic dataset. The authors contribute the resulting dataset as a new benchmark.</p>
<p>Overall, the approach is sound, the paper is well-written, and it represents an important contribution to the field.</p>
<p><strong>Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid “weak accepts” or “weak rejects” if possible.)</strong></p>
<p>Strong accept</p>
<p><strong>Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))</strong></p>
<p>All reviewers agree on the significant relevance and value of the contributions in the paper for advancing LALM benchmarks. The paper is well-written and presents a sound methodology, and the reviewers concur on a strong recommendation for acceptance. There are specific questions from reviewers that should be addressed to further improve the paper. We expect relevant changes to be implemented in the camera-ready submission.</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review1" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review1Example">
          <span class="font-weight-bold">Review 1:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Strong accept</li>
</ul>
<h4>Main Reviews</h4>
<p>This paper presents a compelling and much-needed investigation into the perceptual awareness of audio language models within music question-answering benchmarks. The authors convincingly demonstrate that MuchoMusic can often be solved by text-only models relying on reasoning and prior knowledge, rather than genuine audio perception. The proposed RUListening framework, which introduces a perceptual index metric to quantify a question’s reliance on audio and a method to generate more challenging, perceptually-demanding distractors, is a valuable and well-reasoned approach, and can be very useful to the community.. </p>
<p>The experiments are well designed and an extensive selection of models, including very large ones, are tested. The results support the central hypothesis, showing a significant reduction in text-only model performance and increased sensitivity of audio language models to audio input. The authors also qualitatively investigate possible issues with MuchoMusic, both in how it might encourage text reasoning, but also in how some questions seem to be invalid. While the authors mention the reliance on MuchoMusic as a limitation given these issues, their method is still applicable to other datasets, so it still remains valuable.</p>
<p>The paper is generally very well written, particularly in the explanation of the motivation, methods, and results. Personally, I found the writing style in the introduction a bit distracting, compared to the otherwise scientific and precise style for the rest of the paper. The language can be simplified a bit, both in complexity and “grandiosity”. I understand part of the intensity/confidence stems from the unconventional decision of starting off with some results to prove your motivation (which, while unexpected, I felt by the end that it was a good choice). Part of this is also the starting quote, which I really don’t think adds anything meaningful to the paper, and the starting sentence, which is long and convoluted. I think simpler language and a clearer explanation of the situation would make the paper much easier to read - it could be more clearly stated that the text models are simply responding to the text question without the audio. Persisting on them not being able to perceive and being “deaf” prolonged my confusion about whether you are, for example, investigating providing some text encoding of a waveform to text models. A clearer explanation of the task (and dataset) would also benefit new readers to the area.</p>
<p>Still, as mentioned, I very much enjoyed reading the rest of the paper and think it’s a very strong and valuable contribution.</p>
<p>A paper that I think is relevant and could be discussed in related work is “I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition” from last year’s ISMIR.</p>
<p>I am not sure the ISMIR template is strictly followed. There are larger than usual page margins, but I can’t tell if there’s just added padding or if the column sizes are affected. References could be cleaner (links, capitalization, consistency).</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review2" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review2Example">
          <span class="font-weight-bold">Review 2:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Strong accept</li>
</ul>
<h4>Main Reviews</h4>
<p>I recommend acceptance of this paper. </p>
<p>Strenghts: 
* Paper is easy to read and follow. Figures are appropriate and well-discussed in my opinion.
* Authors conduct extensive experiments to validate their Perceptual Index.
* Results indicate that the new questions makes it harder for LLMs to have comparable performance with LALMs, which is what we should see in MusicQA benchmarks.
Weaknesses: I don't see any major weaknesses in this paper. </p>
<p>Questions:
* L186-L190: Is the prompt also changed for LALMs?
* L304-L316: How many distractors were generated per question? What was the prompt used to generate them? Was there any comparison with other LLMs to generate such distractors?
* L329-L335: It would be interesting to have some examples of high and low semantic similarity score for CLAP. 
* L346-L348: Are you constraining the model output to only one token? If no, how are you controlling for the case in which the model does not follow the output format. For example, instead of answering the correct answer option, the model just write a very extensive answer. 
* L443-L450: Do authors see any relationship between the length of the response and the accuracy? In my experience, MuChoMusic evaluation criteria search for common terms between the answer words and the model output. I am not sure what is the impact of this on the evaluation.</p>
<ul>
<li>Appendix B: Maybe I am missing something, but I don't understand the role of the audio description in the model inspection. Was the description provided to the models together with the question? If yes, why? </li>
</ul>
<p>Minor comments:
* MuchoMusic -&gt; MuChoMusic
* L203, L227, L405, L444: remove space before the \footnote command.
* Is there a specific reason why authors decide to refer to LLMs as text-only LMs, but Audio LLMs as LALMs?</p>
        </div>
      </div>
    </div> 
  </div>
  
    <div id="review3" class="pp-card m-3 collapse">
      <div class="card-body">
        <div class="card-text">
          <div id="review3Example">
            <span class="font-weight-bold">Review 3:</span>
            <br>
            <ul>
<li><strong>Overall Evaluation</strong>: Strong accept</li>
</ul>
<h4>Main Reviews</h4>
<p>This paper focuses on evaluating the validity of multimodal system evaluation protocols and more specifically, MuchoMusic. In that sense, they ask the question, are truly Multimodal Question Answering datasets testing the multimodal capabilities of such systems? In what way is the reasoning capabilities of language models enough to come up with the right answer without regarding the audio signal?</p>
<p>Apart from that, in lines &lt;216-221&gt; the authors provide a statement as true, but no valid evaluation has been tested as far as I'm concerned. Therefore, they should change the phrasing to explicitly say that this is a hypothesis that models to posses the "world-prior" information for music with an existing corpus.</p>
<p>In line &lt;262&gt; the definition of the total probability in the parentheses is incorrect, as the total probability is defined as the multiplication between the posterior (which is in the formula) and the prior of each answer (for which we don't have access to). Also, it is implied that softmax has been applied and should be clearly stated because in multiclass classification you don't need to have that. The final answer is the most plausible one, i.e. the one with the most likelihood. Therefore, the conditional probabilities of answers conditioned to the questions are not bound to be added to one. As such, more details must be included or explicitly state the softmax approximation of the total probability.</p>
<p>In the paragraph between lines &lt;284-302&gt; it is hardly to follow the argument over the choice of perceptual index over entropy. First, evidence for correlation between PI and entropy is needed and therefore the argument between lines &lt;292-295&gt; should be rephrased to explicitly state that this is a hypothesis rather than a given. Also, the noun is missing from sentence at lines &lt;296-302&gt;, probably they imply 'confidently wrong' and correct answers?. Also the claim that high PI answers require perceptual information while high entropy ones are not, is not justified. For that to be completely true, experiments with filtering based on entropy are needed and then comparison between PI and entropy based filtered performance and similarity distributions need to be added. Otherwise, I would rephrase the paragraph in order to explicitly state that this is a hypothesis left to be explored in future work.</p>
<p>In line &lt;308&gt; context packages are not a term that is known and therefore need explicit definition. Changing the phrasing from 'with question...' to 'as a list of question...' would introduce more clarity that a package is the set of the triplets. Line &lt;319&gt; has a typo. I would change the x-axis label of figure-3 to explicitly state the similarity that has been chosen, which is the cosine similarity.</p>
<p>There is evidence that CLAP is not properly understanding musical terms and a paper was out for Instrument recognition in last years ISMIR, where the text encoder of CLAP failed to semantically understand music relationships between instruments which is proposing the directly opposite phenomenon of the example proposed. Also, the distributions themselves do not provide context. A better alternative would be to plot the distributions between c and distractors in MuchoMusic, the generated distractors and even random ones. I believe that there won't be a significant change between random distractors and chosen ones. With that, an aggregation function (such as the overlapping index) could be calculated to hint that there isn't any big difference between the distributions. Also, in these distributions, the similarity between distractors and the correct answer is approaching 1 and those cases should be considered or it should be explicitly be stated so. </p>
<p>In figure 4b, there are a lot of outliers with very small perceptual index and Pearson's coefficient is known to be sensitive to them. A lasso analysis should be performed given the visualization, and the R factor would be almost 0 in that case (with a threshold of 0.3). This doesn't happen with 4a, so the correlation value is mostly true. Also, it should be good to include the mean accuracy for both of the datasets to signify the difference in both cases.</p>
<p>In line &lt;393&gt; hardness value are not properly defined and in line &lt;394-397&gt; the argument doesn't have grounding, as this is the under investigation hypothesis. Up until this point, audio is only partially correlated with PI. Also, Figure 4 should include the same plots for MuchoMusic without the generation to properly compare between them two. I suspect that the change in the music PR correlation will be pretty much the same. </p>
<p>Apart from that, the paper was an interesting read, very precise and definitely addressed a large gap in the music-language multmodal comprehension. I think its a work properly addressing a main gap of our domain and community, the lack of rigorously defined evaluation sets. Also, a way of estimating the informativeness of specific question answer pairs and the problematic nature of evaluating multimodal systems was addressed succesfuly! As a remark, more experiments should be performed with less models on random distractors, different subsets from several steps of the RUListening framework (with variable distance from the optimal D*) and even test the entropy based filtering. That would further solidify that their approach is successful and that choosing PI is the right surrogate task for finding the right set of distarctors. With minimal phrases changed, I'm thinking that this paper is a nice addition to the ISMIR conference!</p>
          </div>
        </div>
      </div> 
    </div>
  
  
</div>


<script src="static/js/poster.js"></script>
<script src="static/js/video_selection.js"></script>

      </div>
    </div>
    
    

    <!-- Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "UA-");
    </script>

    <!-- Footer -->
    <footer class="footer bg-light p-4">
      <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2023 ISMIR Organization Committee</p>
      </div>
    </footer>

    <!-- Code for hash tags -->
    <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });
      });
    </script>
    <script src="static/js/lazy_load.js"></script>
    
  </body>
</html>