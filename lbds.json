[
  {
    "content": {
      "TLDR": "The music industry attracts a large number of investors due to its high turnover. Releasing hit songs can garner profits, whereas flop songs lead to losses. Thus, predicting the popularity of a song before its release can help in promotion plans. Can we really predict hit Songs? This is the main motivation of the work. Extensive work is being done for western songs, but Indian music is relatively less explored. Hence, our work aims to predict hit songs of Indian origin using acoustic features. To analyze tracks, a data set is created from data provided by the Spotify Web API. The features are extracted using Spotify, available libraries such as Librosa and aubio, which are passed to machine learning algorithms for prediction. Along with available features, melodic features based on patterns are proposed and extracted. A comparative analysis is done for four acoustic feature sets containing timbral, pitch, rhythm and melodic features proposed. Further experimentation is performed with the combined features sets resulting in the improved performance. The results are encouraging and hit song prediction can be a reality in the near future.",
      "abstract": "The music industry attracts a large number of investors due to its high turnover. Releasing hit songs can garner profits, whereas flop songs lead to losses. Thus, predicting the popularity of a song before its release can help in promotion plans. Can we really predict hit Songs? This is the main motivation of the work. Extensive work is being done for western songs, but Indian music is relatively less explored. Hence, our work aims to predict hit songs of Indian origin using acoustic features. To analyze tracks, a data set is created from data provided by the Spotify Web API. The features are extracted using Spotify, available libraries such as Librosa and aubio, which are passed to machine learning algorithms for prediction. Along with available features, melodic features based on patterns are proposed and extracted. A comparative analysis is done for four acoustic feature sets containing timbral, pitch, rhythm and melodic features proposed. Further experimentation is performed with the combined features sets resulting in the improved performance. The results are encouraging and hit song prediction can be a reality in the near future.",
      "authors": [
        "Shreya Kale, Makarand Velankar, Rameshwari Joshi, Vaishnavi Ingole, Aparna Dhaygude"
      ],
      "bilibili_id": "",
      "channel_name": "lp-1-kale",
      "channel_url": "https://slack.com/app_redirect?channel=C04CHSMK0A1",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000001.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000001.pdf",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "000001.png",
      "title": "Hit Song Prediction for Indian Popular Music",
      "youtube_id": ""
    },
    "forum": "362",
    "id": "362",
    "position": "1"
  },
  {
    "content": {
      "TLDR": "FixMatch, a semi-supervised learning method proposed for image classification, includes unlabeled data instances into the training procedure by predicting labels for differently augmented versions of the unlabeled data. In our previous work, we adapted FixMatch to audio classification by applying image augmentations to spectral representations of the audio signal. While this approach matched the performance of the supervised baseline with only a fraction of the training data, the performance of audio-specific augmentation techniques, and their effect on the FixMatch approach was not evaluated. In this work, we replace all image-based augmentation techniques with audio-specific ones and keep the feature extraction unchanged. The audio-specific approach improved upon the supervised baseline which confirms the effectiveness of the FixMatch approach for semi-supervised learning even with a completely different set of augmentations. However, the image-based approach outperforms the audio-based approach on the three audio classification tasks evaluated.",
      "abstract": "FixMatch, a semi-supervised learning method proposed for image classification, includes unlabeled data instances into the training procedure by predicting labels for differently augmented versions of the unlabeled data. In our previous work, we adapted FixMatch to audio classification by applying image augmentations to spectral representations of the audio signal. While this approach matched the performance of the supervised baseline with only a fraction of the training data, the performance of audio-specific augmentation techniques, and their effect on the FixMatch approach was not evaluated. In this work, we replace all image-based augmentation techniques with audio-specific ones and keep the feature extraction unchanged. The audio-specific approach improved upon the supervised baseline which confirms the effectiveness of the FixMatch approach for semi-supervised learning even with a completely different set of augmentations. However, the image-based approach outperforms the audio-based approach on the three audio classification tasks evaluated.",
      "authors": [
        "Sascha Grollmisch, Estefan\u00eda Cano, Jakob Abe\u00dfer"
      ],
      "bilibili_id": "",
      "channel_name": "lv-2-grollmisch",
      "channel_url": "https://slack.com/app_redirect?channel=C04CLMZJPEF",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000002.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000002.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000002.png",
      "title": "Audio Augmentations for Semi-Supervised Learning with FixMatch",
      "youtube_id": ""
    },
    "forum": "363",
    "id": "363",
    "position": "2"
  },
  {
    "content": {
      "TLDR": "The extraction of fundamental frequency (F0) information from music recordings is a crucial task in the field of music information retrieval. The sequence of F0-estimates over successive time frames (also called F0-trajectory) often corresponds to a melodic phrase and serves as a representation for downstream tasks such as automatic music transcription and performance analysis. A large number of algorithms and tools for F0-estimation have been proposed in the literature and implemented in various programming languages. However, these heterogeneous implementations are often not easily comparable and may vary considerably in performance and accuracy, which is problematic for reproducible research. In this contribution, we introduce libf0, a Python library of reference implementations that can conveniently be used to apply, compare, and develop F0-estimation algorithms in a reproducible way.",
      "abstract": "The extraction of fundamental frequency (F0) information from music recordings is a crucial task in the field of music information retrieval. The sequence of F0-estimates over successive time frames (also called F0-trajectory) often corresponds to a melodic phrase and serves as a representation for downstream tasks such as automatic music transcription and performance analysis. A large number of algorithms and tools for F0-estimation have been proposed in the literature and implemented in various programming languages. However, these heterogeneous implementations are often not easily comparable and may vary considerably in performance and accuracy, which is problematic for reproducible research. In this contribution, we introduce libf0, a Python library of reference implementations that can conveniently be used to apply, compare, and develop F0-estimation algorithms in a reproducible way.",
      "authors": [
        "Sebastian Rosenzweig, Simon J Schw\u00e4r, Meinard M\u00fcller"
      ],
      "bilibili_id": "",
      "channel_name": "lv-3-rosenzweig",
      "channel_url": "https://slack.com/app_redirect?channel=C04CZEEH4QZ",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000003.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000003.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000003.png",
      "title": "libf0: A Python Library for Fundamental Frequency Estimation",
      "youtube_id": ""
    },
    "forum": "364",
    "id": "364",
    "position": "3"
  },
  {
    "content": {
      "TLDR": "We investigate musical genre classification for three forms of Marathi vocal music each with a distinct socio-cultural context, namely devotional, poetic and folk dance. We present a dataset of songs covering the three genres, and discuss their musical and acoustic characteristics. We consider timbre and chroma based features for the 3-way classification. We specifically examine whether the source-separated vocal and instrumental accompaniment components can help improve genre recognition accuracy over that obtained with acoustic features extracted from the original mix audio track.",
      "abstract": "We investigate musical genre classification for three forms of Marathi vocal music each with a distinct socio-cultural context, namely devotional, poetic and folk dance. We present a dataset of songs covering the three genres, and discuss their musical and acoustic characteristics. We consider timbre and chroma based features for the 3-way classification. We specifically examine whether the source-separated vocal and instrumental accompaniment components can help improve genre recognition accuracy over that obtained with acoustic features extracted from the original mix audio track.",
      "authors": [
        "Shreyas M Nadkarni, Preeti Rao"
      ],
      "bilibili_id": "",
      "channel_name": "lp-4-nadkarni",
      "channel_url": "https://slack.com/app_redirect?channel=C04CZEEHQ8Z",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000004.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000004.pdf",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "000004.png",
      "title": "Genre Classification and Analysis of Marathi Songs",
      "youtube_id": ""
    },
    "forum": "365",
    "id": "365",
    "position": "4"
  },
  {
    "content": {
      "TLDR": "Recent research has shown that the analysis of musical performance videos benefits from a multimodal learning due to the close interaction between audio characteristics and visual gestures by the performer. However, every performer displays idiosyncrasies in both modalities. Thus, general tasks like raga classification are affected by the introduction of singer specific information into the data which can adversely affect the results. In this work-in-progress discussion, we attempt to disentangle the singer specific characteristics from both audio and video modalities in unimodal and multimodal settings. In particular, we build on approaches used earlier in emotion recognition and racial bias removal tasks based on a multitask framework where we consider joint optimisation of classification loss and a loss to enable performer disentanglement. We build on our previous work on multimodal raga classification with various fusion strategies in this work to understand the impact of individual characteristics superimposed in both modalities.",
      "abstract": "Recent research has shown that the analysis of musical performance videos benefits from a multimodal learning due to the close interaction between audio characteristics and visual gestures by the performer. However, every performer displays idiosyncrasies in both modalities. Thus, general tasks like raga classification are affected by the introduction of singer specific information into the data which can adversely affect the results. In this work-in-progress discussion, we attempt to disentangle the singer specific characteristics from both audio and video modalities in unimodal and multimodal settings. In particular, we build on approaches used earlier in emotion recognition and racial bias removal tasks based on a multitask framework where we consider joint optimisation of classification loss and a loss to enable performer disentanglement. We build on our previous work on multimodal raga classification with various fusion strategies in this work to understand the impact of individual characteristics superimposed in both modalities.",
      "authors": [
        "Adbhut V Bhardwaj, Sujoy Roychowdhury, Preeti Rao"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000005.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000005.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000005.png",
      "title": "Towards Singer-independent Raga Classification from Audiovisual Recordings",
      "youtube_id": ""
    },
    "forum": "366",
    "id": "366",
    "position": "5"
  },
  {
    "content": {
      "TLDR": "EGFxSet contains recordings of all clean tones in a Stratocaster guitar, with augmentations by processing through twelve electric guitar effects. Similar datasets apply effects using software, EGFxSet in contrast uses real guitar effects hardware, making it relevant to develop MIR tools with applications on real music. Annotations include all guitar and effect parameters controlled during our dataset recording. EGFxSet contains 8970 unique, annotated guitar tones, and is published with full open-access rights.",
      "abstract": "EGFxSet contains recordings of all clean tones in a Stratocaster guitar, with augmentations by processing through twelve electric guitar effects. Similar datasets apply effects using software, EGFxSet in contrast uses real guitar effects hardware, making it relevant to develop MIR tools with applications on real music. Annotations include all guitar and effect parameters controlled during our dataset recording. EGFxSet contains 8970 unique, annotated guitar tones, and is published with full open-access rights.",
      "authors": [
        "Hegel Emmanuel Pedroza, Gerardo Meza, Iran R Roman"
      ],
      "bilibili_id": "",
      "channel_name": "lv-6-pedroza",
      "channel_url": "https://slack.com/app_redirect?channel=C04CHSMMGJH",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000006.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000006.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000006.png",
      "title": "EGFxSet: Electric Guitar Tones Processed Through Real Effects of Distortion, Modulation, Delay and Reverb",
      "youtube_id": "https://drive.google.com/uc?export=preview&id=1XR0nGnwGZmrfEk3BMeBEzLAEI2JirpIc"
    },
    "forum": "367",
    "id": "367",
    "position": "6"
  },
  {
    "content": {
      "TLDR": "Language models have made great progress in symbolic music generation. However, to the best of our knowledge, without large human-annotated datasets, music generation constrained to a predefined musical form has not been well studied. In this demo paper, we present a GPT2-based melody generation system for generating well-structured melodies via controllable similarity and length. We design several embeddings and tokens to modelling similarity and length information within the music, and train the GPT-2 model with a large dataset in ABC notation. With preliminary experimental results and examples of self-similarity matrices, we demonstrate the potential of this system for generating melodies with long-term repetitive structures.",
      "abstract": "Language models have made great progress in symbolic music generation. However, to the best of our knowledge, without large human-annotated datasets, music generation constrained to a predefined musical form has not been well studied. In this demo paper, we present a GPT2-based melody generation system for generating well-structured melodies via controllable similarity and length. We design several embeddings and tokens to modelling similarity and length information within the music, and train the GPT-2 model with a large dataset in ABC notation. With preliminary experimental results and examples of self-similarity matrices, we demonstrate the potential of this system for generating melodies with long-term repetitive structures.",
      "authors": [
        "Shangda Wu, Yuanliang Dong, Maosong Sun"
      ],
      "bilibili_id": "",
      "channel_name": "lv-7-sun",
      "channel_url": "https://slack.com/app_redirect?channel=C04CLMZM7EX",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000007.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000007.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000007.png",
      "title": "Generating Melodies with Controllable Similarity and Length in ABC Notation",
      "youtube_id": "https://drive.google.com/uc?export=preview&id=1mgtdrkBct6M2gpv37g6M7hOrwe8k66K_"
    },
    "forum": "368",
    "id": "368",
    "position": "7"
  },
  {
    "content": {
      "TLDR": "Training sequence models such as transformers with symbolic music necessitates a representation of music as sequences of atomic elements called tokens. State-of-the-art music tokenizations encode pitch values explicitly, which complicates the ability of a machine learning model to generalize musical knowledge at different keys. We propose tracks for a tokenization encoding pitch intervals rather than pitch values, resulting in transposition invariant representations. The musical expressivity of this new tokenization is evaluated through two MIR classification tasks: composer classification and end of phrase detection.",
      "abstract": "Training sequence models such as transformers with symbolic music necessitates a representation of music as sequences of atomic elements called tokens. State-of-the-art music tokenizations encode pitch values explicitly, which complicates the ability of a machine learning model to generalize musical knowledge at different keys. We propose tracks for a tokenization encoding pitch intervals rather than pitch values, resulting in transposition invariant representations. The musical expressivity of this new tokenization is evaluated through two MIR classification tasks: composer classification and end of phrase detection.",
      "authors": [
        "Louis Bigo, Mikaela Keller"
      ],
      "bilibili_id": "",
      "channel_name": "lv-8-bigo",
      "channel_url": "https://slack.com/app_redirect?channel=C04CE7FKVU6",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000008.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000008.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000008.png",
      "title": "Improving tokenization expressiveness with pitch intervals",
      "youtube_id": ""
    },
    "forum": "369",
    "id": "369",
    "position": "8"
  },
  {
    "content": {
      "TLDR": "We created a visualization tool that helps Automatic Chord Recognition (ACR) developers to characterize system performance across a test data set. Our system's design uses Information Visualization (InfoVis) principles to communicate accuracy more effectively than a table of mean metric scores. We share some of the insights we developed while building our tool, and hope our findings may help inform the design of figures used in future publications, and affect how future ACR system designers improve and present their systems.",
      "abstract": "We created a visualization tool that helps Automatic Chord Recognition (ACR) developers to characterize system performance across a test data set. Our system's design uses Information Visualization (InfoVis) principles to communicate accuracy more effectively than a table of mean metric scores. We share some of the insights we developed while building our tool, and hope our findings may help inform the design of figures used in future publications, and affect how future ACR system designers improve and present their systems.",
      "authors": [
        "Christopher Liscio,  Dan Brown"
      ],
      "bilibili_id": "",
      "channel_name": "lv-9-liscio",
      "channel_url": "https://slack.com/app_redirect?channel=C04CHSMPMEZ",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000009.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000009.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000009.png",
      "title": "Visualizing Chord Recognition Performance",
      "youtube_id": "https://drive.google.com/uc?export=preview&id=1Cr_3MLyeipNPicyRbyj0O9SmYEr8yE3p"
    },
    "forum": "370",
    "id": "370",
    "position": "9"
  },
  {
    "content": {
      "TLDR": "A monophonic instrument can play at once several voices or lines, for example when interleaving pedal notes and scales. Such patterns may bear both melodic, harmonic, and rhythmic elements and are frequent in cello music. We propose a model of alternating patterns, where regularly spaced pitches are linked with some relation. We also propose an algorithm to list all such patterns. Perspectives include better corpus analysis, and extending and benchmarking such algorithms.",
      "abstract": "A monophonic instrument can play at once several voices or lines, for example when interleaving pedal notes and scales. Such patterns may bear both melodic, harmonic, and rhythmic elements and are frequent in cello music. We propose a model of alternating patterns, where regularly spaced pitches are linked with some relation. We also propose an algorithm to list all such patterns. Perspectives include better corpus analysis, and extending and benchmarking such algorithms.",
      "authors": [
        "Perrine Vantalon, Mathieu Giraud, Richard Groult, Thierry Lecroq"
      ],
      "bilibili_id": "",
      "channel_name": "lv-10-giraud",
      "channel_url": "https://slack.com/app_redirect?channel=C04DAG9S8MN",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000010.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000010.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000010.png",
      "title": "Towards modeling alternating patterns through inter-notes relations",
      "youtube_id": ""
    },
    "forum": "371",
    "id": "371",
    "position": "10"
  },
  {
    "content": {
      "TLDR": "Popularity bias is the idea that a music recommender system will unduly favor popular artists when recommending artists to users. In this paper, we attempt to measure popularity bias on three commercial music streaming services (Spotify, Amazon Music, YouTube). We find no significant evidence of popularity bias in the commercial recommendations based on a simulated user experiment.",
      "abstract": "Popularity bias is the idea that a music recommender system will unduly favor popular artists when recommending artists to users. In this paper, we attempt to measure popularity bias on three commercial music streaming services (Spotify, Amazon Music, YouTube). We find no significant evidence of popularity bias in the commercial recommendations based on a simulated user experiment.",
      "authors": [
        "Douglas Turnbull, Vera Crabtree, Sean McQuillan"
      ],
      "bilibili_id": "",
      "channel_name": "lv-11-turnbull",
      "channel_url": "https://slack.com/app_redirect?channel=C04CLR1UKK4",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000011.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000011.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000011.png",
      "title": "Exploring Popularity Bias in Music Steaming Services",
      "youtube_id": ""
    },
    "forum": "372",
    "id": "372",
    "position": "11"
  },
  {
    "content": {
      "TLDR": "Expert musicians can mould a musical piece to convey specific emotions that they intend to communicate. In this paper, we place a mid-level features based music emotion model in this performer-to-listener communication scenario, and demonstrate via a small visualisation music emotion decoding in real time. We also extend the existing set of mid-level features using analogues of perceptual speed and perceived dynamics.",
      "abstract": "Expert musicians can mould a musical piece to convey specific emotions that they intend to communicate. In this paper, we place a mid-level features based music emotion model in this performer-to-listener communication scenario, and demonstrate via a small visualisation music emotion decoding in real time. We also extend the existing set of mid-level features using analogues of perceptual speed and perceived dynamics.",
      "authors": [
        "Shreyan Chowdhury, Gerhard Widmer"
      ],
      "bilibili_id": "",
      "channel_name": "lv-13-chowdhury",
      "channel_url": "https://slack.com/app_redirect?channel=C04CP988JVA",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000013.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000013.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000013.png",
      "title": "Decoding and Visualising Intended Emotion in an Expressive Piano Performance",
      "youtube_id": ""
    },
    "forum": "374",
    "id": "374",
    "position": "13"
  },
  {
    "content": {
      "TLDR": "We introduce the Classical Concert Video Shot (CCVS) dataset with concert videos of classical music performance and annotations available for the MIR research on concert videography. In this dataset, we annotate the start time, end time, shot class and musical instruments of each shot in the video. Eight classes of video shot and 33 classes of instruments are considered in the annotation. Totally 5,527 shot tags with instruments are then collected from 207 YouTube videos. The total length of the dataset reaches 12.68 hours.",
      "abstract": "We introduce the Classical Concert Video Shot (CCVS) dataset with concert videos of classical music performance and annotations available for the MIR research on concert videography. In this dataset, we annotate the start time, end time, shot class and musical instruments of each shot in the video. Eight classes of video shot and 33 classes of instruments are considered in the annotation. Totally 5,527 shot tags with instruments are then collected from 207 YouTube videos. The total length of the dataset reaches 12.68 hours.",
      "authors": [
        "Hsin-Min Chou, Ting-Wei Lin, Jen-Chun Lin, Ching Te Chiu, Li Su"
      ],
      "bilibili_id": "",
      "channel_name": "lv-14-su",
      "channel_url": "https://slack.com/app_redirect?channel=C04CE7FPJG6",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000014.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000014.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000014.png",
      "title": "CCVS: A dataset for concert videography research",
      "youtube_id": ""
    },
    "forum": "375",
    "id": "375",
    "position": "14"
  },
  {
    "content": {
      "TLDR": "We contribute two design studies for augmented reality visualizations that support learning musical instruments. First, we designed simple, glanceable encodings for drum kits, which we display through a projector. As second instrument, we chose guitar and designed visualizations to be displayed either on a screen as an augmented mirror or an an optical see-through AR headset. These modalities allow us to also show information around the instrument and in 3D. We evaluated our prototypes through case studies and our results demonstrate the general effectivity and revealed design-related and technical limitations.",
      "abstract": "We contribute two design studies for augmented reality visualizations that support learning musical instruments. First, we designed simple, glanceable encodings for drum kits, which we display through a projector. As second instrument, we chose guitar and designed visualizations to be displayed either on a screen as an augmented mirror or an an optical see-through AR headset. These modalities allow us to also show information around the instrument and in 3D. We evaluated our prototypes through case studies and our results demonstrate the general effectivity and revealed design-related and technical limitations.",
      "authors": [
        "Frank Heyen, Michael Sedlmair"
      ],
      "bilibili_id": "",
      "channel_name": "lp-15-heyen",
      "channel_url": "https://slack.com/app_redirect?channel=C04CE7FQBAA",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000015.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000015.pdf",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "000015.png",
      "title": "Augmented Reality Visualization for Musical Instrument Learning",
      "youtube_id": ""
    },
    "forum": "376",
    "id": "376",
    "position": "15"
  },
  {
    "content": {
      "TLDR": "AudioLoader is a PyTorch package which helps users to auto-download, unzip and prepossess (audio re-sampling, segmenting, data splitting) common audio datasets that are still not available in the official torchaudio dataset collection yet. AudioLoader supports a wide rage of datasets for different applications such as speech recognition (Multilingual LibriSpeech (MLS), TIMIT, SpeechCommands v2 with 12 classes), automatic music transcription (MAPS, MusicNet, MAESTRO), and music source separation (MusdbHQ). Slakh2100 will also be included in our future release. \n \n AudioLoader is designed to be hassle-free. Once called, it returns a pytorch dataset class, and it can be combined with \\verb | torch.utils.data.DataLoader |  as usual. This design allows users and researchers to spend less time on dataset preparation so that they can focus more on the research and model development part. In this paper, we will demonstrate the usage of AudioLoader by using various datasets such as MLS, MAESTEO, and MusdbHQ as the examples. AudioLoader is an on-going open source project available on GitHub, more datasets will be supported in the future and contributions from the community is highly welcomed.",
      "abstract": "AudioLoader is a PyTorch package which helps users to auto-download, unzip and prepossess (audio re-sampling, segmenting, data splitting) common audio datasets that are still not available in the official torchaudio dataset collection yet. AudioLoader supports a wide rage of datasets for different applications such as speech recognition (Multilingual LibriSpeech (MLS), TIMIT, SpeechCommands v2 with 12 classes), automatic music transcription (MAPS, MusicNet, MAESTRO), and music source separation (MusdbHQ). Slakh2100 will also be included in our future release. \n \n AudioLoader is designed to be hassle-free. Once called, it returns a pytorch dataset class, and it can be combined with \\verb | torch.utils.data.DataLoader |  as usual. This design allows users and researchers to spend less time on dataset preparation so that they can focus more on the research and model development part. In this paper, we will demonstrate the usage of AudioLoader by using various datasets such as MLS, MAESTEO, and MusdbHQ as the examples. AudioLoader is an on-going open source project available on GitHub, more datasets will be supported in the future and contributions from the community is highly welcomed.",
      "authors": [
        "Kin Wai Cheuk, Kwan Yee Heung, Dorien Herremans"
      ],
      "bilibili_id": "",
      "channel_name": "lv-16-cheuk",
      "channel_url": "https://slack.com/app_redirect?channel=C04DAGA0LGY",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000016.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000016.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000016.png",
      "title": "AudioLoader: a hassle-free Pytorch audio dataset loader",
      "youtube_id": "https://drive.google.com/uc?export=preview&id=1W0OLDwovWJ3dRiCLQntiTphhgDY2ak7Z"
    },
    "forum": "377",
    "id": "377",
    "position": "16"
  },
  {
    "content": {
      "TLDR": "We contribute an interactive visual frontend to live coding environments, which allows live coders and performers to influence the behavior of their code more quickly and efficiently. Users can trigger actions and change parameters via instruments, buttons, and sliders, instead of only inside the code. For instance, toggling a loop or controlling a fading effect through mouse or touch interaction on a screen is faster than editing code. While this kind of control has already been possible with hardware MIDI devices, we provide a more accessible, easy-to-use, and customizable alternative that only requires a web browser. With examples, we show how users perform live-coded music faster and more easily with our design compared to using pure code.",
      "abstract": "We contribute an interactive visual frontend to live coding environments, which allows live coders and performers to influence the behavior of their code more quickly and efficiently. Users can trigger actions and change parameters via instruments, buttons, and sliders, instead of only inside the code. For instance, toggling a loop or controlling a fading effect through mouse or touch interaction on a screen is faster than editing code. While this kind of control has already been possible with hardware MIDI devices, we provide a more accessible, easy-to-use, and customizable alternative that only requires a web browser. With examples, we show how users perform live-coded music faster and more easily with our design compared to using pure code.",
      "authors": [
        "Frank Heyen, Michael Sedlmair"
      ],
      "bilibili_id": "",
      "channel_name": "lp-18-heyen",
      "channel_url": "https://slack.com/app_redirect?channel=C04CE7FRT3Q",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000018.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000018.pdf",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "000018.png",
      "title": "A Web-Based MIDI Controller for Music Live Coding",
      "youtube_id": ""
    },
    "forum": "379",
    "id": "379",
    "position": "18"
  },
  {
    "content": {
      "TLDR": "Gamaka (note ornamentation) is an essential element of Carnatic music. Earlier works in computer-generated gamakas focused on developing mathematical models of each gamaka, which fails to capture the intricate changes in pitch and thus does not sound natural. To address this challenge, this work approaches the synthesis of gamaka for kalpitha swaras (composed notes) in Carnatic music using a data-driven system. The model uses masked latent space representation in an auto-encoder architecture with features extracted using convolutional layers. \n It takes as input, the pitch contour extracted from symbolic data to generate a pitch contour with gamaka information embedded in it. The model is successful in synthesizing gamaka with nuances that closely follow the ground truth.",
      "abstract": "Gamaka (note ornamentation) is an essential element of Carnatic music. Earlier works in computer-generated gamakas focused on developing mathematical models of each gamaka, which fails to capture the intricate changes in pitch and thus does not sound natural. To address this challenge, this work approaches the synthesis of gamaka for kalpitha swaras (composed notes) in Carnatic music using a data-driven system. The model uses masked latent space representation in an auto-encoder architecture with features extracted using convolutional layers. \n It takes as input, the pitch contour extracted from symbolic data to generate a pitch contour with gamaka information embedded in it. The model is successful in synthesizing gamaka with nuances that closely follow the ground truth.",
      "authors": [
        "Raghavasimhan Sankaranarayanan, Gil Weinberg"
      ],
      "bilibili_id": "",
      "channel_name": "lv-19-sankaranarayanan",
      "channel_url": "https://slack.com/app_redirect?channel=C04CLR23ML2",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000019.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000019.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000019.png",
      "title": "Gamaka Synthesis for Kalpitha Swaras in Carnatic music",
      "youtube_id": ""
    },
    "forum": "380",
    "id": "380",
    "position": "19"
  },
  {
    "content": {
      "TLDR": "In this paper, we propose a new paradigm to learn audio features for the task of Music Structure Analysis (MSA).We train a deep encoder to learn features such that the Self-Similarity-Matrix (SSM) resulting from those approximates a ground-truth SSM. This is done by minimizing a loss between both SSMs. Since this loss is differentiable w.r.t. its input features we can train the encoder in a straightforward way. We successfully demonstrate the use of this training paradigm using the AUC on the RWC-Pop dataset.",
      "abstract": "In this paper, we propose a new paradigm to learn audio features for the task of Music Structure Analysis (MSA).We train a deep encoder to learn features such that the Self-Similarity-Matrix (SSM) resulting from those approximates a ground-truth SSM. This is done by minimizing a loss between both SSMs. Since this loss is differentiable w.r.t. its input features we can train the encoder in a straightforward way. We successfully demonstrate the use of this training paradigm using the AUC on the RWC-Pop dataset.",
      "authors": [
        "Geoffroy Peeters,  Florian Angulo"
      ],
      "bilibili_id": "",
      "channel_name": "lp-20-peeters",
      "channel_url": "https://slack.com/app_redirect?channel=C04DAGA2NEL",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000020.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000020.pdf",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "000020.png",
      "title": "SSM-NET: Feature Learning for Music Structure Analysis Using a Self-similarity-matrix Based Loss",
      "youtube_id": "https://drive.google.com/uc?export=preview&id=18hTtiMpRRFuctTg7rdpMhzJbnwP-QI2W"
    },
    "forum": "381",
    "id": "381",
    "position": "20"
  },
  {
    "content": {
      "TLDR": "The pitch bend is used as a tool in a virtual instrument/synth that allows one to glide smoothly from one note to another. Almost all virtual instruments today employ the pitch bend joystick/wheel on a keyboard to solely manipulate the fundamental frequency or the pitch of the note being played. While this feature allows a perceptually realistic reproduction of slides in a few instruments, it largely fails to do so with plucked string instruments, especially non-Western traditional instruments. In this abstract, I chose to study the meend technique in Sitar performance in order to understand in finer detail the factors (including, but not limited to, the fundamental frequency) that characterize the glide between two notes.",
      "abstract": "The pitch bend is used as a tool in a virtual instrument/synth that allows one to glide smoothly from one note to another. Almost all virtual instruments today employ the pitch bend joystick/wheel on a keyboard to solely manipulate the fundamental frequency or the pitch of the note being played. While this feature allows a perceptually realistic reproduction of slides in a few instruments, it largely fails to do so with plucked string instruments, especially non-Western traditional instruments. In this abstract, I chose to study the meend technique in Sitar performance in order to understand in finer detail the factors (including, but not limited to, the fundamental frequency) that characterize the glide between two notes.",
      "authors": [
        "Suhit Chiruthapudi"
      ],
      "bilibili_id": "",
      "channel_name": "lp-22-chiruthapudi",
      "channel_url": "https://slack.com/app_redirect?channel=C04CHSN1EP7",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000022.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000022.pdf",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "000022.png",
      "title": "Differentiating the Pitch Bend Function Between the Sitar as a Real and Virtual Instrument",
      "youtube_id": ""
    },
    "forum": "383",
    "id": "383",
    "position": "22"
  },
  {
    "content": {
      "TLDR": "In the present research work an attempt was made to construct the body of the daya component of tabla with processed fibre. The scientific parameters were analysed and compared with the standard instrument. The tonal quality calibration of the instrument was digitally performed in a studio environment with the help of a computer and an interface. The analysis showed that there was difference in the single tone \u201ctha\u201d and \u201cthi\u201d in terms of the peak frequency. The maximum frequency of \u201ctha\u201d in wooden daya was 770 Hz whereas in fibre daya it was 526 Hz. The peak frequency of \u201cthin\u201d in wooden daya was 236 and that in fibre daya was 278Hz. Frequency analysis of combined tones also showed difference in the peak frequencies. Generally, it could be observed that the peak frequency difference was more in open notes than in closed notes.",
      "abstract": "In the present research work an attempt was made to construct the body of the daya component of tabla with processed fibre. The scientific parameters were analysed and compared with the standard instrument. The tonal quality calibration of the instrument was digitally performed in a studio environment with the help of a computer and an interface. The analysis showed that there was difference in the single tone \u201ctha\u201d and \u201cthi\u201d in terms of the peak frequency. The maximum frequency of \u201ctha\u201d in wooden daya was 770 Hz whereas in fibre daya it was 526 Hz. The peak frequency of \u201cthin\u201d in wooden daya was 236 and that in fibre daya was 278Hz. Frequency analysis of combined tones also showed difference in the peak frequencies. Generally, it could be observed that the peak frequency difference was more in open notes than in closed notes.",
      "authors": [
        "Retnasree Iyer"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000023.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000023.pdf",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "000023.png",
      "title": "Improvisation of tabla the indian percussion using fibre",
      "youtube_id": ""
    },
    "forum": "384",
    "id": "384",
    "position": "23"
  },
  {
    "content": {
      "TLDR": "It has been shown in a recent publication that words in human-produced English language tend to have an information content close to the conditional entropy. In this paper, we show that the same is true for events in human-produced monophonic musical sequences. We also show how \"typical sampling\" influences the distribution of information around the entropy for single events and sequences.",
      "abstract": "It has been shown in a recent publication that words in human-produced English language tend to have an information content close to the conditional entropy. In this paper, we show that the same is true for events in human-produced monophonic musical sequences. We also show how \"typical sampling\" influences the distribution of information around the entropy for single events and sequences.",
      "authors": [
        "Mathais Rose Bjare, Stefan Lattner"
      ],
      "bilibili_id": "",
      "channel_name": "lp-24-bjare",
      "channel_url": "https://slack.com/app_redirect?channel=C04DAGA40AU",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000024.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000024.pdf",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "000024.png",
      "title": "On the Typicality of Music",
      "youtube_id": ""
    },
    "forum": "385",
    "id": "385",
    "position": "24"
  },
  {
    "content": {
      "TLDR": "To date, little is known about the impact of different source locations on the listener's emotional response to music. Here we investigated through machine learning whether four music source locations (front, back, left, and right) could be accurately differentiated according to the type of valence in a subject-wise manner using spectral features extracted from electrophysiological (EEG) data. The results show that the four source locations can accurately be classified by different EEG correlates and that the effect is stronger when music characterized by negative emotional valence is played outside the listener's field of view. This proof-of-concept study may pave the way for advanced spatial audio analysis approaches in music information retrieval by taking into account the listener's emotional impact depending on the source direction of incidence.",
      "abstract": "To date, little is known about the impact of different source locations on the listener's emotional response to music. Here we investigated through machine learning whether four music source locations (front, back, left, and right) could be accurately differentiated according to the type of valence in a subject-wise manner using spectral features extracted from electrophysiological (EEG) data. The results show that the four source locations can accurately be classified by different EEG correlates and that the effect is stronger when music characterized by negative emotional valence is played outside the listener's field of view. This proof-of-concept study may pave the way for advanced spatial audio analysis approaches in music information retrieval by taking into account the listener's emotional impact depending on the source direction of incidence.",
      "authors": [
        "Eleonora De Filippi, Timothy Schmele, ARIJIT NANDI"
      ],
      "bilibili_id": "",
      "channel_name": "lp-25-filippi",
      "channel_url": "https://slack.com/app_redirect?channel=C04CP98EYJ0",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000025.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000025.pdf",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "000025.png",
      "title": "Towards a machine-learning approach to analyse the emotional impact of source localization in music",
      "youtube_id": ""
    },
    "forum": "386",
    "id": "386",
    "position": "25"
  },
  {
    "content": {
      "TLDR": "One of the ambitions of computational musicology consists in characterising music harmony in a symbol-ic system. In the context of the EU project Polifonia, we are exploring the possibility to associate EEG data to characterise harmony from a cognitive and emo-tional point of view. Data will be collected using a Brain Computer Interface (BCI). In a further step, we aim to train a ML classifier to automate the pattern recognition process.\n To obtain the EEG characterisation, we will consider chord sequences. This choice represents per s\u00e8 a nov-elty, considering that in literature mainly sounds and tracks have been explored with BCI interfaces. We present preliminary findings and on that basis sketch research hypotheses to be further developed.",
      "abstract": "One of the ambitions of computational musicology consists in characterising music harmony in a symbol-ic system. In the context of the EU project Polifonia, we are exploring the possibility to associate EEG data to characterise harmony from a cognitive and emo-tional point of view. Data will be collected using a Brain Computer Interface (BCI). In a further step, we aim to train a ML classifier to automate the pattern recognition process.\n To obtain the EEG characterisation, we will consider chord sequences. This choice represents per s\u00e8 a nov-elty, considering that in literature mainly sounds and tracks have been explored with BCI interfaces. We present preliminary findings and on that basis sketch research hypotheses to be further developed.",
      "authors": [
        "Raffaella Folgieri, Enrico Daga, Claudio Lucchiari, Paul Arnold"
      ],
      "bilibili_id": "",
      "channel_name": "",
      "channel_url": "",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000026.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000026.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000026.png",
      "title": "A CHORD PROGRESSION LIBRARY FOR MEASURING EMOTIONS BY BCIs",
      "youtube_id": ""
    },
    "forum": "387",
    "id": "387",
    "position": "26"
  },
  {
    "content": {
      "TLDR": "Music recommendation systems are researched from multiple aspects, and the recent focus has been on the relationship between physiological measures and music preference. These studies, however, face the challenge of varying acoustic features among stimuli, which can con-found with the effect of preference. Also, access to physiological signals is often limited in real-life applications such as smart devices. In this study, we aimed to reduce the effect of acoustic variability by presenting different expressions of the same musical piece while connecting the study to daily use by shortening the stimulus length. We predicted participants\u2019 preference for musical expressions from cardiac and respiratory data measured from 30 subjects in a psychophysiological experiment. We identified a non-linear relationship between physiological signals and musical preference over an ultrashort time interval (~15 s). This result suggests that music recommendation systems can use biological signals to adapt their model rapidly on minimal data retrieval.",
      "abstract": "Music recommendation systems are researched from multiple aspects, and the recent focus has been on the relationship between physiological measures and music preference. These studies, however, face the challenge of varying acoustic features among stimuli, which can con-found with the effect of preference. Also, access to physiological signals is often limited in real-life applications such as smart devices. In this study, we aimed to reduce the effect of acoustic variability by presenting different expressions of the same musical piece while connecting the study to daily use by shortening the stimulus length. We predicted participants\u2019 preference for musical expressions from cardiac and respiratory data measured from 30 subjects in a psychophysiological experiment. We identified a non-linear relationship between physiological signals and musical preference over an ultrashort time interval (~15 s). This result suggests that music recommendation systems can use biological signals to adapt their model rapidly on minimal data retrieval.",
      "authors": [
        "Shu Sakamoto,  Vincent Cheung, Shinichi Furuya"
      ],
      "bilibili_id": "",
      "channel_name": "lv-27-sakamoto",
      "channel_url": "https://slack.com/app_redirect?channel=C04CLN02FC3",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000027.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000027.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000027.png",
      "title": "Rapidly Predicting Music Artistic Expression Preference From Heart Rate and Respiration Rate",
      "youtube_id": "https://drive.google.com/uc?export=preview&id=1VcJ_oqtoYZMx3PCR5XSDmPa0uTvN5Qgh"
    },
    "forum": "388",
    "id": "388",
    "position": "27"
  },
  {
    "content": {
      "TLDR": "Music structure analysis (MSA) systems aim to segment a song recording into non-overlapping sections with useful labels. Previous MSA systems typically predict abstract labels in a post-processing step and require the full context of the song. By contrast, we recently proposed a supervised framework, called \"Music Structural Function Analysis\" (MuSFA), that models and predicts meaningful labels like 'verse' and 'chorus' directly from audio, without requiring the full context of a song. However, the performance of this system depends on the amount and quality of training data. In this paper, we propose to repurpose a public dataset, HookTheory Lead Sheet Dataset (HLSD), to improve the performance. HLSD contains over 18K excerpts of music sections originally collected for studying automatic melody harmonization. We treat each excerpt as a partially labeled song and provide a label mapping, so that HLSD can be used together with other public datasets, such as SALAMI, RWC, and Isophonics. In cross-dataset evaluations, we find that including HLSD in training can improve state-of-the-art boundary detection and section labeling scores by ~3% and ~1% respectively.",
      "abstract": "Music structure analysis (MSA) systems aim to segment a song recording into non-overlapping sections with useful labels. Previous MSA systems typically predict abstract labels in a post-processing step and require the full context of the song. By contrast, we recently proposed a supervised framework, called \"Music Structural Function Analysis\" (MuSFA), that models and predicts meaningful labels like 'verse' and 'chorus' directly from audio, without requiring the full context of a song. However, the performance of this system depends on the amount and quality of training data. In this paper, we propose to repurpose a public dataset, HookTheory Lead Sheet Dataset (HLSD), to improve the performance. HLSD contains over 18K excerpts of music sections originally collected for studying automatic melody harmonization. We treat each excerpt as a partially labeled song and provide a label mapping, so that HLSD can be used together with other public datasets, such as SALAMI, RWC, and Isophonics. In cross-dataset evaluations, we find that including HLSD in training can improve state-of-the-art boundary detection and section labeling scores by ~3% and ~1% respectively.",
      "authors": [
        "Ju-Chiang Wang, Jordan B. L. Smith, Yun-Ning Hung"
      ],
      "bilibili_id": "",
      "channel_name": "lp-28-wang",
      "channel_url": "https://slack.com/app_redirect?channel=C04CZEF1A9X",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000028.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000028.pdf",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "000028.png",
      "title": "MuSFA: Improving Music Structural Function Analysis with Partially Labeled Data",
      "youtube_id": "https://drive.google.com/uc?export=preview&id=1yXHzmaYZ6XuXdu0M78gI_M8Rle8QBVwd"
    },
    "forum": "389",
    "id": "389",
    "position": "28"
  },
  {
    "content": {
      "TLDR": "The Audio Metaphor (AUME) demonstration invites participants to explore an interactive system for automatic soundscape composition. AUME is built as an online prompt-based system which produces audio soundscapes given a textual and affective query. The user can set valence and arousal curves which the system uses to generate a soundscape composition matching the desired eventfulness and mood. By streamlining the composition process, AUME aims to relieve some of the cognitive work done by sound designers. AUME's latest iteration draws from a database of nearly half a million audio files, each paired with a list of textual sound descriptors. Along with this expanded capacity, comes a refined ability to interpret a wider lexicon describing sound. Improvements to AUME\u2019s algorithms and architecture for sound retrieval, segmentation, background and foreground classification, automatic mixing and automatic soundscape affect recognition, makes it a powerful system that generates believable soundscapes at interactive rates.",
      "abstract": "The Audio Metaphor (AUME) demonstration invites participants to explore an interactive system for automatic soundscape composition. AUME is built as an online prompt-based system which produces audio soundscapes given a textual and affective query. The user can set valence and arousal curves which the system uses to generate a soundscape composition matching the desired eventfulness and mood. By streamlining the composition process, AUME aims to relieve some of the cognitive work done by sound designers. AUME's latest iteration draws from a database of nearly half a million audio files, each paired with a list of textual sound descriptors. Along with this expanded capacity, comes a refined ability to interpret a wider lexicon describing sound. Improvements to AUME\u2019s algorithms and architecture for sound retrieval, segmentation, background and foreground classification, automatic mixing and automatic soundscape affect recognition, makes it a powerful system that generates believable soundscapes at interactive rates.",
      "authors": [
        "Renaud Bougueng Tchemeube, Joshua Kranabetter, Craig Carpenter, Philippe Pasquier, Miles Thorogood"
      ],
      "bilibili_id": "",
      "channel_name": "lp-29-pasquier",
      "channel_url": "https://slack.com/app_redirect?channel=C04DAGA746L",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000029.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000029.pdf",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "000029.png",
      "title": "Audio Metaphor 2.0: An Improved System for Automatic Sound Design",
      "youtube_id": ""
    },
    "forum": "390",
    "id": "390",
    "position": "29"
  },
  {
    "content": {
      "TLDR": "With the rise of artificial intelligence has come an increase in its application towards creative domains, including music. Many systems have been built that apply machine learning approaches to the problem of computer-assisted music composition (CAC). Calliope is a web application that assists composers in performing multi-track composition tasks in the symbolic domain. The composer can upload Musical Instrument Digital Interface (MIDI) files, visualize and edit MIDI tracks, and generate partial (via bar in-filling) or complete multi-track content using the Multi-Track Music Machine (MMM). Generation can be done in batch, and can be combined with active playback listening and ranking for an enhanced workflow. Generated MIDI files can be exported or streamed to any standard Digital Audio Workstation (DAW). We present the system\u2019s features and describe its co-creative workflow. Calliope can be used for creative ideation, musical exploration or for full multi-track music compositions.",
      "abstract": "With the rise of artificial intelligence has come an increase in its application towards creative domains, including music. Many systems have been built that apply machine learning approaches to the problem of computer-assisted music composition (CAC). Calliope is a web application that assists composers in performing multi-track composition tasks in the symbolic domain. The composer can upload Musical Instrument Digital Interface (MIDI) files, visualize and edit MIDI tracks, and generate partial (via bar in-filling) or complete multi-track content using the Multi-Track Music Machine (MMM). Generation can be done in batch, and can be combined with active playback listening and ranking for an enhanced workflow. Generated MIDI files can be exported or streamed to any standard Digital Audio Workstation (DAW). We present the system\u2019s features and describe its co-creative workflow. Calliope can be used for creative ideation, musical exploration or for full multi-track music compositions.",
      "authors": [
        "Renaud Bougueng Tchemeube, Jeffrey Ens, Cale Plut, Philippe Pasquier"
      ],
      "bilibili_id": "",
      "channel_name": "lp-30-pasquier",
      "channel_url": "https://slack.com/app_redirect?channel=C04C693AWT1",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000030.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000030.pdf",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "000030.png",
      "title": "Calliope: An Online Interface for Generative Music Co-Creation",
      "youtube_id": ""
    },
    "forum": "391",
    "id": "391",
    "position": "30"
  },
  {
    "content": {
      "TLDR": "This paper presents the wmn4j Java library for handling Western music notation. The central goal of wmn4j is to provide a simple model of musical scores and an intuitive API that allows efficient access to their contents. Wmn4j supports fully concurrent and parallel access to all contents of scores and is intended for implementing large scale server-side music analysis applications. Wmn4j is licensed under the MIT license and is available on Github and Maven Central.",
      "abstract": "This paper presents the wmn4j Java library for handling Western music notation. The central goal of wmn4j is to provide a simple model of musical scores and an intuitive API that allows efficient access to their contents. Wmn4j supports fully concurrent and parallel access to all contents of scores and is intended for implementing large scale server-side music analysis applications. Wmn4j is licensed under the MIT license and is available on Github and Maven Central.",
      "authors": [
        "Otso Bj\u00f6rklund"
      ],
      "bilibili_id": "",
      "channel_name": "lv-31-bj\u221a\u2202rklund",
      "channel_url": "https://slack.com/app_redirect?channel=C04CLN05K7D",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000031.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000031.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000031.png",
      "title": "Western Music Notation for Java: A library for music notation on the JVM",
      "youtube_id": "https://drive.google.com/uc?export=preview&id=1MAGhoRVHRuIh6quM1u9FS9OZTd75DV6P"
    },
    "forum": "392",
    "id": "392",
    "position": "31"
  },
  {
    "content": {
      "TLDR": "Strumming is a guitar playing technique where instead of\n playing individual notes, the player strokes over the strings\n of a chord in an up- or downwards direction. Due to the\n very short time difference between the string hits, conventional\n note-based transcription methods cannot distinguish\n the direction of the strumming movement and hence is not\n suited for rhythm guitar transcription. Therefore, a multimodal\n approach to strumming action transcription is proposed,\n combining audio recordings with the motion of the\n hand. The audio signal is used for strumming event detection\n and the motion signal is used for direction classification.\n To measure the motion of the hand, a 6-axis\n gyroscope and accelerometer is mounted on the back of\n the hand which sends measurements to a computer. For\n evaluation purposes, a recording of five minutes has been\n labelled manually and is published along with this work.\n On this test dataset, the presented multimodal approach\n reaches an F1 score of 85% for the up and 92% for the\n down strumming transcription task.",
      "abstract": "Strumming is a guitar playing technique where instead of\n playing individual notes, the player strokes over the strings\n of a chord in an up- or downwards direction. Due to the\n very short time difference between the string hits, conventional\n note-based transcription methods cannot distinguish\n the direction of the strumming movement and hence is not\n suited for rhythm guitar transcription. Therefore, a multimodal\n approach to strumming action transcription is proposed,\n combining audio recordings with the motion of the\n hand. The audio signal is used for strumming event detection\n and the motion signal is used for direction classification.\n To measure the motion of the hand, a 6-axis\n gyroscope and accelerometer is mounted on the back of\n the hand which sends measurements to a computer. For\n evaluation purposes, a recording of five minutes has been\n labelled manually and is published along with this work.\n On this test dataset, the presented multimodal approach\n reaches an F1 score of 85% for the up and 92% for the\n down strumming transcription task.",
      "authors": [
        "Sebastian Murgul, Michael Heizmann"
      ],
      "bilibili_id": "",
      "channel_name": "lv-32-murgul",
      "channel_url": "https://slack.com/app_redirect?channel=C04DKLHB04V",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000032.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000032.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000032.png",
      "title": "A Multimodal Approach to Acoustic Guitar Strumming Action Transcription",
      "youtube_id": ""
    },
    "forum": "393",
    "id": "393",
    "position": "32"
  },
  {
    "content": {
      "TLDR": "We describe a proof-of-principle implementation of a system for drawing melodies that abstracts away from a note-level input representation via melodic contours. The aim is to allow users to express their musical intentions without requiring prior knowledge of how notes fit together melodiously. Current approaches to controllable melody generation often require users to choose parameters that are static across a whole sequence, via buttons or sliders. In contrast, our method allows users to quickly specify how parameters should change over time by drawing a contour.",
      "abstract": "We describe a proof-of-principle implementation of a system for drawing melodies that abstracts away from a note-level input representation via melodic contours. The aim is to allow users to express their musical intentions without requiring prior knowledge of how notes fit together melodiously. Current approaches to controllable melody generation often require users to choose parameters that are static across a whole sequence, via buttons or sliders. In contrast, our method allows users to quickly specify how parameters should change over time by drawing a contour.",
      "authors": [
        "Tashi Namgyal"
      ],
      "bilibili_id": "",
      "channel_name": "lv-33-namgyal",
      "channel_url": "https://slack.com/app_redirect?channel=C04D82YJBMM",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000033.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000033.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000033.png",
      "title": "MIDI-DRAW: SKETCHING TO CONTROL MELODY GENERATION",
      "youtube_id": ""
    },
    "forum": "394",
    "id": "394",
    "position": "33"
  },
  {
    "content": {
      "TLDR": "The piano-roll has been the de-facto standard representation for melodic and harmonic content in DAWs for decades, yet direct manipulation of those requires expert knowledge of music theory to begin with, and additionally becomes physically impractical when switching to the smaller, touch-based screens of modern mobile devices, too coarse for the precision required by micro-timings and the unforgiving discrete placement of pitches. Imbuing these interfaces with machine learning and offloading their precise and error-prone aspects to style-adaptive AI assistants may allow the design of more intuitive interactions whilst maintaining a high level of control, helping lower the cost of entry to composition for novices and offer stimulating new creative tools for professional musicians.\n We introduce PIANOTO, a touch-ready, responsive web interface for creating expressive piano performances through AI-assisted inpainting, all via simple swipe operations. This open-source, model-agnostic prototype is designed with both novice and expert users in mind, for usage either as a standalone tool or in conjunction with existing DAWs, on desktop or mobile.",
      "abstract": "The piano-roll has been the de-facto standard representation for melodic and harmonic content in DAWs for decades, yet direct manipulation of those requires expert knowledge of music theory to begin with, and additionally becomes physically impractical when switching to the smaller, touch-based screens of modern mobile devices, too coarse for the precision required by micro-timings and the unforgiving discrete placement of pitches. Imbuing these interfaces with machine learning and offloading their precise and error-prone aspects to style-adaptive AI assistants may allow the design of more intuitive interactions whilst maintaining a high level of control, helping lower the cost of entry to composition for novices and offer stimulating new creative tools for professional musicians.\n We introduce PIANOTO, a touch-ready, responsive web interface for creating expressive piano performances through AI-assisted inpainting, all via simple swipe operations. This open-source, model-agnostic prototype is designed with both novice and expert users in mind, for usage either as a standalone tool or in conjunction with existing DAWs, on desktop or mobile.",
      "authors": [
        "Th\u00e9is Bazin, Ga\u00ebtan Hadjeres, Mikhail Malt"
      ],
      "bilibili_id": "",
      "channel_name": "lv-34-bazin",
      "channel_url": "https://slack.com/app_redirect?channel=C04DNJXA466",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000034.pdf",
      "poster_link": "",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000034.png",
      "title": "AI-driven, mobile-first web UI for controllable expressive piano performance composition",
      "youtube_id": "https://drive.google.com/uc?export=preview&id=1MAGhoRVHRuIh6quM1u9FS9OZTd75DV6P"
    },
    "forum": "395",
    "id": "395",
    "position": "34"
  },
  {
    "content": {
      "TLDR": "Independent content creation is becoming highly present in our daily lives, from educational videos to social media influencers who reach celebrity status. With the increasing importance of social media and, consequently, content creation, music is taking a vital role in accompanying the created media, being able to capture the consumer's attention or set a mood. \n \n However, new challenges arise regarding obtaining music to accompany the created media. While royalty-free production music allows for its usage without significant issues, navigating extensive collections for a music piece suitable to the content being made can be tiresome.\n \n In this late-breaking demo, we present Beatoven.ai, an online tool for creating royalty-free music according to high-level parameters which do not require expert music creation knowledge. The user control enables the generated music piece to fit exactly what content creators have in mind and the content's requirements.",
      "abstract": "Independent content creation is becoming highly present in our daily lives, from educational videos to social media influencers who reach celebrity status. With the increasing importance of social media and, consequently, content creation, music is taking a vital role in accompanying the created media, being able to capture the consumer's attention or set a mood. \n \n However, new challenges arise regarding obtaining music to accompany the created media. While royalty-free production music allows for its usage without significant issues, navigating extensive collections for a music piece suitable to the content being made can be tiresome.\n \n In this late-breaking demo, we present Beatoven.ai, an online tool for creating royalty-free music according to high-level parameters which do not require expert music creation knowledge. The user control enables the generated music piece to fit exactly what content creators have in mind and the content's requirements.",
      "authors": [
        "Ant\u00f3nio Ramires, Siddharth Bhardwaj"
      ],
      "bilibili_id": "",
      "channel_name": "lp-35-ramires",
      "channel_url": "https://slack.com/app_redirect?channel=C04E189RT2M",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000035.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000035.pdf",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "000035.png",
      "title": "Beatoven.ai: AI-powered music creation",
      "youtube_id": ""
    },
    "forum": "396",
    "id": "396",
    "position": "35"
  },
  {
    "content": {
      "TLDR": "This paper presents FACETS, a tool that helps to manage and navigate large digital music libraries, aiming to aid the work of musicologists, composers, MIR researchers and the interested public. It supports queries based on melody, rhythm and metadata, as well as management of symbolic music datasets in MusicXML, MEI, Humdrum and ABC formats. It provides fast collection index and search functions based on Elasticsearch, with ranking methods by relevancy and music similarity, and an easy-accessible GUI with score visualization. FACETS is distributed as a Docker Image, with source code available on Github.",
      "abstract": "This paper presents FACETS, a tool that helps to manage and navigate large digital music libraries, aiming to aid the work of musicologists, composers, MIR researchers and the interested public. It supports queries based on melody, rhythm and metadata, as well as management of symbolic music datasets in MusicXML, MEI, Humdrum and ABC formats. It provides fast collection index and search functions based on Elasticsearch, with ranking methods by relevancy and music similarity, and an easy-accessible GUI with score visualization. FACETS is distributed as a Docker Image, with source code available on Github.",
      "authors": [
        "Tiange Zhu, Raphael Fournier-S'niehotta"
      ],
      "bilibili_id": "",
      "channel_name": "lv-36-zhu",
      "channel_url": "https://slack.com/app_redirect?channel=C04ECA5ETL0",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000036.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000036.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000036.png",
      "title": "FACETS: A TOOL FOR MANAGEMENT AND NAVIGATION OF SYMBOLIC MUSIC COLLECTIONS",
      "youtube_id": "https://drive.google.com/uc?export=preview&id=1C1sa4ZgxXEJhfXtH-UDzQA-tULP2tR05"
    },
    "forum": "397",
    "id": "397",
    "position": "36"
  },
  {
    "content": {
      "TLDR": "Melody Slot Machine HD is an application that allows one to generate melodies using music structures while playing a slot machine. The user can create new melody combinations by rearranging melody variations using the slot machine. When the user composes a melody with variations, the structure of the melody is analyzed using a theory of music known as the generative theory of tonal music. Even if the melody switches variations in the middle, the overall structure will not be disrupted. Our original Melody Slot Machine has been exhibited in academic conference as a demo system. Due to Covid-19, however, it has become difficult to exhibit in places where many people gather, so we developed an app version for the iPad, Melody Slot Machine HD, to give many people access to it through the Appstore. We conducted an experiment to evaluate Melody Slot Machine HD, and the results indicate that when five participants listened to new variations of melodies generated by random combinations using Melody Slot Machine HD, 85.6% of the melodies were judged as natural or almost natural.",
      "abstract": "Melody Slot Machine HD is an application that allows one to generate melodies using music structures while playing a slot machine. The user can create new melody combinations by rearranging melody variations using the slot machine. When the user composes a melody with variations, the structure of the melody is analyzed using a theory of music known as the generative theory of tonal music. Even if the melody switches variations in the middle, the overall structure will not be disrupted. Our original Melody Slot Machine has been exhibited in academic conference as a demo system. Due to Covid-19, however, it has become difficult to exhibit in places where many people gather, so we developed an app version for the iPad, Melody Slot Machine HD, to give many people access to it through the Appstore. We conducted an experiment to evaluate Melody Slot Machine HD, and the results indicate that when five participants listened to new variations of melodies generated by random combinations using Melody Slot Machine HD, 85.6% of the melodies were judged as natural or almost natural.",
      "authors": [
        "Masatoshi Hamanaka"
      ],
      "bilibili_id": "",
      "channel_name": "lp-37-hamanaka",
      "channel_url": "https://slack.com/app_redirect?channel=C04DKLHFMGV",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000037.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000037.pdf",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "000037.png",
      "title": "Melody Slot Machine HD",
      "youtube_id": "https://drive.google.com/uc?export=preview&id=1bQUmQmXlTIRsceQT3byiA_GM0twiBGdR"
    },
    "forum": "398",
    "id": "398",
    "position": "37"
  },
  {
    "content": {
      "TLDR": "Sheet music, which contains precise instructions for performers, remains a primary mechanism for communicating musical ideas. While digital scans of sheet music (represented as images) and recordings of performances (represented as audio) are both abundant sources of musical data, there remains a surprising paucity of aligned data, mappings between pixels in sheet music and the corresponding timestamps in associated performances. While several existing MIR datasets contain alignments between performances and structured scores (formats like MIDI and MusicXML), no current resources align performances with more commonplace raw-image sheet music, possibly due to obstacles like expressive timing and repeat signs that make alignment challenging and time-consuming even for trained musicians. To overcome these obstacles, we developed an interactive system, MeSA , which leverages off-the-shelf measure and beat detection software to aid musicians in quickly producing measure-level alignments (ones which map bounding boxes of measures in the sheet music to timestamps in the performance audio). We verified MeSA \u2019s functionality by using it to create a small proof-of-concept dataset, MeSA-13.",
      "abstract": "Sheet music, which contains precise instructions for performers, remains a primary mechanism for communicating musical ideas. While digital scans of sheet music (represented as images) and recordings of performances (represented as audio) are both abundant sources of musical data, there remains a surprising paucity of aligned data, mappings between pixels in sheet music and the corresponding timestamps in associated performances. While several existing MIR datasets contain alignments between performances and structured scores (formats like MIDI and MusicXML), no current resources align performances with more commonplace raw-image sheet music, possibly due to obstacles like expressive timing and repeat signs that make alignment challenging and time-consuming even for trained musicians. To overcome these obstacles, we developed an interactive system, MeSA , which leverages off-the-shelf measure and beat detection software to aid musicians in quickly producing measure-level alignments (ones which map bounding boxes of measures in the sheet music to timestamps in the performance audio). We verified MeSA \u2019s functionality by using it to create a small proof-of-concept dataset, MeSA-13.",
      "authors": [
        "Michael Feffer, Chris Donahue, Zachary Lipton"
      ],
      "bilibili_id": "",
      "channel_name": "lv-38-feffer",
      "channel_url": "https://slack.com/app_redirect?channel=C04ECA5GPNC",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000038.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000038.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000038.png",
      "title": "Assistive alignment of in-the-wild sheet music and performances",
      "youtube_id": "https://drive.google.com/uc?export=preview&id=1XrFo-uZYL6ptL5HDCh-uR8kxNHdk8hcp"
    },
    "forum": "399",
    "id": "399",
    "position": "38"
  },
  {
    "content": {
      "TLDR": "Piano is one of the most popular instruments among people to play music. \n During the piano performance, loudness is an important factor for expressiveness. It gives the audience a different impression by different dynamics of loudness.\n Due to the polyphonic characteristics of piano having many information such as melody line and accompaniment line, it is more meaningful to find loudness for each note than looking at accumulated loudness of a certain time frame. \n \n There have been a few researches having tackled to this topic by a Non-Negative Matrix Factorization (NMF) to find note level loudness. This research provides a novel method by Deep Neural Network (DNN) with score information by Feature-wise Linear Modulation (FiLM) to estimate the loudness based on MIDI velocity for each note performed by piano players. \n This is the first research which tried to estimate the MIDI velocity by a DNN model in end to end fashion between audio to MIDI velocity.",
      "abstract": "Piano is one of the most popular instruments among people to play music. \n During the piano performance, loudness is an important factor for expressiveness. It gives the audience a different impression by different dynamics of loudness.\n Due to the polyphonic characteristics of piano having many information such as melody line and accompaniment line, it is more meaningful to find loudness for each note than looking at accumulated loudness of a certain time frame. \n \n There have been a few researches having tackled to this topic by a Non-Negative Matrix Factorization (NMF) to find note level loudness. This research provides a novel method by Deep Neural Network (DNN) with score information by Feature-wise Linear Modulation (FiLM) to estimate the loudness based on MIDI velocity for each note performed by piano players. \n This is the first research which tried to estimate the MIDI velocity by a DNN model in end to end fashion between audio to MIDI velocity.",
      "authors": [
        "Hyon Kim, Marius Miron, Xavier Serra"
      ],
      "bilibili_id": "",
      "channel_name": "lv-39-kim",
      "channel_url": "https://slack.com/app_redirect?channel=C04D82YQ0ET",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000039.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000039.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000039.png",
      "title": "Note level MIDI velocity estimation for piano performance",
      "youtube_id": ""
    },
    "forum": "400",
    "id": "400",
    "position": "39"
  },
  {
    "content": {
      "TLDR": "We present Essentia API, a web API to access a collection of state-of-the-art music audio analysis and description algorithms based on Essentia, an open-source library and machine learning (ML) models for audio and music analysis. We are developing it as part of a broader project in which we explore strategies for the commercial viability of technologies developed at Music Technology Group (MTG) following open science and open source practices, which involves finding licensing schemes and building custom solutions. Currently, the API supports music auto-tagging and classification algorithms (for genre, instrumentation, mood/emotion, danceability, approachability, and engagement), and algorithms for musical key, tempo, loudness, and many more. In the future, we envision expanding it with new machine learning models developed by the MTG and our collaborators to facilitate their access for a broader community of users.",
      "abstract": "We present Essentia API, a web API to access a collection of state-of-the-art music audio analysis and description algorithms based on Essentia, an open-source library and machine learning (ML) models for audio and music analysis. We are developing it as part of a broader project in which we explore strategies for the commercial viability of technologies developed at Music Technology Group (MTG) following open science and open source practices, which involves finding licensing schemes and building custom solutions. Currently, the API supports music auto-tagging and classification algorithms (for genre, instrumentation, mood/emotion, danceability, approachability, and engagement), and algorithms for musical key, tempo, loudness, and many more. In the future, we envision expanding it with new machine learning models developed by the MTG and our collaborators to facilitate their access for a broader community of users.",
      "authors": [
        "Albin Andrew Correya, Dmitry Bogdanov, Pablo Alonso-Jim\u00e9nez, Xavier Serra"
      ],
      "bilibili_id": "",
      "channel_name": "lp-40-bogdanov",
      "channel_url": "https://slack.com/app_redirect?channel=C04DNFV609H",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000040.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000040.pdf",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "000040.png",
      "title": "Essentia API: a web API for music audio analysis",
      "youtube_id": ""
    },
    "forum": "401",
    "id": "401",
    "position": "40"
  },
  {
    "content": {
      "TLDR": "We introduce a differentiable WORLD synthesizer and demonstrate its use in end-to-end audio style transfer tasks. Our baseline differentiable synthesizer has no model parameters, yet it yields adequate synthesis quality. We extend the baseline synthesizer by appending lightweight black-box postnets applying further processing to the baseline output in order to improve fidelity. An alternative differentiable approach relies on the extraction of the source excitation spectrum directly, and results in improved naturalness, albeit for a narrower class of style transfer applications. Accordingly, we illustrate our methods for (singing) voice conversion and the DDSP timbre transfer task.",
      "abstract": "We introduce a differentiable WORLD synthesizer and demonstrate its use in end-to-end audio style transfer tasks. Our baseline differentiable synthesizer has no model parameters, yet it yields adequate synthesis quality. We extend the baseline synthesizer by appending lightweight black-box postnets applying further processing to the baseline output in order to improve fidelity. An alternative differentiable approach relies on the extraction of the source excitation spectrum directly, and results in improved naturalness, albeit for a narrower class of style transfer applications. Accordingly, we illustrate our methods for (singing) voice conversion and the DDSP timbre transfer task.",
      "authors": [
        "Shahan Nercessian"
      ],
      "bilibili_id": "",
      "channel_name": "lv-41-nercessian",
      "channel_url": "https://slack.com/app_redirect?channel=C04DNFV792P",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000041.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000041.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000041.png",
      "title": "Differentiable WORLD Synthesizer-Based Neural Vocoder With Application To End-To-End Audio Style Transfer",
      "youtube_id": ""
    },
    "forum": "402",
    "id": "402",
    "position": "41"
  },
  {
    "content": {
      "TLDR": "Denoising Diffusion Probabilistic Models (DDPMs) have\n shown great success generating high quality samples in\n both discrete and continuous domains. How-\n ever, Discrete Denoising Diffusion Probabilistic Models\n (D3PMs) have not yet been shown to be directly appli-\n cable to the domain of Symbolic Music. In this work\n we present the direct generation of Polyphonic Symbolic\n Music using D3PMs. Our model does not only exhibit\n state of the art sample quality, but also allows for var-\n ious conditioning methods at sample time without ex-\n tra training. As the model is trained to reconstruct ran-\n domly masked out tokens, conditioning on an existing\n piece of symbolic music is possible. Such condition-\n ing scenarios include, but are not limited to, accom-\n paniment (one track is provided, accompaniment tracks\n are masked out) and infilling/completion (one or multi-\n ple tracks with temporal gaps are provided). We provide\n our implementation, trained model weights and some se-\n lected samples at https://github.com/plassma/symbolic-music-discrete-diffusion.",
      "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) have\n shown great success generating high quality samples in\n both discrete and continuous domains. How-\n ever, Discrete Denoising Diffusion Probabilistic Models\n (D3PMs) have not yet been shown to be directly appli-\n cable to the domain of Symbolic Music. In this work\n we present the direct generation of Polyphonic Symbolic\n Music using D3PMs. Our model does not only exhibit\n state of the art sample quality, but also allows for var-\n ious conditioning methods at sample time without ex-\n tra training. As the model is trained to reconstruct ran-\n domly masked out tokens, conditioning on an existing\n piece of symbolic music is possible. Such condition-\n ing scenarios include, but are not limited to, accom-\n paniment (one track is provided, accompaniment tracks\n are masked out) and infilling/completion (one or multi-\n ple tracks with temporal gaps are provided). We provide\n our implementation, trained model weights and some se-\n lected samples at https://github.com/plassma/symbolic-music-discrete-diffusion.",
      "authors": [
        "Matthias Plasser, Silvan Peter"
      ],
      "bilibili_id": "",
      "channel_name": "lv-43-plasser",
      "channel_url": "https://slack.com/app_redirect?channel=C04DNFV8GBD",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000043.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000043.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000043.png",
      "title": "SCHMUBERT: A SYMBOLIC CREATIVE HARMONIC MUSIC UNMASKING BIDIRECTIONAL ENCODER REPRESENTATION TRANSFORMER",
      "youtube_id": "https://drive.google.com/uc?export=preview&id=1eTe4EBMh0xfvZR1Y8P0LKNiDPDZAz6F1"
    },
    "forum": "404",
    "id": "404",
    "position": "43"
  },
  {
    "content": {
      "TLDR": "We present Song Describer, an open-source data annotation platform for crowdsourcing textual descriptions of music recordings. Through this tool, we propose to collect annotations with the goal of creating the first public dataset of audio-caption pairs in the music domain. We believe that such a dataset will be useful in supporting the growing interest in the integration of natural language processing within music information retrieval systems.\n In this paper, we describe our approach to designing Song Describer, outline the data collection protocol, and illustrate the main steps involved in using the platform.",
      "abstract": "We present Song Describer, an open-source data annotation platform for crowdsourcing textual descriptions of music recordings. Through this tool, we propose to collect annotations with the goal of creating the first public dataset of audio-caption pairs in the music domain. We believe that such a dataset will be useful in supporting the growing interest in the integration of natural language processing within music information retrieval systems.\n In this paper, we describe our approach to designing Song Describer, outline the data collection protocol, and illustrate the main steps involved in using the platform.",
      "authors": [
        "Ilaria Manco, Benno Weck, Philip Tovstogan, Minz Won, Dmitry Bogdanov"
      ],
      "bilibili_id": "",
      "channel_name": "lp-44-manco",
      "channel_url": "https://slack.com/app_redirect?channel=C04DR30PY4C",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000044.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000044.pdf",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "000044.png",
      "title": "Song Describer: a Platform for Collecting Textual Descriptions of Music Recordings",
      "youtube_id": ""
    },
    "forum": "405",
    "id": "405",
    "position": "44"
  },
  {
    "content": {
      "TLDR": "Explainability for the behavior of deep learning models has been a topic of increasing interest, as these black box networks have become ubiquitous across machine learning tasks in general and in Music Information Retrieval. Our past research into learned feature representations of Predominant Instrument Recognition (PIR) models using adversarial training showed that these models are quite fragile. As a result, we propose a PIR framework that is interpretable by nature, using a Generative Adversarial Network (GAN) architecture to iteratively include adversarial advancements in the PIR classifier. To do this we reframe PIR as a conditional-generative task using an Auxiliary Classifier GAN (ACGAN) and train a PIR classifier concurrently with an instrument synthesis model, which is then used to explore the feature space learned by the ACGAN. Preliminary results show that the ACGAN is capable of generating Mel-spectrogram outputs that closely resemble the input data. Based on these findings, we propose a novel modified ACGAN Classifier with a Wasserstein GAN with Gradient Penalty (WGAN-GP) framework addition to address instability in the ACGAN training.",
      "abstract": "Explainability for the behavior of deep learning models has been a topic of increasing interest, as these black box networks have become ubiquitous across machine learning tasks in general and in Music Information Retrieval. Our past research into learned feature representations of Predominant Instrument Recognition (PIR) models using adversarial training showed that these models are quite fragile. As a result, we propose a PIR framework that is interpretable by nature, using a Generative Adversarial Network (GAN) architecture to iteratively include adversarial advancements in the PIR classifier. To do this we reframe PIR as a conditional-generative task using an Auxiliary Classifier GAN (ACGAN) and train a PIR classifier concurrently with an instrument synthesis model, which is then used to explore the feature space learned by the ACGAN. Preliminary results show that the ACGAN is capable of generating Mel-spectrogram outputs that closely resemble the input data. Based on these findings, we propose a novel modified ACGAN Classifier with a Wasserstein GAN with Gradient Penalty (WGAN-GP) framework addition to address instability in the ACGAN training.",
      "authors": [
        "Charis Cochran, Youngmoo Kim"
      ],
      "bilibili_id": "",
      "channel_name": "lv-45-cochran",
      "channel_url": "https://slack.com/app_redirect?channel=C04DKLHP8SH",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000045.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000045.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000045.png",
      "title": "A Framework for Instrument Recognition Using Conditional Generative Adversarial Networks",
      "youtube_id": ""
    },
    "forum": "406",
    "id": "406",
    "position": "45"
  },
  {
    "content": {
      "TLDR": "We introduce a prototype model for differentiable acoustic guitar synthesis and share initial results. Our model takes in tablature conditioning in a 6-channel MIDI-like format and outputs synthesized audio. We utilize DDSP-style synthesis, in which parameters are predicted which drive differentiable implementations of harmonic oscillator and filtered noise synthesizer modules. A monophonic decoder network is used to predict harmonic amplitudes and noise filters for each guitar-string voice from the tablature conditioning. Finally, the 6 channels of synthesized guitar string audio are summed and fed through a reverb module with trainable parameters. In this work, we investigate the use of a guitar-string embedding layer which encodes the guitar-string ID for each note in the conditioning. We compare 2 model variations, the basic model and the string embedding model. While this research is ongoing, our preliminary results indicate that the use of string embeddings results in improved synthesis. Moving forward, we aim to incorporate this differentiable guitar synthesis model into a an unsupervised system for automatic guitar transcription.",
      "abstract": "We introduce a prototype model for differentiable acoustic guitar synthesis and share initial results. Our model takes in tablature conditioning in a 6-channel MIDI-like format and outputs synthesized audio. We utilize DDSP-style synthesis, in which parameters are predicted which drive differentiable implementations of harmonic oscillator and filtered noise synthesizer modules. A monophonic decoder network is used to predict harmonic amplitudes and noise filters for each guitar-string voice from the tablature conditioning. Finally, the 6 channels of synthesized guitar string audio are summed and fed through a reverb module with trainable parameters. In this work, we investigate the use of a guitar-string embedding layer which encodes the guitar-string ID for each note in the conditioning. We compare 2 model variations, the basic model and the string embedding model. While this research is ongoing, our preliminary results indicate that the use of string embeddings results in improved synthesis. Moving forward, we aim to incorporate this differentiable guitar synthesis model into a an unsupervised system for automatic guitar transcription.",
      "authors": [
        "Andrew F Wiggins, Youngmoo Kim"
      ],
      "bilibili_id": "",
      "channel_name": "lv-46-wiggins",
      "channel_url": "https://slack.com/app_redirect?channel=C04DR30SWKE",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000046.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000046.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000046.png",
      "title": "Differentiable Acoustic Guitar Synthesis from Tablature",
      "youtube_id": ""
    },
    "forum": "407",
    "id": "407",
    "position": "46"
  },
  {
    "content": {
      "TLDR": "As various studies on symbolic music, it is important to consider the methods for representing note information. Among them, pianoroll is one of the widely used methods. However, pianoroll data is too sparse, and it is difficult to handle chord scale information compared to token-based representation. To solve this problem, we suggest new representation \"relational pianoroll\" for symbolic music data. This is pianoroll-based method with handling only relational pitch changes. We conducted experiments using a relational pianoroll to learn bar classification, music generation, bar clustering, and pianoroll Vector Quantized Variational AutoEncoder(VQ-VAE) model.",
      "abstract": "As various studies on symbolic music, it is important to consider the methods for representing note information. Among them, pianoroll is one of the widely used methods. However, pianoroll data is too sparse, and it is difficult to handle chord scale information compared to token-based representation. To solve this problem, we suggest new representation \"relational pianoroll\" for symbolic music data. This is pianoroll-based method with handling only relational pitch changes. We conducted experiments using a relational pianoroll to learn bar classification, music generation, bar clustering, and pianoroll Vector Quantized Variational AutoEncoder(VQ-VAE) model.",
      "authors": [
        "Seonghyeon Go"
      ],
      "bilibili_id": "",
      "channel_name": "lv-47-go",
      "channel_url": "https://slack.com/app_redirect?channel=C04DNJXP5D0",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000047.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000047.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000047.png",
      "title": "A NEW PIANOROLL WITH RELATIVE PITCH APPROACH",
      "youtube_id": "https://drive.google.com/uc?export=preview&id=1i5-UA-DDvCnMwC5TvtKgxlP2ZDOjY-ii"
    },
    "forum": "408",
    "id": "408",
    "position": "47"
  },
  {
    "content": {
      "TLDR": "Detecting and identifying sound events is a core task with many applications in music. Supervised deep learning-based solutions are effective but require labeled datasets. Finding annotated music datasets is difficult for specific tasks at hand. Moreover, the performance degrades when models trained on synthetic datasets are deployed on real-world audio. We are working towards effective unsupervised or semi-supervised domain adaptation techniques for the above problem. Initial experiments show promising results. The domain adaptation methods we discuss here are fairly general and could be applied to other problems in music information retrieval too.",
      "abstract": "Detecting and identifying sound events is a core task with many applications in music. Supervised deep learning-based solutions are effective but require labeled datasets. Finding annotated music datasets is difficult for specific tasks at hand. Moreover, the performance degrades when models trained on synthetic datasets are deployed on real-world audio. We are working towards effective unsupervised or semi-supervised domain adaptation techniques for the above problem. Initial experiments show promising results. The domain adaptation methods we discuss here are fairly general and could be applied to other problems in music information retrieval too.",
      "authors": [
        "Arkaprava Biswas, Akshay Raina, Vipul Arora"
      ],
      "bilibili_id": "",
      "channel_name": "lp-48-arora",
      "channel_url": "https://slack.com/app_redirect?channel=C04DR30UW0L",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000048.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000048.pdf",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "000048.png",
      "title": "UNSUPERVISED DOMAIN ADAPTATION FOR SOUND EVENT DETECTION IN MUSIC APPLICATIONS",
      "youtube_id": ""
    },
    "forum": "409",
    "id": "409",
    "position": "48"
  },
  {
    "content": {
      "TLDR": "The deep learning community has witnessed an exponentially growing interest in self-supervised learning (SSL). However, it still remains unexplored how to build a framework for learning useful representations of raw music waveforms in a self-supervised manner. In this work, we design MAP-Music2Vec, a framework exploring different self-supervised learning algorithmic components and tricks for music audios. Our model achieves comparable results to the state-of-the-art (SOTA) music SSL model Jukebox, despite being significantly lightweight with less than 2% of parameters of the latter. The model will be released on Huggingface.",
      "abstract": "The deep learning community has witnessed an exponentially growing interest in self-supervised learning (SSL). However, it still remains unexplored how to build a framework for learning useful representations of raw music waveforms in a self-supervised manner. In this work, we design MAP-Music2Vec, a framework exploring different self-supervised learning algorithmic components and tricks for music audios. Our model achieves comparable results to the state-of-the-art (SOTA) music SSL model Jukebox, despite being significantly lightweight with less than 2% of parameters of the latter. The model will be released on Huggingface.",
      "authors": [
        "Yizhi Li, Ruibin Yuan, Ge Zhang, Yinghao MA, Chenghua Lin, Xingran Chen, Anton Ragni, Hanzhi Yin, Zhijie Hu, Haoyu He, Emmanouil Benetos, Norbert Gyenge, Ruibo Liu, Jie Fu"
      ],
      "bilibili_id": "",
      "channel_name": "lv-49-li",
      "channel_url": "https://slack.com/app_redirect?channel=C04E18AA933",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000049.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000049.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000049.png",
      "title": "MAP-Music2Vec: A Simple and Effective Baseline for Self-Supervised Music Audio Representation Learning",
      "youtube_id": ""
    },
    "forum": "410",
    "id": "410",
    "position": "49"
  },
  {
    "content": {
      "TLDR": "Over the past few years, deep Artificial Neural Networks (ANNs) have become more popular due to their great success in various tasks.\n However, their improvements made them more capable but less interpretable.\n To overcome this issue, some introspection techniques have been proposed.\n According to the fact that ANNs are inspired by human brains, we adapt techniques from cognitive neuroscience to easier interpret them.\n Our approach first computes characteristic network responses for groups of input examples, for example, relating to a specific error.\n We then use these to compare network responses between different groups.\n To this end, we compute representational similarity and we visualize the activations as topographic activation maps.\n In this work, we present a graphical user interface called CogXAI ANNalyzer to easily apply our techniques to trained ANNs and to interpret their results.\n Further, we demonstrate our tool using an audio ANN for speech recognition.",
      "abstract": "Over the past few years, deep Artificial Neural Networks (ANNs) have become more popular due to their great success in various tasks.\n However, their improvements made them more capable but less interpretable.\n To overcome this issue, some introspection techniques have been proposed.\n According to the fact that ANNs are inspired by human brains, we adapt techniques from cognitive neuroscience to easier interpret them.\n Our approach first computes characteristic network responses for groups of input examples, for example, relating to a specific error.\n We then use these to compare network responses between different groups.\n To this end, we compute representational similarity and we visualize the activations as topographic activation maps.\n In this work, we present a graphical user interface called CogXAI ANNalyzer to easily apply our techniques to trained ANNs and to interpret their results.\n Further, we demonstrate our tool using an audio ANN for speech recognition.",
      "authors": [
        "Maral Ebrahimzadeh, Valerie Krug, Sebastian Stober"
      ],
      "bilibili_id": "",
      "channel_name": "lv-50-ebrahimzadeh",
      "channel_url": "https://slack.com/app_redirect?channel=C04DNJXRVEW",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000050.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000050.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000050.png",
      "title": "CogXAI ANNalyzer: Cognitive Neuroscience Inspired Techniques for eXplainable AI",
      "youtube_id": ""
    },
    "forum": "411",
    "id": "411",
    "position": "50"
  },
  {
    "content": {
      "TLDR": "One of the challenges in modeling of symbolic musical information is the correct representation and manipulation of pitch, in particular in the Western standard notation system. This late-breaking demo introduces a set of libraries for different programming languages that provide unified types and operations, as well as a common notation system for different pitch and interval types. In particular, these libraries provide an API for working with spelled pitches and intervals as used in the Western notation system. This reduces the complexity of using properly represented pitches and intervals in computational modelling, machine learning, and data analysis, and potentially improves compatibility (and thus reusability) of different models.",
      "abstract": "One of the challenges in modeling of symbolic musical information is the correct representation and manipulation of pitch, in particular in the Western standard notation system. This late-breaking demo introduces a set of libraries for different programming languages that provide unified types and operations, as well as a common notation system for different pitch and interval types. In particular, these libraries provide an API for working with spelled pitches and intervals as used in the Western notation system. This reduces the complexity of using properly represented pitches and intervals in computational modelling, machine learning, and data analysis, and potentially improves compatibility (and thus reusability) of different models.",
      "authors": [
        "Christoph Finkensiep, Robert Lieck, Martin A Rohrmeier"
      ],
      "bilibili_id": "",
      "channel_name": "lv-51-finkensiep",
      "channel_url": "https://slack.com/app_redirect?channel=C04DG1BL6T0",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000051.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000051.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000051.png",
      "title": "A Family of Libraries for Working With Pitches and Intervals",
      "youtube_id": ""
    },
    "forum": "412",
    "id": "412",
    "position": "51"
  },
  {
    "content": {
      "TLDR": "We explore the generation of visualizations of audio latent spaces using an audio to image generation pipeline. We believe this can help with the interpretability of audio latent spaces. We demonstrate a variety of results on the NSynth dataset.",
      "abstract": "We explore the generation of visualizations of audio latent spaces using an audio to image generation pipeline. We believe this can help with the interpretability of audio latent spaces. We demonstrate a variety of results on the NSynth dataset.",
      "authors": [
        "Nicolas Jonason, Bob Sturm"
      ],
      "bilibili_id": "",
      "channel_name": "lv-52-jonason",
      "channel_url": "https://slack.com/app_redirect?channel=C04DG1BM32S",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000052.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000052.pdf",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000052.png",
      "title": "Audio Latent Space Cartography",
      "youtube_id": ""
    },
    "forum": "413",
    "id": "413",
    "position": "52"
  },
  {
    "content": {
      "TLDR": "Before magnetic tape recording was common, acetate discs were the main audio storage medium for radio broadcasters. Acetate discs only had a capacity to record about ten minutes. Longer material was recorded on overlapping discs using (at least) two recorders. Unfortunately, the recorders used were not reliable in terms of recording speed, resulting in audio of variable speed. \n \n To make digitized audio originating from acetate discs fit for reuse, (1) overlapping parts need to be identified, (2) a precise alignment needs to be found and (3) a mixing point suggested. All three steps are challenging due to the audio speed variabilities.\n  \n This paper introduces the ideas behind DiscStitch: which aims to reassemble audio from overlapping parts, even if variable speed is present. The main contribution is a fast and precise audio alignment strategy based on spectral peaks. The method is evaluated on a synthetic data set.",
      "abstract": "Before magnetic tape recording was common, acetate discs were the main audio storage medium for radio broadcasters. Acetate discs only had a capacity to record about ten minutes. Longer material was recorded on overlapping discs using (at least) two recorders. Unfortunately, the recorders used were not reliable in terms of recording speed, resulting in audio of variable speed. \n \n To make digitized audio originating from acetate discs fit for reuse, (1) overlapping parts need to be identified, (2) a precise alignment needs to be found and (3) a mixing point suggested. All three steps are challenging due to the audio speed variabilities.\n  \n This paper introduces the ideas behind DiscStitch: which aims to reassemble audio from overlapping parts, even if variable speed is present. The main contribution is a fast and precise audio alignment strategy based on spectral peaks. The method is evaluated on a synthetic data set.",
      "authors": [
        "Joren Six"
      ],
      "bilibili_id": "",
      "channel_name": "lp-53-six",
      "channel_url": "https://slack.com/app_redirect?channel=C04DKLJ29V3",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000053.pdf",
      "poster_link": "https://archives.ismir.net/ismir2022/latebreaking_poster/000053.pdf",
      "poster_type": "",
      "session": [
        "Physical"
      ],
      "thumbnail_link": "000053.png",
      "title": "DiscStitch: towards audio-to-audio alignment with robustness to playback speed variabilities",
      "youtube_id": ""
    },
    "forum": "414",
    "id": "414",
    "position": "53"
  },
  {
    "content": {
      "TLDR": "Recent advances in deep learning-based audio creation are fueling the rise of a new approach to musical sound design: neural synthesis. In this project, an implementation of neural waveform synthesis on a resource-limited embedded development platform is explored using a modified version of Google Magenta\u2019s Differentiable Digital Signal Processing (DDSP) timbre transfer pipeline on an Nvidia Jetson Nano 2GB embedded prototyping board. This work introduces a standalone hardware/software system for running DDSP timbre transfer models offline. It features a physical control interface for audio input and output, and a selection of five neural timbre models to choose from. The streamlined neural synthesis pipeline uses YIN in lieu of CREPE for input audio feature extraction, resulting in a processing latency reduction, but inferior output quality in noisy environments. Qualitative research data indicate that users find the device to be useful, intuitive, and fun to interact with; however, its audio rendering speed remains too slow for many practical use cases. As this project is designed to be readily replicable by end users, the source code, build instructions, and audio examples are all made freely available.",
      "abstract": "Recent advances in deep learning-based audio creation are fueling the rise of a new approach to musical sound design: neural synthesis. In this project, an implementation of neural waveform synthesis on a resource-limited embedded development platform is explored using a modified version of Google Magenta\u2019s Differentiable Digital Signal Processing (DDSP) timbre transfer pipeline on an Nvidia Jetson Nano 2GB embedded prototyping board. This work introduces a standalone hardware/software system for running DDSP timbre transfer models offline. It features a physical control interface for audio input and output, and a selection of five neural timbre models to choose from. The streamlined neural synthesis pipeline uses YIN in lieu of CREPE for input audio feature extraction, resulting in a processing latency reduction, but inferior output quality in noisy environments. Qualitative research data indicate that users find the device to be useful, intuitive, and fun to interact with; however, its audio rendering speed remains too slow for many practical use cases. As this project is designed to be readily replicable by end users, the source code, build instructions, and audio examples are all made freely available.",
      "authors": [
        "Alexander Tipper"
      ],
      "bilibili_id": "",
      "channel_name": "lv-54-tipper",
      "channel_url": "https://slack.com/app_redirect?channel=C04DG1BPJ6A",
      "day": 4,
      "paper_link": "https://archives.ismir.net/ismir2022/latebreaking/000054.pdf",
      "poster_link": "",
      "poster_type": "",
      "session": [
        "Virtual"
      ],
      "thumbnail_link": "000054.png",
      "title": "Neural Waveform Synthesis on a Low-Resource Embedded Device",
      "youtube_id": "https://drive.google.com/uc?export=preview&id=1iRdH5yejQoJkAXbKSm-1hbmcUw5EX83h"
    },
    "forum": "415",
    "id": "415",
    "position": "54"
  }
]
