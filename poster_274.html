


<!DOCTYPE html>
<html lang="en">
  <head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <link rel="icon" type="image/png" sizes="32x32" href="static/images/ismir_tab_icon.png">

    <link rel="stylesheet" href="static/css/main.css" />
    <link rel="stylesheet" href="static/css/custom.css" />
    <link rel="stylesheet" href="static/css/lazy_load.css" />
    <link rel="stylesheet" href="static/css/typeahead.css" />
    <link rel="stylesheet" href="static/css/poster.css" />
    <link rel="stylesheet" href="static/css/music.css" />
    <link rel="stylesheet" href="static/css/piece.css" />


    <!-- <link rel="stylesheet" href="https://gitcdn.xyz/repo/wingkwong/jquery-chameleon/master/src/chameleon.min.css"> -->

    <!-- External Javascript libs  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

    <script
      src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
      integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
      crossorigin="anonymous"
    ></script>


    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>
    <!-- <script src="static/js/chameleon.js"></script> -->


    <!-- Library libs -->
    <script src="static/js/typeahead.bundle.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
      href="static/css/Lato.css"
      rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet" />
    <link
      href="static/css/Cuprum.css"
      rel="stylesheet"
    />
    <link
      href="static/css/Ambiant.css"
      rel="stylesheet"
    />
    <!-- <link href="static/css/chameleon.css" rel="stylesheet"/> -->
    <title>ISMIR 2025: Adding temporal musical controls on top of pretrained generative models</title>
    
<meta name="citation_title" content="Adding temporal musical controls on top of pretrained generative models" />

<meta name="citation_author" content="Sarah Nabi" />

<meta name="citation_author" content="Nils Demerlé" />

<meta name="citation_author" content="Geoffroy Peeters" />

<meta name="citation_author" content="Frederic Bevilacqua" />

<meta name="citation_author" content="Philippe Esling" />

<meta name="citation_publication_date" content="21-25 September 2025" />
<meta name="citation_conference_title" content="Ismir 2025 Hybrid Conference" />
<meta name="citation_inbook_title" content="Proceedings of the First MiniCon Conference" />
<meta name="citation_abstract" content="Recent advances in deep generative modeling have enabled high-quality models for musical audio synthesis. However, these approaches remain difficult to control, confined to simple, static attributes and, most importantly, entail retraining a different computationally-heavy architecure for each new control. This is inefficient and impractical as it requires substantial computational resources.
In this paper, we propose a novel approach allowing to add time-varying musical controls on top of any pretrained generative models with an exposed latent space (e.g. neural audio codecs), without retraining or finetuning. Our method supports both discrete and continuous attributes by adapting a rectified flow approach with a latent diffusion transformer. We learn an invertible mapping between pretrained latent variables and a new space disentangling explicit control attributes and style variables that capture the remaining factors of variation.
This enables both feature extraction from an input, but also editing those features to generate transformed audio samples. Finally, this also introduces the ability to perform synthesis directly from the audio descriptors. 
We validate our method with 4 datasets going from different musical instruments up to full music recordings, on which we outperform state-of-the-art task-specific baselines in terms of both generation quality and accuracy of the control by inferring transferred attributes. Our code is available on the supporting webpage.&lt;br&gt;&lt;br&gt; &lt;b&gt;&lt;p align=&#34;center&#34;&gt;[Direct link to video]()&lt;/b&gt;" />

<meta name="citation_keywords" content="Generative Tasks" />

<meta name="citation_keywords" content="Representations of music" />

<meta name="citation_keywords" content="Music generation" />

<meta name="citation_keywords" content="Music and audio synthesis" />

<meta name="citation_keywords" content="MIR tasks" />

<meta name="citation_keywords" content="Machine learning/artificial intelligence for music" />

<meta name="citation_keywords" content="Knowledge-driven approaches to MIR" />

<meta name="citation_pdf_url" content="" />
<meta id="yt-id" data-name= />
<meta id="bb-id" data-name= />


  </head>

  <body>
    <!-- NAV -->
    
    <!--

    ('tutorials.html', 'Tutorials'),
    ('index.html, 'Home'),
    ('special_meetings.html', 'Special Meetings'),
    -->

    <nav
      class="navbar sticky-top navbar-expand-lg navbar-light mr-auto"
      id="main-nav"
    >
      <div class="container">
        <a class="navbar-brand" href="https://ismir2025.ismir.net">
          <img
             class="logo" src="static/images/ismir_tabicon.png"
             height="auto"
             width="300px"
          />
        </a>
        
        <button
          class="navbar-toggler"
          type="button"
          data-toggle="collapse"
          data-target="#navbarNav"
          aria-controls="navbarNav"
          aria-expanded="false"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div
          class="collapse navbar-collapse text-right flex-grow-1"
          id="navbarNav"
        >
          <ul class="navbar-nav ml-auto">
            
            <li class="nav-item ">
              <a class="nav-link" href="calendar.html">Schedule</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="papers.html">Papers</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="music.html">Music</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="lbds.html">LBDs</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="industry.html?session=Platinum">Industry</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="jobs.html">Jobs</a>
            </li>
            
          </ul>
        </div>
      </div>
    </nav>
    

    
    <!-- User Overrides -->
     

    <div class="container">
      <!-- Tabs -->
      <div class="tabs">
         
      </div>
      <!-- Content -->
      <div class="content">
        

<!-- Title -->
<div class="pp-card m-3" style="">
  <div class="card-header">
    <h2 class="card-title main-title text-center" style="">
      P7-06: Adding temporal musical controls on top of pretrained generative models
    </h2>
    <h3 class="card-subtitle mb-2 text-muted text-center">
      
      <a href="papers.html?filter=authors&search=Sarah Nabi" class="text-muted"
        >Sarah Nabi</a
      >,
      
      <a href="papers.html?filter=authors&search=Nils Demerlé" class="text-muted"
        >Nils Demerlé</a
      >,
      
      <a href="papers.html?filter=authors&search=Geoffroy Peeters" class="text-muted"
        >Geoffroy Peeters</a
      >,
      
      <a href="papers.html?filter=authors&search=Frederic Bevilacqua" class="text-muted"
        >Frederic Bevilacqua</a
      >,
      
      <a href="papers.html?filter=authors&search=Philippe Esling" class="text-muted"
        >Philippe Esling</a
      >
      
    </h3>
    <p class="card-text text-center">
      <span class="">Subjects:</span>
      
      <a
        href="papers.html?filter=keywords&search=Generative Tasks"
        class="text-secondary text-decoration-none"
        >Generative Tasks</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Representations of music"
        class="text-secondary text-decoration-none"
        >Representations of music</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Music generation"
        class="text-secondary text-decoration-none"
        >Music generation</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Music and audio synthesis"
        class="text-secondary text-decoration-none"
        >Music and audio synthesis</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=MIR tasks"
        class="text-secondary text-decoration-none"
        >MIR tasks</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Machine learning/artificial intelligence for music"
        class="text-secondary text-decoration-none"
        >Machine learning/artificial intelligence for music</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Knowledge-driven approaches to MIR"
        class="text-secondary text-decoration-none"
        >Knowledge-driven approaches to MIR</a
      > 
      
    </p>
    <h4 class="text-muted text-center">
      Presented In-person, in Daejeon
    </h4>
    <h4 class="text-muted text-center">
      
        4-minute short-format presentation
      
    </h4>

  </div>
</div>
<div style="display: flex; justify-content: center;" class="btn-toolbar mt-4" role="toolbar">
  <div class="poster-buttons btn-group btn-group-toggle mb-3" data-toggle="buttons">
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#details">
      Abstract
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#paper">
      Paper
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#poster">
      Poster
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#video">
      Video
    </button>
    
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#meta-review">
        Meta
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review1">
        R1
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review2">
        R2
      </button>
      
        <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review3">
          R3
        </button>
      
    
    <!--  -->
  </div>
  
</div>
<div class="poster-content">
  <div id="details" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="abstractExample">
          <span class="font-weight-bold">Abstract:</span>
          <p>Recent advances in deep generative modeling have enabled high-quality models for musical audio synthesis. However, these approaches remain difficult to control, confined to simple, static attributes and, most importantly, entail retraining a different computationally-heavy architecure for each new control. This is inefficient and impractical as it requires substantial computational resources.
In this paper, we propose a novel approach allowing to add time-varying musical controls on top of any pretrained generative models with an exposed latent space (e.g. neural audio codecs), without retraining or finetuning. Our method supports both discrete and continuous attributes by adapting a rectified flow approach with a latent diffusion transformer. We learn an invertible mapping between pretrained latent variables and a new space disentangling explicit control attributes and style variables that capture the remaining factors of variation.
This enables both feature extraction from an input, but also editing those features to generate transformed audio samples. Finally, this also introduces the ability to perform synthesis directly from the audio descriptors. 
We validate our method with 4 datasets going from different musical instruments up to full music recordings, on which we outperform state-of-the-art task-specific baselines in terms of both generation quality and accuracy of the control by inferring transferred attributes. Our code is available on the supporting webpage.<br><br> <b><p align="center"><a href="">Direct link to video</a></b></p>
        </div>
      </div>
      <p></p>
    </div>
  </div>
  <div id="paper" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "https://drive.google.com/file/d/1s899zMWWZdNyl2KL_LUgGd0BwqmXDrGV/preview#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="poster" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="video" class="collapse">
    
      <div  style="display: flex; justify-content: center;">
      <iframe src="" width="960" height="540" allow="encrypted-media" allowfullscreen></iframe>
      </div>
    
  </div>
  
  <div id="meta-review" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="metaReviewExample">
          <span class="font-weight-bold">Meta Review:</span>
          <br>
          <p><strong>Q2 ( I am an expert on the topic of the paper.)</strong></p>
<p>Disagree</p>
<p><strong>Q3 ( The title and abstract reflect the content of the paper.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q4 (The paper discusses, cites and compares with all relevant related work.)</strong></p>
<p>Agree</p>
<p><strong>Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by “n” pages of references or ethical considerations, references are well formatted). If you selected “No”, please explain the issue in your comments.)</strong></p>
<p>Yes</p>
<p><strong>Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q9 (Scholarly/scientific quality: The content is scientifically correct.)</strong></p>
<p>Agree</p>
<p><strong>Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view "novelty" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)</strong></p>
<p>Agree</p>
<p><strong>Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated “Strongly Agree” and “Agree” can be highlighted, but please do not penalize papers rated “Disagree” or “Strongly Disagree”. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)</strong></p>
<p>Disagree (Standard topic, task, or application)</p>
<p><strong>Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)</strong></p>
<p>Agree</p>
<p><strong>Q15 (Please explain your assessment of reusable insights in the paper.)</strong></p>
<p>Similar generative models could be built using the proposed approach, by using different datasets and defining custom control variables. Also the results provided give some insights on which musical aspects could serve as a good set of user control variables (sufficiently disentangled, interpretable etc.) when designing generation systems</p>
<p><strong>Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)</strong></p>
<p>Time-varying musical controls can be added to a pre-trained generative model with latent variables, without needing to train the generative model, by training an invertible mapping between latent and control space</p>
<p><strong>Q17 (This paper is of award-winning quality.)</strong></p>
<p>No</p>
<p><strong>Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)</strong></p>
<p>Agree</p>
<p><strong>Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)</strong></p>
<p>Strong accept</p>
<p><strong>Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)</strong></p>
<p>This paper pursues a valuable research direction: As pre-trained models become more and more difficult to train or fine-tune, current methods for adding musical controls to them becomes less feasible. </p>
<p>The proposed method, while not entirely novel as it is mostly adapted from PluGeN, does entail a small extension for time-varying controls and makes a convincing case (theoretically and experimentally) for why that extension is needed.</p>
<p>It delivers a good set of experiments, featuring numerous baselines, different types of tasks (retrieval, editing, generation) as well as application domains (monophonic, polyphonic single instrument, full music). Using MSE as evaluation metric for the melody extraction task, while fulfilling its job in the context of the paper, is unusual, so adding some clarification on why the usual F1-based scores (e.g. Overall Accuracy) are not used would be helpful. </p>
<p>Another point that needs a bit more clarification is the choice of control variables. It seems they were carefully chosen, but that might limit applicability to other tasks as it is not clear how (e.g. just four tags for tagging, but all basic pitch features for melody control). Do control variables need to be very independent of each other, and what happens if they are not? Are there any guidelines for selection? </p>
<p>The paper is well written and flows well overall. One potential issue is that I could not fully understand how the SDEdit baseline works, as in, how the edited version is created once an input example is mapped to its corresponding noise vector.</p>
<p>Minor issues:
L37 - Reference needed for this claim
L51 - Reference would be helpful here</p>
<p><strong>Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid “weak accepts” or “weak rejects” if possible.)</strong></p>
<p>Accept</p>
<p><strong>Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))</strong></p>
<p>Summary of Reviews</p>
<p>This paper proposes a method to introduce time-varying control over pretrained generative models by learning an invertible mapping between the latent space and a disentangled control space. The approach avoids retraining the generative model and is demonstrated across multiple tasks including editing, retrieval, and conditional synthesis.</p>
<p>Three reviewers gave strong accept recommendations, and one a weak accept. The reviewers agreed that the paper is clearly written, scientifically sound, and relevant to the ISMIR community. While the model and core ideas build on prior work (notably PluGeN), the extension to temporal control is seen as a meaningful and well-executed contribution. The breadth of evaluation is also a strong point. </p>
<p>Some concerns were raised regarding missing ablation studies, unclear implementation details for continuous controls, and assumptions about the independence of control variables. However, these issues can be considered relatively minor, as they can be addressed by improving the writing for a camera-ready version.</p>
<p>Final recommendation</p>
<p>This is a well-executed paper that makes a timely and practical contribution to controllable music generation. While its methodological novelty is relatively incremental, the practical impact is significant. I recommend acceptance, with the expectation that minor issues be addressed in the final version.</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review1" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review1Example">
          <span class="font-weight-bold">Review 1:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Strong accept</li>
</ul>
<h4>Main Reviews</h4>
<p>The paper is well written with good organization and story-telling ability which allows a reader who is relatively unaware of SOTA progress to follow what has been done. As mentioned earlier in this review, I believe the proposed framework is addressing an important general question and has potentials for a wide range of applications. Below, I list a few comments for the authors to consider:</p>
<ol>
<li>
<p>I suppose the notion of "time" in Eq. (2) is different from the usual definition of time is music or digital signal processing. Though this is not a new idea in machine learning and should be clear in the context, the authors may want to point out the difference explicitly (perhaps with a footnote) to avoid causing any potential confusion. Same suggestion for "frame-rate" near the end of page 3.</p>
</li>
<li>
<p>Somehow, line 163 contains mutiple lines -- but it appears that [0,M_k]^K should be changed to [0, M_k-1]^K if M_k is the number of classes for each attribute.</p>
</li>
<li>
<p>Figure 2: thank you for a very nice illustration.</p>
</li>
<li>
<p>Line 307 and 357: conditionnal -&gt; conditional</p>
</li>
<li>
<p>Regarding the anonymous demo page, here are some comments:
(a) In the "pitch" plots, what does the control variable represent? At first I thought the plots are pitch contours, but then some results do not look right.
(b) for audio editing: I feel that the proposed method is indeed better than AFTER, but there is definitely room for further improvement in the future in terms of the synthesized sound clarity.
(c) Conditional synthesis: the present results are quite thought-provoking. The female singing example especially piques my interest since the results indeed sound like somebody is humming with free pronunciation.</p>
</li>
</ol>
        </div>
      </div>
    </div> 
  </div>
  <div id="review2" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review2Example">
          <span class="font-weight-bold">Review 2:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Weak accept</li>
</ul>
<h4>Main Reviews</h4>
<ol>
<li>
<p>The problem statement is clear. The authors attempt to add time-varying controls on top of existing generative models without requiring retraining.</p>
</li>
<li>
<p>With SDE-Edit and PluGeN, the baselines are sufficient.</p>
</li>
<li>
<p>The theoretical part largely inherits the ideas from PluGeN, with the main novelty being the addition of time-varying control and the use of rectified flow. A few points to consider:</p>
</li>
</ol>
<p>3.1 The core of adding time-varying control primarily lies in the alignment step. It would be helpful to describe the alignment process in more detail. For example, do you tweak the parameters of the libraries to ensure consistent hop sizes? Do you apply high-level descriptors using a single value for the entire song across the temporal axis?</p>
<p>3.2. You may want to explain how you compute \sigma_i.Furthermore, how are a_max and a_min defined? Are they per-batch, per-song, per-dataset, or heuristically defined global values?</p>
<p>3.3 Please write Equation (10) more rigorously. For example: L_\theta = min_\theta E_{t \sim [0, 1]} (||...||_2^2) </p>
<ol>
<li>The experiments can potentially be improved, in particular:</li>
</ol>
<p>4.1 There are no ablation studies. Given the introduction of both time-varying control and rectified flow as extensions of the PluGeN baseline, these two components should be ablated separately before presenting the full comparison in Tables 1 and 2.</p>
<p>4.2 It may also be beneficial to organize the comparison tables in a way that is consistent with Section 4.2.</p>
        </div>
      </div>
    </div> 
  </div>
  
    <div id="review3" class="pp-card m-3 collapse">
      <div class="card-body">
        <div class="card-text">
          <div id="review3Example">
            <span class="font-weight-bold">Review 3:</span>
            <br>
            <ul>
<li><strong>Overall Evaluation</strong>: Strong accept</li>
</ul>
<h4>Main Reviews</h4>
<p>This paper presents a solution to control existing pre-trained generative models by manipulating their latent codes. By learning a mapping between the latent space and a new space where the dimensions correspond to various user defined conditions, the authors show that generations exhibit increased controllability while maintaining fidelity of generation. Through 3 tasks, reconstruction, editing and conditional synthesis, they show that their model performs reasonably better than competitive baselines on multiple datasets. </p>
<p>I think the paper does a very convincing job of presenting the quality of their model with well thought out experiments and discussion. The method is light-weight and impactful due to its ability to work on any exposed latent space. There will also be a release of code which is much appreciated. I only have some small suggestions which I list below:</p>
<p>Handling of continuous variables a_c (section 3.1): Eq 7, multiplies the normal distribution function for values of i ranging from 0 to M_k. These values are discrete class values that the attribute a_k can take (interpreted from the paragraph between lines 162 - 163). However there is no mention of how this equation can be adapted for continuous attributes even though this an important part of experiments and contributions presented in this paper. 
Algorithm 1: a_min and a_max haven’t been defined anywhere. I think it should be clarified that these values are calculated across the dataset and not on a per-sample basis.
Independence of control variables: The PluGeN framework assumes that the control signals are independent. However the control signals defined in this paper aren’t necessarily independent: for instance instrument label could be correlated with the pitch distribution, octave, sharpness and so on. Perhaps these correlations aren’t strong enough to significantly affect performance. Either way, I believe this should be addressed in the text.
Difference between table 1 and table 2: Is the difference just that the models in table 2 have additional continuous descriptors? I’m surprised by how the onset F1 score and instrument accuracy values degrade just by the adding of another control variable. Is there an intuition for why this is the case? It would also be interesting to hear the difference between these samples. 
Listening tests: It would be helpful and more convincing to have preference scores from actual humans, especially for higher level features like emotions. It is difficult to simply trust a quantitative method for such conditions.
Samples page (similar to previous point): I think the samples page is very well organized and is impressive. I would love to see examples of the emotion based generations there as well. Since these features are pretty high-level, I think it is important to ground them in both human preference and highlight examples of them. </p>
<p>Minor corrections
7 a. Line 273: synthetis -&gt; synthesis
7 b. Line 307, 5.1.3 heading: conditionnal -&gt; conditional</p>
          </div>
        </div>
      </div> 
    </div>
  
  
</div>


<script src="static/js/poster.js"></script>
<script src="static/js/video_selection.js"></script>

      </div>
    </div>
    
    

    <!-- Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "UA-");
    </script>

    <!-- Footer -->
    <footer class="footer bg-light p-4">
      <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2023 ISMIR Organization Committee</p>
      </div>
    </footer>

    <!-- Code for hash tags -->
    <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });
      });
    </script>
    <script src="static/js/lazy_load.js"></script>
    
  </body>
</html>