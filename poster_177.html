


<!DOCTYPE html>
<html lang="en">
  <head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <link rel="icon" type="image/png" sizes="32x32" href="static/images/ismir_tab_icon.png">

    <link rel="stylesheet" href="static/css/main.css" />
    <link rel="stylesheet" href="static/css/custom.css" />
    <link rel="stylesheet" href="static/css/lazy_load.css" />
    <link rel="stylesheet" href="static/css/typeahead.css" />
    <link rel="stylesheet" href="static/css/poster.css" />
    <link rel="stylesheet" href="static/css/music.css" />
    <link rel="stylesheet" href="static/css/piece.css" />


    <!-- <link rel="stylesheet" href="https://gitcdn.xyz/repo/wingkwong/jquery-chameleon/master/src/chameleon.min.css"> -->

    <!-- External Javascript libs  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

    <script
      src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
      integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
      crossorigin="anonymous"
    ></script>


    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>
    <!-- <script src="static/js/chameleon.js"></script> -->


    <!-- Library libs -->
    <script src="static/js/typeahead.bundle.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
      href="static/css/Lato.css"
      rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet" />
    <link
      href="static/css/Cuprum.css"
      rel="stylesheet"
    />
    <link
      href="static/css/Ambiant.css"
      rel="stylesheet"
    />
    <!-- <link href="static/css/chameleon.css" rel="stylesheet"/> -->
    <title>ISMIR 2025: Human vs. Machine: Comparing Selection Strategies in Active Learning for Optical Music Recognition</title>
    
<meta name="citation_title" content="Human vs. Machine: Comparing Selection Strategies in Active Learning for Optical Music Recognition" />

<meta name="citation_author" content="Juan Pedro Martinez-Esteso" />

<meta name="citation_author" content="Alejandro Galan-Cuenca" />

<meta name="citation_author" content="Carlos Pérez-Sancho" />

<meta name="citation_author" content="Francisco J. Castellanos" />

<meta name="citation_author" content="Antonio Javier Gallego" />

<meta name="citation_publication_date" content="21-25 September 2025" />
<meta name="citation_conference_title" content="Ismir 2025 Hybrid Conference" />
<meta name="citation_inbook_title" content="Proceedings of the First MiniCon Conference" />
<meta name="citation_abstract" content="Optical Music Recognition (OMR) systems rely on accurate layout analysis (LA) to segment different information layers in music score images. While deep learning approaches have improved performance, they remain heavily dependent on large amounts of annotated data. In this work, we propose the integration of a Few-Shot Learning (FSL) architecture into an active learning framework for LA. This enables interactive and iterative training, allowing the model to progressively improve from minimal annotated data. We evaluate how this approach enhances recognition accuracy and reduces annotation effort, and we study the impact of different sample selection criteria within this framework, comparing data selected by five expert annotators against four automated strategies: random, sequential, ink density-based, and entropy-based. Experiments across three diverse music score datasets show that entropy-based selection consistently outperforms human choices, achieving an F1-score of 81.1% with only 8 labeled patches, while humans required at least 16 to reach similar performance. Our method improves over existing FSL approaches by up to 21.6% and substantially reduces annotation time. These results suggest that automated strategies can offer more efficient alternatives to human selection in OMR annotation workflows.&lt;br&gt;&lt;br&gt; &lt;b&gt;&lt;p align=&#34;center&#34;&gt;[Direct link to video]()&lt;/b&gt;" />

<meta name="citation_keywords" content="Human-computer interaction" />

<meta name="citation_keywords" content="Knowledge-driven approaches to MIR" />

<meta name="citation_keywords" content="Human-centered MIR" />

<meta name="citation_keywords" content="Pattern matching and detection" />

<meta name="citation_keywords" content="Optical music recognition" />

<meta name="citation_keywords" content="Machine learning/artificial intelligence for music" />

<meta name="citation_keywords" content="MIR tasks" />

<meta name="citation_pdf_url" content="" />
<meta id="yt-id" data-name= />
<meta id="bb-id" data-name= />


  </head>

  <body>
    <!-- NAV -->
    
    <!--

    ('tutorials.html', 'Tutorials'),
    ('index.html, 'Home'),
    ('special_meetings.html', 'Special Meetings'),
    -->

    <nav
      class="navbar sticky-top navbar-expand-lg navbar-light mr-auto"
      id="main-nav"
    >
      <div class="container">
        <a class="navbar-brand" href="https://ismir2025.ismir.net">
          <img
             class="logo" src="static/images/ismir_tabicon.png"
             height="auto"
             width="300px"
          />
        </a>
        
        <button
          class="navbar-toggler"
          type="button"
          data-toggle="collapse"
          data-target="#navbarNav"
          aria-controls="navbarNav"
          aria-expanded="false"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div
          class="collapse navbar-collapse text-right flex-grow-1"
          id="navbarNav"
        >
          <ul class="navbar-nav ml-auto">
            
            <li class="nav-item ">
              <a class="nav-link" href="calendar.html">Schedule</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="papers.html">Papers</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="music.html">Music</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="lbds.html">LBDs</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="industry.html?session=Platinum">Industry</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="jobs.html">Jobs</a>
            </li>
            
          </ul>
        </div>
      </div>
    </nav>
    

    
    <!-- User Overrides -->
     

    <div class="container">
      <!-- Tabs -->
      <div class="tabs">
         
      </div>
      <!-- Content -->
      <div class="content">
        

<!-- Title -->
<div class="pp-card m-3" style="">
  <div class="card-header">
    <h2 class="card-title main-title text-center" style="">
      P6-11: Human vs. Machine: Comparing Selection Strategies in Active Learning for Optical Music Recognition
    </h2>
    <h3 class="card-subtitle mb-2 text-muted text-center">
      
      <a href="papers.html?filter=authors&search=Juan Pedro Martinez-Esteso" class="text-muted"
        >Juan Pedro Martinez-Esteso</a
      >,
      
      <a href="papers.html?filter=authors&search=Alejandro Galan-Cuenca" class="text-muted"
        >Alejandro Galan-Cuenca</a
      >,
      
      <a href="papers.html?filter=authors&search=Carlos Pérez-Sancho" class="text-muted"
        >Carlos Pérez-Sancho</a
      >,
      
      <a href="papers.html?filter=authors&search=Francisco J. Castellanos" class="text-muted"
        >Francisco J. Castellanos</a
      >,
      
      <a href="papers.html?filter=authors&search=Antonio Javier Gallego" class="text-muted"
        >Antonio Javier Gallego</a
      >
      
    </h3>
    <p class="card-text text-center">
      <span class="">Subjects:</span>
      
      <a
        href="papers.html?filter=keywords&search=Human-computer interaction"
        class="text-secondary text-decoration-none"
        >Human-computer interaction</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Knowledge-driven approaches to MIR"
        class="text-secondary text-decoration-none"
        >Knowledge-driven approaches to MIR</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Human-centered MIR"
        class="text-secondary text-decoration-none"
        >Human-centered MIR</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Pattern matching and detection"
        class="text-secondary text-decoration-none"
        >Pattern matching and detection</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Optical music recognition"
        class="text-secondary text-decoration-none"
        >Optical music recognition</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Machine learning/artificial intelligence for music"
        class="text-secondary text-decoration-none"
        >Machine learning/artificial intelligence for music</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=MIR tasks"
        class="text-secondary text-decoration-none"
        >MIR tasks</a
      > 
      
    </p>
    <h4 class="text-muted text-center">
      Presented In-person, in Daejeon
    </h4>
    <h4 class="text-muted text-center">
      
        4-minute short-format presentation
      
    </h4>

  </div>
</div>
<div style="display: flex; justify-content: center;" class="btn-toolbar mt-4" role="toolbar">
  <div class="poster-buttons btn-group btn-group-toggle mb-3" data-toggle="buttons">
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#details">
      Abstract
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#paper">
      Paper
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#poster">
      Poster
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#video">
      Video
    </button>
    
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#meta-review">
        Meta
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review1">
        R1
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review2">
        R2
      </button>
      
        <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review3">
          R3
        </button>
      
    
    <!--  -->
  </div>
  
</div>
<div class="poster-content">
  <div id="details" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="abstractExample">
          <span class="font-weight-bold">Abstract:</span>
          <p>Optical Music Recognition (OMR) systems rely on accurate layout analysis (LA) to segment different information layers in music score images. While deep learning approaches have improved performance, they remain heavily dependent on large amounts of annotated data. In this work, we propose the integration of a Few-Shot Learning (FSL) architecture into an active learning framework for LA. This enables interactive and iterative training, allowing the model to progressively improve from minimal annotated data. We evaluate how this approach enhances recognition accuracy and reduces annotation effort, and we study the impact of different sample selection criteria within this framework, comparing data selected by five expert annotators against four automated strategies: random, sequential, ink density-based, and entropy-based. Experiments across three diverse music score datasets show that entropy-based selection consistently outperforms human choices, achieving an F1-score of 81.1% with only 8 labeled patches, while humans required at least 16 to reach similar performance. Our method improves over existing FSL approaches by up to 21.6% and substantially reduces annotation time. These results suggest that automated strategies can offer more efficient alternatives to human selection in OMR annotation workflows.<br><br> <b><p align="center"><a href="">Direct link to video</a></b></p>
        </div>
      </div>
      <p></p>
    </div>
  </div>
  <div id="paper" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "https://drive.google.com/file/d/1zkTtIodC_mwEVHS6UNxseMe8wbTfsbaE/preview#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="poster" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="video" class="collapse">
    
      <div  style="display: flex; justify-content: center;">
      <iframe src="" width="960" height="540" allow="encrypted-media" allowfullscreen></iframe>
      </div>
    
  </div>
  
  <div id="meta-review" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="metaReviewExample">
          <span class="font-weight-bold">Meta Review:</span>
          <br>
          <p><strong>Q2 ( I am an expert on the topic of the paper.)</strong></p>
<p>Disagree</p>
<p><strong>Q3 ( The title and abstract reflect the content of the paper.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q4 (The paper discusses, cites and compares with all relevant related work.)</strong></p>
<p>Disagree</p>
<p><strong>Q5 ( Please justify the previous choice (Required if “Strongly Disagree” or “Disagree” is chosen, otherwise write "n/a"))</strong></p>
<p>The reference on "entropy-based" selection is too loose to just cite a book on information theory.</p>
<p><strong>Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)</strong></p>
<p>Agree</p>
<p><strong>Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by “n” pages of references or ethical considerations, references are well formatted). If you selected “No”, please explain the issue in your comments.)</strong></p>
<p>Yes</p>
<p><strong>Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q9 (Scholarly/scientific quality: The content is scientifically correct.)</strong></p>
<p>Agree</p>
<p><strong>Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view "novelty" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)</strong></p>
<p>Agree</p>
<p><strong>Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)</strong></p>
<p>Disagree</p>
<p><strong>Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated “Strongly Agree” and “Agree” can be highlighted, but please do not penalize papers rated “Disagree” or “Strongly Disagree”. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)</strong></p>
<p>Disagree (Standard topic, task, or application)</p>
<p><strong>Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)</strong></p>
<p>Agree</p>
<p><strong>Q15 (Please explain your assessment of reusable insights in the paper.)</strong></p>
<p>The experiments comparing different selection methods is informative for future OMR research, as well as the baseline measurements of human annotation time.</p>
<p><strong>Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)</strong></p>
<p>Being aware of the selection method in active learning is beneficial for the layout analysis in OMR.</p>
<p><strong>Q17 (This paper is of award-winning quality.)</strong></p>
<p>No</p>
<p><strong>Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)</strong></p>
<p>Disagree</p>
<p><strong>Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)</strong></p>
<p>Weak accept</p>
<p><strong>Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)</strong></p>
<p>The paper is well written and structured in general. From motivation to how the experiments were organized are clear. Given that certain details are clarified and provided, this paper provided informative experiment results on how different selection method in active learning would impact a few shot learning training scheme in layout analysis for OMR. </p>
<p>The details that needed to be provided and/or clarified.
- How does entropy-based selection method work? how is entropy calculated? Either provide a more specific/precise reference, or provide the technical definitions in the paper.
- Instructions/criteria for human annotaters.</p>
<p><strong>Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid “weak accepts” or “weak rejects” if possible.)</strong></p>
<p>Accept</p>
<p><strong>Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))</strong></p>
<p>Main revisions that the reviewers suggested. 
- Clarify how the entropy is calculated for the entropy-based method. 
- Make it clear what information or criteria human annotators used when selecting patches. For instance, did they have access to the current model's segmentation results to identify areas with errors, or were they instructed to select diverse data? Understanding the human baseline is important.</p>
<p>Please also address the comments by all reviewers.</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review1" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review1Example">
          <span class="font-weight-bold">Review 1:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Weak accept</li>
</ul>
<h4>Main Reviews</h4>
<p>This paper proposes the adaptation of a Few-Shot Learning (FSL) architecture to an active learning setting for layout analysis (LA). Specifically, the work explores several patch selection methods, demonstrating that some automated approaches can outperform human selection as iterations progress.</p>
<p>The base model is based on the few-shot learning framework previously proposed by Castellanos et al. (ISMIR 2023). The authors enhance this model through the use of active learning strategies and achieve up to a 21.6% performance improvement using different patch selection techniques. One of the key contributions of this paper is the comparative analysis between human-driven and automated sample selection strategies, showing that a well-designed selection method can significantly impact the overall performance of the OMR (Optical Music Recognition) pipeline.</p>
<p>The paper is clearly written, and the experiments are well explained. However, I recommend that the authors provide more detailed explanations of the patch selection strategies, especially the entropy-based method, which ultimately achieved the best results. Additionally, the paper specifies a fixed patch size of 256×256 pixels. It would be helpful to include a discussion about how different patch sizes or shapes might affect the performance or selection quality.</p>
<p>Although this work does not introduce a novel architecture or fundamentally new topic, it is significant in demonstrating how the integration of active learning strategies and intelligent sample selection can enhance the effectiveness of OMR systems. I believe the insights and results presented in this paper are valuable and merit presentation at the conference.</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review2" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review2Example">
          <span class="font-weight-bold">Review 2:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Weak accept</li>
</ul>
<h4>Main Reviews</h4>
<p>The paper is well-motivated, addressing a realistic problem in OMR, the annotation scarcity. The paper is clearly written and easy to follow.</p>
<p>There are two main contributions: the proposed adjusted approach and the experimental setup with the comparison of human vs. algorithmic annotators and thev tracked time spent. In addition, the finding that entropy-based selection surpasses human annotation in both efficiency and performance is a bit surprising, but supported by the provided data. In a more philosophical aspect (or maybe even economic in a different way), the paper does open a significant question of considering systems to replace the annotators. However, there are still several challenges open before achieving this goal in a form of an automatised pipeline for annotation.
Nevertheless, the paper includes a sufficiently significant contribution of the proposed approach to be considered for publication.</p>
        </div>
      </div>
    </div> 
  </div>
  
    <div id="review3" class="pp-card m-3 collapse">
      <div class="card-body">
        <div class="card-text">
          <div id="review3Example">
            <span class="font-weight-bold">Review 3:</span>
            <br>
            <ul>
<li><strong>Overall Evaluation</strong>: Weak accept</li>
</ul>
<h4>Main Reviews</h4>
<p>The paper applies active learning for deep learning-based layout analysis. Starting with little labeled data, an initial model is trained. Afterwards, further annotations can be requested from an "oracle", where the regions to be annotated are either selected by a human or using automated strategies. The model is then re-trained with the extended labeled data and this process is repeated. The central question addressed in this paper is whether the data to be annotated should be selected by humans or using automated strategies. </p>
<p>The paper is well-written and generally easy to follow. However, a few details regarding the selection process remain unclear to me:</p>
<p>1) Entropy-based selection: As far as I understand, the model outputs for each pixel 4 values between 0 and 1, corresponding to the 4 considered layers. Is the entropy calculated for each layer separately, or is the entropy calculated after jointly normalizing the 4 values?</p>
<p>2) Human selection: Based on which information or criteria do the human annotators select patches to be annotated? Do they have access to the segmentation of the current model so that they can specifically select a region with many errors? Are they instructed to select a diverse set of labeled data? It would be important for this to be made clear.</p>
<p>Some minor issues:
- l. 114: The introduction of sigma seems unnecessary, as the performance-based stopping criterion is not used in this paper.
- l. 131ff: It is unclear how sequential selection works; is the goal to distribute the patches uniformly across all images? How are patches selected within an image? While this can be read up on in the given reference, the paper would benefit from a bit more detail.
- l. 159ff: Sounds like the patch order would be predefined for entropy-based selection.
- l. 316ff: I would suggest indicating annotation time in person-hours, which would allow for a better understanding of the annotation effort.</p>
<p>Overall, I suggest to accept the paper, as it appears to be one of the first applications of active learning to optical music recognition, showing the potential of this research direction.</p>
          </div>
        </div>
      </div> 
    </div>
  
  
</div>


<script src="static/js/poster.js"></script>
<script src="static/js/video_selection.js"></script>

      </div>
    </div>
    
    

    <!-- Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "UA-");
    </script>

    <!-- Footer -->
    <footer class="footer bg-light p-4">
      <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2023 ISMIR Organization Committee</p>
      </div>
    </footer>

    <!-- Code for hash tags -->
    <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });
      });
    </script>
    <script src="static/js/lazy_load.js"></script>
    
  </body>
</html>