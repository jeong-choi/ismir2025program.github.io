


<!DOCTYPE html>
<html lang="en">
  <head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <link rel="icon" type="image/png" sizes="32x32" href="static/images/ismir_tab_icon.png">

    <link rel="stylesheet" href="static/css/main.css" />
    <link rel="stylesheet" href="static/css/custom.css" />
    <link rel="stylesheet" href="static/css/lazy_load.css" />
    <link rel="stylesheet" href="static/css/typeahead.css" />
    <link rel="stylesheet" href="static/css/poster.css" />
    <link rel="stylesheet" href="static/css/music.css" />
    <link rel="stylesheet" href="static/css/piece.css" />


    <!-- <link rel="stylesheet" href="https://gitcdn.xyz/repo/wingkwong/jquery-chameleon/master/src/chameleon.min.css"> -->

    <!-- External Javascript libs  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

    <script
      src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
      integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
      crossorigin="anonymous"
    ></script>


    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>
    <!-- <script src="static/js/chameleon.js"></script> -->


    <!-- Library libs -->
    <script src="static/js/typeahead.bundle.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
      href="static/css/Lato.css"
      rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet" />
    <link
      href="static/css/Cuprum.css"
      rel="stylesheet"
    />
    <link
      href="static/css/Ambiant.css"
      rel="stylesheet"
    />
    <!-- <link href="static/css/chameleon.css" rel="stylesheet"/> -->
    <title>ISMIR 2025: ITO-Master: Inference-Time Optimization for Audio Effects Modeling of Music Mastering Processors</title>
    
<meta name="citation_title" content="ITO-Master: Inference-Time Optimization for Audio Effects Modeling of Music Mastering Processors" />

<meta name="citation_author" content="Junghyun Koo" />

<meta name="citation_author" content="Marco Martinez-Ramirez" />

<meta name="citation_author" content="WeiHsiang Liao" />

<meta name="citation_author" content="Giorgio Fabbro" />

<meta name="citation_author" content="Michele Mancusi" />

<meta name="citation_author" content="Yuki Mitsufuji" />

<meta name="citation_publication_date" content="21-25 September 2025" />
<meta name="citation_conference_title" content="Ismir 2025 Hybrid Conference" />
<meta name="citation_inbook_title" content="Proceedings of the First MiniCon Conference" />
<meta name="citation_abstract" content="Music mastering style transfer aims to model and apply the mastering characteristics of a reference track to a target track, simulating the professional mastering process. However, existing methods apply fixed processing based on a reference track, limiting users&#39; ability to fine-tune the results to match their artistic intent.In this paper, we introduce the ITO-Master framework, a reference-based mastering style transfer system that integrates Inference-Time Optimization (ITO) to enable finer user control over the mastering process. By optimizing the reference embedding during inference, our approach allows users to refine the output dynamically, making micro-level adjustments to achieve more precise mastering results. 
We explore both black-box and white-box methods for modeling mastering processors and demonstrate that ITO improves mastering performance across different styles. Through objective evaluation, subjective listening tests, and qualitative analysis using text-based conditioning with CLAP embeddings, we validate that ITO enhances mastering style similarity while offering increased adaptability. Our framework provides an effective and user-controllable solution for mastering style transfer, allowing users to refine their results beyond the initial style transfer.&lt;br&gt;&lt;br&gt; &lt;b&gt;&lt;p align=&#34;center&#34;&gt;[Direct link to video]()&lt;/b&gt;" />

<meta name="citation_keywords" content="MIR fundamentals and methodology" />

<meta name="citation_keywords" content="Applications" />

<meta name="citation_keywords" content="Creativity" />

<meta name="citation_keywords" content="Human-ai co-creativity" />

<meta name="citation_keywords" content="Music composition, performance, and production" />

<meta name="citation_keywords" content="Tools for artists" />

<meta name="citation_keywords" content="Music signal processing" />

<meta name="citation_keywords" content="MIR tasks" />

<meta name="citation_keywords" content="Music synthesis and transformation" />

<meta name="citation_keywords" content="Machine learning/artificial intelligence for music" />

<meta name="citation_keywords" content="Knowledge-driven approaches to MIR" />

<meta name="citation_pdf_url" content="" />
<meta id="yt-id" data-name= />
<meta id="bb-id" data-name= />


  </head>

  <body>
    <!-- NAV -->
    
    <!--

    ('tutorials.html', 'Tutorials'),
    ('index.html, 'Home'),
    ('special_meetings.html', 'Special Meetings'),
    -->

    <nav
      class="navbar sticky-top navbar-expand-lg navbar-light mr-auto"
      id="main-nav"
    >
      <div class="container">
        <a class="navbar-brand" href="https://ismir2025.ismir.net">
          <img
             class="logo" src="static/images/ismir_tabicon.png"
             height="auto"
             width="300px"
          />
        </a>
        
        <button
          class="navbar-toggler"
          type="button"
          data-toggle="collapse"
          data-target="#navbarNav"
          aria-controls="navbarNav"
          aria-expanded="false"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div
          class="collapse navbar-collapse text-right flex-grow-1"
          id="navbarNav"
        >
          <ul class="navbar-nav ml-auto">
            
            <li class="nav-item ">
              <a class="nav-link" href="calendar.html">Schedule</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="papers.html">Papers</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="music.html">Music</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="lbds.html">LBDs</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="industry.html?session=Platinum">Industry</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="jobs.html">Jobs</a>
            </li>
            
          </ul>
        </div>
      </div>
    </nav>
    

    
    <!-- User Overrides -->
     

    <div class="container">
      <!-- Tabs -->
      <div class="tabs">
         
      </div>
      <!-- Content -->
      <div class="content">
        

<!-- Title -->
<div class="pp-card m-3" style="">
  <div class="card-header">
    <h2 class="card-title main-title text-center" style="">
      P2-02: ITO-Master: Inference-Time Optimization for Audio Effects Modeling of Music Mastering Processors
    </h2>
    <h3 class="card-subtitle mb-2 text-muted text-center">
      
      <a href="papers.html?filter=authors&search=Junghyun Koo" class="text-muted"
        >Junghyun Koo</a
      >,
      
      <a href="papers.html?filter=authors&search=Marco Martinez-Ramirez" class="text-muted"
        >Marco Martinez-Ramirez</a
      >,
      
      <a href="papers.html?filter=authors&search=WeiHsiang Liao" class="text-muted"
        >WeiHsiang Liao</a
      >,
      
      <a href="papers.html?filter=authors&search=Giorgio Fabbro" class="text-muted"
        >Giorgio Fabbro</a
      >,
      
      <a href="papers.html?filter=authors&search=Michele Mancusi" class="text-muted"
        >Michele Mancusi</a
      >,
      
      <a href="papers.html?filter=authors&search=Yuki Mitsufuji" class="text-muted"
        >Yuki Mitsufuji</a
      >
      
    </h3>
    <p class="card-text text-center">
      <span class="">Subjects:</span>
      
      <a
        href="papers.html?filter=keywords&search=MIR fundamentals and methodology"
        class="text-secondary text-decoration-none"
        >MIR fundamentals and methodology</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Applications"
        class="text-secondary text-decoration-none"
        >Applications</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Creativity"
        class="text-secondary text-decoration-none"
        >Creativity</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Human-ai co-creativity"
        class="text-secondary text-decoration-none"
        >Human-ai co-creativity</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Music composition, performance, and production"
        class="text-secondary text-decoration-none"
        >Music composition, performance, and production</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Tools for artists"
        class="text-secondary text-decoration-none"
        >Tools for artists</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Music signal processing"
        class="text-secondary text-decoration-none"
        >Music signal processing</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=MIR tasks"
        class="text-secondary text-decoration-none"
        >MIR tasks</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Music synthesis and transformation"
        class="text-secondary text-decoration-none"
        >Music synthesis and transformation</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Machine learning/artificial intelligence for music"
        class="text-secondary text-decoration-none"
        >Machine learning/artificial intelligence for music</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Knowledge-driven approaches to MIR"
        class="text-secondary text-decoration-none"
        >Knowledge-driven approaches to MIR</a
      > 
      
    </p>
    <h4 class="text-muted text-center">
      Presented In-person, in Daejeon
    </h4>
    <h4 class="text-muted text-center">
      
        4-minute short-format presentation
      
    </h4>

  </div>
</div>
<div style="display: flex; justify-content: center;" class="btn-toolbar mt-4" role="toolbar">
  <div class="poster-buttons btn-group btn-group-toggle mb-3" data-toggle="buttons">
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#details">
      Abstract
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#paper">
      Paper
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#poster">
      Poster
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#video">
      Video
    </button>
    
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#meta-review">
        Meta
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review1">
        R1
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review2">
        R2
      </button>
      
        <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review3">
          R3
        </button>
      
    
    <!--  -->
  </div>
  
</div>
<div class="poster-content">
  <div id="details" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="abstractExample">
          <span class="font-weight-bold">Abstract:</span>
          <p>Music mastering style transfer aims to model and apply the mastering characteristics of a reference track to a target track, simulating the professional mastering process. However, existing methods apply fixed processing based on a reference track, limiting users' ability to fine-tune the results to match their artistic intent.In this paper, we introduce the ITO-Master framework, a reference-based mastering style transfer system that integrates Inference-Time Optimization (ITO) to enable finer user control over the mastering process. By optimizing the reference embedding during inference, our approach allows users to refine the output dynamically, making micro-level adjustments to achieve more precise mastering results. 
We explore both black-box and white-box methods for modeling mastering processors and demonstrate that ITO improves mastering performance across different styles. Through objective evaluation, subjective listening tests, and qualitative analysis using text-based conditioning with CLAP embeddings, we validate that ITO enhances mastering style similarity while offering increased adaptability. Our framework provides an effective and user-controllable solution for mastering style transfer, allowing users to refine their results beyond the initial style transfer.<br><br> <b><p align="center"><a href="">Direct link to video</a></b></p>
        </div>
      </div>
      <p></p>
    </div>
  </div>
  <div id="paper" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "https://drive.google.com/file/d/1lQMMfHXdwwYIobpWM-Z3yV0nIll8GTgg/preview#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="poster" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="video" class="collapse">
    
      <div  style="display: flex; justify-content: center;">
      <iframe src="" width="960" height="540" allow="encrypted-media" allowfullscreen></iframe>
      </div>
    
  </div>
  
  <div id="meta-review" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="metaReviewExample">
          <span class="font-weight-bold">Meta Review:</span>
          <br>
          <p><strong>Q2 ( I am an expert on the topic of the paper.)</strong></p>
<p>Agree</p>
<p><strong>Q3 ( The title and abstract reflect the content of the paper.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q4 (The paper discusses, cites and compares with all relevant related work.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by “n” pages of references or ethical considerations, references are well formatted). If you selected “No”, please explain the issue in your comments.)</strong></p>
<p>Yes</p>
<p><strong>Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q9 (Scholarly/scientific quality: The content is scientifically correct.)</strong></p>
<p>Agree</p>
<p><strong>Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view "novelty" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)</strong></p>
<p>Agree</p>
<p><strong>Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)</strong></p>
<p>Agree</p>
<p><strong>Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated “Strongly Agree” and “Agree” can be highlighted, but please do not penalize papers rated “Disagree” or “Strongly Disagree”. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)</strong></p>
<p>Disagree (Standard topic, task, or application)</p>
<p><strong>Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)</strong></p>
<p>Agree</p>
<p><strong>Q15 (Please explain your assessment of reusable insights in the paper.)</strong></p>
<p>This paper focuses pretty specifically on reference-based automatic music mastering, but has some specific insights that may apply to work on ITO more broadly. Specifically, the insight to differentiably optimize z_ref instead of employing black box optimization parameters for audio FX chains</p>
<p><strong>Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)</strong></p>
<p>Inference-time optimization is a promising family of methods for automatic music mastering based on a reference track</p>
<p><strong>Q17 (This paper is of award-winning quality.)</strong></p>
<p>No</p>
<p><strong>Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)</strong></p>
<p>Disagree</p>
<p><strong>Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)</strong></p>
<p>Weak reject</p>
<p><strong>Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)</strong></p>
<p>This paper proposes a novel strategy for automatic music mastering based on a reference track. Following past work on controlling audio effects, the authors explore a self-supervised strategy for training FX chain encoders, and inference-time optimization for iteratively refining the automatic mastering process based on the reference. The authors perform extensive quantitative and qualitative evaluation on their proposed approach.</p>
<p>Overall, this is an interesting paper demonstrating promising results on a difficult task. It will be of interest to several sub-communities of researchers within ISMIR working on automatic mastering, inference-time optimization, and differentiable signal processing. However, there is a key issue around the <em>motivation for exploring differentiable synthesis</em> that limit the ability to judge the promise of the proposed method in the broader landscape of recent MIR research. Moreover, there are some areas for improvement around <em>missing baselines</em>, the <em>quantitative evaluation protocol</em>, <em>minor methodological novelty</em>, and the presentation of <em>incomplete follow-up work</em>.</p>
<p><em>Motivation for differentiable synthesis</em>. L58 says "white box methods ... are often constrained by the simplicity of their differentiable processors, which may not fully replicate the complex tools in professional mastering". However, this paper then goes on to explore slightly more sophisticated differentiable primitives, which are likely always going to have a lower performance ceiling than professional chains. An apples-to-apples comparison against an off-the-shelf black box optimization framework (e.g. ST-ITO) on the parameters of a <em>professional</em> (non-differentiable) mastering toolchain is essential here to justify the decision to go with a differentiable approach.</p>
<p><em>Additional baselines</em>. It would be very helpful to see a few additional simple baselines: (1) randomized z_ref for black box model, (2) randomized z_ref for white box model, and (3) randomized FX parameters for white box model. As it stands, it's unclear to what extent the benefits of the proposed approach are coming from the audio manipulation primitives vs. the encoding / optimization procedures. </p>
<p><em>Quantitative evaluation protocol</em>. In the white box setting, why not just directly evaluate the ability of the model to exactly reconstruct the original FX parameters? I.e., just report the error between the ground truth FX parameters and the estimated ones.</p>
<p><em>Minor methodological novelty</em>. The idea of differentiable optimization of z_ref is interesting, though ultimately a bit limited in its novelty and applicability to non-differentiable settings. Moreover, it is unclear if this result would hold for other audio production / effects matching scenarios. This isn't a huge issue but as it stands it's unclear how reusable this insight is outside of the specific context in this paper</p>
<p><em>Incomplete follow-up work</em>. Section 5.3 is somewhat interesting but currently unjustified (no experiments or evaluation). Also, why is it important to match a text prompt if an audio reference can be provided? I would much rather see that extra page devoted to more thorough investigation or analysis of the non-text-conditioned setting</p>
<p>Errata
- L161 xin = fnorm(f1(A)) based in Figure 1, here you wrote it the other way</p>
<p><strong>Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid “weak accepts” or “weak rejects” if possible.)</strong></p>
<p>Weak reject</p>
<p><strong>Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))</strong></p>
<p>Overall, the recommendation for this paper is weak reject. R1/R2 and myself (MR) leaned negative on the work in our initial reviews, while R3 leaned strongly positive. During the discussion, R1 reiterated concerns from their review about the evaluation / real world practicality / limited methodological innovation, and R2 reiterated a lack of clarity in the writing. R3 eventually adjusted their score to weakly positive in light of criticisms raised by other reviewers.</p>
<p>Summary of strengths / weaknesses from reviews:</p>
<p>Strengths: well-motivated (R3), diverse set of experiments and baselines (R3)
Weaknesses: Arbitrary choice of reference tracks (R1), concerns about generalization from synthetic setup to real-world setting (R1), writing clarity (R2), concerns with practical usefulness (R3/MR), insufficient evaluation protocol (MR)</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review1" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review1Example">
          <span class="font-weight-bold">Review 1:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Weak reject</li>
</ul>
<h4>Main Reviews</h4>
<p>As with many MIR tasks, evaluating "similarity" between a mastered track and a reference master is tricky. The devil is usually in the details when it comes to bridging the gap between evaluation metrics in a paper and real-world applicability or impact. For example, what kind of reference tracks are conceivable choices for real-world mastering engineers to use? In some of the audio samples on the demo page, the choices of references seem difficult to justify as useful choices given the stylistic/genre differences. Perhaps a bigger concern is that the mastering FX chains that are being modeled in the data are themselves random rather than specifically targeted because they were used intentionally for mastering. This seems to me to create a significant gap between the actual task at hand (matching a dataset of randomized audio FX chains) and the real-world musical activity used to justify the technical work (mastering music with reference tracks).</p>
<p>While the technical method proposed in the paper does seem to perform comparatively well next to the baselines, I don't think that the application of the method and reporting of these metrics alone are enough to justify publishing this paper in ISMIR. If this is primarily a paper about music mastering, I think that the proxy-mastering task would need to be refined or justified further; otherwise, I could imagine this being reframed to focus more on the ITO method.</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review2" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review2Example">
          <span class="font-weight-bold">Review 2:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Weak reject</li>
</ul>
<h4>Main Reviews</h4>
<p>This paper describes a mastering-by-example system, created using the following components: a transfer network called the Mastering Style Converter, a differentiable effects chain, retraining of the reference encoder, and ITO, which we take to mean direct optimization of the embedding target for the transfer network.</p>
<p>An evaluation is described, in which white vs. black box approaches were compared, including retraining of the reference encoder, and proposed ITO procedure. The comparisons are done using a suite of objective metrics, as well as with a subjective test. A further test was conducted using embeddings from text prompts.</p>
<p>In this work, a complex new system was trained and many experiments were conducted. However, I had some problems understanding the main ideas behind ITO and its motivation. I feel that what ITO is was not clearly explained, neither in the introduction, nor in section 2.2. Some clear statement about what ITO is, combined with a reference, would be helpful. Similarly, a technical definition of FX normalization, and why it is used, would be helpful.</p>
<p>After reading the references I eventually settled that ITO must mean something like in [16], in which ITO means optimizing a latent noise reference through a diffusion process (which additionally requires gradient checkpointing through iterative steps of the algorithm). But the use of ITO here is more like an analogy, because instead of optimizing latent noise, we are optimizing the reference embedding. Furthermore, since diffusion doesn’t seem to be involved, implementation of the gradients is more straightforward than [16]. </p>
<p>In general, I would have appreciated greater clarity in the text and direct mathematical expression (with citations), to better understand the contributions of what seems like a unique system.</p>
<p>Additional questions:
Are there any regularizers preventing that z_ref goes directly to the new reference? In Section 3.4 Inference-Time Optimization on Reference Embedding, it mentions using the Audio-Feature loss from [13] but doesn’t give more details.</p>
<p>In the evaluation, ITO was only performed as an additional step when the reference encoder Phi was retrained. As it seemed to show little marginal benefit for some cases, was it possible to test using a fixed reference encoder?</p>
        </div>
      </div>
    </div> 
  </div>
  
    <div id="review3" class="pp-card m-3 collapse">
      <div class="card-body">
        <div class="card-text">
          <div id="review3Example">
            <span class="font-weight-bold">Review 3:</span>
            <br>
            <ul>
<li><strong>Overall Evaluation</strong>: Weak accept</li>
</ul>
<h4>Main Reviews</h4>
<p>The paper proposes a novel approach to improving post-production style transfer using inference-time optimization. It presents a range of experiments with and without optimization, introduces strong baselines and novel metrics, and offers valuable insights into optimization techniques involving reference encoders and effect parameters. Overall, I really enjoyed reading the paper.</p>
<p>Strengths:</p>
<p>The use of inference-time optimization for post-production style transfer is both novel and well-motivated. A diverse set of experiments and baselines help contextualize the effectiveness of the proposed approach. The comparison of various optimization techniques (reference encoder and effect parameters) provide a useful reference for future research.</p>
<p>Criticisms and Suggestions:
Real-world usability vs. reference alignment: While the proposed method aims to align the output with a given reference, I found that masters from E2Emastering and Matchering subjectively sounded more usable in real-world scenarios, even though they were less aligned with the reference. Future work should consider not only similarity to the reference but also the production value and usability of the output. Reference-based mastering is a curatorial task, if the reference is poorly chosen, the result may be undesirable.</p>
<p>Uncited similarity in Figure 1:
Figure 1 appears quite similar to the one in this ISMIR 2024 LBD paper (https://ismir2024program.ismir.net/lbd_446.html), but this is not cited. Please include the citation or clarify the relationship between the figures.</p>
<p>Fx-normalization claims (Lines 151–152):
The paper states that Fx-normalization improves model performance, but no supporting metrics or references are provided. Please clarify or provide evidence. Notably, Fx-normalization is typically used in mixing tasks involving wet stems- if this paper builds on that prior work, relevant citations should be included.</p>
<p>Clarification on distortion removal (Line 159):
Please elaborate on the reasoning behind the need to remove all distortion. </p>
<p>Percentages in Line 196:
The methodology behind the percentage calculations is unclear. Please explain how these values were derived.</p>
<p>Subjective listening tests (Line 414):
Based on my listening experience, I preferred the outputs from E2Emastering and Matchering in terms of usability. However, I understand that the objective of your listening tests may have been reference similarity, not user preference. I would encourage future evaluations to include subjective preference ratings in addition to similarity, especially since mastering involves aesthetic and perceptual judgments.
Section 5.3: The paper evaluated only one song and though the figures show how the CLAP embedding is able to drive the system to produce different masters, it is unclear if they sound usable as no audio examples for the same were shared. Further, this was not evaluated using subjective listening tests. 
Additional Comment:
The introduction of new evaluation metrics for post-production style transfer is commendable, especially given the difficulty of objective evaluation in this domain. However, incorporating user preference and real-world usability into the evaluation pipeline will strengthen the practical relevance of this work.</p>
          </div>
        </div>
      </div> 
    </div>
  
  
</div>


<script src="static/js/poster.js"></script>
<script src="static/js/video_selection.js"></script>

      </div>
    </div>
    
    

    <!-- Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "UA-");
    </script>

    <!-- Footer -->
    <footer class="footer bg-light p-4">
      <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2023 ISMIR Organization Committee</p>
      </div>
    </footer>

    <!-- Code for hash tags -->
    <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });
      });
    </script>
    <script src="static/js/lazy_load.js"></script>
    
  </body>
</html>