


<!DOCTYPE html>
<html lang="en">
  <head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <link rel="icon" type="image/png" sizes="32x32" href="static/images/ismir_tab_icon.png">

    <link rel="stylesheet" href="static/css/main.css" />
    <link rel="stylesheet" href="static/css/custom.css" />
    <link rel="stylesheet" href="static/css/lazy_load.css" />
    <link rel="stylesheet" href="static/css/typeahead.css" />
    <link rel="stylesheet" href="static/css/poster.css" />
    <link rel="stylesheet" href="static/css/music.css" />
    <link rel="stylesheet" href="static/css/piece.css" />


    <!-- <link rel="stylesheet" href="https://gitcdn.xyz/repo/wingkwong/jquery-chameleon/master/src/chameleon.min.css"> -->

    <!-- External Javascript libs  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

    <script
      src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
      integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
      crossorigin="anonymous"
    ></script>


    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>
    <!-- <script src="static/js/chameleon.js"></script> -->


    <!-- Library libs -->
    <script src="static/js/typeahead.bundle.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
      href="static/css/Lato.css"
      rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet" />
    <link
      href="static/css/Cuprum.css"
      rel="stylesheet"
    />
    <link
      href="static/css/Ambiant.css"
      rel="stylesheet"
    />
    <!-- <link href="static/css/chameleon.css" rel="stylesheet"/> -->
    <title>ISMIR 2025: Joint Object Detection and Sound Source Separation</title>
    
<meta name="citation_title" content="Joint Object Detection and Sound Source Separation" />

<meta name="citation_author" content="Sunyoo Kim" />

<meta name="citation_author" content="Yunjeong Choi" />

<meta name="citation_author" content="Doyeon Lee" />

<meta name="citation_author" content="Seoyoung Lee" />

<meta name="citation_author" content="Eunyi Lyou" />

<meta name="citation_author" content="Seungju Kim" />

<meta name="citation_author" content="Junhyug Noh" />

<meta name="citation_author" content="Joonseok Lee" />

<meta name="citation_publication_date" content="21-25 September 2025" />
<meta name="citation_conference_title" content="Ismir 2025 Hybrid Conference" />
<meta name="citation_inbook_title" content="Proceedings of the First MiniCon Conference" />
<meta name="citation_abstract" content="We propose See2Hear (S2H), a framework that jointly learns audio-visual representations for object detection and sound source separation from videos. Existing methods do not fully exploit the synergy between the detection and separation tasks, often relying on disjointly pre-trained visual encoders. In this paper, S2H integrates both tasks in an end-to-end trainable unified structure using transformer-based architectures. A naive combination of them, however, results in suboptimal performance. We propose a dynamic filtering mechanism that selects relevant object queries from the object detector to resolve this issue. We conduct extensive experiments to verify that our approach achieves the state-of-the-art performance in audio source separation on the MUSIC and MUSIC-21 datasets, while maintaining competitive object detection performance. Ablation studies confirm that the joint training of detection and separation is mutually beneficial for both tasks.&lt;br&gt;&lt;br&gt; &lt;b&gt;&lt;p align=&#34;center&#34;&gt;[Direct link to video]()&lt;/b&gt;" />

<meta name="citation_keywords" content="MIR fundamentals and methodology" />

<meta name="citation_keywords" content="Sound source separation" />

<meta name="citation_keywords" content="MIR tasks" />

<meta name="citation_keywords" content="Multimodality" />

<meta name="citation_pdf_url" content="" />
<meta id="yt-id" data-name= />
<meta id="bb-id" data-name= />


  </head>

  <body>
    <!-- NAV -->
    
    <!--

    ('tutorials.html', 'Tutorials'),
    ('index.html, 'Home'),
    ('special_meetings.html', 'Special Meetings'),
    -->

    <nav
      class="navbar sticky-top navbar-expand-lg navbar-light mr-auto"
      id="main-nav"
    >
      <div class="container">
        <a class="navbar-brand" href="https://ismir2025.ismir.net">
          <img
             class="logo" src="static/images/ismir_tabicon.png"
             height="auto"
             width="300px"
          />
        </a>
        
        <button
          class="navbar-toggler"
          type="button"
          data-toggle="collapse"
          data-target="#navbarNav"
          aria-controls="navbarNav"
          aria-expanded="false"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div
          class="collapse navbar-collapse text-right flex-grow-1"
          id="navbarNav"
        >
          <ul class="navbar-nav ml-auto">
            
            <li class="nav-item ">
              <a class="nav-link" href="calendar.html">Schedule</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="papers.html">Papers</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="music.html">Music</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="lbds.html">LBDs</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="industry.html?session=Platinum">Industry</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="jobs.html">Jobs</a>
            </li>
            
          </ul>
        </div>
      </div>
    </nav>
    

    
    <!-- User Overrides -->
     

    <div class="container">
      <!-- Tabs -->
      <div class="tabs">
         
      </div>
      <!-- Content -->
      <div class="content">
        

<!-- Title -->
<div class="pp-card m-3" style="">
  <div class="card-header">
    <h2 class="card-title main-title text-center" style="">
      P7-10: Joint Object Detection and Sound Source Separation
    </h2>
    <h3 class="card-subtitle mb-2 text-muted text-center">
      
      <a href="papers.html?filter=authors&search=Sunyoo Kim" class="text-muted"
        >Sunyoo Kim</a
      >,
      
      <a href="papers.html?filter=authors&search=Yunjeong Choi" class="text-muted"
        >Yunjeong Choi</a
      >,
      
      <a href="papers.html?filter=authors&search=Doyeon Lee" class="text-muted"
        >Doyeon Lee</a
      >,
      
      <a href="papers.html?filter=authors&search=Seoyoung Lee" class="text-muted"
        >Seoyoung Lee</a
      >,
      
      <a href="papers.html?filter=authors&search=Eunyi Lyou" class="text-muted"
        >Eunyi Lyou</a
      >,
      
      <a href="papers.html?filter=authors&search=Seungju Kim" class="text-muted"
        >Seungju Kim</a
      >,
      
      <a href="papers.html?filter=authors&search=Junhyug Noh" class="text-muted"
        >Junhyug Noh</a
      >,
      
      <a href="papers.html?filter=authors&search=Joonseok Lee" class="text-muted"
        >Joonseok Lee</a
      >
      
    </h3>
    <p class="card-text text-center">
      <span class="">Subjects:</span>
      
      <a
        href="papers.html?filter=keywords&search=MIR fundamentals and methodology"
        class="text-secondary text-decoration-none"
        >MIR fundamentals and methodology</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Sound source separation"
        class="text-secondary text-decoration-none"
        >Sound source separation</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=MIR tasks"
        class="text-secondary text-decoration-none"
        >MIR tasks</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Multimodality"
        class="text-secondary text-decoration-none"
        >Multimodality</a
      > 
      
    </p>
    <h4 class="text-muted text-center">
      Presented In-person, in Daejeon
    </h4>
    <h4 class="text-muted text-center">
      
        4-minute short-format presentation
      
    </h4>

  </div>
</div>
<div style="display: flex; justify-content: center;" class="btn-toolbar mt-4" role="toolbar">
  <div class="poster-buttons btn-group btn-group-toggle mb-3" data-toggle="buttons">
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#details">
      Abstract
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#paper">
      Paper
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#poster">
      Poster
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#video">
      Video
    </button>
    
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#meta-review">
        Meta
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review1">
        R1
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review2">
        R2
      </button>
      
        <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review3">
          R3
        </button>
      
    
    <!--  -->
  </div>
  
</div>
<div class="poster-content">
  <div id="details" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="abstractExample">
          <span class="font-weight-bold">Abstract:</span>
          <p>We propose See2Hear (S2H), a framework that jointly learns audio-visual representations for object detection and sound source separation from videos. Existing methods do not fully exploit the synergy between the detection and separation tasks, often relying on disjointly pre-trained visual encoders. In this paper, S2H integrates both tasks in an end-to-end trainable unified structure using transformer-based architectures. A naive combination of them, however, results in suboptimal performance. We propose a dynamic filtering mechanism that selects relevant object queries from the object detector to resolve this issue. We conduct extensive experiments to verify that our approach achieves the state-of-the-art performance in audio source separation on the MUSIC and MUSIC-21 datasets, while maintaining competitive object detection performance. Ablation studies confirm that the joint training of detection and separation is mutually beneficial for both tasks.<br><br> <b><p align="center"><a href="">Direct link to video</a></b></p>
        </div>
      </div>
      <p></p>
    </div>
  </div>
  <div id="paper" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "https://drive.google.com/file/d/18mNiWr7qRykY_ENyGE9HLkfPP_ElqRP1/preview#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="poster" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="video" class="collapse">
    
      <div  style="display: flex; justify-content: center;">
      <iframe src="" width="960" height="540" allow="encrypted-media" allowfullscreen></iframe>
      </div>
    
  </div>
  
  <div id="meta-review" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="metaReviewExample">
          <span class="font-weight-bold">Meta Review:</span>
          <br>
          <p><strong>Q2 ( I am an expert on the topic of the paper.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q3 ( The title and abstract reflect the content of the paper.)</strong></p>
<p>Agree</p>
<p><strong>Q4 (The paper discusses, cites and compares with all relevant related work.)</strong></p>
<p>Strongly disagree</p>
<p><strong>Q5 ( Please justify the previous choice (Required if “Strongly Disagree” or “Disagree” is chosen, otherwise write "n/a"))</strong></p>
<p>The related work section and more generally the whole paper refers to work published in ML and computer vision conferences but only scarcely refers to work published in audio/music conferences/journals. There are a very large number of references (but many are not adequate) and clearly a significant number of relevant references are missing. 
For instance from work by Sanjeel Parekh &amp; al., Zhiyao Duan &amp; al., Cynthia Liem &amp; al., and others</p>
<p><strong>Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)</strong></p>
<p>Agree</p>
<p><strong>Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by “n” pages of references or ethical considerations, references are well formatted). If you selected “No”, please explain the issue in your comments.)</strong></p>
<p>Yes</p>
<p><strong>Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)</strong></p>
<p>Agree</p>
<p><strong>Q9 (Scholarly/scientific quality: The content is scientifically correct.)</strong></p>
<p>Disagree</p>
<p><strong>Q10 (Please justify the previous choice (Required if “Strongly Disagree” or “Disagree” is chose, otherwise write "n/a"))</strong></p>
<p>The evaluation is not convincing. There are missing details on the datasets used. This does not permit to well evaluate the difficulty of the task (how many concurrent sources are played ? what are the initial SDR for a naive separator (only outputting the mix) ? how would perform a pure SoA audio separator ? The demo example provided is not explained and it is difficult to understand what is seen/heard and as such it is not a convincing demo.</p>
<p><strong>Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view "novelty" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)</strong></p>
<p>Disagree</p>
<p><strong>Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)</strong></p>
<p>Strongly disagree</p>
<p><strong>Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated “Strongly Agree” and “Agree” can be highlighted, but please do not penalize papers rated “Disagree” or “Strongly Disagree”. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)</strong></p>
<p>Disagree (Standard topic, task, or application)</p>
<p><strong>Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)</strong></p>
<p>Disagree</p>
<p><strong>Q15 (Please explain your assessment of reusable insights in the paper.)</strong></p>
<p>The paper is building on previous work [14] and does not integrate new concepts. Besides, the reproducibility is rather low (no publication of code and the datasets used seems to be only partially accessible)</p>
<p><strong>Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)</strong></p>
<p>A joint object detection and audio source separation trained in an end-to-end fashion.</p>
<p><strong>Q17 (This paper is of award-winning quality.)</strong></p>
<p>No</p>
<p><strong>Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)</strong></p>
<p>Disagree</p>
<p><strong>Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)</strong></p>
<p>Strong reject</p>
<p><strong>Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)</strong></p>
<p>This paper addresses the relevant problem of audio-visual music source separation and is generally well written. However, it suffers from several significant limitations.</p>
<p>Literature Positioning: The related work review is skewed toward computer vision literature, with insufficient coverage of prior work in the audio/music source separation domain.</p>
<p>Experimental Validation: The experiments lack rigor. Dataset details are minimal, task complexity is not discussed, and the number of concurrent sources is unclear. There is no comparison with established music (only) source separation baselines, and the demo is neither clearly described nor convincing.</p>
<p>Novelty: The proposed method appears to be a minor extension of existing work, possibly by the same authors.</p>
<p>Reproducibility: The absence of code and partial dataset availability significantly hinders reproducibility. The implementation detials provided in the paper are not sufficient to easily reproduce the work.</p>
<p><strong>Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid “weak accepts” or “weak rejects” if possible.)</strong></p>
<p>Weak accept</p>
<p><strong>Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))</strong></p>
<p>The different reviewers had quite different opinions. But, after discussion, it was agreed that a substantial number of weaknesses pointed out by the reviewers are minor and could be improved in the final submission and that the paper has merits which could justify acceptance.</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review1" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review1Example">
          <span class="font-weight-bold">Review 1:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Weak accept</li>
</ul>
<h4>Main Reviews</h4>
<p>The paper introduces <strong>See2Hear (S2H)</strong>, an end-to-end transformer-based framework that jointly performs object detection and sound source separation in videos. Unlike prior methods that treat these tasks separately, S2H integrates them using shared visual and auditory representations. The authors design a dynamic filtering mechanism to prune irrelevant object queries before cross-modal fusion, enhancing both efficiency and accuracy. Evaluated on the MUSIC and MUSIC-21 datasets, S2H seems to achieve convincing results in sound separation while jointly training detection. Ablation studies validate the benefit of this joint learning.</p>
<h3><strong>Core Contributions</strong></h3>
<ol>
<li><strong>Unified End-to-End Framework:</strong></li>
</ol>
<p>Proposes a joint architecture that simultaneously performs object detection and sound source separation using shared transformer encoders and decoders.</p>
<ol>
<li><strong>Dynamic Query Filtering Mechanism:</strong></li>
</ol>
<p>Introduces a method to filter out low-confidence or redundant object detections (bounding boxes) before fusion, improving both sound separation quality and detection precision.</p>
<ol>
<li><strong>Transformer-Based Audio-Visual Fusion:</strong></li>
</ol>
<p>Utilizes cross-attention between visual object queries and audio tokens, allowing fine-grained association between objects and their sounds.</p>
<ol>
<li><strong>State-of-the-Art Results:</strong></li>
</ol>
<p>Achieves superior SDR, SIR, and SAR performance over existing baselines (e.g., iQuery, Sound-of-Pixels) on MUSIC and MUSIC-21 datasets.</p>
<h2><strong>Concerns &amp; Suggestions</strong></h2>
<ol>
<li><strong>Dependence on pre-trained models:</strong> The model relies on pseudo-ground truth bounding boxes generated via an external detector (<code>detic</code>). For the audio part the model relies on pre-trained weights from the AST model (likely pre-trained on noisy audioset). And for the core visual classifier the model relies on pre-trained <code>DETR</code> model. This weakens claims of full end-to-end learning and could limit the applicability in less structured domains. As the authors mentioned, they retrained “all baselines on the same set of currently available videos to ensure a fair comparison” but to me it would have been more fair if the pre-trained models would be take out of the equation. This could mean to train all models from scratch or use the same original datasets (like AudioSet in this case).</li>
<li>I’m not deeply familiar with the MUSIC/MUSIC-21 dataset, so, I find it difficult to understand the objective of the task. Specifically if permutation plays a role in the task itself: are two instruments of the same kind (e.g. violin + viola) mixed? In that wouldn’t the loss function have to be permutation invariant to produce meaningful results? I would encourage the authors to to discuss if that is the case or whether the query based visual branch make it possible to not require PIT loss functions. </li>
<li><strong>Limited Evaluation Scope:</strong> The evaluation is restricted to musical instruments in controlled settings (MUSIC/MUSIC-21). It remains unclear how well the approach generalizes to more complex scenes, diverse object categories, or real-world noise. Also as far as the evaluation and ablation study goes, it is not clear which ablation completely disables the video branch. As I understand, this kind of ablation is common is audio-visual speech separation to demonstrate how well the separation with just audio would work. If this is not easily possible in this framework I would at least suggest to do an ablation experiment where the bounding boxes are pertubated with random noise or the input image itself is destructed. </li>
<li><strong>Audio separation baseline missing:</strong> In the same direction as above: i would highly suggest to also add an audio-only baseline separation model so that the reader can understand the benefit of the audio models. This should be trained with the same data to have a fair comparison. </li>
<li><strong>Computational Complexity &amp; Scalability:</strong> The transformer-based architecture and fusion module may be computationally heavy, especially in multi-object or high-resolution settings. Runtime and memory usage analysis would strengthen the paper.</li>
<li><strong>Lack of clarity on audio analysis and synthesis:</strong> the paper describes in paragraaph 5.1 how the masking is taking part in practice. However, I found it difficult to understand how exactly the mask is computed, is the mask complex valued or real-valued? What parameters of the STFT have been used? How does the model deal with higher sampling rates like 44k if the model was only trained on 11khz inputs? How were the video frames selected when the hop size / sampling rate of video and audio are different? Was interpolation used?</li>
<li><strong>Temporal positional encoding of video frames:</strong> the paper mentions that for each sample, 3 frames of video are sampled. From each of the 3 frames, a number of bounding boxes is estimated and features are inferred. If this is correct, I wonder how the bounding boxes were tracked over the course of the 3 frames and how the model would be able to utilize temporal video information. Imagining a violinist would move his/her bow 3 frames might be enough to already encode temporal information. Therefore I wonder if (or if not: why) temporal positional encoding was used for the video frame index.</li>
<li><strong>Lack of Qualitative Detection Results:</strong> While separation results are illustrated, visual detection outcomes are not qualitatively analyzed or benchmarked beyond mAP/mIoU, making it hard to assess detection reliability.</li>
<li>Please use a spell checker</li>
</ol>
        </div>
      </div>
    </div> 
  </div>
  <div id="review2" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review2Example">
          <span class="font-weight-bold">Review 2:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Strong accept</li>
</ul>
<h4>Main Reviews</h4>
<p>The main strength of this paper is that this paper shows how audio and visual tokens could be jointly used to train in a single model, allowing gradients from both tasks, object detection and source separation, to update the shared representation space. Such update is not easy task.
The main weaknesses or limiations of this paper are 
- Although 2 publicly available datasets are used and there is no more challenging datasets available, such as having multiple instruments in a same video, evaluating the proposed method on such challenging dataset is necessary. To advocate the purposed method, such dataset should be created first.
- Source code should be said to be released soon, as such joint training is not easily to be reproduced.</p>
        </div>
      </div>
    </div> 
  </div>
  
    <div id="review3" class="pp-card m-3 collapse">
      <div class="card-body">
        <div class="card-text">
          <div id="review3Example">
            <span class="font-weight-bold">Review 3:</span>
            <br>
            <ul>
<li><strong>Overall Evaluation</strong>: Weak accept</li>
</ul>
<h4>Main Reviews</h4>
<p>The article presents a new framework to perform audio source separation from audio-visual learning. The presentation is clear and the experimental analysis is strong. Some areas are missing further exploration and clarification, which would make this article stronger.</p>
<p>Remarks to address:</p>
<p>Line 226 defines \theta but it does not mention what this threshold is.</p>
<p>In 263 the embedding fusion of audio and video is mentioned. Figure 1 also describes it. The dimensions of the video embedding O_o are not the same as the dimensions of S_out. This has to be fixed for clarity.</p>
<p>Please cite the standard protocol for the data splitting on the MUSIC dataset (line 326).</p>
<p>Data processing. It is not clear what is the video sampling rate. The standard is 30fps. If 3 frames per video are sampled that is roughly a window of 100 milliseconds. However, in audio 6 seconds are sampled. This indicates that the audio and video are not time aligned. This needs to be further explained. Also the choice of the audio sampling rate at 11kHz seems arbitrary. This also needs to be explained in detail as it is critical for experiment reproducibility. </p>
<p>Results Section: The ablations presented are very clear. However, one ablation missing is what happens if the visual branch is missing the bounding box information. Opposite to having no b-box filtering, which means there will be a lot of visual information being merged into the network, what happens if there is no b-box information at all, leaving only audio and class information being processed by the network? This should also clarify the benefits of using bounding boxes.</p>
<p>The supplementary video needs to be more self-explanatory. It is not clear when each sound source should be separated. A timeline of the sound events and the task would make this clearer.</p>
<p>Is the code to reproduce experiments going to be open sourced?</p>
          </div>
        </div>
      </div> 
    </div>
  
  
</div>


<script src="static/js/poster.js"></script>
<script src="static/js/video_selection.js"></script>

      </div>
    </div>
    
    

    <!-- Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "UA-");
    </script>

    <!-- Footer -->
    <footer class="footer bg-light p-4">
      <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2023 ISMIR Organization Committee</p>
      </div>
    </footer>

    <!-- Code for hash tags -->
    <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });
      });
    </script>
    <script src="static/js/lazy_load.js"></script>
    
  </body>
</html>