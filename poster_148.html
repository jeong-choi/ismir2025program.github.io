


<!DOCTYPE html>
<html lang="en">
  <head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <link rel="icon" type="image/png" sizes="32x32" href="static/images/ismir_tab_icon.png">

    <link rel="stylesheet" href="static/css/main.css" />
    <link rel="stylesheet" href="static/css/custom.css" />
    <link rel="stylesheet" href="static/css/lazy_load.css" />
    <link rel="stylesheet" href="static/css/typeahead.css" />
    <link rel="stylesheet" href="static/css/poster.css" />
    <link rel="stylesheet" href="static/css/music.css" />
    <link rel="stylesheet" href="static/css/piece.css" />


    <!-- <link rel="stylesheet" href="https://gitcdn.xyz/repo/wingkwong/jquery-chameleon/master/src/chameleon.min.css"> -->

    <!-- External Javascript libs  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

    <script
      src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
      integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
      crossorigin="anonymous"
    ></script>


    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>
    <!-- <script src="static/js/chameleon.js"></script> -->


    <!-- Library libs -->
    <script src="static/js/typeahead.bundle.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
      href="static/css/Lato.css"
      rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet" />
    <link
      href="static/css/Cuprum.css"
      rel="stylesheet"
    />
    <link
      href="static/css/Ambiant.css"
      rel="stylesheet"
    />
    <!-- <link href="static/css/chameleon.css" rel="stylesheet"/> -->
    <title>ISMIR 2025: Fx-Encoder++: Extracting Instrument-Wise Audio Effects Representations from Mixtures</title>
    
<meta name="citation_title" content="Fx-Encoder++: Extracting Instrument-Wise Audio Effects Representations from Mixtures" />

<meta name="citation_author" content="Yen-Tung Yeh" />

<meta name="citation_author" content="Junghyun Koo" />

<meta name="citation_author" content="Marco Martínez-Ramírez" />

<meta name="citation_author" content="Wei-Hsiang Liao" />

<meta name="citation_author" content="Yi-Hsuan Yang" />

<meta name="citation_author" content="Yuki Mitsufuji" />

<meta name="citation_publication_date" content="21-25 September 2025" />
<meta name="citation_conference_title" content="Ismir 2025 Hybrid Conference" />
<meta name="citation_inbook_title" content="Proceedings of the First MiniCon Conference" />
<meta name="citation_abstract" content="General-purpose audio representations have proven effective across diverse music information retrieval applications, yet their utility in intelligent music production remains limited by insufficient understanding of audio effects (Fx). Although previous approaches have emphasized audio effects analysis at the mixture level, this focus falls short for tasks demanding instrument-wise audio effects understanding, such as automatic mixing. In this work, we present Fx-Encoder++, a novel model designed to extract instrument-wise audio effects representations from music mixtures. Our approach leverages a contrastive learning framework and introduces an ``extractor&#39;&#39; mechanism that, when provided with instrument queries (audio or text), transforms mixture-level audio effect embeddings into instrument-wise audio effect embeddings. We evaluated our model across retrieval and audio effect parameter matching tasks, testing its performance across a diverse range of instruments. The results demonstrate that Fx-Encoder++ outperforms previous approaches at mixture level and show a novel ability to extract effects representation instrument-wise, addressing a critical capability gap in intelligent music production systems.&lt;br&gt;&lt;br&gt; &lt;b&gt;&lt;p align=&#34;center&#34;&gt;[Direct link to video]()&lt;/b&gt;" />

<meta name="citation_keywords" content="Representations of music" />

<meta name="citation_keywords" content="Knowledge-driven approaches to MIR" />

<meta name="citation_keywords" content="Musical features and properties" />

<meta name="citation_keywords" content="Music composition, performance, and production" />

<meta name="citation_keywords" content="Applications" />

<meta name="citation_pdf_url" content="" />
<meta id="yt-id" data-name= />
<meta id="bb-id" data-name= />


  </head>

  <body>
    <!-- NAV -->
    
    <!--

    ('tutorials.html', 'Tutorials'),
    ('index.html, 'Home'),
    ('special_meetings.html', 'Special Meetings'),
    -->

    <nav
      class="navbar sticky-top navbar-expand-lg navbar-light mr-auto"
      id="main-nav"
    >
      <div class="container">
        <a class="navbar-brand" href="https://ismir2025.ismir.net">
          <img
             class="logo" src="static/images/ismir_tabicon.png"
             height="auto"
             width="300px"
          />
        </a>
        
        <button
          class="navbar-toggler"
          type="button"
          data-toggle="collapse"
          data-target="#navbarNav"
          aria-controls="navbarNav"
          aria-expanded="false"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div
          class="collapse navbar-collapse text-right flex-grow-1"
          id="navbarNav"
        >
          <ul class="navbar-nav ml-auto">
            
            <li class="nav-item ">
              <a class="nav-link" href="calendar.html">Schedule</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="papers.html">Papers</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="music.html">Music</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="lbds.html">LBDs</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="industry.html?session=Platinum">Industry</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="jobs.html">Jobs</a>
            </li>
            
          </ul>
        </div>
      </div>
    </nav>
    

    
    <!-- User Overrides -->
     

    <div class="container">
      <!-- Tabs -->
      <div class="tabs">
         
      </div>
      <!-- Content -->
      <div class="content">
        

<!-- Title -->
<div class="pp-card m-3" style="">
  <div class="card-header">
    <h2 class="card-title main-title text-center" style="">
      P5-14: Fx-Encoder++: Extracting Instrument-Wise Audio Effects Representations from Mixtures
    </h2>
    <h3 class="card-subtitle mb-2 text-muted text-center">
      
      <a href="papers.html?filter=authors&search=Yen-Tung Yeh" class="text-muted"
        >Yen-Tung Yeh</a
      >,
      
      <a href="papers.html?filter=authors&search=Junghyun Koo" class="text-muted"
        >Junghyun Koo</a
      >,
      
      <a href="papers.html?filter=authors&search=Marco Martínez-Ramírez" class="text-muted"
        >Marco Martínez-Ramírez</a
      >,
      
      <a href="papers.html?filter=authors&search=Wei-Hsiang Liao" class="text-muted"
        >Wei-Hsiang Liao</a
      >,
      
      <a href="papers.html?filter=authors&search=Yi-Hsuan Yang" class="text-muted"
        >Yi-Hsuan Yang</a
      >,
      
      <a href="papers.html?filter=authors&search=Yuki Mitsufuji" class="text-muted"
        >Yuki Mitsufuji</a
      >
      
    </h3>
    <p class="card-text text-center">
      <span class="">Subjects:</span>
      
      <a
        href="papers.html?filter=keywords&search=Representations of music"
        class="text-secondary text-decoration-none"
        >Representations of music</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Knowledge-driven approaches to MIR"
        class="text-secondary text-decoration-none"
        >Knowledge-driven approaches to MIR</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Musical features and properties"
        class="text-secondary text-decoration-none"
        >Musical features and properties</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Music composition, performance, and production"
        class="text-secondary text-decoration-none"
        >Music composition, performance, and production</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Applications"
        class="text-secondary text-decoration-none"
        >Applications</a
      > 
      
    </p>
    <h4 class="text-muted text-center">
      Presented In-person, in Daejeon
    </h4>
    <h4 class="text-muted text-center">
      
        4-minute short-format presentation
      
    </h4>

  </div>
</div>
<div style="display: flex; justify-content: center;" class="btn-toolbar mt-4" role="toolbar">
  <div class="poster-buttons btn-group btn-group-toggle mb-3" data-toggle="buttons">
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#details">
      Abstract
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#paper">
      Paper
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#poster">
      Poster
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#video">
      Video
    </button>
    
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#meta-review">
        Meta
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review1">
        R1
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review2">
        R2
      </button>
      
        <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review3">
          R3
        </button>
      
    
    <!--  -->
  </div>
  
</div>
<div class="poster-content">
  <div id="details" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="abstractExample">
          <span class="font-weight-bold">Abstract:</span>
          <p>General-purpose audio representations have proven effective across diverse music information retrieval applications, yet their utility in intelligent music production remains limited by insufficient understanding of audio effects (Fx). Although previous approaches have emphasized audio effects analysis at the mixture level, this focus falls short for tasks demanding instrument-wise audio effects understanding, such as automatic mixing. In this work, we present Fx-Encoder++, a novel model designed to extract instrument-wise audio effects representations from music mixtures. Our approach leverages a contrastive learning framework and introduces an ``extractor'' mechanism that, when provided with instrument queries (audio or text), transforms mixture-level audio effect embeddings into instrument-wise audio effect embeddings. We evaluated our model across retrieval and audio effect parameter matching tasks, testing its performance across a diverse range of instruments. The results demonstrate that Fx-Encoder++ outperforms previous approaches at mixture level and show a novel ability to extract effects representation instrument-wise, addressing a critical capability gap in intelligent music production systems.<br><br> <b><p align="center"><a href="">Direct link to video</a></b></p>
        </div>
      </div>
      <p></p>
    </div>
  </div>
  <div id="paper" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "https://drive.google.com/file/d/1KFjtKnn8F5aJ6BNu0rQmA1qZALlsYcMz/preview#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="poster" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="video" class="collapse">
    
      <div  style="display: flex; justify-content: center;">
      <iframe src="" width="960" height="540" allow="encrypted-media" allowfullscreen></iframe>
      </div>
    
  </div>
  
  <div id="meta-review" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="metaReviewExample">
          <span class="font-weight-bold">Meta Review:</span>
          <br>
          <p><strong>Q2 ( I am an expert on the topic of the paper.)</strong></p>
<p>Agree</p>
<p><strong>Q3 ( The title and abstract reflect the content of the paper.)</strong></p>
<p>Agree</p>
<p><strong>Q4 (The paper discusses, cites and compares with all relevant related work.)</strong></p>
<p>Agree</p>
<p><strong>Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)</strong></p>
<p>Agree</p>
<p><strong>Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by “n” pages of references or ethical considerations, references are well formatted). If you selected “No”, please explain the issue in your comments.)</strong></p>
<p>Yes</p>
<p><strong>Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)</strong></p>
<p>Agree</p>
<p><strong>Q9 (Scholarly/scientific quality: The content is scientifically correct.)</strong></p>
<p>Agree</p>
<p><strong>Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view "novelty" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)</strong></p>
<p>Agree</p>
<p><strong>Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)</strong></p>
<p>Disagree</p>
<p><strong>Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated “Strongly Agree” and “Agree” can be highlighted, but please do not penalize papers rated “Disagree” or “Strongly Disagree”. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)</strong></p>
<p>Disagree (Standard topic, task, or application)</p>
<p><strong>Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)</strong></p>
<p>Agree</p>
<p><strong>Q15 (Please explain your assessment of reusable insights in the paper.)</strong></p>
<p>The paper has some resuable insights, e.g., that the overall approach of combining mixture-level and instrument-level (through querying) effects representation learning can yield better results on both levels, work well on complex effects chains, but suffer in the presence of novel timbral characteristics.</p>
<p><strong>Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)</strong></p>
<p>Contrastive FX representation learning augmented with query-based extraction results in improved representation learning at both the instrument and mix level.</p>
<p><strong>Q17 (This paper is of award-winning quality.)</strong></p>
<p>No</p>
<p><strong>Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)</strong></p>
<p>Disagree</p>
<p><strong>Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)</strong></p>
<p>Weak accept</p>
<p><strong>Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)</strong></p>
<h2>Summary</h2>
<p>This paper presents an evolution of FX-Encoder to extract audio effects representations from both mixtures and queried instruments from within mixtures. They evaluate this task on both effect parameter retrieval, as well as effect parameter estimation via inference time optimization. They find that the proposed method </p>
<h2>Comments</h2>
<p>Overall, this paper presents an advancement of audio effects representation learning models. While prior models operated at either the mixture or instrument level, this work proposes a solution that works at both levels. The proposed methods in the paper seem sound and perform better than prior work in almost all scenarios, but there is still much room for improvement. Unfortunately, the paper has minimal error analysis, providing little insight into how such models can be improved in the future. Furthermore, many details about the evaluation are missing, including those about the evaluation dataset as well as a whole table of results related to Section 5.2. Such omissions reduce both the reproducibility of the paper and its reusable insights.</p>
<h2>Specific Comments</h2>
<p>Section 4.1 - More detail is needed about the construction of the evaluation dataset, e.g., how the effects and parameters are sampled. How were the mixtures constructed (are there ever multiple instances of the same class)? Can there be single instruments? Is the effect parameter evaluation set distinct from the training set?</p>
<p>Section 5.1 - The statement 'Notably, even when using high-quality source separation, we observe a clear gap between the "Target Instrument" and "MSS(m)"' seems to be incorrect. ST-ITO w/ MSS(m) performs better than target extraction.</p>
<p>Section 5.2 - The table referred to in this section seems to be completely absent. The results are not present.</p>
<p>Section 6 - What is 'VA modeling'? This has not been defined in this paper.</p>
<p><strong>Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid “weak accepts” or “weak rejects” if possible.)</strong></p>
<p>Accept</p>
<p><strong>Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))</strong></p>
<p>The majority of reviewers agreed that this paper is addressing an important topic and is generally well-written. Some noted that the evaluation could be stronger, e.g., with an ablation study, but the majority agreed that this should be included in the ISMIR program. That said, reviewers noted aspects of the writing that could be improved before a final submission. For example, while reviewers could envision the application and utility of this work, they noted that the authors should more clearly communicate the motivation and potential applications of the work, particularly regarding the instrument-specific embeddings. Furthermore, reviewers said that authors should provide a clearer interpretation of the instrument-specific embedding evaluation.</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review1" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review1Example">
          <span class="font-weight-bold">Review 1:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Weak reject</li>
</ul>
<h4>Main Reviews</h4>
<p>This paper presents FX-Encoder++, an innovative model addressing a key challenge in music technology: extracting instrument-specific audio effect representations directly from full music mixtures without the need for source separation. The method leverages a pretrained CLAP encoder alongside an “extractor” mechanism, enabling both audio and text queries. A carefully designed contrastive learning framework, underpins the training process, (FX-Normalization, consistent instrument composition, and hand-crafted hard negatives ensures robust learning, the dual-objective). However, I have several concerns and questions regarding the manuscript:</p>
<p>Motivation and Problem Definition
The main concern is: why is it important to extract instrument-specific embeddings from mixtures? </p>
<p>The authors state in lines [39–45]:
“Applications in this domain require..., how they shape the overall sound of a complete mixture (‘mixture level’) and how they transform individual instruments within that mixture (‘instrument level’).”
This statement reads more as a definition constructed for the purpose of this work rather than a widely accepted requirement of FX-specific representation learning. Based on my understanding, the goal of FX-specific representation learning is to embeddings that are specific to audio effect transformations (rather than invariant to them, as in general-purpose embeddings). 
Therefore, the motivation for why versatile understanding of both mixture and single-instrument content is necessary should be clarified and better grounded in prior literature. From the citations in the introduction [16,17], it seems that only guitar FX classification requires instrument-wise embeddings, which are not directly evaluated in this work. (also this task not extract embedding from mixture)
In summary, how is the instrument embedding extracted from a mixture used in music production? </p>
<p>Instrument-Specific Conditioning
- Line [275]: How is conditioning performed via the MLP layer? and How to attend to effect-related features?</p>
<p>Are the mixture-level embedding and CLAP query embedding concatenated? Is Adaptive Layer Normalization used? What specific operations are involved?</p>
<p>Writing and Presentation
- Line [160]: The phrase “is generate” is inappropriate here. Since your model is not a generative model or does not involve stochastic sampling, please revise this terminology.
- Figure 1: The figure and caption are confusing. It’s unclear whether "splitting source tracks" refers to time-based segmentation or instrument-wise source separation. The use of inconsistent icons (e.g., guitar, drums, mixture) adds to the confusion. 
- Lines [239–240]: CLAP supports both audio and text queries during training and inference. Why does the paper only mention audio in line 239-240? And the biggest advantage of using CLAP is that 1) it can handle multi-modal input (audio and text) and 2) it works even with zeroshot (unseen) words. Other than that, it is no different from using random text embedding (one-hot instruments class embedding) or any audio embedding. I think it would be good to write down the purpose of using CALP Encoder.</p>
<p>Evaluation
- Lines [361–362]: Are there any overlaps between the 8 instrument queries and MoisesDB?. Please clarify whether this evaluation setup is in-domain or out-of-domain.
- Line [372]: What are the queries and targets in the mixture-level retrieval evaluation? The phrase “testing effect identification in complete mixtures or isolated recordings” is ambiguous. Are these the query types, the retrieval targets?</p>
<p>Minor Comments
- Line [357]: The task of query-by-audio retrieval should be supported by references in audio retrieval [ref1] or audio fingerprinting [ref2], not solely CLAP. CLAP primarily addresses text-to-audio retrieval. (If I were to explain metrics rather than retrieval tasks in this part, I would state that Recall and Precision are not proposed in CLAP paper.)
[ref1] Disentangled multidimensional metric learning for music similarity, ICASSP 2020
[ref2] Neural Audio Fingerprint for High-specific Audio Retrieval based on Contrastive Learning, ICASSP 2021</p>
<ul>
<li>Line [376]: The phrase “directly from mixtures” is vague and should be rephrased for clarity.</li>
</ul>
<p>I believe this paper presents impressive experiments and result tables, and it is a valuable contribution. However, due to (1) the insufficient discussion on the necessity of instrument-specific audio effects and (2) the relatively weak writing quality in the methods and evaluation sections, I recommend a weak reject.</p>
<p>To strengthen the work, I suggest the following for reframing the paper:
1. Clearly articulate how fine-grained, instrument-wise information can improve the quality of mixture embeddings.
2. Consider adding an instrument-specific music production task as a downstream application.</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review2" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review2Example">
          <span class="font-weight-bold">Review 2:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Strong accept</li>
</ul>
<h4>Main Reviews</h4>
<p>The authors propose Fx-Encoder++, a novel model that extracts instrument-wise audio effects representations from music mixtures using a contrastive learning framework and an extractor mechanism guided by audio or text queries. They demonstrate that their approach outperforms previous methods on retrieval and parameter matching tasks, effectively advancing intelligent music production through improved understanding of audio effects at the instrument level.</p>
<p>I find the work very interesting and well-written, however there are some points that are not clear. I have some comments that I would like the authors to address in order to improve the quality of the manuscript.</p>
<ul>
<li>Introduction, line 55: what do you mean with “However, they focus only on modeling the aggregate result, rather than identifying how effects have been applied to each instrument”? Do you mean they model only the full Fx-chain and not each single Fx? </li>
<li>Sec. 3.1, line 177: “iff” -&gt; “if”.</li>
<li>Equation (2): please, define the operator \sim{.} and N.</li>
<li>Sec. 3.1, line 200: what do you mean with “negative” and “positive” pairs?</li>
<li>Sec. 3.1, line 211: Is there a requirement for the effects to be differentiable? </li>
<li>Sec. 3.1, line 290: can you please tell more about the scheduling you used to introduce the instrument-level loss? Is \lambda_{inst} increasing linearly? Or following cosine raising? Can you also tell more about the results you obtained instead without introducing such a scheduling or by swapping the paradigm?</li>
<li>I do not know if I missed it, but can you give information on how the audio query is supposed to be?</li>
<li>Sec. 5.1, line 421: please, define USS. You should also explicitly define q_{text} and q_{audio}. To be consistent and precise, define also MSS.</li>
<li>Sec. 5.2, line 453: I do not find any definition for L_d, which you use to quantify the performance of the methods as far as matching the effect parameters is concerned. Please, add more information.</li>
<li>I really suggest to add a github page with audio examples in order to clarify even better the performance of the models.</li>
<li>Can you also provide applications of such instrument-specific embeddings?</li>
</ul>
        </div>
      </div>
    </div> 
  </div>
  
    <div id="review3" class="pp-card m-3 collapse">
      <div class="card-body">
        <div class="card-text">
          <div id="review3Example">
            <span class="font-weight-bold">Review 3:</span>
            <br>
            <ul>
<li><strong>Overall Evaluation</strong>: Strong accept</li>
</ul>
<h4>Main Reviews</h4>
<p>This paper tackles the important and novel problem of extracting instrument-wise audio effect representations from mixtures, an emerging challenge in the field of intelligent music production, through an original approach that includes mechanisms such as the extractor module. The demonstrated high performance on the audio effect retrieval task mark a great advancement in the field. Particularly noteworthy is the finding illustrated in Figure 2, where retrieval performance improves as the number of effects increases, a compelling result. Although there are some limitations, such as parameter matching performance and understanding of single effects, these are challenges that could be addressed in future work. Given the novelty and potential impact of the proposed approach, I consider this paper a valuable contribution to the ISMIR community.</p>
<p>That said, one point of concern is the lack of ablation studies for the proposed method. As a result, it is unclear which techniques contribute to the observed performance improvements. For instance, in the contrastive learning setup, techniques like Fx-Normalization and Hand-Crafted Hard Negative Samples are employed, but the individual effects of these components on retrieval and classification performance have not been evaluated. Investigating these aspects is important to validate the soundness of the method. Therefore, I do not believe this paper is suitable for an award recommendation.</p>
          </div>
        </div>
      </div> 
    </div>
  
  
</div>


<script src="static/js/poster.js"></script>
<script src="static/js/video_selection.js"></script>

      </div>
    </div>
    
    

    <!-- Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "UA-");
    </script>

    <!-- Footer -->
    <footer class="footer bg-light p-4">
      <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2023 ISMIR Organization Committee</p>
      </div>
    </footer>

    <!-- Code for hash tags -->
    <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });
      });
    </script>
    <script src="static/js/lazy_load.js"></script>
    
  </body>
</html>