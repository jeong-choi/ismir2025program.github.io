


<!DOCTYPE html>
<html lang="en">
  <head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <link rel="icon" type="image/png" sizes="32x32" href="static/images/ismir_tab_icon.png">

    <link rel="stylesheet" href="static/css/main.css" />
    <link rel="stylesheet" href="static/css/custom.css" />
    <link rel="stylesheet" href="static/css/lazy_load.css" />
    <link rel="stylesheet" href="static/css/typeahead.css" />
    <link rel="stylesheet" href="static/css/poster.css" />
    <link rel="stylesheet" href="static/css/music.css" />
    <link rel="stylesheet" href="static/css/piece.css" />


    <!-- <link rel="stylesheet" href="https://gitcdn.xyz/repo/wingkwong/jquery-chameleon/master/src/chameleon.min.css"> -->

    <!-- External Javascript libs  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

    <script
      src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
      integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
      crossorigin="anonymous"
    ></script>


    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>
    <!-- <script src="static/js/chameleon.js"></script> -->


    <!-- Library libs -->
    <script src="static/js/typeahead.bundle.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
      href="static/css/Lato.css"
      rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet" />
    <link
      href="static/css/Cuprum.css"
      rel="stylesheet"
    />
    <link
      href="static/css/Ambiant.css"
      rel="stylesheet"
    />
    <!-- <link href="static/css/chameleon.css" rel="stylesheet"/> -->
    <title>ISMIR 2025: GOAT: A Large Dataset of Paired Guitar Audio Recordings and Tablatures</title>
    
<meta name="citation_title" content="GOAT: A Large Dataset of Paired Guitar Audio Recordings and Tablatures" />

<meta name="citation_author" content="Jackson Loth" />

<meta name="citation_author" content="Pedro Sarmento" />

<meta name="citation_author" content="Saurjya Sarkar" />

<meta name="citation_author" content="Zixun Guo" />

<meta name="citation_author" content="Mathieu Barthet" />

<meta name="citation_author" content="Mark Sandler" />

<meta name="citation_publication_date" content="21-25 September 2025" />
<meta name="citation_conference_title" content="Ismir 2025 Hybrid Conference" />
<meta name="citation_inbook_title" content="Proceedings of the First MiniCon Conference" />
<meta name="citation_abstract" content="In recent years, the guitar has received increased attention from the music information retrieval (MIR) community driven by the challenges posed by its diverse playing techniques and sonic characteristics. Mainly fueled by deep learning approaches, progress has been limited by the scarcity and limited annotations of datasets. To address this, we present the Guitar On Audio and Tablatures (GOAT) dataset,  comprising 5.9 hours of unique high-quality direct input audio recordings of electric guitars from a variety of different guitars and players. We also present an effective data augmentation strategy using guitar amplifiers which delivers near-unlimited tonal variety, of which we provide a starting 29.5 hours of audio. Each recording is annotated using guitar tablatures, a guitar-specific symbolic format supporting string and fret numbers, as well as numerous playing techniques. For this we utilise both the Guitar Pro format, a software for tablature playback and editing, and a text-like token encoding. Furthermore, we present competitive results using GOAT for MIDI transcription and preliminary results for a novel approach to automatic guitar tablature transcription. We hope that GOAT opens up the possibilities to train novel models on a wide variety of guitar-related MIR tasks, from synthesis to transcription to playing technique detection.&lt;br&gt;&lt;br&gt; &lt;b&gt;&lt;p align=&#34;center&#34;&gt;[Direct link to video]()&lt;/b&gt;" />

<meta name="citation_keywords" content="Symbolic music processing" />

<meta name="citation_keywords" content="Generative Tasks" />

<meta name="citation_keywords" content="Musical features and properties" />

<meta name="citation_keywords" content="Music transcription and annotation" />

<meta name="citation_keywords" content="Representations of music" />

<meta name="citation_keywords" content="Novel datasets and use cases" />

<meta name="citation_keywords" content="Music and audio synthesis" />

<meta name="citation_keywords" content="MIR tasks" />

<meta name="citation_keywords" content="Expression and performative aspects of music" />

<meta name="citation_keywords" content="Evaluation, datasets, and reproducibility" />

<meta name="citation_keywords" content="MIR fundamentals and methodology" />

<meta name="citation_pdf_url" content="" />
<meta id="yt-id" data-name= />
<meta id="bb-id" data-name= />


  </head>

  <body>
    <!-- NAV -->
    
    <!--

    ('tutorials.html', 'Tutorials'),
    ('index.html, 'Home'),
    ('special_meetings.html', 'Special Meetings'),
    -->

    <nav
      class="navbar sticky-top navbar-expand-lg navbar-light mr-auto"
      id="main-nav"
    >
      <div class="container">
        <a class="navbar-brand" href="https://ismir2025.ismir.net">
          <img
             class="logo" src="static/images/ismir_tabicon.png"
             height="auto"
             width="300px"
          />
        </a>
        
        <button
          class="navbar-toggler"
          type="button"
          data-toggle="collapse"
          data-target="#navbarNav"
          aria-controls="navbarNav"
          aria-expanded="false"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div
          class="collapse navbar-collapse text-right flex-grow-1"
          id="navbarNav"
        >
          <ul class="navbar-nav ml-auto">
            
            <li class="nav-item ">
              <a class="nav-link" href="calendar.html">Schedule</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="papers.html">Papers</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="music.html">Music</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="lbds.html">LBDs</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="industry.html?session=Platinum">Industry</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="jobs.html">Jobs</a>
            </li>
            
          </ul>
        </div>
      </div>
    </nav>
    

    
    <!-- User Overrides -->
     

    <div class="container">
      <!-- Tabs -->
      <div class="tabs">
         
      </div>
      <!-- Content -->
      <div class="content">
        

<!-- Title -->
<div class="pp-card m-3" style="">
  <div class="card-header">
    <h2 class="card-title main-title text-center" style="">
      P6-05: GOAT: A Large Dataset of Paired Guitar Audio Recordings and Tablatures
    </h2>
    <h3 class="card-subtitle mb-2 text-muted text-center">
      
      <a href="papers.html?filter=authors&search=Jackson Loth" class="text-muted"
        >Jackson Loth</a
      >,
      
      <a href="papers.html?filter=authors&search=Pedro Sarmento" class="text-muted"
        >Pedro Sarmento</a
      >,
      
      <a href="papers.html?filter=authors&search=Saurjya Sarkar" class="text-muted"
        >Saurjya Sarkar</a
      >,
      
      <a href="papers.html?filter=authors&search=Zixun Guo" class="text-muted"
        >Zixun Guo</a
      >,
      
      <a href="papers.html?filter=authors&search=Mathieu Barthet" class="text-muted"
        >Mathieu Barthet</a
      >,
      
      <a href="papers.html?filter=authors&search=Mark Sandler" class="text-muted"
        >Mark Sandler</a
      >
      
    </h3>
    <p class="card-text text-center">
      <span class="">Subjects:</span>
      
      <a
        href="papers.html?filter=keywords&search=Symbolic music processing"
        class="text-secondary text-decoration-none"
        >Symbolic music processing</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Generative Tasks"
        class="text-secondary text-decoration-none"
        >Generative Tasks</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Musical features and properties"
        class="text-secondary text-decoration-none"
        >Musical features and properties</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Music transcription and annotation"
        class="text-secondary text-decoration-none"
        >Music transcription and annotation</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Representations of music"
        class="text-secondary text-decoration-none"
        >Representations of music</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Novel datasets and use cases"
        class="text-secondary text-decoration-none"
        >Novel datasets and use cases</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Music and audio synthesis"
        class="text-secondary text-decoration-none"
        >Music and audio synthesis</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=MIR tasks"
        class="text-secondary text-decoration-none"
        >MIR tasks</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Expression and performative aspects of music"
        class="text-secondary text-decoration-none"
        >Expression and performative aspects of music</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Evaluation, datasets, and reproducibility"
        class="text-secondary text-decoration-none"
        >Evaluation, datasets, and reproducibility</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=MIR fundamentals and methodology"
        class="text-secondary text-decoration-none"
        >MIR fundamentals and methodology</a
      > 
      
    </p>
    <h4 class="text-muted text-center">
      Presented In-person, in Daejeon
    </h4>
    <h4 class="text-muted text-center">
      
        4-minute short-format presentation
      
    </h4>

  </div>
</div>
<div style="display: flex; justify-content: center;" class="btn-toolbar mt-4" role="toolbar">
  <div class="poster-buttons btn-group btn-group-toggle mb-3" data-toggle="buttons">
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#details">
      Abstract
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#paper">
      Paper
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#poster">
      Poster
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#video">
      Video
    </button>
    
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#meta-review">
        Meta
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review1">
        R1
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review2">
        R2
      </button>
      
        <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review3">
          R3
        </button>
      
    
    <!--  -->
  </div>
  
</div>
<div class="poster-content">
  <div id="details" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="abstractExample">
          <span class="font-weight-bold">Abstract:</span>
          <p>In recent years, the guitar has received increased attention from the music information retrieval (MIR) community driven by the challenges posed by its diverse playing techniques and sonic characteristics. Mainly fueled by deep learning approaches, progress has been limited by the scarcity and limited annotations of datasets. To address this, we present the Guitar On Audio and Tablatures (GOAT) dataset,  comprising 5.9 hours of unique high-quality direct input audio recordings of electric guitars from a variety of different guitars and players. We also present an effective data augmentation strategy using guitar amplifiers which delivers near-unlimited tonal variety, of which we provide a starting 29.5 hours of audio. Each recording is annotated using guitar tablatures, a guitar-specific symbolic format supporting string and fret numbers, as well as numerous playing techniques. For this we utilise both the Guitar Pro format, a software for tablature playback and editing, and a text-like token encoding. Furthermore, we present competitive results using GOAT for MIDI transcription and preliminary results for a novel approach to automatic guitar tablature transcription. We hope that GOAT opens up the possibilities to train novel models on a wide variety of guitar-related MIR tasks, from synthesis to transcription to playing technique detection.<br><br> <b><p align="center"><a href="">Direct link to video</a></b></p>
        </div>
      </div>
      <p></p>
    </div>
  </div>
  <div id="paper" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "https://drive.google.com/file/d/1Q_EHvjCdRn8EME98EiiwNg6iEya62Lpm/preview#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="poster" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="video" class="collapse">
    
      <div  style="display: flex; justify-content: center;">
      <iframe src="" width="960" height="540" allow="encrypted-media" allowfullscreen></iframe>
      </div>
    
  </div>
  
  <div id="meta-review" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="metaReviewExample">
          <span class="font-weight-bold">Meta Review:</span>
          <br>
          <p><strong>Q2 ( I am an expert on the topic of the paper.)</strong></p>
<p>Agree</p>
<p><strong>Q3 ( The title and abstract reflect the content of the paper.)</strong></p>
<p>Agree</p>
<p><strong>Q4 (The paper discusses, cites and compares with all relevant related work.)</strong></p>
<p>Agree</p>
<p><strong>Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)</strong></p>
<p>Agree</p>
<p><strong>Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by “n” pages of references or ethical considerations, references are well formatted). If you selected “No”, please explain the issue in your comments.)</strong></p>
<p>Yes</p>
<p><strong>Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)</strong></p>
<p>Strongly disagree</p>
<p><strong>Q9 (Scholarly/scientific quality: The content is scientifically correct.)</strong></p>
<p>Agree</p>
<p><strong>Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view "novelty" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)</strong></p>
<p>Agree</p>
<p><strong>Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)</strong></p>
<p>Disagree</p>
<p><strong>Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated “Strongly Agree” and “Agree” can be highlighted, but please do not penalize papers rated “Disagree” or “Strongly Disagree”. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)</strong></p>
<p>Agree (Novel topic, task, or application)</p>
<p><strong>Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)</strong></p>
<p>Disagree</p>
<p><strong>Q15 (Please explain your assessment of reusable insights in the paper.)</strong></p>
<p>As this is a dataset paper, the focus right now is on dataset documentation rather than offering broader reusable insights; I would expect for these to emerge later when the dataset will be used.</p>
<p><strong>Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)</strong></p>
<p>GOAT offers a new, rich dataset of guitar audio, associated tablatures, and various augmentations that are realistic for guitar players (such as different amplifier renderings)</p>
<p><strong>Q17 (This paper is of award-winning quality.)</strong></p>
<p>No</p>
<p><strong>Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)</strong></p>
<p>Disagree</p>
<p><strong>Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)</strong></p>
<p>Weak accept</p>
<p><strong>Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)</strong></p>
<p>This dataset paper offers new, rich data on guitar playing, including guitar audio by different players, associated aligned tablature, and different amplifier renderings. I appreciate the care that seems to have gone into the creation of the dataset, and the way in which the authors really seem to consider actual playing considerations specific to the guitar.</p>
<p>While I think this article would fit well at ISMIR and benefit the ISMIR community, I do have a few questions or remarks. Generally, as also acknowledged in the ethics statement and in the paper itself, use permission (and possible copyright/licensing issues) may be a possible problem for data like this. As mitigation, the authors indicate data only will be released for research purposes, and metadata removal/anonymization was performed as described in Section 3.2. I wonder to what extent that would be sufficient to avoid identifiability - and relating to this, what informed the choice of data in the first place.</p>
<p>The choice of data/songs is less clearly documented in the paper, which I again assume is for protection against possible copyright challenges, but some further justification would be welcome - why a cover of an existing piece, rather than e.g. new material that is not yet under protection, or test samples that represent playing in relation to tablature but would have less possible controversy, such as arpeggios, scales or other exercise fragments? What would generally be considered a comprehensive dataset in the eyes of the creator, and to what extent is GOAT meeting this in its choice of material?</p>
<p><strong>Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid “weak accepts” or “weak rejects” if possible.)</strong></p>
<p>Accept</p>
<p><strong>Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))</strong></p>
<p>This paper presents a new dataset that will clearly have value to the ISMIR community. While the reviews were not unanimous in their verdicts, in the discussion phase, no strong objections were raised against acceptance, but rather, the positive contributions of the paper were further emphasized. As such I consider it well above the acceptance bar, and recommend inclusion at ISMIR.</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review1" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review1Example">
          <span class="font-weight-bold">Review 1:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Strong accept</li>
</ul>
<h4>Main Reviews</h4>
<p>This is an interesting paper on a novel guitar transcription dataset, which provides both MIDI and Tab representations and is therefore useful for several task.
The variety of different file formats and the active attempt to ensure compatability with other research as well as the critical discussion in 4.3. is appreciated.
I like the hypothesis-driven experimental design.
The paper is well structured and easy to read.</p>
<p>I have the following comments / suggestions:</p>
<p>Introduction
- "(3) an evaluation of results" -&gt; evaluation of methods 
- preliminary results -&gt; how strong of a contribution is this, explain why preliminary (maybe this term can be avoided to make a stronger point even if the method is improved later)</p>
<p>Sec 2.1.
- the list "[12][5][6][18][19]" would benefit from a short list of general approaches for tab generation, to be a bit more specific</p>
<p>Sec 3.1.
- "community-created tablatures" -&gt; how are copyright concerns adressed?, same question for 3.2., first sentence</p>
<p>Sec 3.2.
- give some insight which method for fine-aligning the MIDI notes to the performance was used
- it is not clear, why the tuning was added as additional annotation</p>
<p>Sec. 3.3.
- to what detail are re-amping model parameters stored as well?</p>
<p>Fig. 3
- change y-axis to logarithmic scale (left three subplots)
- in the right subplot -&gt; add the number of no playing techniques as reference / for comparison</p>
<p>Sec 4.1.
- you might consider adding subsubsections for better structuring</p>
<p>Sec. 5.1
- "Following [4] [2], we finetune ..." &gt; gie some more details about network architecture
- "its zero-shot learning capabilities" &gt; I think this needs some justification, why the same task (transcription) but on different instrument timbres is really zero-shot learning</p>
<p>Sec. 5.2.1.
- "The transcription results ... do show some promise" &gt; which metric values would you consider satisfactory for the task?
- "...it is possible for the model to simply learn ..." &gt; did you check, was that the case?
- "The model tends to overfit ..." &gt; can you share some quantitative evidence for this?</p>
<p>Sec. 8
- "...we intend to make the dataset ... upon request" &gt; is this allowed? Would you not need consent from the original artists / their labels?</p>
<p>References
- revise for consistent labeling of conference names ([5], [6] "The ...")
- [8] has been published at ICASSP 2023 and should replace the arxiv pre-print
- [15, 16] add page range
- [17] -&gt; give full name for PMLR
- remove publisher for IEEE</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review2" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review2Example">
          <span class="font-weight-bold">Review 2:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Strong accept</li>
</ul>
<h4>Main Reviews</h4>
<ul>
<li>GOAT fills a major gap in guitar MIR by offering real-world, expressive annotations beyond basic MIDI.</li>
<li>Careful collection, alignment, tone augmentation, and documentation practices make the dataset credible.</li>
<li>Proposing DadaGP-based audio-to-text transcription expands the MIR frontier, aligning with current trends in large language/audio models.</li>
<li>Clear structure, accessible explanations, thorough statistical analysis of the dataset.</li>
</ul>
<p>Minor Suggestions:
* Since GOAT primarily uses covers and popular songs, it would be helpful to provide a short paragraph or table summarizing the genre/style diversity (e.g., proportion of rock, metal, pop, etc.) to give future users a clearer idea of dataset biases.
* Even a tiny preliminary human evaluation (e.g., 5–10 samples rated for tablature transcription quality) could have illustrated the qualitative potential of the Whisper fine-tuning approach, alongside WER scores.
* Showing a few sample outputs of the DadaGP transcription (both successful and failed examples) would help readers better understand the kinds of errors and structure the model is learning.</p>
        </div>
      </div>
    </div> 
  </div>
  
    <div id="review3" class="pp-card m-3 collapse">
      <div class="card-body">
        <div class="card-text">
          <div id="review3Example">
            <span class="font-weight-bold">Review 3:</span>
            <br>
            <ul>
<li><strong>Overall Evaluation</strong>: Weak reject</li>
</ul>
<h4>Main Reviews</h4>
<p>This paper introduces GOAT, a large dataset of paired electric guitar audio and tablature annotations, created using real performances and extended through a comprehensive amplifier-based data augmentation pipeline. It provides real DI (direct input) recordings with symbolic Guitar Pro annotations and offers experiments for guitar MIDI transcription and automatic guitar tablature transcription (AGTT) using Whisper.</p>
<p>Strengths: 
The introductory sections and dataset description are clearly written with detail, statistics, and metadata provided.</p>
<p>The dataset provides annotation richness, the inclusion of expressive guitar techniques (e.g., bends, mutes, legato) in tablatures, is a valuable addition over typical MIDI-only datasets.</p>
<p>Suggestions:</p>
<p>The real value lies in the dataset creation tool and pipeline, not necessarily the data itself. This distinction should be emphasized. A lot of existing datasets (e.g., GuitarSet, GAPS) could be transformed similarly. For all the midi transcription experiments it would be nice to also include ablation studies on the pipeline applied to other datasets, assessing actual performance improvements from this synthetic tonal variety. </p>
<p>For the MIDI transcription experiments, while the test set from GuitarSet is fair, I would suggest comparing with a separately created test set (recorded with real amps and effects, if possible), or at least processed with different effects and not the same pipeline. Even with proper splitting, having test data created using the same synthetic procedure as the training data might bias results, as the model could have learned artefacts specific to that pipeline. For example, in the case of AMP-XL, you might have seen more meaningful improvements if it had been tested on real data or data created using a different amping plugin.</p>
<p>The AGTT task is underspecified. The audio-to-text formulation of AGTT (automatic guitar tablature transcription) is novel, but it’s not well-explained what this task is. If this is a first introduction of AGTT as audio-to-text, more explanation and justification is needed. For example, what makes DadaGP suited to Whisper? How are timing, rhythm, or note grouping handled in tokenization? What is the ultimate downstream application? This approach may confuse readers unfamiliar with Whisper, token-based encodings, or tablature formats.</p>
          </div>
        </div>
      </div> 
    </div>
  
  
</div>


<script src="static/js/poster.js"></script>
<script src="static/js/video_selection.js"></script>

      </div>
    </div>
    
    

    <!-- Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "UA-");
    </script>

    <!-- Footer -->
    <footer class="footer bg-light p-4">
      <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2023 ISMIR Organization Committee</p>
      </div>
    </footer>

    <!-- Code for hash tags -->
    <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });
      });
    </script>
    <script src="static/js/lazy_load.js"></script>
    
  </body>
</html>