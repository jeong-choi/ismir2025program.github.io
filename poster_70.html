


<!DOCTYPE html>
<html lang="en">
  <head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <link rel="icon" type="image/png" sizes="32x32" href="static/images/ismir_tab_icon.png">

    <link rel="stylesheet" href="static/css/main.css" />
    <link rel="stylesheet" href="static/css/custom.css" />
    <link rel="stylesheet" href="static/css/lazy_load.css" />
    <link rel="stylesheet" href="static/css/typeahead.css" />
    <link rel="stylesheet" href="static/css/poster.css" />
    <link rel="stylesheet" href="static/css/music.css" />
    <link rel="stylesheet" href="static/css/piece.css" />


    <!-- <link rel="stylesheet" href="https://gitcdn.xyz/repo/wingkwong/jquery-chameleon/master/src/chameleon.min.css"> -->

    <!-- External Javascript libs  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

    <script
      src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
      integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
      crossorigin="anonymous"
    ></script>


    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>
    <!-- <script src="static/js/chameleon.js"></script> -->


    <!-- Library libs -->
    <script src="static/js/typeahead.bundle.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
      href="static/css/Lato.css"
      rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet" />
    <link
      href="static/css/Cuprum.css"
      rel="stylesheet"
    />
    <link
      href="static/css/Ambiant.css"
      rel="stylesheet"
    />
    <!-- <link href="static/css/chameleon.css" rel="stylesheet"/> -->
    <title>ISMIR 2025: Sheet Music Benchmark: Standardized Optical Music Recognition Evaluation</title>
    
<meta name="citation_title" content="Sheet Music Benchmark: Standardized Optical Music Recognition Evaluation" />

<meta name="citation_author" content="Juan C. Martinez-Sevilla" />

<meta name="citation_author" content="Joan Cerveto-Serrano" />

<meta name="citation_author" content="Noelia Luna-Barahona" />

<meta name="citation_author" content="Greg Chapman" />

<meta name="citation_author" content="Craig Sapp" />

<meta name="citation_author" content="David Rizo" />

<meta name="citation_author" content="Jorge Calvo-Zaragoza" />

<meta name="citation_publication_date" content="21-25 September 2025" />
<meta name="citation_conference_title" content="Ismir 2025 Hybrid Conference" />
<meta name="citation_inbook_title" content="Proceedings of the First MiniCon Conference" />
<meta name="citation_abstract" content="In this work, we introduce the Sheet Music Benchmark (SMB), a dataset of six hundred and eighty-five pages specifically designed to benchmark Optical Music Recognition (OMR) research. SMB encompasses a diverse array of musical textures, including monophony, pianoform, quartet, and others, all encoded in Common Western Modern Notation using the Humdrum **kern format. Alongside SMB, we introduce the OMR Normalized Edit Distance (OMR-NED), a new metric tailored explicitly for evaluating OMR performance. OMR-NED builds upon the widely-used Symbol Error Rate (SER), offering a fine-grained and detailed error analysis that covers individual musical elements such as note heads, beams, pitches, accidentals, and other critical notation features. The resulting numeric score provided by OMR-NED facilitates clear comparisons, enabling researchers and end-users alike to identify optimal OMR approaches. Our work thus addresses a long-standing gap in OMR evaluation, and we support our contributions with baseline experiments using standardized SMB dataset splits for training and assessing state-of-the-art methods.&lt;br&gt;&lt;br&gt; &lt;b&gt;&lt;p align=&#34;center&#34;&gt;[Direct link to video]()&lt;/b&gt;" />

<meta name="citation_keywords" content="Knowledge-driven approaches to MIR" />

<meta name="citation_keywords" content="Evaluation, datasets, and reproducibility" />

<meta name="citation_keywords" content="Optical music recognition" />

<meta name="citation_keywords" content="Novel datasets and use cases" />

<meta name="citation_keywords" content="Music transcription and annotation" />

<meta name="citation_keywords" content="Evaluation methodology" />

<meta name="citation_keywords" content="Machine learning/artificial intelligence for music" />

<meta name="citation_keywords" content="Evaluation metrics" />

<meta name="citation_keywords" content="MIR tasks" />

<meta name="citation_pdf_url" content="" />
<meta id="yt-id" data-name= />
<meta id="bb-id" data-name= />


  </head>

  <body>
    <!-- NAV -->
    
    <!--

    ('tutorials.html', 'Tutorials'),
    ('index.html, 'Home'),
    ('special_meetings.html', 'Special Meetings'),
    -->

    <nav
      class="navbar sticky-top navbar-expand-lg navbar-light mr-auto"
      id="main-nav"
    >
      <div class="container">
        <a class="navbar-brand" href="https://ismir2025.ismir.net">
          <img
             class="logo" src="static/images/ismir_tabicon.png"
             height="auto"
             width="300px"
          />
        </a>
        
        <button
          class="navbar-toggler"
          type="button"
          data-toggle="collapse"
          data-target="#navbarNav"
          aria-controls="navbarNav"
          aria-expanded="false"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div
          class="collapse navbar-collapse text-right flex-grow-1"
          id="navbarNav"
        >
          <ul class="navbar-nav ml-auto">
            
            <li class="nav-item ">
              <a class="nav-link" href="calendar.html">Schedule</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="papers.html">Papers</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="music.html">Music</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="lbds.html">LBDs</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="industry.html?session=Platinum">Industry</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="jobs.html">Jobs</a>
            </li>
            
          </ul>
        </div>
      </div>
    </nav>
    

    
    <!-- User Overrides -->
     

    <div class="container">
      <!-- Tabs -->
      <div class="tabs">
         
      </div>
      <!-- Content -->
      <div class="content">
        

<!-- Title -->
<div class="pp-card m-3" style="">
  <div class="card-header">
    <h2 class="card-title main-title text-center" style="">
      P5-13: Sheet Music Benchmark: Standardized Optical Music Recognition Evaluation
    </h2>
    <h3 class="card-subtitle mb-2 text-muted text-center">
      
      <a href="papers.html?filter=authors&search=Juan C. Martinez-Sevilla" class="text-muted"
        >Juan C. Martinez-Sevilla</a
      >,
      
      <a href="papers.html?filter=authors&search=Joan Cerveto-Serrano" class="text-muted"
        >Joan Cerveto-Serrano</a
      >,
      
      <a href="papers.html?filter=authors&search=Noelia Luna-Barahona" class="text-muted"
        >Noelia Luna-Barahona</a
      >,
      
      <a href="papers.html?filter=authors&search=Greg Chapman" class="text-muted"
        >Greg Chapman</a
      >,
      
      <a href="papers.html?filter=authors&search=Craig Sapp" class="text-muted"
        >Craig Sapp</a
      >,
      
      <a href="papers.html?filter=authors&search=David Rizo" class="text-muted"
        >David Rizo</a
      >,
      
      <a href="papers.html?filter=authors&search=Jorge Calvo-Zaragoza" class="text-muted"
        >Jorge Calvo-Zaragoza</a
      >
      
    </h3>
    <p class="card-text text-center">
      <span class="">Subjects:</span>
      
      <a
        href="papers.html?filter=keywords&search=Knowledge-driven approaches to MIR"
        class="text-secondary text-decoration-none"
        >Knowledge-driven approaches to MIR</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Evaluation, datasets, and reproducibility"
        class="text-secondary text-decoration-none"
        >Evaluation, datasets, and reproducibility</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Optical music recognition"
        class="text-secondary text-decoration-none"
        >Optical music recognition</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Novel datasets and use cases"
        class="text-secondary text-decoration-none"
        >Novel datasets and use cases</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Music transcription and annotation"
        class="text-secondary text-decoration-none"
        >Music transcription and annotation</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Evaluation methodology"
        class="text-secondary text-decoration-none"
        >Evaluation methodology</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Machine learning/artificial intelligence for music"
        class="text-secondary text-decoration-none"
        >Machine learning/artificial intelligence for music</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Evaluation metrics"
        class="text-secondary text-decoration-none"
        >Evaluation metrics</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=MIR tasks"
        class="text-secondary text-decoration-none"
        >MIR tasks</a
      > 
      
    </p>
    <h4 class="text-muted text-center">
      Presented In-person, in Daejeon
    </h4>
    <h4 class="text-muted text-center">
      
        4-minute short-format presentation
      
    </h4>

  </div>
</div>
<div style="display: flex; justify-content: center;" class="btn-toolbar mt-4" role="toolbar">
  <div class="poster-buttons btn-group btn-group-toggle mb-3" data-toggle="buttons">
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#details">
      Abstract
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#paper">
      Paper
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#poster">
      Poster
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#video">
      Video
    </button>
    
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#meta-review">
        Meta
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review1">
        R1
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review2">
        R2
      </button>
      
        <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review3">
          R3
        </button>
      
    
    <!--  -->
  </div>
  
</div>
<div class="poster-content">
  <div id="details" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="abstractExample">
          <span class="font-weight-bold">Abstract:</span>
          <p>In this work, we introduce the Sheet Music Benchmark (SMB), a dataset of six hundred and eighty-five pages specifically designed to benchmark Optical Music Recognition (OMR) research. SMB encompasses a diverse array of musical textures, including monophony, pianoform, quartet, and others, all encoded in Common Western Modern Notation using the Humdrum **kern format. Alongside SMB, we introduce the OMR Normalized Edit Distance (OMR-NED), a new metric tailored explicitly for evaluating OMR performance. OMR-NED builds upon the widely-used Symbol Error Rate (SER), offering a fine-grained and detailed error analysis that covers individual musical elements such as note heads, beams, pitches, accidentals, and other critical notation features. The resulting numeric score provided by OMR-NED facilitates clear comparisons, enabling researchers and end-users alike to identify optimal OMR approaches. Our work thus addresses a long-standing gap in OMR evaluation, and we support our contributions with baseline experiments using standardized SMB dataset splits for training and assessing state-of-the-art methods.<br><br> <b><p align="center"><a href="">Direct link to video</a></b></p>
        </div>
      </div>
      <p></p>
    </div>
  </div>
  <div id="paper" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "https://drive.google.com/file/d/1vIRtBhRSleT_xnSuXXNAJQBfOEZGaydQ/preview#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="poster" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="video" class="collapse">
    
      <div  style="display: flex; justify-content: center;">
      <iframe src="" width="960" height="540" allow="encrypted-media" allowfullscreen></iframe>
      </div>
    
  </div>
  
  <div id="meta-review" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="metaReviewExample">
          <span class="font-weight-bold">Meta Review:</span>
          <br>
          <p><strong>Q2 ( I am an expert on the topic of the paper.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q3 ( The title and abstract reflect the content of the paper.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q4 (The paper discusses, cites and compares with all relevant related work.)</strong></p>
<p>Agree</p>
<p><strong>Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)</strong></p>
<p>Agree</p>
<p><strong>Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by “n” pages of references or ethical considerations, references are well formatted). If you selected “No”, please explain the issue in your comments.)</strong></p>
<p>Yes</p>
<p><strong>Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q9 (Scholarly/scientific quality: The content is scientifically correct.)</strong></p>
<p>Agree</p>
<p><strong>Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view "novelty" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)</strong></p>
<p>Agree</p>
<p><strong>Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated “Strongly Agree” and “Agree” can be highlighted, but please do not penalize papers rated “Disagree” or “Strongly Disagree”. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)</strong></p>
<p>Agree (Novel topic, task, or application)</p>
<p><strong>Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)</strong></p>
<p>Agree</p>
<p><strong>Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)</strong></p>
<p>The paper introduces a high-quality benchmark dataset (SMB) and a detailed evaluation metric (OMR-NED) that together offer standardized, fine-grained assessment of modern OMR systems.</p>
<p><strong>Q17 (This paper is of award-winning quality.)</strong></p>
<p>No</p>
<p><strong>Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)</strong></p>
<p>Agree</p>
<p><strong>Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)</strong></p>
<p>Strong accept</p>
<p><strong>Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)</strong></p>
<p>This paper fills an important gap in OMR. offering a much-needed benchmark dataset (SMB) and a novel evaluation metric (OMR-NED) that provides fine-grained error analysis across musical symbol categories. The construction and annotation of the dataset are rigorous, and the metric builds meaningfully on existing tools like MusicDiff. The baseline experiments, while not yielding high accuracy, are appropriate and indicative of the dataset's complexity.</p>
<p>Strengths:
- Timely and highly relevant contributions to the MIR/OMR community
- Solid technical methodology for both dataset and metric
- Clear writing and thoughtful organization</p>
<p>Suggestions for improvement:
- Consider adding a summary table comparing SMB to previous datasets (e.g., size, scope, textures, and coverage).
- Provide visual examples or schema for OMR-NED categories and scoring to aid understanding.
- Ensure that links to dataset/code are well-documented and accessible post-acceptance.</p>
<p><strong>Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid “weak accepts” or “weak rejects” if possible.)</strong></p>
<p>Strong accept</p>
<p><strong>Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))</strong></p>
<p>This paper introduces a significant and much-needed contribution to the Optical Music Recognition (OMR) research community through the development of a standardized benchmark dataset (SMB) and a novel evaluation metric (OMR-NED). The work is timely, addressing a longstanding gap in reproducible, fine-grained evaluation for OMR systems, and has clear potential to become a foundational reference for future research in the field.</p>
<p>Across the board, reviewers acknowledge the technical soundness, relevance, and clarity of the work. The dataset construction and annotation processes are described as thorough, and the OMR-NED metric is appreciated for its granularity and thoughtful design.</p>
<p>Some reviewers expressed concerns about details that could be improved in the camera-ready version, including:</p>
<ul>
<li>Clarifying the licensing terms of the dataset.</li>
<li>Providing per-category evaluation scores to highlight the variability in task difficulty.</li>
<li>Offering more clarity on the tokenization process and how OMR-NED treats substitutions.</li>
<li>Expanding the explanation of differences between SER and OMR-NED and motivating the design choices in the latter.</li>
<li>Verifying the accuracy of labels in the dataset (e.g., use of “monophonic”).</li>
</ul>
<p>These are constructive and actionable suggestions, none of which undermine the overall scholarly contribution of the work. Rather, they indicate areas for refinement that will enhance the impact and usability of the benchmark.</p>
<p>Importantly, the paper meets ISMIR’s criteria for reproducibility and openness, and it presents reusable infrastructure—dataset, metric, and tools—that will serve the community for years to come. It is rare to see a benchmark paper that is both technically solid and practically impactful in the way this one is.</p>
<p>The paper represents an exemplary effort in infrastructure building for MIR, especially in a field where robust benchmarks have been lacking. It is likely to catalyze future work, foster comparability between methods, and stimulate further discourse on evaluation methodologies in OMR.</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review1" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review1Example">
          <span class="font-weight-bold">Review 1:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Strong accept</li>
</ul>
<h4>Main Reviews</h4>
<p>The paper introduces a new, large and diverse benchmark for the optimal music recognition (OMR) task, with detailed dataset construction, metric definition, and baseline results. The key contributions are:</p>
<ul>
<li>The dataset is large and diverse, and the paper provides a thorough statistical analysis</li>
<li>The process of annotation and post-processing is well-described, helping the users better understand the characteristics and any limitations of the dataset.</li>
<li>A new suggested standard metric (OMR-NED) for the benchmark and the improved tools for calculating them, with discussions on the pros and cons of each.</li>
</ul>
<p>Minor limitations that could be addressed for camera-ready:</p>
<ul>
<li>The paper doesn't seem to clarify the license for the data, other than a mention of public-domain uploads.</li>
<li>In addition to the dataset, a reference implementation of the OMR-NED and SER could further facilitate easier and more reproducible comparison of the benchmark.</li>
<li>While having a competitive baseline is not the primary goal of a dataset paper, and the low performance is expected without a vision model as the authors note, OMR-NED often being over 90% is somewhat discouraging. Using a pretrained encoder (by using one of publicly available vision models or pre-training on GrandStaff) would have produced more competitive baseline metrics that can help better understand how difficult the benchmark is and how likely it may become saturated soon and no longer useful.</li>
<li>Also, per-category baseline metrics would've been also good to include, basically a version of Table 3 for the models/data in Table 2, which would be informative on the expected difficulty in each category and also encourage the researchers using this benchmark to report per-category details.</li>
</ul>
        </div>
      </div>
    </div> 
  </div>
  <div id="review2" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review2Example">
          <span class="font-weight-bold">Review 2:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Weak accept</li>
</ul>
<h4>Main Reviews</h4>
<p>The paper introduces a dataset and an evaluation measure to benchmark OMR systems. As such, it is a valuable contribution to the community, as there is a lack of annotated evaluation datasets in OMR. 
The description of the dataset is suitable, however the description of the proposed evaluation metric and the baseline experiments lack in detail. To improve the paper, I suggest the authors to:</p>
<ul>
<li>
<p>describe the limitations of the chosen **kern format - it is not a fully-fledged music notation format such as MusicXML, so you should describe what it can and can't represent</p>
</li>
<li>
<p>provide more details on how the proposed evaluation metric is calculated, as it is not completely clear</p>
</li>
<li>
<p>the baseline system's error rates are so high, that it makes me wonder if there is any use in including the results - over 90% error rate means that almost all symbols are wrong, so the output is almost random? If so, it makes no sense in including this evaluation, if not, you should make this clearer in the paper.</p>
</li>
<li>
<p>you should elaborate more deeply on the differences between SER and the proposed measure, as they can differ over 50%?</p>
</li>
</ul>
        </div>
      </div>
    </div> 
  </div>
  
    <div id="review3" class="pp-card m-3 collapse">
      <div class="card-body">
        <div class="card-text">
          <div id="review3Example">
            <span class="font-weight-bold">Review 3:</span>
            <br>
            <ul>
<li><strong>Overall Evaluation</strong>: Weak accept</li>
</ul>
<h4>Main Reviews</h4>
<p>The proposed dataset and evaluation metric are a meaningful contribution to OMR research. Overall the paper is written clearly, but some sections are lacking, and the experiments are not entirely convincing.</p>
<p>Strengths:
* New dataset for OMR of scanned classical Western music (685 pages)
* Standardization of kern notation for consistent tokenization
* Proposed OMR-NED evaluation gives information on type of error</p>
<p>Weaknesses:
* Discrepancy between experiments and stated dataset purpose: Due to the small size of the dataset, training from scratch gave bad results. Authors could have fine-tuned a model trained on more data, or used the dataset as a benchmark for existing models, which is what they proposed this dataset for in the first place.
* Not clear whether author release all their code (Humdrum standardization, MusicDiff modifications, evaluation metric calculation). Just making sure.
* In OMR-NED, does it really make sense to treat a substitution as insertion+deletion, counting it as two errors?
* Tokenization scheme is not stated (is every full ‘word’ a token?), yet dataset statistics are reported on tokens rather than e.g. notes.</p>
<p>Additional comments:
* Table 2: Interestingly, OMR-NED and SED give a different ordering of the rows. For example, if we look at the ekern results only, then Monophony got the worst results under OMR-NED, but placed second under SED. How do authors explain this discrepancy? Can they somehow show that OMR-NED matches human judgement better than SED, which counts substitutions differently?
* Table 2: How come Monophony (which is supposedly the easiest task) got worse scores than Pianoform, Quartet, and Other?
* I downloaded the dataset and noticed that many scores labeled as Monophonic (in the mono_scores folder) are in fact not monophonic (they have chord/voices). Did authors (mis-)use the term monophonic throughout the paper to mean ‘single staff’?
* Figure 4: The lyrics seem made up and don’t match the music. This is not a very good illustration of OMR-NED.
* Authors mentioned dataset splits in the abstract, but where are they? I see only a ‘train’ split.
* Line 211: ‘particellas’ is not a standard term in English. The term is ‘parts’.</p>
          </div>
        </div>
      </div> 
    </div>
  
  
</div>


<script src="static/js/poster.js"></script>
<script src="static/js/video_selection.js"></script>

      </div>
    </div>
    
    

    <!-- Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "UA-");
    </script>

    <!-- Footer -->
    <footer class="footer bg-light p-4">
      <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2023 ISMIR Organization Committee</p>
      </div>
    </footer>

    <!-- Code for hash tags -->
    <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });
      });
    </script>
    <script src="static/js/lazy_load.js"></script>
    
  </body>
</html>