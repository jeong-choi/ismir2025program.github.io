


<!DOCTYPE html>
<html lang="en">
  <head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <link rel="icon" type="image/png" sizes="32x32" href="static/images/ismir_tab_icon.png">

    <link rel="stylesheet" href="static/css/main.css" />
    <link rel="stylesheet" href="static/css/custom.css" />
    <link rel="stylesheet" href="static/css/lazy_load.css" />
    <link rel="stylesheet" href="static/css/typeahead.css" />
    <link rel="stylesheet" href="static/css/poster.css" />
    <link rel="stylesheet" href="static/css/music.css" />
    <link rel="stylesheet" href="static/css/piece.css" />


    <!-- <link rel="stylesheet" href="https://gitcdn.xyz/repo/wingkwong/jquery-chameleon/master/src/chameleon.min.css"> -->

    <!-- External Javascript libs  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

    <script
      src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
      integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
      crossorigin="anonymous"
    ></script>


    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>
    <!-- <script src="static/js/chameleon.js"></script> -->


    <!-- Library libs -->
    <script src="static/js/typeahead.bundle.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
      href="static/css/Lato.css"
      rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet" />
    <link
      href="static/css/Cuprum.css"
      rel="stylesheet"
    />
    <link
      href="static/css/Ambiant.css"
      rel="stylesheet"
    />
    <!-- <link href="static/css/chameleon.css" rel="stylesheet"/> -->
    <title>ISMIR 2025: Measuring Sensory Dissonance In Multi-Track Music Recordings: A Case Study with Wind Quartets</title>
    
<meta name="citation_title" content="Measuring Sensory Dissonance In Multi-Track Music Recordings: A Case Study with Wind Quartets" />

<meta name="citation_author" content="Simon Schwär" />

<meta name="citation_author" content="Stefan Balke" />

<meta name="citation_author" content="Meinard Müller" />

<meta name="citation_publication_date" content="21-25 September 2025" />
<meta name="citation_conference_title" content="Ismir 2025 Hybrid Conference" />
<meta name="citation_inbook_title" content="Proceedings of the First MiniCon Conference" />
<meta name="citation_abstract" content="Sensory dissonance (SD) quantifies the interference between partials in a mixture of simultaneously sounding tones and correlates with the perceived dissonance or unpleasantness of this mixture. While it is mainly studied in music perception, often using synthetic signals or symbolic inputs, in this paper, we focus on a practical application and investigate SD as a tool for analyzing the interactions between voices in multi-track music recordings. Using visualization and statistical analysis on an existing dataset of four-part chorales recorded with various wind instruments, we examine how timbre, tuning, and score influences SD. To do this, we introduce the notion of relative SD, which quantifies how individual voices in a multi-track recording contribute to overall SD of their polyphonic mixture. In addition to discussing practical aspects of measuring SD between and within real music signals, our case study shows potential benefits and limitations of using SD as an analysis tool in music production, for example, to inform or automate tasks like take selection or equalization.&lt;br&gt;&lt;br&gt; &lt;b&gt;&lt;p align=&#34;center&#34;&gt;[Direct link to video]()&lt;/b&gt;" />

<meta name="citation_keywords" content="MIR fundamentals and methodology" />

<meta name="citation_keywords" content="Timbre, instrumentation, and singing voice" />

<meta name="citation_keywords" content="Harmony, chords and tonality" />

<meta name="citation_keywords" content="Musical features and properties" />

<meta name="citation_keywords" content="Music composition, performance, and production" />

<meta name="citation_keywords" content="Music signal processing" />

<meta name="citation_keywords" content="Applications" />

<meta name="citation_pdf_url" content="" />
<meta id="yt-id" data-name= />
<meta id="bb-id" data-name= />


  </head>

  <body>
    <!-- NAV -->
    
    <!--

    ('tutorials.html', 'Tutorials'),
    ('index.html, 'Home'),
    ('special_meetings.html', 'Special Meetings'),
    -->

    <nav
      class="navbar sticky-top navbar-expand-lg navbar-light mr-auto"
      id="main-nav"
    >
      <div class="container">
        <a class="navbar-brand" href="https://ismir2025.ismir.net">
          <img
             class="logo" src="static/images/ismir_tabicon.png"
             height="auto"
             width="300px"
          />
        </a>
        
        <button
          class="navbar-toggler"
          type="button"
          data-toggle="collapse"
          data-target="#navbarNav"
          aria-controls="navbarNav"
          aria-expanded="false"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div
          class="collapse navbar-collapse text-right flex-grow-1"
          id="navbarNav"
        >
          <ul class="navbar-nav ml-auto">
            
            <li class="nav-item ">
              <a class="nav-link" href="calendar.html">Schedule</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="papers.html">Papers</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="music.html">Music</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="lbds.html">LBDs</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="industry.html?session=Platinum">Industry</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="jobs.html">Jobs</a>
            </li>
            
          </ul>
        </div>
      </div>
    </nav>
    

    
    <!-- User Overrides -->
     

    <div class="container">
      <!-- Tabs -->
      <div class="tabs">
         
      </div>
      <!-- Content -->
      <div class="content">
        

<!-- Title -->
<div class="pp-card m-3" style="">
  <div class="card-header">
    <h2 class="card-title main-title text-center" style="">
      P1-14: Measuring Sensory Dissonance In Multi-Track Music Recordings: A Case Study with Wind Quartets
    </h2>
    <h3 class="card-subtitle mb-2 text-muted text-center">
      
      <a href="papers.html?filter=authors&search=Simon Schwär" class="text-muted"
        >Simon Schwär</a
      >,
      
      <a href="papers.html?filter=authors&search=Stefan Balke" class="text-muted"
        >Stefan Balke</a
      >,
      
      <a href="papers.html?filter=authors&search=Meinard Müller" class="text-muted"
        >Meinard Müller</a
      >
      
    </h3>
    <p class="card-text text-center">
      <span class="">Subjects:</span>
      
      <a
        href="papers.html?filter=keywords&search=MIR fundamentals and methodology"
        class="text-secondary text-decoration-none"
        >MIR fundamentals and methodology</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Timbre, instrumentation, and singing voice"
        class="text-secondary text-decoration-none"
        >Timbre, instrumentation, and singing voice</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Harmony, chords and tonality"
        class="text-secondary text-decoration-none"
        >Harmony, chords and tonality</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Musical features and properties"
        class="text-secondary text-decoration-none"
        >Musical features and properties</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Music composition, performance, and production"
        class="text-secondary text-decoration-none"
        >Music composition, performance, and production</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Music signal processing"
        class="text-secondary text-decoration-none"
        >Music signal processing</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Applications"
        class="text-secondary text-decoration-none"
        >Applications</a
      > 
      
    </p>
    <h4 class="text-muted text-center">
      Presented In-person, in Daejeon
    </h4>
    <h4 class="text-muted text-center">
      
        4-minute short-format presentation
      
    </h4>

  </div>
</div>
<div style="display: flex; justify-content: center;" class="btn-toolbar mt-4" role="toolbar">
  <div class="poster-buttons btn-group btn-group-toggle mb-3" data-toggle="buttons">
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#details">
      Abstract
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#paper">
      Paper
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#poster">
      Poster
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#video">
      Video
    </button>
    
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#meta-review">
        Meta
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review1">
        R1
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review2">
        R2
      </button>
      
        <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review3">
          R3
        </button>
      
    
    <!--  -->
  </div>
  
</div>
<div class="poster-content">
  <div id="details" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="abstractExample">
          <span class="font-weight-bold">Abstract:</span>
          <p>Sensory dissonance (SD) quantifies the interference between partials in a mixture of simultaneously sounding tones and correlates with the perceived dissonance or unpleasantness of this mixture. While it is mainly studied in music perception, often using synthetic signals or symbolic inputs, in this paper, we focus on a practical application and investigate SD as a tool for analyzing the interactions between voices in multi-track music recordings. Using visualization and statistical analysis on an existing dataset of four-part chorales recorded with various wind instruments, we examine how timbre, tuning, and score influences SD. To do this, we introduce the notion of relative SD, which quantifies how individual voices in a multi-track recording contribute to overall SD of their polyphonic mixture. In addition to discussing practical aspects of measuring SD between and within real music signals, our case study shows potential benefits and limitations of using SD as an analysis tool in music production, for example, to inform or automate tasks like take selection or equalization.<br><br> <b><p align="center"><a href="">Direct link to video</a></b></p>
        </div>
      </div>
      <p></p>
    </div>
  </div>
  <div id="paper" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "https://drive.google.com/file/d/17BiprOPuEUSqYWlYrn0XWNKRSlxqXBG1/preview#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="poster" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="video" class="collapse">
    
      <div  style="display: flex; justify-content: center;">
      <iframe src="" width="960" height="540" allow="encrypted-media" allowfullscreen></iframe>
      </div>
    
  </div>
  
  <div id="meta-review" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="metaReviewExample">
          <span class="font-weight-bold">Meta Review:</span>
          <br>
          <p><strong>Q2 ( I am an expert on the topic of the paper.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q3 ( The title and abstract reflect the content of the paper.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q4 (The paper discusses, cites and compares with all relevant related work.)</strong></p>
<p>Agree</p>
<p><strong>Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by “n” pages of references or ethical considerations, references are well formatted). If you selected “No”, please explain the issue in your comments.)</strong></p>
<p>Yes</p>
<p><strong>Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)</strong></p>
<p>Agree</p>
<p><strong>Q9 (Scholarly/scientific quality: The content is scientifically correct.)</strong></p>
<p>Agree</p>
<p><strong>Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view "novelty" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)</strong></p>
<p>Agree</p>
<p><strong>Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)</strong></p>
<p>Agree</p>
<p><strong>Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated “Strongly Agree” and “Agree” can be highlighted, but please do not penalize papers rated “Disagree” or “Strongly Disagree”. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)</strong></p>
<p>Disagree (Standard topic, task, or application)</p>
<p><strong>Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)</strong></p>
<p>Agree</p>
<p><strong>Q15 (Please explain your assessment of reusable insights in the paper.)</strong></p>
<p>The idea of exploring automatic ways to measure dissonance and use that information for different tasks is a novel and interesting one.</p>
<p><strong>Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)</strong></p>
<p>Sensory Dissonance can be computed automatically and initial experiments with a dataset of wind quintets shows some promise.</p>
<p><strong>Q17 (This paper is of award-winning quality.)</strong></p>
<p>No</p>
<p><strong>Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)</strong></p>
<p>Disagree</p>
<p><strong>Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)</strong></p>
<p>Weak reject</p>
<p><strong>Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)</strong></p>
<p>The paper is well written and clearly describes the concept proposed and the experiments conducted. The main idea is to compute automatically a measure of time-varying sensory dissonance and use visualization and statistical analysis to support/validate the approach. </p>
<p>Minor comments: 
There is no Figure 1e </p>
<p>Although dissonance has been extensively explored using synthetic and psychological stimuli, a more automatic approach as proposed by the authors is something interesting and potentially useful.</p>
<p>The experiments described show some promise but are limited by the dataset and a specific metric. They basically show that some approximation of dissonance can be computed and potentially be used. In my opinion that is not enough to warrant publication. Ideally the authors should have done some listening studies to verify that what they are measuring correlates to human perception and have some concrete examples of how it could be used for take selection and/or equalization as proposed. </p>
<p>I really like the idea but there needs to be more work for publication.</p>
<p><strong>Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid “weak accepts” or “weak rejects” if possible.)</strong></p>
<p>Weak accept</p>
<p><strong>Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))</strong></p>
<p>There is considerable agreement among the reviewers and the meta-reviewers regarding the work described in this paper. They all agree that the paper is well written and the idea of automating calculating dissonance has novelty. They also raise some important questions regarding limitations of the approach such as the experiments being conducted on a limited dataset, no listener/user study validation, and limited comparison of different approaches for example the pitch detection is somewhat unusual and it is unclear what are the benefits. </p>
<p>It is more of a pilot proof-of concept paper that show there is promise in the approach proposed. The future work and application are potentially very interesting. </p>
<p>Based on these considerations I recommend weak acceptance as we would have liked either more experimental evaluation of various parameters and algorithm choices or more exploration of potential applications or both.</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review1" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review1Example">
          <span class="font-weight-bold">Review 1:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Weak accept</li>
</ul>
<h4>Main Reviews</h4>
<p>The paper presents a case study of applying sensory dissonance (SD) models to multitrack audios of wind instrument recordings. The writing is clear and the exploratory findings are interesting, with careful analyses of the contributions of timbre, tuning and score to SD. Authors argue that SD would be helpful in automating studio recording tasks such as take selection and equalization, but these applications are delegated to future work. More specific comments follow.</p>
<p>In equation (1) there is a subscript missing in the term w(a_i,a_j).</p>
<p>Some motivation should be given for the use of the mean frequency (f_i+f_j)/2 in adapting the pairwise dissonance curve. It is understood that for f_i close to f_j the two sinusoids may be perceived as one with the mean frequency and a beating associated to (f_i-f_j)/2, but in this adaptation, the dissonance measures for either f_i or f_j would reflect this latter halved distance, which is equivalent to a warping in the dissonance kernel, making your actual dissonance curve wider (i.e. closer to Vassilakis'). If symmetry is the goal here, then a redefined expression for d(f_i,f_j) could be given, explaining how the frequency distance |f_i-f_j| is preserved.</p>
<p>The dependence of the SD values on the hypothesis that all instruments have harmonic spectra is very important, and in my opinion should appear earlier than the discussion of peak picking strategies (Section 2.2). Since it is a natural consequence of the case study scenario, mentioning it in the introduction (in the context of Fig. 1) would help the reader understand that this is a requirement of the method (and not an accidental property of an otherwise illustrative example).</p>
<p>In Fig. 4 there is a mention to blue rectangles, but it appears that only blue markings are visible.</p>
<p>The terminology "mean tuning variability" is very confusing for the numbers discussed after Fig. 7, because these variabilities are of SD values rather than of tuning. Maybe "mean SD variability" would help the reader avoid this confusion.</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review2" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review2Example">
          <span class="font-weight-bold">Review 2:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Strong accept</li>
</ul>
<h4>Main Reviews</h4>
<p>This paper introduces the concept of relative sensory dissonance (SD) as a tool for analyzing multi-track music recordings by quantifying how individual voices contribute to the dissonance. By exploring the relationship between SD and timbre, tuning, and score, the authors demonstrate that timbre is the dominant factor influencing SD, and they propose practical applications for music production.</p>
<p>The paper is well-written and easy to follow. The main contribution, the formalization of relative SD, is highly relevant for readers and ISMIR attendees. Additionally, the inclusion of a Python library for reproducibility significantly strengthens the contribution to the research community. Therefore, my overall recommendation is a strong accept.</p>
<p>My main suggestion, or perhaps a recommendation for future work, is that conducting listener studies to correlate SD with perceived dissonance would greatly enhance the proposed method.</p>
<p>Some minor issues:</p>
<ul>
<li>
<p>Section 1: The authors could briefly clarify whether dissonance in this context is the same as frequency masking, and if not, explain the differences.</p>
</li>
<li>
<p>Line 61: A brief definition and reference for salient tonal components would help non-expert readers.</p>
</li>
<li>
<p>Line 63: "we can calculate the overall SD (see Section 2)" </p>
</li>
<li>
<p>Line 121: What is the main motivation for modifying the kernels proposed in [15]?</p>
</li>
<li>
<p>Line 128: Define "high salience" </p>
</li>
<li>
<p>Line 182: Figure 1d?</p>
</li>
<li>
<p>Line 192: Define "DFT"</p>
</li>
<li>
<p>Line 199: The paper could benefit from a brief discussion of the consequences of assuming a quasi-harmonic overtone structure. How would SD perform with different types of overtone structures, and is dissonance still relevant for such musical sources?</p>
</li>
<li>
<p>Sections 2.2 and 3.1: Loudness normalization of Pv is mentioned, but it would be helpful to discuss why an audio-based loudness normalization approach would not achieve loudness invariance in the experiments. For example, Figure 6 shows that the loudness of the take has a significant effect on SD.</p>
</li>
<li>
<p>Figure 7: It is unclear which plots correspond to Dv,v and Dv,v_hat</p>
</li>
</ul>
        </div>
      </div>
    </div> 
  </div>
  
    <div id="review3" class="pp-card m-3 collapse">
      <div class="card-body">
        <div class="card-text">
          <div id="review3Example">
            <span class="font-weight-bold">Review 3:</span>
            <br>
            <ul>
<li><strong>Overall Evaluation</strong>: Weak reject</li>
</ul>
<h4>Main Reviews</h4>
<p>The paper presents the concept of "sensory dissonance," which measures the weighted difference between a pair of tonal peaks in the spectrum. Experiments are based on ChoraleBricks dataset, which plays ten Baroque chorales using different wind instruments. </p>
<p>It appears that the authors' main purpose is to provide an analysis of the proposed metric on this dataset, but I found the study is limited only to this particular dataset and the proposed metric, while there could be clearer benefits of using this metric or some practical applications that can benefit from the concept. </p>
<ul>
<li>
<p>First, the definition of the dissonance kernel (Figure 2) lacks supporting arguments. It is different from the existing methods, but it's not clearly defined, to begin with, (i.e., there's no equation), while it is not clearly explained why and how it has to be different from other methods. </p>
</li>
<li>
<p>Meanwhile, the comparison between two tonal peaks, f_i, and f_j, are based on the assumption that their harmonics are not involved in the comparison. For Figure 2, for example, there can be most probably another peak at 400Hz but how does the kernel capture it?</p>
</li>
<li>
<p>Actually, the proposed method relies heavily on peak detection algorithms proposed in Sec. 2.2, where harmonic components are taken into account separately. While the process appears to be okay-ish, it's not proven to be robust to various real-world recording environments, e.g., music with percussive instruments, where finding f_0 and tonal peaks is known to be difficult. Essentially, the ChoraleBricks dataset is a favorable one for this kind of concept but the algorithm seems to be limited only to the dataset. </p>
</li>
<li>
<p>There are a few hyperparameters that could greatly affect the accuracy of the algorithm, e.g., \gamma. </p>
</li>
<li>
<p>Most importantly, the proposed SD concept doesn't seem to be able to represent/correlate to perceived dissonance by human listeners. Provided results compare some results by replacing instruments, but given that the results are not correlated to the perceived dissonance, it is challenging to grasp the main insight the authors want to provide. </p>
</li>
</ul>
<p>Minor issues. </p>
<ul>
<li>Figure 1e doesn't exist, although it was referred in line 281. </li>
<li>Eq 1. w(a, a_j) -&gt; w(a_i, a_j)</li>
</ul>
          </div>
        </div>
      </div> 
    </div>
  
  
</div>


<script src="static/js/poster.js"></script>
<script src="static/js/video_selection.js"></script>

      </div>
    </div>
    
    

    <!-- Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "UA-");
    </script>

    <!-- Footer -->
    <footer class="footer bg-light p-4">
      <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2023 ISMIR Organization Committee</p>
      </div>
    </footer>

    <!-- Code for hash tags -->
    <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });
      });
    </script>
    <script src="static/js/lazy_load.js"></script>
    
  </body>
</html>