


<!DOCTYPE html>
<html lang="en">
  <head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <link rel="icon" type="image/png" sizes="32x32" href="static/images/ismir_tab_icon.png">

    <link rel="stylesheet" href="static/css/main.css" />
    <link rel="stylesheet" href="static/css/custom.css" />
    <link rel="stylesheet" href="static/css/lazy_load.css" />
    <link rel="stylesheet" href="static/css/typeahead.css" />
    <link rel="stylesheet" href="static/css/poster.css" />
    <link rel="stylesheet" href="static/css/music.css" />
    <link rel="stylesheet" href="static/css/piece.css" />


    <!-- <link rel="stylesheet" href="https://gitcdn.xyz/repo/wingkwong/jquery-chameleon/master/src/chameleon.min.css"> -->

    <!-- External Javascript libs  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

    <script
      src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
      integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
      crossorigin="anonymous"
    ></script>


    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>
    <!-- <script src="static/js/chameleon.js"></script> -->


    <!-- Library libs -->
    <script src="static/js/typeahead.bundle.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
      href="static/css/Lato.css"
      rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet" />
    <link
      href="static/css/Cuprum.css"
      rel="stylesheet"
    />
    <link
      href="static/css/Ambiant.css"
      rel="stylesheet"
    />
    <!-- <link href="static/css/chameleon.css" rel="stylesheet"/> -->
    <title>ISMIR 2025: LiLAC: A Lightweight Latent ControlNet for Musical Audio Generation</title>
    
<meta name="citation_title" content="LiLAC: A Lightweight Latent ControlNet for Musical Audio Generation" />

<meta name="citation_author" content="Tom Baker" />

<meta name="citation_author" content="Javier Nistal" />

<meta name="citation_publication_date" content="21-25 September 2025" />
<meta name="citation_conference_title" content="Ismir 2025 Hybrid Conference" />
<meta name="citation_inbook_title" content="Proceedings of the First MiniCon Conference" />
<meta name="citation_abstract" content="Text-to-audio diffusion models produce high-quality and diverse music but many, if not most, of the SOTA models lack the fine-grained, time-varying controls essential for music production. ControlNet enables attaching external controls to a pre-trained generative model by cloning and fine-tuning its encoder on new conditionings. However, this approach incurs a large memory footprint and restricts users to a fixed set of controls. We propose a lightweight, modular architecture that considerably reduces parameter count while matching ControlNet in audio quality and condition adherence. Our method offers greater flexibility and significantly lower memory usage, enabling more efficient training and deployment of independent controls. We conduct extensive objective and subjective evaluations and provide numerous audio examples on the accompanying website.&lt;br&gt;&lt;br&gt; &lt;b&gt;&lt;p align=&#34;center&#34;&gt;[Direct link to video]()&lt;/b&gt;" />

<meta name="citation_keywords" content="Melody and motives" />

<meta name="citation_keywords" content="Generative Tasks" />

<meta name="citation_keywords" content="Musical features and properties" />

<meta name="citation_keywords" content="Music generation" />

<meta name="citation_keywords" content="Harmony, chords and tonality" />

<meta name="citation_keywords" content="Music and audio synthesis" />

<meta name="citation_keywords" content="MIR tasks" />

<meta name="citation_pdf_url" content="" />
<meta id="yt-id" data-name= />
<meta id="bb-id" data-name= />


  </head>

  <body>
    <!-- NAV -->
    
    <!--

    ('tutorials.html', 'Tutorials'),
    ('index.html, 'Home'),
    ('special_meetings.html', 'Special Meetings'),
    -->

    <nav
      class="navbar sticky-top navbar-expand-lg navbar-light mr-auto"
      id="main-nav"
    >
      <div class="container">
        <a class="navbar-brand" href="https://ismir2025.ismir.net">
          <img
             class="logo" src="static/images/ismir_tabicon.png"
             height="auto"
             width="300px"
          />
        </a>
        
        <button
          class="navbar-toggler"
          type="button"
          data-toggle="collapse"
          data-target="#navbarNav"
          aria-controls="navbarNav"
          aria-expanded="false"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div
          class="collapse navbar-collapse text-right flex-grow-1"
          id="navbarNav"
        >
          <ul class="navbar-nav ml-auto">
            
            <li class="nav-item ">
              <a class="nav-link" href="calendar.html">Schedule</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="papers.html">Papers</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="music.html">Music</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="lbds.html">LBDs</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="industry.html?session=Platinum">Industry</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="jobs.html">Jobs</a>
            </li>
            
          </ul>
        </div>
      </div>
    </nav>
    

    
    <!-- User Overrides -->
     

    <div class="container">
      <!-- Tabs -->
      <div class="tabs">
         
      </div>
      <!-- Content -->
      <div class="content">
        

<!-- Title -->
<div class="pp-card m-3" style="">
  <div class="card-header">
    <h2 class="card-title main-title text-center" style="">
      P3-05: LiLAC: A Lightweight Latent ControlNet for Musical Audio Generation
    </h2>
    <h3 class="card-subtitle mb-2 text-muted text-center">
      
      <a href="papers.html?filter=authors&search=Tom Baker" class="text-muted"
        >Tom Baker</a
      >,
      
      <a href="papers.html?filter=authors&search=Javier Nistal" class="text-muted"
        >Javier Nistal</a
      >
      
    </h3>
    <p class="card-text text-center">
      <span class="">Subjects:</span>
      
      <a
        href="papers.html?filter=keywords&search=Melody and motives"
        class="text-secondary text-decoration-none"
        >Melody and motives</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Generative Tasks"
        class="text-secondary text-decoration-none"
        >Generative Tasks</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Musical features and properties"
        class="text-secondary text-decoration-none"
        >Musical features and properties</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Music generation"
        class="text-secondary text-decoration-none"
        >Music generation</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Harmony, chords and tonality"
        class="text-secondary text-decoration-none"
        >Harmony, chords and tonality</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Music and audio synthesis"
        class="text-secondary text-decoration-none"
        >Music and audio synthesis</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=MIR tasks"
        class="text-secondary text-decoration-none"
        >MIR tasks</a
      > 
      
    </p>
    <h4 class="text-muted text-center">
      Presented In-person, in Daejeon
    </h4>
    <h4 class="text-muted text-center">
      
        4-minute short-format presentation
      
    </h4>

  </div>
</div>
<div style="display: flex; justify-content: center;" class="btn-toolbar mt-4" role="toolbar">
  <div class="poster-buttons btn-group btn-group-toggle mb-3" data-toggle="buttons">
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#details">
      Abstract
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#paper">
      Paper
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#poster">
      Poster
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#video">
      Video
    </button>
    
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#meta-review">
        Meta
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review1">
        R1
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review2">
        R2
      </button>
      
        <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review3">
          R3
        </button>
      
    
    <!--  -->
  </div>
  
</div>
<div class="poster-content">
  <div id="details" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="abstractExample">
          <span class="font-weight-bold">Abstract:</span>
          <p>Text-to-audio diffusion models produce high-quality and diverse music but many, if not most, of the SOTA models lack the fine-grained, time-varying controls essential for music production. ControlNet enables attaching external controls to a pre-trained generative model by cloning and fine-tuning its encoder on new conditionings. However, this approach incurs a large memory footprint and restricts users to a fixed set of controls. We propose a lightweight, modular architecture that considerably reduces parameter count while matching ControlNet in audio quality and condition adherence. Our method offers greater flexibility and significantly lower memory usage, enabling more efficient training and deployment of independent controls. We conduct extensive objective and subjective evaluations and provide numerous audio examples on the accompanying website.<br><br> <b><p align="center"><a href="">Direct link to video</a></b></p>
        </div>
      </div>
      <p></p>
    </div>
  </div>
  <div id="paper" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "https://drive.google.com/file/d/1zvlWdyX_GU-1qc7THLmfWUEJ5IG5fXVL/preview#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="poster" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="video" class="collapse">
    
      <div  style="display: flex; justify-content: center;">
      <iframe src="" width="960" height="540" allow="encrypted-media" allowfullscreen></iframe>
      </div>
    
  </div>
  
  <div id="meta-review" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="metaReviewExample">
          <span class="font-weight-bold">Meta Review:</span>
          <br>
          <p><strong>Q2 ( I am an expert on the topic of the paper.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q3 ( The title and abstract reflect the content of the paper.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q4 (The paper discusses, cites and compares with all relevant related work.)</strong></p>
<p>Agree</p>
<p><strong>Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)</strong></p>
<p>Agree</p>
<p><strong>Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by “n” pages of references or ethical considerations, references are well formatted). If you selected “No”, please explain the issue in your comments.)</strong></p>
<p>Yes</p>
<p><strong>Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q9 (Scholarly/scientific quality: The content is scientifically correct.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view "novelty" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)</strong></p>
<p>Agree</p>
<p><strong>Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated “Strongly Agree” and “Agree” can be highlighted, but please do not penalize papers rated “Disagree” or “Strongly Disagree”. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)</strong></p>
<p>Disagree (Standard topic, task, or application)</p>
<p><strong>Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)</strong></p>
<p>Agree</p>
<p><strong>Q15 (Please explain your assessment of reusable insights in the paper.)</strong></p>
<p>the use of identity-initialized and zero-initialized conv layers for fine-tuning seems interesting</p>
<p><strong>Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)</strong></p>
<p>we can use lightweight conv layers to fine-tune a text-to-music generation model to learn new conditions</p>
<p><strong>Q17 (This paper is of award-winning quality.)</strong></p>
<p>No</p>
<p><strong>Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)</strong></p>
<p>Agree</p>
<p><strong>Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)</strong></p>
<p>Weak accept</p>
<p><strong>Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)</strong></p>
<p>This paper presents a lightweight alternative to the ControlNet approach for fine-tuning a text-to-music generation model to handle new conditions. Figure 1 clearly illustrates the idea, which involves learning identity-init and zero-init convolutional layers instead of using cloned encoder blocks. The authors implemented their method using Diff-a-Riff (432M parameters) as the backbone and tested variants with learnable parameters ranging from 32M to 64M, all smaller than the ControlNet baseline (165M). Both objective and subjective evaluations show that the proposed method matches the performance of the ControlNet baseline, though it does not surpass it.</p>
<p>Strengths:
* Fresh and interesting and interesting use of zero-init and identity-init convolutional layers for fine-tuning.
* Achieves similar performance to the Music ControlNet baseline while using about 1/4 to 1/2 of the learnable parameters.
* Solid set of experiments, with nice use of APA and MUSHRA evaluations.</p>
<p>Weakness:
* The backbone, Diff-a-Riff, is not open source.
* The reduction in trainable parameters is not substantial.
* It’s unclear if the idea of identity-init convolution is novel, as the authors don’t clarify this.
* No comparison with a relevant recent work [11] (Hou et al., ICASSP 2025), though this is understandable since [11] is new.
* Insufficient explanation of how their method differs from an important prior work, ControlNet-XS [13].
* Limited description of how their approach differs from another key prior work, Sketch2Sound [36], from a methodology point-of-view, although the authors did implement Sketch2Sound (labeled LiLAC* in Table 1).</p>
<p>Minor issues:
* References are not consistently formatted.</p>
<p><strong>Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid “weak accepts” or “weak rejects” if possible.)</strong></p>
<p>Accept</p>
<p><strong>Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))</strong></p>
<p>The reviewers are generally positive about this submission, noting issues like the problematic abstract opening and several points needing clarification. I encourage the authors to use the reviewers' feedback constructively to enhance the paper's quality while preparing for the camera-ready version.</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review1" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review1Example">
          <span class="font-weight-bold">Review 1:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Weak accept</li>
</ul>
<h4>Main Reviews</h4>
<p>This paper proposes a lightweight, modular variation of the method proposed in controlnet [9] and evaluate the approach using an example music generation model. The evaluation focusses on time-varying control over music generation—something lacking in current systems. The approach is validated through objective and subjective evaluations and demonstrated with audio examples online.</p>
<p>General remark:</p>
<p>Overall I feel that the method is explained clearly and motivated sufficiently. Experimental results are a bit limited. In table 1 all methods achieve APA == 1, and this independently of the fact that MSE is quite different. I wonder whether the APA metric is very helpful here. </p>
<p>Sugested modifications:</p>
<p>Table 2 appears a bit confusing. Section 5.3.4 explains the motivation of the experiment. </p>
<blockquote>
<p>306-309: achitecture is more susceptible to CLAP leakage—where over-specified control signals (e.g., chroma) can dominate or obscure CLAP’s condition.</p>
</blockquote>
<p>I am not sure to understand this. My question would be: what would one want? If you have to conflicting control signals (here CLAP and chroma), then there is a design problem. I do not think it is generally better if CLAP or chroma wins. So the arrows in table 2 do not seem justified, or at least I don't see why one would favor one over the other. One could for example say that the chroma feature is more specific and therefore it should overrule the more general specification (CLAP). It appears table 2 reflects an understanding that is the other way around. It would be helpful to understand why.</p>
<p>I am also a bit disturbed by the discussion of the SCA results in paragraph in 509-516. There we find that comparing to SAQ we the differences for SCA are clearer. However, in the table 2 it is the other way around. the differences are more pronounced for SAQ (max diff 4.2) then in SCA (max diff 2.7). </p>
<p>If I understand correctly the results displayed in table 4 col SCA, it appears that the control efficiency is still somewhat weak.</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review2" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review2Example">
          <span class="font-weight-bold">Review 2:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Strong accept</li>
</ul>
<h4>Main Reviews</h4>
<p>This paper builds off of ControlNet in introducing controllability to pre-trained generative music models. To reduce the number of parameters, the authors introduce light weight convolutions in the adaptor branch. Through the experiments, the authors show that (1) the quality of generation is not affected by their method, (2) effects of conflictive conditioning (conflicts between post-hoc conditions, i.e. chroma and pre-existing conditions, i.e. CLAP embeddings), (3) the effect of specificity of conditions, i.e. chroma and chord, and (4) the quality of the post-hoc conditions, i.e. chords and chroma. </p>
<p>I think the paper is well organized and does a good job of presenting a compelling argument for their light-weight controllable architecture. No mention has been made of the code being made available and I hope this could be done to help the community build on it. I also appreciate the extensive samples page. Below I have listed a few points I would like clarification on:</p>
<ol>
<li>
<p>Fig 1: I think the figure does a good job of communicating the idea. I was confused about a couple of things. (1) In the right subfigure, what is the zero convolution (in the center of the figure, along the y-axis) doing and how is the output of the convolution integrated with the output from the identity convolution? I also don’t see this zero convolution in the equation defined in line 168. Additionally, I think it would be helpful to have a legend for the colours indicating clearly which parts are trained and which aren’t.</p>
</li>
<li>
<p>Regarding conditioning:</p>
</li>
</ol>
<p>2 a. Training details, CFG on conditioning: Lines 229 - 234: My understanding is that CFG is being used on the conditions introduced in this paper, i.e. chroma and chords when training the adapter branch. The pre-trained model takes in CLAP embedding and the audio context as conditioning signals as well. Is CFG being used for these conditions as well during the training of the adapter branch? If so, this should be made clear. It is unclear to me what the “new c” refers to in line 232 as “c” is defined as “the condition c” (chroma and chord) that is introduced to the adapter branch in line 191. </p>
<p>2 b. Table 1: My understanding is that the audio context is not used for all the models in table 1 except for the Diff-a-Riff + Context model. Is this correct? I have comments based on if this assumption is correct or not:</p>
<p>If this is the case, then I’m a little confused what the relevance of APA is, since the model is not seeing the audio prompt at all. Perhaps the goal here is to show that even without the audio prompt the generated samples adhere to the context audio with just a chromagram input, if this is the case it should be explicitly stated. 
If my assumption is incorrect, then lines 384 - 386 seem to attribute alignment to the context to the chroma conditioning which wouldn’t make sense if the context is also seen as conditioning. </p>
<p>Additionally, I think cMSE being reported for the Diff-a-Riff models is similarly a little confusing. While I see the value of a standard metric across all models, I believe that if the models are not seeing the chroma conditioning during inference, it should be explicitly stated to avoid confusion. </p>
<p>2 c. Chroma MSE as a metric: I wonder if a per-frame chroma overlap would be a more reliable metric in the context of this work, similar to that used in Music ControlNet [1]. I think MSE introduces biases by penalizing notes that are closer less than notes that are further away, which could be problematic.</p>
<ol>
<li>
<p>LiLAC<em>: LiLAC</em> is said to be similar to Sketch2Sound [2]. However Sketch2Sound finetunes the backbone after adding the input condition. Is that being done for this baseline? If so, that should be explicitly stated since this is not the case for the other LiLAC models. </p>
</li>
<li>
<p>Samples page: I would be very interested to listen to samples from the misaligned conditions which I don’t believe are on the samples website right now. Especially examples which highlight the behavior stated in lines 436-442.</p>
</li>
<li>
<p>Minor corrections:
In section 3.2.1, the term ‘adaptor branch’ is never explicitly linked to G_l (..). The term is termed as ‘its cloned counterpart’, I would recommend making the link in the text.
There is a space missing between Table and 2 in line 411</p>
</li>
</ol>
<p>[1] Wu, Shih-Lun, et al. "Music controlnet: Multiple time-varying controls for music generation." IEEE/ACM Transactions on Audio, Speech, and Language Processing 32 (2024): 2692-2703.
[2] García, Hugo Flores, et al. "Sketch2sound: Controllable audio generation via time-varying signals and sonic imitations." ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2025.</p>
        </div>
      </div>
    </div> 
  </div>
  
    <div id="review3" class="pp-card m-3 collapse">
      <div class="card-body">
        <div class="card-text">
          <div id="review3Example">
            <span class="font-weight-bold">Review 3:</span>
            <br>
            <ul>
<li><strong>Overall Evaluation</strong>: Weak reject</li>
</ul>
<h4>Main Reviews</h4>
<h1>I. Strengths</h1>
<p>I'm highly supportive of the direction that this work explores, i.e., to shave off the resources/parameter needed to achieve fine-grained control. It is especially important for musical applications as different users might have drastically different types of controls they'd like to achieve. The lightweightness of these methods can meaningfully reduce the barrier to achieve plug-and-play controls for various use cases.</p>
<p>I also appreciate the exploration on conflicting controls (Sec 5.3.4 and Table 2) since these use cases are potentially central to musicians' creative inquiries, e.g., making playing techniques / phrases (controlled via fine-grained signals) that are technically impossible with some instruments (controlled via text / CLAP). And, it's nice to see that LiLAC seems to have an edge over the heavier-weight ControlNet.</p>
<p>Besides, some architectural ablations and a listening study are both conducted, which are commendable. However, I think overall the work is less than ready for publication in its current state.</p>
<h1>II. Weaknesses</h1>
<p>(W1) Missing analyses on memory/efficiency improvements
While the advantage on memory is a repeated claim in the manuscript, there isn't any comment or experiment supporting this claim. Also, Figure 1 sort of tells that the proposed design actually has more components, and hence total parameters, than ControlNet -- if I understand correctly, this would worsen inference-time speed and memory footprint (although it is likely still advantageous at training thanks to fewer trainable parameters hence fewer optimizer states).</p>
<p>Some speed/memory stats compared to ControlNet should be reported. Besides that, the authors could consider other ways where the proposed architecture could shine more efficiency-wise -- perhaps it's combining multiple fine-grained controls, since all controls can share the same encoder backbone, or demonstrating improved sample efficiency (i.e., the required amount of training data to make controls work) which implies better applicability in cases where controls are costly to obtain (e.g., require hand labeling).</p>
<p>(W2) Limited exploration on controls and output space
This work primarily explored single-instrument outputs and harmonic controls (chroma and chords), which I feel is a little narrow especially considering that prior works like Music ControlNet and DITTO have tackled multi-instrument audios and a wider range of controls (dynamics, rhythm, structure, etc.).</p>
<p>(W3) Insufficient motivation &amp; demonstration on additional experiments
It's great to see experiments that discuss the interactions/conflicts between heterogeneous controls which might contain overlapping information (Tables 2 &amp; 3). Yet, I think more could be done to better ground these explorations to applications, through, for example, motivating why "less bleeding" is important (and in what use cases) and/or providing generated samples to show that the differences are qualitatively substantial. I have these comments because from the metric gaps in Tables 2 and 3, it's difficult to judge how much better LiLAC is over ControlNet as these metrics are, after all, proxies to the actual desiderata, which in turn depend on specific application goals.</p>
<p>(W4) Problematic abstract opening
I would strongly advise revising the first sentence in abstract ("Text-to-audio ... lack fine-grained controls") as this is not true anymore. Plenty of citations in this manuscript also refute this statement.</p>
          </div>
        </div>
      </div> 
    </div>
  
  
</div>


<script src="static/js/poster.js"></script>
<script src="static/js/video_selection.js"></script>

      </div>
    </div>
    
    

    <!-- Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "UA-");
    </script>

    <!-- Footer -->
    <footer class="footer bg-light p-4">
      <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2023 ISMIR Organization Committee</p>
      </div>
    </footer>

    <!-- Code for hash tags -->
    <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });
      });
    </script>
    <script src="static/js/lazy_load.js"></script>
    
  </body>
</html>