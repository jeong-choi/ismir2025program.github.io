


<!DOCTYPE html>
<html lang="en">
  <head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <link rel="icon" type="image/png" sizes="32x32" href="static/images/ismir_tab_icon.png">

    <link rel="stylesheet" href="static/css/main.css" />
    <link rel="stylesheet" href="static/css/custom.css" />
    <link rel="stylesheet" href="static/css/lazy_load.css" />
    <link rel="stylesheet" href="static/css/typeahead.css" />
    <link rel="stylesheet" href="static/css/poster.css" />
    <link rel="stylesheet" href="static/css/music.css" />
    <link rel="stylesheet" href="static/css/piece.css" />


    <!-- <link rel="stylesheet" href="https://gitcdn.xyz/repo/wingkwong/jquery-chameleon/master/src/chameleon.min.css"> -->

    <!-- External Javascript libs  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

    <script
      src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
      integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
      crossorigin="anonymous"
    ></script>


    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>
    <!-- <script src="static/js/chameleon.js"></script> -->


    <!-- Library libs -->
    <script src="static/js/typeahead.bundle.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
      href="static/css/Lato.css"
      rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet" />
    <link
      href="static/css/Cuprum.css"
      rel="stylesheet"
    />
    <link
      href="static/css/Ambiant.css"
      rel="stylesheet"
    />
    <!-- <link href="static/css/chameleon.css" rel="stylesheet"/> -->
    <title>ISMIR 2025: FRETBOARDFLOW: A DUAL-MODEL APPROACH TO OPTIMIZE CHORD VOICINGS ON THE GUITAR FRETBOARD</title>
    
<meta name="citation_title" content="FRETBOARDFLOW: A DUAL-MODEL APPROACH TO OPTIMIZE CHORD VOICINGS ON THE GUITAR FRETBOARD" />

<meta name="citation_author" content="Marcel Vélez Vásquez" />

<meta name="citation_author" content="Mariëlle Baelemans" />

<meta name="citation_author" content="Jonathan Driedger" />

<meta name="citation_author" content="John Ashley Burgoyne" />

<meta name="citation_publication_date" content="21-25 September 2025" />
<meta name="citation_conference_title" content="Ismir 2025 Hybrid Conference" />
<meta name="citation_inbook_title" content="Proceedings of the First MiniCon Conference" />
<meta name="citation_abstract" content="Smoothly transitioning between chords on the guitar can be a major challenge for beginners, especially when they rely on just a few common chord diagrams. Yet many chords can be played in multiple ways (i.e., voicings), which can facilitate more comfortable hand movements on the fretboard. To address this, we present the FretboardFlow dataset, featuring 97 songs recorded with a hexaphonic pickup to capture multiple chord voicings as performed by expert guitarists. Our dataset builds upon the GuitarSet pipeline, incorporating a Python translation of Prätzlich et al&#39;s KAMIR algorithm for interference reduction, for automated hexaphonic transcriptions, thereby capturing harmonic structure and performance-driven voicing choices that implicitly reflect muscle memory and ergonomic habits, providing a rich resource for analyzing real-world chord transitions.
To predict the most convenient voicing within progressions, we propose a dual-model approach integrating both chord and voicing history, and loss functions well-suited to the flexible nature of voicings. Our research expands on prior chord prediction work by incorporating expert-recorded voicing variations of the same progressions and introducing a novel machine learning approach to fretboard navigation. We publicly release this dataset as a living resource to support data-driven exploration of context-aware guitar instructions.&lt;br&gt;&lt;br&gt; &lt;b&gt;&lt;p align=&#34;center&#34;&gt;[Direct link to video]()&lt;/b&gt;" />

<meta name="citation_keywords" content="Evaluation metrics" />

<meta name="citation_keywords" content="Applications" />

<meta name="citation_keywords" content="Musical features and properties" />

<meta name="citation_keywords" content="Harmony, chords and tonality" />

<meta name="citation_keywords" content="Novel datasets and use cases" />

<meta name="citation_keywords" content="Music training and education" />

<meta name="citation_keywords" content="Evaluation, datasets, and reproducibility" />

<meta name="citation_keywords" content="Machine learning/artificial intelligence for music" />

<meta name="citation_keywords" content="Knowledge-driven approaches to MIR" />

<meta name="citation_pdf_url" content="" />
<meta id="yt-id" data-name= />
<meta id="bb-id" data-name= />


  </head>

  <body>
    <!-- NAV -->
    
    <!--

    ('tutorials.html', 'Tutorials'),
    ('index.html, 'Home'),
    ('special_meetings.html', 'Special Meetings'),
    -->

    <nav
      class="navbar sticky-top navbar-expand-lg navbar-light mr-auto"
      id="main-nav"
    >
      <div class="container">
        <a class="navbar-brand" href="https://ismir2025.ismir.net">
          <img
             class="logo" src="static/images/ismir_tabicon.png"
             height="auto"
             width="300px"
          />
        </a>
        
        <button
          class="navbar-toggler"
          type="button"
          data-toggle="collapse"
          data-target="#navbarNav"
          aria-controls="navbarNav"
          aria-expanded="false"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div
          class="collapse navbar-collapse text-right flex-grow-1"
          id="navbarNav"
        >
          <ul class="navbar-nav ml-auto">
            
            <li class="nav-item ">
              <a class="nav-link" href="calendar.html">Schedule</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="papers.html">Papers</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="music.html">Music</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="lbds.html">LBDs</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="industry.html?session=Platinum">Industry</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="jobs.html">Jobs</a>
            </li>
            
          </ul>
        </div>
      </div>
    </nav>
    

    
    <!-- User Overrides -->
     

    <div class="container">
      <!-- Tabs -->
      <div class="tabs">
         
      </div>
      <!-- Content -->
      <div class="content">
        

<!-- Title -->
<div class="pp-card m-3" style="">
  <div class="card-header">
    <h2 class="card-title main-title text-center" style="">
      P7-04: FRETBOARDFLOW: A DUAL-MODEL APPROACH TO OPTIMIZE CHORD VOICINGS ON THE GUITAR FRETBOARD
    </h2>
    <h3 class="card-subtitle mb-2 text-muted text-center">
      
      <a href="papers.html?filter=authors&search=Marcel Vélez Vásquez" class="text-muted"
        >Marcel Vélez Vásquez</a
      >,
      
      <a href="papers.html?filter=authors&search=Mariëlle Baelemans" class="text-muted"
        >Mariëlle Baelemans</a
      >,
      
      <a href="papers.html?filter=authors&search=Jonathan Driedger" class="text-muted"
        >Jonathan Driedger</a
      >,
      
      <a href="papers.html?filter=authors&search=John Ashley Burgoyne" class="text-muted"
        >John Ashley Burgoyne</a
      >
      
    </h3>
    <p class="card-text text-center">
      <span class="">Subjects:</span>
      
      <a
        href="papers.html?filter=keywords&search=Evaluation metrics"
        class="text-secondary text-decoration-none"
        >Evaluation metrics</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Applications"
        class="text-secondary text-decoration-none"
        >Applications</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Musical features and properties"
        class="text-secondary text-decoration-none"
        >Musical features and properties</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Harmony, chords and tonality"
        class="text-secondary text-decoration-none"
        >Harmony, chords and tonality</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Novel datasets and use cases"
        class="text-secondary text-decoration-none"
        >Novel datasets and use cases</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Music training and education"
        class="text-secondary text-decoration-none"
        >Music training and education</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Evaluation, datasets, and reproducibility"
        class="text-secondary text-decoration-none"
        >Evaluation, datasets, and reproducibility</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Machine learning/artificial intelligence for music"
        class="text-secondary text-decoration-none"
        >Machine learning/artificial intelligence for music</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Knowledge-driven approaches to MIR"
        class="text-secondary text-decoration-none"
        >Knowledge-driven approaches to MIR</a
      > 
      
    </p>
    <h4 class="text-muted text-center">
      Presented In-person, in Daejeon
    </h4>
    <h4 class="text-muted text-center">
      
        4-minute short-format presentation
      
    </h4>

  </div>
</div>
<div style="display: flex; justify-content: center;" class="btn-toolbar mt-4" role="toolbar">
  <div class="poster-buttons btn-group btn-group-toggle mb-3" data-toggle="buttons">
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#details">
      Abstract
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#paper">
      Paper
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#poster">
      Poster
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#video">
      Video
    </button>
    
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#meta-review">
        Meta
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review1">
        R1
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review2">
        R2
      </button>
      
        <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review3">
          R3
        </button>
      
    
    <!--  -->
  </div>
  
</div>
<div class="poster-content">
  <div id="details" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="abstractExample">
          <span class="font-weight-bold">Abstract:</span>
          <p>Smoothly transitioning between chords on the guitar can be a major challenge for beginners, especially when they rely on just a few common chord diagrams. Yet many chords can be played in multiple ways (i.e., voicings), which can facilitate more comfortable hand movements on the fretboard. To address this, we present the FretboardFlow dataset, featuring 97 songs recorded with a hexaphonic pickup to capture multiple chord voicings as performed by expert guitarists. Our dataset builds upon the GuitarSet pipeline, incorporating a Python translation of Prätzlich et al's KAMIR algorithm for interference reduction, for automated hexaphonic transcriptions, thereby capturing harmonic structure and performance-driven voicing choices that implicitly reflect muscle memory and ergonomic habits, providing a rich resource for analyzing real-world chord transitions.
To predict the most convenient voicing within progressions, we propose a dual-model approach integrating both chord and voicing history, and loss functions well-suited to the flexible nature of voicings. Our research expands on prior chord prediction work by incorporating expert-recorded voicing variations of the same progressions and introducing a novel machine learning approach to fretboard navigation. We publicly release this dataset as a living resource to support data-driven exploration of context-aware guitar instructions.<br><br> <b><p align="center"><a href="">Direct link to video</a></b></p>
        </div>
      </div>
      <p></p>
    </div>
  </div>
  <div id="paper" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "https://drive.google.com/file/d/1m0sWG8o2RtCL0kYzTzeOZxOllkTh7E3c/preview#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="poster" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="video" class="collapse">
    
      <div  style="display: flex; justify-content: center;">
      <iframe src="" width="960" height="540" allow="encrypted-media" allowfullscreen></iframe>
      </div>
    
  </div>
  
  <div id="meta-review" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="metaReviewExample">
          <span class="font-weight-bold">Meta Review:</span>
          <br>
          <p><strong>Q2 ( I am an expert on the topic of the paper.)</strong></p>
<p>Agree</p>
<p><strong>Q3 ( The title and abstract reflect the content of the paper.)</strong></p>
<p>Agree</p>
<p><strong>Q4 (The paper discusses, cites and compares with all relevant related work.)</strong></p>
<p>Agree</p>
<p><strong>Q5 ( Please justify the previous choice (Required if “Strongly Disagree” or “Disagree” is chosen, otherwise write "n/a"))</strong></p>
<p>Avoid references in languages other than English, unless they absolutely contribute something no other source can. In this case, a relatively random news article about Chordify seems cited instead of a simple link to their homepage.</p>
<p><strong>Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)</strong></p>
<p>Agree</p>
<p><strong>Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by “n” pages of references or ethical considerations, references are well formatted). If you selected “No”, please explain the issue in your comments.)</strong></p>
<p>No</p>
<p><strong>Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q9 (Scholarly/scientific quality: The content is scientifically correct.)</strong></p>
<p>Agree</p>
<p><strong>Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view "novelty" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)</strong></p>
<p>Disagree</p>
<p><strong>Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated “Strongly Agree” and “Agree” can be highlighted, but please do not penalize papers rated “Disagree” or “Strongly Disagree”. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)</strong></p>
<p>Disagree (Standard topic, task, or application)</p>
<p><strong>Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)</strong></p>
<p>Disagree</p>
<p><strong>Q15 (Please explain your assessment of reusable insights in the paper.)</strong></p>
<p>Few insights are derived from the raw numeric data.</p>
<p><strong>Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)</strong></p>
<p>New data available for fret board voicing prediction, accompanied by experiments with an alternative architecture</p>
<p><strong>Q17 (This paper is of award-winning quality.)</strong></p>
<p>No</p>
<p><strong>Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)</strong></p>
<p>Disagree</p>
<p><strong>Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)</strong></p>
<p>Weak reject</p>
<p><strong>Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)</strong></p>
<h2>Summary</h2>
<p>The submission presents a new collection of fret board voicing data, obtained from hexaphonic recordings. Experiments on the prediction of chord voicing are conducted with dual branch models that process chord symbols and temporally preceding fretboard data separately before being merged by a linear layer. Those branch networks use either Bi-LSTMs or DeepGRUs, where the latter proves to be generally more performant according to a number of metrics.</p>
<h2>Positives</h2>
<ul>
<li>New data to work with is always welcome.</li>
<li>The proposed DeepGRU architecture is interesting, and the results are promising.</li>
</ul>
<h2>Negatives</h2>
<ul>
<li>A main issue is that the evaluation is strongly numeric, without providing any insight what this means for the actual real-world problem. What does a typical prediction look like? What type of errors are being made? Is the output perceived to be good enough in practice or completely unworkable? An application or human centric addition to the evaluation would be very insightful for this kind of problem without clear ground truth.</li>
<li>The dataset could be better curated. Now it is dominated by a single person who recorded more than the rest combined. None of the challenges/opportunities coming from multiple annotators are currently explored. Only hard songs are recorded by multiple people, and the effect of difficulty, number of variations per song per player, etc. is not examined.</li>
<li>The whole discussion of "proper scoring rule" does not seem to lead to an approach that is different from earlier work. It seems a justification for a non-existent problem.</li>
<li>The term "history length" suddenly appears on line 421 and plays a prominent role in the experiments, but is not properly explained. I interpret it as a strict cut-off of the input to the recurrent layers, but see no obvious reason why that is necessary and a contradiction with the justification of using recurrent layers. At minimum a comparison with using the complete history would be needed.</li>
<li>Both datasets are only used in isolation with cross-validation, whereas there is an opportunity to do cross-dataset evaluation.</li>
<li>Representing a fretboard as a binary matrix, where the fact that only the highest fretted note on a string produces sound is not explicitly encoded, seems subobtimal compared to an integer vector representation. At least something worth exploring.</li>
</ul>
<h2>Overall</h2>
<p>Given that the dataset is described as a living dataset, and extended analysis and experiments would be welcome, the current submission feels very much like work in progress. Addressing the raised points would lead to a very valuable contribution to the field, but this year's ISMIR might be too soon for that.</p>
<h2>Additional comments</h2>
<ul>
<li>The supplementary material should have been submitted as a separate file, not as part of the main paper.</li>
<li>Confusing usage of both "Amsterdam Playability Dataset" and "Billboard Playability Dataset" to refer to what seems to be the same thing.</li>
<li>The references could use cleaning up, e.g. [11] is missing its publication venue and there's a typo in [23] ("toplay").</li>
<li>Mentioning first assigning on l. 224 and then selecting on l. 238 of the songs is unnecessarily confusing. I interpret it as a free selection out of a predefined subset, but this should be explained once instead of splitting this information over multiple paragraphs.</li>
</ul>
<p><strong>Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid “weak accepts” or “weak rejects” if possible.)</strong></p>
<p>Weak accept</p>
<p><strong>Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))</strong></p>
<p>All reviewers appreciate the work put into collecting the data for this submission and making it publicly available. The experiments show promise, though would benefit from more human interpretation and insights. We recommend looking at the individual reviews to address the points made there. Do keep expanding this resource!</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review1" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review1Example">
          <span class="font-weight-bold">Review 1:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Strong accept</li>
</ul>
<h4>Main Reviews</h4>
<p>Overall, this is a very nicely written paper, and the very first time I see the task of chord voicing estimation being formalized. 
While this is obviously a very niche topic, with little possibility to expand the learnings from this paper to other MIR tasks, I believe it does qualify as a valid and novel research area.</p>
<p>The dataset collected is, in itself, an impressive contribution. As a guitar player myself, I do understand the value in widening the possibilities of chord voicings to facilitate interpretation and I look forward to analyzing the various rendering of the same piece that were collected.</p>
<p>The experimental part of chord voicing estimation through machine learning technique is a bit more confusing. It's not clear from Table 1 (yet alone from table S1) what elements of the proposed method are most effective to the task. Metrics seem to be hard to jointly optimize, and while authors choose to highlight the test loss, I would intuitively have thought that ease of transition and playability should be most important here.</p>
<p>The discussion on the MSE being a "proper scoring rule" is not very convincing (tbh it reads a bit like a posteriori justification of why it should be the metric we trust the most). I am no expert in Brier score, but I believe it is mostly applicable in a binary setup. In a multi-label case such as the task at hand, and considering the rather limited amount of samples and the large imbalances, I wouldn't put too much trust on it being an unbiased estimator of actual expert annotation probabilities</p>
<p>We're also lacking in some respect an understanding of the type of predictions that these various systems make. There are several occurrences in the paper where author state that changing from open chords to barre can be perceived as suboptimal. Are there some configurations that effectively limits such transitions more than others? Maybe a quick qualitative analysis of some of the results could have been insightful.</p>
<p>details</p>
<p>Abstract
* the last sentence (l14) is odd and lacks a verb
* "data-driven exploration of personalized guitar instruction." I'm a bit puzzled by "personalized" as I see no evidence in the paper about this</p>
<p>Introduction
* Quick note on Ultimate Guitar, users do still have comments and ratings to help decide which versions they might find more suitable
* the transition on l50 is a bit weird, what's the relation with the previous paragraph?
*"up to five voicing variations for each of 35 songs," -&gt; for 35 songs</p>
<p>Section 5
could a simpler approach, optimizing for hand movement (e.g a wasserstein distance on the fretboard) be also tested here? </p>
<p>l366 this bidirectionality mirrorring player's choices seems reasonable but is it also due to lower perfs of the causal models?</p>
<p>l400 would achived -&gt; would be achived</p>
<p>Bibliography
* The link chosen for chordify (2) is a dutch blog post celebrating their 10 year anniversary.. maybe a wikipedia link be more adapted
* references 11, 12, 18 are incomplete</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review2" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review2Example">
          <span class="font-weight-bold">Review 2:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Weak accept</li>
</ul>
<h4>Main Reviews</h4>
<h1>Overall Comments</h1>
<p>A lot of work has gone into this paper, from building a new dataset to comparing it to an existing one, as well as using state-of-the-art methods and introducing new ones. 
As detailed below, my two main remarks are on the lack of information regarding the recorded guitarists (an Ethical Statement would be particularly relevant too) and the tendency to compare datasets only on ratios even though they are so different in size.
Apart from that, the paper is well written and could surely help future research in the field.</p>
<h2>Refinements required</h2>
<ul>
<li>l14-15: how can one capture muscle memory through microphone recordings?</li>
<li>l151-156: I agree that it's important to have varied voicings, but is it really lacking in DadaGP?</li>
<li>l208: It's great to release the dataset! What will the licence be?</li>
<li>l236: making a web interface is a lot of work, you could show it!</li>
<li>l249-259: I think the participants' presentation should arrive earlier. We also lack a lot of information about them, like how they were recruited (was it approved by an IRB?), were they paid, what's their musical background, etc. It would also be interesting discussing why the number of recordings is so unbalanced between participants.</li>
<li>l290: rhythm* data quantised? And what is a quarter-measure interval? Is it a quarter note? How does it work in time signatures other than 4-4?</li>
<li>l301-312: the criticisms made towards DadaGP are a bit fallacious. I don't think it really makes sense comparing ratios between the two datasets and not discussing the fact that DadaGP is at least ten times larger</li>
<li>Figure 2: Same comment</li>
<li>l366-367 &amp; 369-370: the claim that a guitarist thinks bidirectionally should be backed up/explained (or removed)</li>
<li>Table1 has a lot of values, maybe only a subset is required in the main text, especially if not everything is discussed</li>
<li>Results analysis: I feel the discussion section lacks a qualitative analysis of the predictions to better understand why the proposed models generally perform worse on string-fret F1 even though the loss is better. </li>
</ul>
<h2>Typesetting and Language issues</h2>
<ul>
<li>l4: the use of single is not very clear, should be rephrased</li>
<li>l158: natural rather than naturalistic?</li>
<li>Some references are incomplete: at least [11] [12] [18]</li>
</ul>
        </div>
      </div>
    </div> 
  </div>
  
    <div id="review3" class="pp-card m-3 collapse">
      <div class="card-body">
        <div class="card-text">
          <div id="review3Example">
            <span class="font-weight-bold">Review 3:</span>
            <br>
            <ul>
<li><strong>Overall Evaluation</strong>: Weak accept</li>
</ul>
<h4>Main Reviews</h4>
<p>The paper can serve a valuable insight and dataset in the ISMIR community. The contribution is above acceptance threshold and will help future work on performance‑aware voicing, auto‑arrangement, and guitar pedagogy.</p>
<p>Strengths:
Novel Dataset. While similar datasets exist, the introduction of the FretboardFlow dataset addresses a clear gap. It offers a well-structured and resourceful dataset for guitar chord voicings and progressions. With potential future extensions, it can serve as a strong resource for modeling realistic chord transitions.</p>
<p>Practical Application. The dual-model architecture, which integrates chord-symbol sequences and voicing history, is well-motivated and offers a data-driven solution to challenges in chord position selection.</p>
<p>Citations: The paper cites the most relevant prior work in the area and actively addresses potential gaps the reader might have, usually backed by proper citation. </p>
<p>Suggestions:
Dual path concats and then passes through a linear layer. Fusion or attention variants can be tried in future work.</p>
<p>The comparison between the DadaGP dataset and FretboardFlow isn't fully direct. You would need a separately collected test set with well-defined structure to properly evaluate and present a table of losses on that set. Claims that DadaGP scores better due to its simpler format are reasonable, but they require experimental backing. The augmentation shifting you're applying may already address many of the issues DadaGP has with uniformity and chord variation. You could try test the models trained on FretboardFlow on a smaller subset of DadaGP and the other way around to confirm your claims. </p>
<p>For completeness you need to explicitly list all types of augmentation you are using, rather than only referencing related work that applies similar methods. (One augmentation you could possibly consider in future work: for chords that span 5 or more strings, you could randomly sample 3 notes, and then for the next chord, sample the 3 closest notes. This will maybe create the variations that are missing from DadaGP and also add more variations to your dataset).</p>
<p>Human validity. The loss metric is automated, a small perceptual/user study with guitarists would strengthen claims.</p>
          </div>
        </div>
      </div> 
    </div>
  
  
</div>


<script src="static/js/poster.js"></script>
<script src="static/js/video_selection.js"></script>

      </div>
    </div>
    
    

    <!-- Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "UA-");
    </script>

    <!-- Footer -->
    <footer class="footer bg-light p-4">
      <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2023 ISMIR Organization Committee</p>
      </div>
    </footer>

    <!-- Code for hash tags -->
    <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });
      });
    </script>
    <script src="static/js/lazy_load.js"></script>
    
  </body>
</html>