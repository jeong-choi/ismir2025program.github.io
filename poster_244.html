


<!DOCTYPE html>
<html lang="en">
  <head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <link rel="icon" type="image/png" sizes="32x32" href="static/images/ismir_tab_icon.png">

    <link rel="stylesheet" href="static/css/main.css" />
    <link rel="stylesheet" href="static/css/custom.css" />
    <link rel="stylesheet" href="static/css/lazy_load.css" />
    <link rel="stylesheet" href="static/css/typeahead.css" />
    <link rel="stylesheet" href="static/css/poster.css" />
    <link rel="stylesheet" href="static/css/music.css" />
    <link rel="stylesheet" href="static/css/piece.css" />


    <!-- <link rel="stylesheet" href="https://gitcdn.xyz/repo/wingkwong/jquery-chameleon/master/src/chameleon.min.css"> -->

    <!-- External Javascript libs  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

    <script
      src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
      integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
      crossorigin="anonymous"
    ></script>


    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>
    <!-- <script src="static/js/chameleon.js"></script> -->


    <!-- Library libs -->
    <script src="static/js/typeahead.bundle.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
      href="static/css/Lato.css"
      rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet" />
    <link
      href="static/css/Cuprum.css"
      rel="stylesheet"
    />
    <link
      href="static/css/Ambiant.css"
      rel="stylesheet"
    />
    <!-- <link href="static/css/chameleon.css" rel="stylesheet"/> -->
    <title>ISMIR 2025: Estimating Musical Surprisal from Audio in Autoregressive Diffusion Model Noise Spaces</title>
    
<meta name="citation_title" content="Estimating Musical Surprisal from Audio in Autoregressive Diffusion Model Noise Spaces" />

<meta name="citation_author" content="Mathias Rose Bjare" />

<meta name="citation_author" content="Stefan Lattner" />

<meta name="citation_author" content="Gerhard Widmer" />

<meta name="citation_publication_date" content="21-25 September 2025" />
<meta name="citation_conference_title" content="Ismir 2025 Hybrid Conference" />
<meta name="citation_inbook_title" content="Proceedings of the First MiniCon Conference" />
<meta name="citation_abstract" content="Recently, the information content (IC) of predictions from a Generative Infinite-Vocabulary Transformer (GIVT) has been used to model musical expectancy and surprisal in audio. We investigate the effectiveness of such modelling using IC calculated with autoregressive diffusion models (ADMs). We empirically show that IC estimates of models based on two different diffusion ordinary differential equations (ODEs) describe diverse data better, in terms of negative log-likelihood, than a GIVT. We evaluate diffusion model IC&#39;s effectiveness in capturing surprisal aspects by examining two tasks: (1) capturing monophonic pitch surprisal, and (2) detecting segment boundaries in multi-track audio. In both tasks, the diffusion models match or exceed the performance of a GIVT. We hypothesize that the surprisal estimated at different diffusion process noise levels corresponds to the surprisal of music and audio features present at different audio granularities. Testing our hypothesis, we find that, for appropriate noise levels, the studied musical surprisal tasks&#39; results improve. Code is provided on github.com/SonyCSLParis/audioic.&lt;br&gt;&lt;br&gt; &lt;b&gt;&lt;p align=&#34;center&#34;&gt;[Direct link to video]()&lt;/b&gt;" />

<meta name="citation_keywords" content="Knowledge-driven approaches to MIR" />

<meta name="citation_keywords" content="MIR fundamentals and methodology" />

<meta name="citation_keywords" content="Musical features and properties" />

<meta name="citation_keywords" content="Machine learning/artificial intelligence for music" />

<meta name="citation_keywords" content="Music signal processing" />

<meta name="citation_pdf_url" content="" />
<meta id="yt-id" data-name= />
<meta id="bb-id" data-name= />


  </head>

  <body>
    <!-- NAV -->
    
    <!--

    ('tutorials.html', 'Tutorials'),
    ('index.html, 'Home'),
    ('special_meetings.html', 'Special Meetings'),
    -->

    <nav
      class="navbar sticky-top navbar-expand-lg navbar-light mr-auto"
      id="main-nav"
    >
      <div class="container">
        <a class="navbar-brand" href="https://ismir2025.ismir.net">
          <img
             class="logo" src="static/images/ismir_tabicon.png"
             height="auto"
             width="300px"
          />
        </a>
        
        <button
          class="navbar-toggler"
          type="button"
          data-toggle="collapse"
          data-target="#navbarNav"
          aria-controls="navbarNav"
          aria-expanded="false"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div
          class="collapse navbar-collapse text-right flex-grow-1"
          id="navbarNav"
        >
          <ul class="navbar-nav ml-auto">
            
            <li class="nav-item ">
              <a class="nav-link" href="calendar.html">Schedule</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="papers.html">Papers</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="music.html">Music</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="lbds.html">LBDs</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="industry.html?session=Platinum">Industry</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="jobs.html">Jobs</a>
            </li>
            
          </ul>
        </div>
      </div>
    </nav>
    

    
    <!-- User Overrides -->
     

    <div class="container">
      <!-- Tabs -->
      <div class="tabs">
         
      </div>
      <!-- Content -->
      <div class="content">
        

<!-- Title -->
<div class="pp-card m-3" style="">
  <div class="card-header">
    <h2 class="card-title main-title text-center" style="">
      P6-08: Estimating Musical Surprisal from Audio in Autoregressive Diffusion Model Noise Spaces
    </h2>
    <h3 class="card-subtitle mb-2 text-muted text-center">
      
      <a href="papers.html?filter=authors&search=Mathias Rose Bjare" class="text-muted"
        >Mathias Rose Bjare</a
      >,
      
      <a href="papers.html?filter=authors&search=Stefan Lattner" class="text-muted"
        >Stefan Lattner</a
      >,
      
      <a href="papers.html?filter=authors&search=Gerhard Widmer" class="text-muted"
        >Gerhard Widmer</a
      >
      
    </h3>
    <p class="card-text text-center">
      <span class="">Subjects:</span>
      
      <a
        href="papers.html?filter=keywords&search=Knowledge-driven approaches to MIR"
        class="text-secondary text-decoration-none"
        >Knowledge-driven approaches to MIR</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=MIR fundamentals and methodology"
        class="text-secondary text-decoration-none"
        >MIR fundamentals and methodology</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Musical features and properties"
        class="text-secondary text-decoration-none"
        >Musical features and properties</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Machine learning/artificial intelligence for music"
        class="text-secondary text-decoration-none"
        >Machine learning/artificial intelligence for music</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Music signal processing"
        class="text-secondary text-decoration-none"
        >Music signal processing</a
      > 
      
    </p>
    <h4 class="text-muted text-center">
      Presented In-person, in Daejeon
    </h4>
    <h4 class="text-muted text-center">
      
        4-minute short-format presentation
      
    </h4>

  </div>
</div>
<div style="display: flex; justify-content: center;" class="btn-toolbar mt-4" role="toolbar">
  <div class="poster-buttons btn-group btn-group-toggle mb-3" data-toggle="buttons">
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#details">
      Abstract
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#paper">
      Paper
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#poster">
      Poster
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#video">
      Video
    </button>
    
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#meta-review">
        Meta
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review1">
        R1
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review2">
        R2
      </button>
      
        <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review3">
          R3
        </button>
      
    
    <!--  -->
  </div>
  
</div>
<div class="poster-content">
  <div id="details" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="abstractExample">
          <span class="font-weight-bold">Abstract:</span>
          <p>Recently, the information content (IC) of predictions from a Generative Infinite-Vocabulary Transformer (GIVT) has been used to model musical expectancy and surprisal in audio. We investigate the effectiveness of such modelling using IC calculated with autoregressive diffusion models (ADMs). We empirically show that IC estimates of models based on two different diffusion ordinary differential equations (ODEs) describe diverse data better, in terms of negative log-likelihood, than a GIVT. We evaluate diffusion model IC's effectiveness in capturing surprisal aspects by examining two tasks: (1) capturing monophonic pitch surprisal, and (2) detecting segment boundaries in multi-track audio. In both tasks, the diffusion models match or exceed the performance of a GIVT. We hypothesize that the surprisal estimated at different diffusion process noise levels corresponds to the surprisal of music and audio features present at different audio granularities. Testing our hypothesis, we find that, for appropriate noise levels, the studied musical surprisal tasks' results improve. Code is provided on github.com/SonyCSLParis/audioic.<br><br> <b><p align="center"><a href="">Direct link to video</a></b></p>
        </div>
      </div>
      <p></p>
    </div>
  </div>
  <div id="paper" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "https://drive.google.com/file/d/1IcrPek0vcQdYgt20GuagKFGkiyeQHonp/preview#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="poster" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="video" class="collapse">
    
      <div  style="display: flex; justify-content: center;">
      <iframe src="" width="960" height="540" allow="encrypted-media" allowfullscreen></iframe>
      </div>
    
  </div>
  
  <div id="meta-review" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="metaReviewExample">
          <span class="font-weight-bold">Meta Review:</span>
          <br>
          <p><strong>Q2 ( I am an expert on the topic of the paper.)</strong></p>
<p>Agree</p>
<p><strong>Q3 ( The title and abstract reflect the content of the paper.)</strong></p>
<p>Agree</p>
<p><strong>Q4 (The paper discusses, cites and compares with all relevant related work.)</strong></p>
<p>Agree</p>
<p><strong>Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)</strong></p>
<p>Disagree</p>
<p><strong>Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by “n” pages of references or ethical considerations, references are well formatted). If you selected “No”, please explain the issue in your comments.)</strong></p>
<p>Yes</p>
<p><strong>Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q9 (Scholarly/scientific quality: The content is scientifically correct.)</strong></p>
<p>Agree</p>
<p><strong>Q10 (Please justify the previous choice (Required if “Strongly Disagree” or “Disagree” is chose, otherwise write "n/a"))</strong></p>
<p>see below</p>
<p><strong>Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view "novelty" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)</strong></p>
<p>Disagree</p>
<p><strong>Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated “Strongly Agree” and “Agree” can be highlighted, but please do not penalize papers rated “Disagree” or “Strongly Disagree”. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)</strong></p>
<p>Agree (Novel topic, task, or application)</p>
<p><strong>Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)</strong></p>
<p>Agree</p>
<p><strong>Q15 (Please explain your assessment of reusable insights in the paper.)</strong></p>
<p>The paper proposes a new way for modeling musical suprise which might open up interesting direction of future research (not really discussed in the paper)</p>
<p><strong>Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)</strong></p>
<p>information content (IC) computed via neural ODE-based likelihood estimation in autoregressive diffusion models
can be used to predict monophonic pitch surprisal and segment boundaries in multi-track audio</p>
<p><strong>Q17 (This paper is of award-winning quality.)</strong></p>
<p>No</p>
<p><strong>Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)</strong></p>
<p>Agree</p>
<p><strong>Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)</strong></p>
<p>Weak reject</p>
<p><strong>Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)</strong></p>
<p>There is a very interesting review of related literature that is well presented (given the limited amount of space). This alone provides a value to the readers.</p>
<p>However, the paper could be very much improved in terms accessibility. To begin with, the abstact already contains acronyms (IC, ODEs) that should not be taken for granted but be properly introduced. The same issue is repeated in the text (GIVT, EEG, D-REX, MEG, GNN, MLP). Further, the presentation is very heavy on math. I understand this comes with the approach chosen. Still I feel very little effort was made to explain things. It rather felt like the descriptions were just thrown at the reader with the content being very obvious.</p>
<p>I am very familiar with diffusion and score models and also with Normalizing Flow approaches. But I never really learned about ODEs and really struggled to follow the method section of the paper. I am sure it will be much worse for the average ISMIR attendee. There has to be a better way to present this.</p>
<p>NLL was used to measure how well the music data is decribed by the model. This was taken as a given. A short discussion would be nice to justify this choice. Are there any alternatives?</p>
<p>There is also no human listening / perception study. </p>
<p>Given the hypothesis from l231, shouldn't one be able to actually hear the pitch be better preserved than the timbre at moderate noise levels? It appears as if the noisy outputs were not actually listened to. Also, some listening examples for the readers would have been nice - for this specifically but also in general.</p>
<p>minor points:
- In Eq. 1, it is unclear what the t in dz(t) means.
- l63 differ in how they noise details
- l231 4.5we
- Reproducibility limited because one of the datasets used in the experiments is private. The authors should at least consider sharing the Music2Latent embeddings.</p>
<p><strong>Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid “weak accepts” or “weak rejects” if possible.)</strong></p>
<p>Weak accept</p>
<p><strong>Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))</strong></p>
<p>The review generally argue in favor of accepting the paper. Still, several issues are raised that need to be addressed for the final revision. 
In particular, to increase the value of this paper to the ISMIR community, the clairty and accessibility of the descriptions should be improved. Furthermore, adding listening examples / studies would be appreciated.</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review1" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review1Example">
          <span class="font-weight-bold">Review 1:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Weak accept</li>
</ul>
<h4>Main Reviews</h4>
<p>The paper proposes a framework for estimating musical surprisal directly from audio using autoregressive diffusion models in a continuous latent space, allowing surprisal to be modeled without symbolic input or fixed output distributions. The key innovation lies in using diffusion-based negative log-likelihood (information content) as a frame-level signal of musical unexpectedness, while leveraging noise-level control to modulate abstraction — from timbre-sensitive to pitch-sensitive to structure-sensitive representations. This enables the model to isolate different layers of musical information depending on the noise level, a novel idea with implications for multi-layered music understanding. The paper is well-structured and technically sound, with a clear modeling pipeline and evaluations that span model fitness (NLL), pitch surprise correlation (IDyOM), timbre invariance, and segment boundary detection. These experiments offer meaningful insight into how noise levels affect the perceived resolution of musical content, and position the work as a valuable contribution to audio-based MIR.</p>
<p>Although I believe the paper introduces a novel and promising direction, I would like to share a few points that could be clarified or expanded, along with some broader suggestions to strengthen future iterations.</p>
<p>(1.) Definition of “musical surprise”:
While the paper consistently refers to its output as “musical surprisal,” the measured quantity is more accurately a form of information-theoretic surprise or prediction error. Without perceptual evaluation or alignment with listener-annotated moments of musical salience (e.g., drops, climaxes, transitions), the use of “surprise” risks being misleading. I encourage the authors to either clarify the distinction or better ground the term in listener-centered or cognitive models.</p>
<p>(2.) Evaluation of noise-level abstraction hypothesis:
One of the paper’s most compelling ideas is that adjusting the noise level changes the model’s sensitivity to different musical dimensions (timbre, pitch, structure). However, this claim is not evaluated systematically — the results are more observational than diagnostic. I suggest including controlled probes or attribution studies to show what kinds of musical features dominate IC estimates at different noise levels.</p>
<p>(3.) Limited qualitative musical examples:
The evaluation relies largely on synthetic melodies and statistical metrics (e.g., correlation with IDyOM, boundary F1). To support broader MIR relevance, I suggest including qualitative case studies using real music (e.g., genre transitions, DJ cuts, expressive phrasing) to demonstrate how IC curves reflect human-perceived surprise or structure.</p>
<p>(4.) No human perceptual validation:
Given that the paper positions its contribution around “musical surprise,” the absence of even a small-scale listener study or alignment with behavioral annotations is a missed opportunity. A few listener-marked surprise points would significantly strengthen the perceptual claim and help calibrate the model’s outputs to real human responses.</p>
<p>(5.) Interpretability of IC spikes:
The IC signal is promising, but what do its spikes mean musically? Are they driven by pitch jumps, texture changes, or dynamics? Introducing simple attribution (e.g., correlation with chroma shifts, energy flux, or timbral novelty) could help demystify the model’s behavior and make it more interpretable for downstream use.</p>
<p>Beyond these main points, I would like to suggest a few broader ideas for future work:</p>
<p>(1.) While the paper focuses on high-IC moments as indicators of surprise, low-IC regions may be equally valuable as indicators of regularity. These could correspond to loops, grooves, or repeated motifs, enabling tasks such as loop extraction, pattern discovery, or motif clustering. Treating surprisal as a continuous signal — where both peaks and plateaus are musically meaningful — could unlock broader applications in structure-aware generation, DJ-style segmentation, or form analysis.</p>
<p>(2.) Rather than treating surprisal as a scalar, this work opens the door to multi-dimensional surprisal — for example, separate IC signals for pitch, timbre, and rhythm, each derived via tailored noise-level filters. I encourage the authors to explore or at least discuss this possibility.</p>
<p>(3.) Since diffusion models are commonly used in generation, one exciting direction would be to use IC as a control signal in generative models — for example, regulating musical surprise over time, or aligning generative transitions with IC peaks. This could bridge analysis and creative synthesis.</p>
<p>Overall, I believe this paper contributes a valuable and technically novel framework for unsupervised surprisal estimation in music audio. It opens important conversations around abstraction control, audio-based expectation modeling, and the relationship between statistical prediction and perceptual response. However, I am assigning a Weak Accept rather than a Strong Accept because two key aspects remain underdeveloped: (1) the definition and framing of “musical surprise” lacks grounding in perceptual or listener-based evidence, and (2) the abstraction-control claim via noise level is intriguing but not yet supported with systematic, feature-specific evaluation. Still, the direction is promising, the methodology is solid, and I believe it should be included at ISMIR as a strong foundation for future work.</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review2" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review2Example">
          <span class="font-weight-bold">Review 2:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Strong accept</li>
</ul>
<h4>Main Reviews</h4>
<p>Strengths:
1. Overall, the writing is very coherent and logically clear, friendly for even an audience not so close to the field (like me).
2. Section 4 shows comprehensive experiments, and the logic for experiment design looks solid to me. The discussions show very clear relationships between each table of results and corresponding findings /claims with adequate justifications.
3. The research perspective is interesting and insightful, especially from 4.5 to 4.6, showing the qualities and potential of diffusion noise space in the music audio domain.</p>
<p>Weakness-ish:
1. Small writing suggestions: in the abstract and conclusion, the authors like to refer to "previous models" or "alternative methods" without specifically pointing out what they are comparing against exactly, I find it more useful if they make it clear about the difference between diffusion and past models in both sections.
2. Line 462, isn't the highest correlation for EDM t=20 instead of 50, according to my understanding of Table 4? I can understand that this sentence is trying to make a connection to Table 3, but I feel it will be more helpful if they point this out directly so the audience will know where to look for the evidence.
3. In sec 4.6, line 482 says "extra peaks not attributed to segment boundaries", interesting findings, but I wonder if the authors can offer some insights/analysis on why and how this appears, especially when in line 77 they say "peaks related to other musical events". I also wonder if there's a relationship between IC estimations and segmentation performance across "t," possibly helpful for explaining any findings like line 487.
4. Out of curiosity for future work, I wonder if the contribution to IC estimation is coming from timbre invariance. How would other models that specifically extract pitch information from audio perform in such tasks, compared to these methods? 
5. In Figure 1, it's visually not intuitive for me to compare blue line with others across the two plots, at a glance all colors look similar between above and below, if the point is to say blue is more sensitive to timbre change compared to other colors, might be good to overlay the same approach in different samples vs. now different approaches overlaid in same sample.</p>
<p>Summary:
Within my bias in a field that I'm not so familiar with, I believe this work is technically solid and insightful for our field with some minor improvements.</p>
        </div>
      </div>
    </div> 
  </div>
  
    <div id="review3" class="pp-card m-3 collapse">
      <div class="card-body">
        <div class="card-text">
          <div id="review3Example">
            <span class="font-weight-bold">Review 3:</span>
            <br>
            <ul>
<li><strong>Overall Evaluation</strong>: Weak accept</li>
</ul>
<h4>Main Reviews</h4>
<p>Strengths:</p>
<p>Methodological novelty—The work is the first to use autoregressive diffusion ODEs (EDM &amp; Rectified-Flow) to compute frame-level log-likelihood and Information Content (IC) in a continuous audio-latent space, eliminating the Gaussian-mixture assumption of the prior GIVT baseline.</p>
<p>Weaknesses:</p>
<ol>
<li>
<p>Lack of perceptual studies—No behavioral or listening-test evidence is provided to confirm that lower NLL or higher ρ actually correlate with human-felt surprise.</p>
</li>
<li>
<p>Limited evaluation scope—Pitch-surprise tests rely on just 49 synthetic Irish tunes and 18 recorded vocal tracks, while segmentation is evaluated only at Salami’s lowercase level; broader genre coverage would strengthen the claims</p>
</li>
<li>
<p>No comparison to flow or VQ-VAE likelihoods—without comparisons to modern flow-based and VQ-VAE likelihood models—current standards for explicit audio density—the reported gains lack context, leaving it unclear whether diffusion truly surpasses state-of-the-art alternatives.</p>
</li>
</ol>
<p>Comment:
This paper introduces an autoregressive diffusion-ODE framework for estimating musical surprisal directly in a continuous audio latent space and demonstrates clear quantitative gains over the current GMM baseline. Strengths include a well-motivated methodological innovation, rigorous derivation, thorough error-speed analysis and convincing improvements in NLL, pitch-surprise correlation and structure boundary detection. The noise-level–as-semantic-granularity insight is particularly compelling and should inspire follow-up work. Weaknesses lie in the limited baselines—no comparison with flow or VQ-VAE likelihoods—and a narrow evaluation corpus lacking perceptual validation. Runtime statistics and a public replication dataset would also strengthen reproducibility.</p>
          </div>
        </div>
      </div> 
    </div>
  
  
</div>


<script src="static/js/poster.js"></script>
<script src="static/js/video_selection.js"></script>

      </div>
    </div>
    
    

    <!-- Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "UA-");
    </script>

    <!-- Footer -->
    <footer class="footer bg-light p-4">
      <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2023 ISMIR Organization Committee</p>
      </div>
    </footer>

    <!-- Code for hash tags -->
    <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });
      });
    </script>
    <script src="static/js/lazy_load.js"></script>
    
  </body>
</html>