[
  {
    "content": {
      "TLDR": "Training deep neural networks for music information retrieval (MIR) often relies on strongly aligned data, where each frame has a precisely annotated target label. To reduce this dependency, soft dynamic time warping (SDTW) enables training with weakly aligned data by replacing hard decisions with weighted sums, allowing for gradient-based learning while aligning feature sequences to shorter, often binary, target sequences. However, SDTW introduces gradient artifacts that can cause blurring and degrade predictions, impacting the learning process. In this work, we analyze the sources and effects of these artifacts and propose a reformulation of SDTW that expresses its gradient in terms of an equivalent strongly aligned target representation. This reformulation provides an intuitive interpretation of learned representations and insights into the impact of SDTW hyperparameters on the prediction quality. Using multi-pitch estimation as a case study, we systematically investigate these modified targets and demonstrate their potential for improving training stability, interpretability, and alignment quality in MIR tasks.",
      "abstract": "Training deep neural networks for music information retrieval (MIR) often relies on strongly aligned data, where each frame has a precisely annotated target label. To reduce this dependency, soft dynamic time warping (SDTW) enables training with weakly aligned data by replacing hard decisions with weighted sums, allowing for gradient-based learning while aligning feature sequences to shorter, often binary, target sequences. However, SDTW introduces gradient artifacts that can cause blurring and degrade predictions, impacting the learning process. In this work, we analyze the sources and effects of these artifacts and propose a reformulation of SDTW that expresses its gradient in terms of an equivalent strongly aligned target representation. This reformulation provides an intuitive interpretation of learned representations and insights into the impact of SDTW hyperparameters on the prediction quality. Using multi-pitch estimation as a case study, we systematically investigate these modified targets and demonstrate their potential for improving training stability, interpretability, and alignment quality in MIR tasks.<br><br> <b><p align=\"center\">[Direct link to video](https://drive.google.com/file/d/1XTHgFrju4TGNAT9_jyiSpPaEhu823ZXE/preview)</b>",
      "authors": [
        "Johannes Zeitler",
        "Meinard M\u00fcller"
      ],
      "authors_and_affil": [
        "Johannes Zeitler (International Audio Laboratories Erlangen)*",
        "Meinard M\u00fcller (International Audio Laboratories Erlangen)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Alignment, synchronization, and score following",
        "MIR tasks",
        "Music signal processing",
        "Machine learning/artificial intelligence for music",
        "MIR fundamentals and methodology",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "TRUE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1lZ-PE0Aj5U6R-vlK56TOoBDIPKHc8Nxz/preview",
      "poster_pdf": "https://drive.google.com/file/d/1JW0Iluw0V5L2q3BoHphdrqat-FR67xuw/preview",
      "session": [
        "2"
      ],
      "slack_channel": "p2-1-reformulating-soft-dynamic",
      "title": "Reformulating Soft Dynamic Time Warping: Insights into Target Artifacts and Prediction Quality",
      "video": "https://drive.google.com/file/d/1XTHgFrju4TGNAT9_jyiSpPaEhu823ZXE/preview"
    },
    "forum": "4",
    "id": "4",
    "pic_id": "https://drive.google.com/open?id=1O6l9lOpxPw51cQVWYphSkyyJsrFZHvOd&usp=drive_copy",
    "position": "01",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nStrongly agree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nSDTW is a quite common way of dealing with weak labels in a number of tasks, thus this investigation into its properties provides reusable insights for anyone who wants to use SDTW (and potentially also other, similar techniques).\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nParametrisation of SDTW has a direct and very noticeable impact on the predictions, thus parameters should be chosen carefully.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThe paper presents a reformulation of the Soft Dynamic Time Warping (SDTW) loss used in neural network training in weakly supervised settings. By expressing the SDTW gradient in terms of modified targets with standard element-wise loss functions like MSE and BCE, the authors offer enhanced interpretability and practical insights into SDTW-based learning.\n\nMain Strengths:\n\n- Theoretical Contribution: provides a reformulation of the SDTW gradient into interpretable, modified target representations. Demonstrates equivalence between SDTW training and standard loss training with these modified targets.\n- Interpretability: the reformulated gradient enables a closer look into what neural networks learn using SDTW. The modified targets enable intuitive qualitative evaluation through visualisation.\n- Practical Relevance: offers practical guidelines for initialising and training DNNs with SDTW in weakly supervised tasks. The controlled experiments on multi-pitch estimation are illustrative and offer valuable insight into SDTW hyperparameter effects\n- Clarity: especially the provided figures are informative and aid comprehension of abstract concepts\n\n\nMain Weaknesses:\n\n- Limited evaluation and ablation: evaluation is limited to one task (multi-pitch estimation) and ablation is limited to the effect one parameter (\u03b3). As the paper's focus is on introducing the reformulation, showing its equivalency, and demonstrating its usefulness in the analysis of SDTW, in my mind this is fine. Nonetheless, it would be very helpful to include further case studies plus a more thorough ablation study (e.g. what about the step weights in SDTW?), maybe in an extended / future paper.\n- Reproducibility: some implementation details are described but no code is provided, potentially hindering replication\n\nSummary:\n\nThe paper introduces a very useful way to get insights into the properties of the SDTW loss. The manuscript is very well written and technical concepts are conveyed with clarity, aided by well-chosen examples and illustrations. Overall, this is a very strong contribution.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nStrong accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThe paper presents a novel reformulation of the Soft Dynamic Time Warping (SDTW) loss, making the training process more interpretable by representing gradients as modified targets. The reformulation enables visualization and qualitative inspection of what the model is optimizing, providing insight into training dynamics. \n\nAll reviewers agree that this is a well-written and highly insightful paper. For the final version, please take into account the individual reviewer comments. Additionally, we encourage you to consider publishing your code to support reproducibility and facilitate future research.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper presents a novel approach to monitoring the training dynamics of the soft dynamic time warping (SDTW) loss by reformulating its gradients into an equivalent representation based on modified targets. These targets are strongly aligned with the network output. This reformulation enables the visualization of which targets the model is optimizing towards at each training iteration.\n\nThe authors effectively demonstrates the utility of this method in the multi-pitch estimation (MPE) task, particularly in analyzing the impact of the soft-min temperature hyperparameter on the prediction of short and long notes. The synthetic example presented in Section 4 is thoughtfully designed and is further validated through real-world examples in Section 5.\n\nSTRENGTHS:\n- The paper is clearly written and well-organized, with a coherent flow from theoretical formulation to synthetic demonstrations and real-world applications. \n \nWEAKNESSES:\n- The proposed technique is limited to monitoring the training process rather than influencing or improving it. While the modified target visualization reveals what the model is optimizing toward, it does not provide a mechanism to steer training if the optimization path is suboptimal.\n \n- The recommendations (except \"2. Target inspection\") in the last paragraph of Section 5 has already been suggested in prior work (see reference [4]) and is not novel here.\n \nMINOR COMMENT:\n- Since the soft-minimum operation is not novel to this paper, the original source should be properly credited, e.g. referring to [9] when introducing with Equation (4).",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nIn this paper, the authors reformulate soft-dynamic time warping targets used to train DNNs with so-called weak targets. This appears to be useful in multiple pitch estimation, but there are probably many other applications in music processing. The reformulation leads to computable modified targets that show the effect of the softmin hyperparameter, as well as a deeper understanding of the weak target training itself. \n\nThe paper is clearly written. Equations are clearly presented for the derivation at the correct level of detail. The work is properly motivated and antecedents are cited. In the penultimate section, concrete suggestions are given for weak target training.\n\nThe paper leads to a potentially more interpretable weak target training, as well as a deeper understanding of weak target training.",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis is mainly a theoretical paper on understanding softdtw on some common loss functions. The authors show that mathematically, when computing the gradient of softdtw with respect to one of the vectors in the input sequence x_n, both MSE and BCE point to point alignment losses both move x_n closer to \"y_n^{mod}\". y_n^{mod} is a convex combination of all target frames y, and it can be thought of as an expected value of what the softmax alignment plan aligns x_n to (in the case of gamma=0, a hard single alignment plan, y_n^{mod} is exactly the target point it's aligning x_n to). This interpretation is very useful during training, because one can sonify y^{mod} and hear how similar it sounds to x to hear if training is progressing. Likewise, it gives a way to examine the quality of the final result. Furthermore, this formulation elucidates a tradeoff of the smoothing parameter gamma in soft-DTW; a larger gamma improves smoothness in differentiability, but it blurs transient events more. As a result, the authors conclude by suggesting starting with a high gamma to stabilize training, but then to lower it as much as possible based on periodic manual inspection of ymod on some examples.\n\nOverall, I really like this paper: the mathematical analysis is simple but deep, and it goes a long way to bring the black box and less-interpretable soft alignments back into something that can be sonified and examined. My main constructive point is about the evaluation: while it's helpful to see a specific example in figure 5 for multi pitch tracking, it would also be nice to see test loss on the multi pitch examples for different fixed gammas, as well as a \"gamma schedule\" following the heuristics the authors suggest in the conclusion. For future work, it would also be very cool to hear sonifications for more complex cross-domain alignment problems like version identification, and possibly even cross-modal alignments like text to audio! I know it isn't the point of the paper, but I feel like multi pitch tracking doesn't totally show off the potential of this idea.\nBeyond that, a minor comment is that since the paper relies on it so heavily, it might be helpful to give the reader some more intuition about why the gradient is like a pseduo-probability matrix.\nBut again, this is overall a very cool work. If it gets in, it would be great to provide some sonifications as supplementary material!"
    },
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Many audio synthesizers can produce the same signal given different parameter configurations, meaning the inversion from sound to parameters is an inherently ill-posed problem. We show that this is largely due to intrinsic symmetries of the synthesizer, and focus in particular on permutation invariance. First, we demonstrate on a synthetic task that regressing point estimates under permutation symmetry degrades performance, even when using a permutation-invariant loss function or symmetry-breaking heuristics. Then, viewing equivalent solutions as modes of a probability distribution, we show that a conditional generative model substantially improves performance. Further, acknowledging the invariance of the implicit parameter distribution, we find that performance is further improved by using a permutation equivariant continuous normalizing flow. To accommodate intriciate symmetries in real synthesizers, we also propose a relaxed equivariance strategy that adaptively discovers relevant symmetries from data. Applying our method to Surge XT, a full-featured open source synthesizer used in real world audio production, we find our method outperforms regression and generative baselines across audio reconstruction metrics.",
      "abstract": "Many audio synthesizers can produce the same signal given different parameter configurations, meaning the inversion from sound to parameters is an inherently ill-posed problem. We show that this is largely due to intrinsic symmetries of the synthesizer, and focus in particular on permutation invariance. First, we demonstrate on a synthetic task that regressing point estimates under permutation symmetry degrades performance, even when using a permutation-invariant loss function or symmetry-breaking heuristics. Then, viewing equivalent solutions as modes of a probability distribution, we show that a conditional generative model substantially improves performance. Further, acknowledging the invariance of the implicit parameter distribution, we find that performance is further improved by using a permutation equivariant continuous normalizing flow. To accommodate intriciate symmetries in real synthesizers, we also propose a relaxed equivariance strategy that adaptively discovers relevant symmetries from data. Applying our method to Surge XT, a full-featured open source synthesizer used in real world audio production, we find our method outperforms regression and generative baselines across audio reconstruction metrics.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Ben Hayes",
        "Charalampos Saitis",
        "Gy\u00f6rgy Fazekas"
      ],
      "authors_and_affil": [
        "Ben Hayes (Queen Mary University of London)*",
        "Charalampos Saitis (Queen Mary University of London)",
        "Gy\u00f6rgy Fazekas (Queen Mary University of London)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "MIR fundamentals and methodology",
        "Generative Tasks",
        "Applications",
        "Creativity",
        "Music and audio synthesis",
        "Music composition, performance, and production",
        "Tools for artists",
        "Music signal processing",
        "MIR tasks",
        "Music synthesis and transformation",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "TRUE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1w6jS9hO7ZFZdlmNwOoYYqHPZ-AuxzCaN/preview",
      "poster_pdf": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-1-audio-synthesizer-inversion",
      "title": "Audio synthesizer inversion in symmetric parameter spaces with approximately equivariant flow matching",
      "video": ""
    },
    "forum": "7",
    "id": "7",
    "pic_id": "",
    "position": "01",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nStrongly agree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nStrongly agree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nStrongly agree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nStrongly agree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nEstimating the parameters of a synthesizer from sound is challenging due to the symmetries of parameter space which leads to a similar sound output.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThe equivariant generative approach to parameter estimation is effective in solving such problems.\n\n**Q17 (This paper is of award-winning quality.)**\n\nYes\n\n**Q18 ( If yes, please explain why it should be awarded.)**\n\nIndicating an issue of symmetries in synthesizer parameter space and providing alternative solutions for the problem could widely contribute to other MIR tasks, such as instrument recognition and genre recognition. Considering the potential influence of the discussion in this paper, I believe the paper has an award-winning quality.\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThe highlight of the paper is the logical explanation of why the proposed method works. Unlike other papers that present a method only by comparing it to other previous methods, this paper explains the core problem of estimating synthesizer parameters from acoustic signals and presents a solution for it. \n\nThe weakness could be the lack of a rough abstract at the beginning of the method section, which would enable readers to prepare for the mathematical formulation.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nStrong accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThis paper Indicates an issue of symmetries in synthesizer parameter space and providing alternative solutions for the problem could widely contribute to other MIR tasks. Unlike other papers that present a method only by comparing it to other previous methods, this paper explains the core problem of estimating synthesizer parameters from acoustic signals and presents a solution for it. We reviewers would like to suggest strong accept for this paper. \n\nAs reviewers indicate weaknesses in readability (reviewer 1), typos and notation problems (reviewer 3), and the unclearness of the initialization process and Figure 2 (reviewer 4), I strongly suggest checking through the draft and modifying the points in the camera-ready version.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper explores the problem of symmetries in synthesizer parameter sets within the task of in-domain sound matching.\n\nThe paper addresses a relevant problem and proposes several solutions. The results obtained with a real-world synthesizer show an improvement over other methods, although most of the improvement seems to come from the use of continuous normalizing flows.\n\nThe text is generally hard to read, and the authors should make an effort to make it more accessible. The problem of synthesizer inversion is quite niche, and it should be defined much earlier. The abstract should allow non-experts to understand what the paper is about more generally. The approach in 3.1 could be explained more clearly, and generally more diagrams (e.g. from the supplementary materials) would help, whereas even more of the mathematical detail could also be moved to the supplementary material.\n\nThe need of automatic handling of permutations could also be justified better. First, the problem assumes uniform sampling of parameters, whereas many works have been based on real-world parameter databases. It is unclear how the symmetries affect the models in this case, and what is the best solution for existing synthesizers. In this sense, the difference in results observed at the end of 4.2.3 should be further clarified or corrected. Second, the authors disregard manual approaches, and claim that automation would scale better, but this is not as obvious as it may seem. As an example, in the first experiment the asymmetric variants (which are a sort of manual mapping of the parameters) perform better, which is not discussed. It is also not clear enough in this experiment that the proposed methods are much better than existing approaches, so it seems at least some should also be used in the second experiment. Generally there seem to be many differences between the two experiments (e.g. missing CNF(Equivariant), AST conditioning), which should be justified. Some discussion of computational cost would help understanding the benefits/cost of this approach against a manual mapping or non-random training data.\nFinally, given the performance of CNF (MLP), the authors should discuss this model more in depth in relation to previous work using normalizing flows, and acknowledge the limitations of the proposed approach.",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper presents a technique to estimate the parameters of a synthesizer from an audio generated with such instrument by taking into account the symmetries of the problem and equivariant flow matching. While I highly enjoyed the topic and the innovative solution, my main concerns are relative to the accessibility of the paper. The topic is not simple and a lot of concepts are taken into account. I wonder if the paper is not more well suited to a longer and more in-depth journal paper than to a conference. Nonetheless it is an interesting work.\n\n3.1\nwhat are S_k\n\nFig. 2 \ntypo: Synthesiser \u2014> Synthesizer\n\n\n\\matbhf{omega} , \\matbhf{\\alpha} an \\matbhf{\\gamma} to what domain do they belong to?\n\nshouldn\u2019t vector \u201cx\u201d be in bold? (col right row 250) \nalso, are the elements concatenated all together in a single column vector? Do they all belong to the range -1,1 \n\n4.1 Results\n\u201cCNF (PA R A M 2 T O K) performs on par with the best models across conditions,\u201d \u2014> this is not true, in some situations it actually performs worse e.g. k=32 for LAC Symmetric and MSE symmetric in all cases\n\n4.2.2\nFor more detail on this point, see the supplementary material. \u2014> should be a journal paper",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nstrengths: the problem is relevant in many inverse problems and essential to consider if performing sound matching task on industry-scale synthesizers. the approach is novel. The insights are significant. the metrics adopted are state-of-the-art approaches refined upon previous metrics. \nweaknesses: in section 3.2, it is unclear how z z\u2019 and A are initialized such that param2tok is invariant to permutation of the parameter vector. its unclear to me how figure 2 illustrated the fact that \u201ctransformer\u2019s equivariance can be used if a permutation symmetry is present in the data but will not be enforced if not\u201d."
    },
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Automatic Music Transcription (AMT) converts audio recordings into symbolic musical representations. Training deep neural networks (DNNs) for AMT typically requires strongly aligned training pairs with precise frame-level annotations. Since creating such datasets is costly and impractical for many musical contexts, weakly aligned approaches using segment-level annotations have gained traction. However, existing methods often rely on Dynamic Time Warping (DTW) or soft alignment loss functions, both of which still require local semantic correspondences, making them error-prone and computationally expensive. In this article, we introduce CountEM, a novel AMT framework that eliminates the need for explicit local alignment by leveraging note event histograms as supervision, enabling lighter computations and greater flexibility. Using an Expectation-Maximization (EM) approach, CountEM iteratively refines predictions based solely on note occurrence counts, significantly reducing annotation efforts while maintaining high transcription accuracy. Experiments on piano, guitar, and multi-instrument datasets demonstrate that CountEM matches or surpasses existing weakly supervised methods, improving AMT's robustness, scalability, and efficiency. Our project page is available at https://yoni-yaffe.github.io/count-the-notes.",
      "abstract": "Automatic Music Transcription (AMT) converts audio recordings into symbolic musical representations. Training deep neural networks (DNNs) for AMT typically requires strongly aligned training pairs with precise frame-level annotations. Since creating such datasets is costly and impractical for many musical contexts, weakly aligned approaches using segment-level annotations have gained traction. However, existing methods often rely on Dynamic Time Warping (DTW) or soft alignment loss functions, both of which still require local semantic correspondences, making them error-prone and computationally expensive. In this article, we introduce CountEM, a novel AMT framework that eliminates the need for explicit local alignment by leveraging note event histograms as supervision, enabling lighter computations and greater flexibility. Using an Expectation-Maximization (EM) approach, CountEM iteratively refines predictions based solely on note occurrence counts, significantly reducing annotation efforts while maintaining high transcription accuracy. Experiments on piano, guitar, and multi-instrument datasets demonstrate that CountEM matches or surpasses existing weakly supervised methods, improving AMT's robustness, scalability, and efficiency. Our project page is available at https://yoni-yaffe.github.io/count-the-notes.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Jonathan Yaffe",
        "Ben Maman",
        "Meinard M\u00fcller",
        "Amit Bermano"
      ],
      "authors_and_affil": [
        "Jonathan Yaffe (Tel Aviv University)*",
        "Ben Maman (International Audio Laboratories Erlangen)",
        "Meinard M\u00fcller (International Audio Laboratories Erlangen)",
        "Amit Bermano (Tel Aviv University)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Music transcription and annotation",
        "Alignment, synchronization, and score following",
        "MIR tasks",
        "Annotation protocols",
        "Evaluation, datasets, and reproducibility",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1elXDXHrJiZ2-Sz6iQCk_SOB0Lqbgfor-/preview",
      "poster_pdf": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-12-count-the-notes",
      "title": "Count The Notes: Histogram-Based Supervision for Automatic Music Transcription",
      "video": ""
    },
    "forum": "10",
    "id": "10",
    "pic_id": "",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nDisagree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nDisagree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nHistograms are a viable alternative to sequence information in AMT training supervision.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nHistograms provide sufficient information as input to a theoretically sound training process.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis work builds entirely on NoteEM (Reference 5). The NoteEM work presents certain limitations of using weakly aligned targets such as repeated cadenza, subtle nuances such as trills (which can easily be rendered differently from the score) and chords or arpeggios where the notes can come with delays in different order. The current works aims to alleviate the assumption of undisturbed event order by using an even weaker form of supervision via a note histogram (i.e. note counting only). Their approach obtains an improvement over NoteEM with the GuitarSet and GAPs datasets. It would have helped if they had discussed the precise contexts where the predictions showed differences and thus provided insights involving real-world score-audio mismatches. The paper presents results on simulated noise added to MAESTRO dataset but it is not clear what real-world musical phenomena are being captured by the noisy histograms generated in this manner. Overall the descriptions and explanations fall short of ideal.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nSummarising the reviews post discussion phase, all reviewers agree that while the approach of histogram supervision is relevant and interesting, more discussion is needed to justify the substantial novelty of the contribution over the NoteEM paper. Clearer discussion is needed on how histogram supervision accounts for real-world score-audio mismatches with examples of specific scenarios where it helps over the DTW method.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper introduces a novel approach for improving automatic music transcription (AMT). To this end, it uses a pitch histogram criterion so-called CountEM which does not require temporal alignment and can fine-tune the estimated parameters of the model. While the idea is conceptually simple, it demonstrates potential usefulness and provides insights that could benefit and complement existing AMT techniques.\n\n\n\nStrengths:\n-The proposed method is simple yet effective, and it shows the potential of the approach without relying on explicit temporal alignment.\n\n-The framework offers reusable insights and can be integrated with existing AMT methods, potentially enhancing their performance.\n\n-The authors provide listening examples and a demo, which help illustrate the practical relevance and perceptual quality of the results.\n\n\n\nWeaknesses:\n-The proposed contribution appears incremental and closely related to NoteEM [5]. The paper should better emphasize the originality of its contributions and clearly delineate what is novel beyond existing work.\n\n-Maybe the CountEM process could be replaced by a regularization term within a standard training pipeline, rather than requiring a dedicated EM algorithm. This alternative should be discussed and justified in more depth.\n\n-The experimental results are limited. The study includes only two baseline methods (Hawthorne et al. [1,2] and Kong et al. [3]), while more recent and state-of-the-art methods such as Transkun [Yujia Yan & Zhiyao Duan, ISMIR 2024] are not considered. A broader comparison would help clarify the benefits of the proposed approach.\n\n-The CountEM method may introduce errors when the histogram prior assumption is not met. This potential limitation should be addressed through targeted experiments and discussed to better understand the trade-offs.\n\n-The paper would benefit from improved organization and clearer exposition, particularly through a more formal problem definition and consistent use of mathematical notation.\n\n-No implementation code is provided, and there is no discussion of the computational complexity or runtime performance of the proposed method, which limits its reproducibility and practical evaluation.\n\n\n\nRecommendation:\nAlthough the paper is borderline in terms of novelty and experimental depth, I recommend a weak accept, as the core idea is potentially useful and could inspire further developments within the ISMIR community.",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nSummary:\nThe paper introduces CountEM, a novel framework for Automatic Music Transcription (AMT) that leverages histogram-based supervision to convert audio recordings into symbolic musical representations. Unlike traditional AMT methods that rely on strongly aligned frame-level annotations or weakly aligned methods using Dynamic Time Warping (DTW) [1], CountEM uses note event histograms (counts of note onsets within time windows) as supervision. This approach eliminates the need for precise temporal alignment, reducing annotation effort and computational complexity. CountEM employs an Expectation-Maximization (EM) algorithm to iteratively refine predictions, starting with a model pre-trained on synthetic data. The framework is evaluated on piano (MAESTRO), guitar (GuitarSet), and multi-instrument (MusicNet, URMP) datasets, demonstrating performance improvements for both single-instrument and multi-instrument transcription.\nReview Comment:\nRecent literature has explored fine-tuning or semi-supervised retraining of transcription models pre-trained on datasets like MAESTRO. This paper offers a novel insight by proposing that DTW-based alignment can be replaced with histogram-based coarse peak picking to provide weak labels for fine-tuning. The experimental design is thorough, with detailed descriptions of the datasets and window sizes used. Notably, the noisy histogram setting is particularly compelling, as it simulates real-world transcription scenarios.\nI suggest the following revisions to strengthen the paper:\nFigure 1 and the corresponding paragraphs in Section 2 are difficult to follow. Terms such as \u201cOrdering Inaccuracy,\u201d \u201cTranslation Inaccuracy,\u201d and \u201cTiming Inaccuracy\u201d are not clearly defined or referenced in the main text of Section 2. Additionally, the \u201c[1*4] histogram supervision\u201d is unclear; it took time to realize this refers to the count of pitches within a time window. The authors should provide a clearer explanation of Figure 1, as it appears central to the paper\u2019s contribution.\nThe highest scores in each table for each setting should be bolded to improve readability and emphasize the best-performing configurations.\nThe paper should compare the training time of DTW-based methods versus histogram-based supervision. In some settings, methods from [1, 2] outperform CountEM, so the authors should emphasize the specific advantages of histogram-based supervision, such as reduced training time or improved performance in certain scenarios. The benefits of this approach need to be more clearly articulated, especially since fine-tuning is known to improve transcription performance based on prior literature.\nOverall, the paper proposes an innovative approach to fine-tuning transcription models, but its performance does not consistently surpass previous works. The advantages of histogram-based supervision should be further highlighted to strengthen the contribution. Based on the current content, I recommend a weak accept for this work.\n\n\n[1] B. Maman and A. H. Bermano, \u201cUnaligned supervision for automatic music transcription in the wild,\u201d in Proceedings of the International Conference on Machine Learning (ICML), Baltimore, Maryland, USA, 2022, pp. 14 918\u201314 934.\n[2] X. Riley, Z. Guo, and S. Edwards, Drew abd Dixon, \u201cGaps: A large and diverse classical guitar dataset and benchmark transcription model,\u201d Proceedings of the International Society for Music Information Retrieval Conference (ISMIR), San Francisco, USA, 2024",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe core text it well written: The structure is logcial; the phrasing is understandable; the grammar is good; punctuation makes it easy to distinguish between individual ideas, and presents the work as easily digestible. I appreciate how they only introduce relevant information and supplies the information with contextual introductions, in case the reader is unaware of how it is relevant. Additionally, they add several examples, to put ideas in context. The figures are good, with figure 1 being self explanatory in conveying the overall idea of the paper. The experiments are well presented in a convincing manner.\n\nAs I understand, the code is not going to be public, and if that is the case, I have a feeling that section 2 might be a little difficult to follow for reproduction. Algorithm 1 could have some more thorough explanation in my opinion, as the presented math and how it fits together takes some time to understand.\nConsidering the argument on the inefficiency of DTW, it would seem obvious to include a discussion on the efficiency increase."
    },
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Despite recent advancements in music generation systems, their application in film production remains limited, as they struggle to capture the nuances of real-world filmmaking, where filmmakers consider multiple factors\u2014such as visual content, dialogue, and emotional tone\u2014when selecting or composing music for a scene. This limitation primarily stems from the absence of comprehensive datasets that integrate these elements. To address this gap, we introduce Open Screen Soundtrack Library (OSSL), a dataset consisting of movie clips from public domain films, totaling approximately 36.5 hours, paired with high-quality soundtracks and human-annotated mood information. To demonstrate the effectiveness of our dataset in improving the performance of pre-trained models on film music generation tasks, we introduce a new video adapter that enhances an autoregressive transformer-based text-to-music model by adding video-based conditioning. Our experimental results demonstrate that our proposed approach effectively enhances MusicGen-Medium in terms of both objective measures of distributional and paired fidelity, and subjective compatibility in mood and genre. To facilitate reproducibility and foster future work, we publicly release the dataset, code, and demo.",
      "abstract": "Despite recent advancements in music generation systems, their application in film production remains limited, as they struggle to capture the nuances of real-world filmmaking, where filmmakers consider multiple factors\u2014such as visual content, dialogue, and emotional tone\u2014when selecting or composing music for a scene. This limitation primarily stems from the absence of comprehensive datasets that integrate these elements. To address this gap, we introduce Open Screen Soundtrack Library (OSSL), a dataset consisting of movie clips from public domain films, totaling approximately 36.5 hours, paired with high-quality soundtracks and human-annotated mood information. To demonstrate the effectiveness of our dataset in improving the performance of pre-trained models on film music generation tasks, we introduce a new video adapter that enhances an autoregressive transformer-based text-to-music model by adding video-based conditioning. Our experimental results demonstrate that our proposed approach effectively enhances MusicGen-Medium in terms of both objective measures of distributional and paired fidelity, and subjective compatibility in mood and genre. To facilitate reproducibility and foster future work, we publicly release the dataset, code, and demo.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Haven Kim",
        "Zachary Novack",
        "Weihan Xu",
        "Julian McAuley",
        "Hao-Wen Dong"
      ],
      "authors_and_affil": [
        "Haven Kim (University of California San Diego)*",
        "Zachary Novack (University of California San Diego)",
        "Weihan Xu (Duke University)",
        "Julian McAuley (University of California San Diego)",
        "Hao-Wen Dong (University of Michigan)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "MIR fundamentals and methodology",
        "Applications",
        "Music generation",
        "Multimodality",
        "Novel datasets and use cases",
        "Music videos, multimodal music systems",
        "MIR tasks",
        "Evaluation, datasets, and reproducibility",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1vTy0QG2GkLoiqbVQapJfVd1R1SrngQB1/preview",
      "poster_pdf": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-3-video-guided-text",
      "title": "Video-Guided Text-to-Music Generation Using Public Domain Movie Collections",
      "video": ""
    },
    "forum": "14",
    "id": "14",
    "pic_id": "",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nDisagree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q5 ( Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chosen, otherwise write \"n/a\"))**\n\nI didn't carefully check this\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q10 (Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chose, otherwise write \"n/a\"))**\n\nI didn't check the experimental details carefully, but I didn't see anything obviously incorrect.\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nDisagree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nDisagree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThere are some potential reusable insights about being able to train a video-to-music system on public domain movies and still generalize to commercial movies; or about using video as an additional/alternative conditioning for generation over text. However both of these potential insights are not fully developed/convincing based on the evidence in the paper.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nA open source dataset of movies & their aligned soundtracks is released, and a model is trained to generate soundtracks using the video as input.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak reject\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nStrengths:\n* The paper addresses an important and challenging problem: generating music for video.\n* The creation and release of the OSSL dataset is a significant contribution. The authors have made a commendable effort towards reproducibility by promising to release the dataset, code, and evaluation sets. \n* The paper is well-written and easy to follow.\n\nAreas for Improvement:\n* Misleading Title: The title suggests that the paper's focus is on a music generation system for films, but the paper focuses mainly on the dataset, plus a proof-of-concept model with limited experiments. Clarifying the emphasis in the title or abstract would improve alignment with the content.\n* Lack of Clear Research Questions: The paper would benefit from explicitly stating its research questions for both the dataset and the model. Currently, the narrative reads somewhat like: \u201cwe made a dataset, and we trained a model on it,\u201d without clearly articulated motivations or hypotheses. For the dataset: while its open-source nature is valuable, the authors could strengthen its contribution by identifying specific gaps it fills compared to existing datasets. For the model: what specific aspects of video-guided music generation are being explored? One possible direction mentioned\u2014but not emphasized\u2014is how well models trained on public domain data generalize to commercial data. Another could be the comparative benefits of video-guided over text-based soundtrack generation, which would benefit from a deeper literature-based argument and more thorough results analysis.\n* Dataset Details:\n - A more thorough comparison of OSSL to other existing film music datasets would improve the paper, including a discussion of dataset sizes and type/quality of the content.\n - The characteristics of the public domain dataset could be discussed in more detail, explicitly addressing the release year and quality of the films, and how they compare to the commercial dataset. (I mention this because I assume most public domain movies that you can find on YouTube are quite old? If that's the case, it would be interesting to discuss these particularities.)\n - How good is the time-alignment between the soundtrack clips and the movie's audio? Also, are there ever subtle differences between the soundtrack audio and the music in the movie's audio?\n* Model Details and Evaluation:\n - The related work section on audio-domain music generation contains inaccuracies (e.g. that diffusion-based generation focuses on generating spectral representations - this is not the case for most systems). The section could simply just focus on introducing MusicGen and conditioned music generation, which is probably enough for this paper.\n - The authors could discuss the strengths and weaknesses of other multimodal generation approaches in the related work, particularly in comparison to their proposed approach.\n - It's unclear if or how script information is used in the model, given the claim in the introduction that video frames alone are insufficient to capture mood, and that scripts often have mood annotations.\n - The paper lacks a comparison to existing video-to-music generation systems, making it difficult to assess the effectiveness of the proposed model. The baselines used are kind of apples-and-oranges comparisons, as the baseline does not use video information. A more direct comparison to other video-to-music models would be more compelling.\n - The connection between the dataset and the proposed adapter architecture needs better motivation (i.e. why are these two sections part of the same paper?). It is not clear why this architecture is particularly suited for this dataset. Said differently - if the paper was only about the model and was based on a different video + soundtrack dataset, would anything be different about the approach / experiments? \n - The human evaluation has a small number of participants & examples, which limits the significance of these results.  \n - The evaluation results are somewhat mixed in terms of trends, making it difficult to draw clear conclusions about the model or the dataset.\n* Minor Points:\n - The citation for pyAudioAnalysis is missing. \n - There appears to be a typo in Table 2 (\"green checkmark on m base\") for \"OSSL-finetuned\" -- should that be a red X instead?  \n - The method used for mapping soundtrack clips to movie clips could be improved by using more robust fingerprinting techniques besides the one that was tested.\n - As a general motivation for video-guided generation, one compelling reason that is not discussed in the paper is that text-based prompts cannot as easily capture important temporal musical elements (e.g. a musical surprise the moment the monster jumps out)\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak reject\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nPaper Summary:\nThis paper introduces the Open Screen Sound Library (OSSL), a new dataset of movie clips from public domain films paired with soundtracks and mood annotations. The authors also propose a model which extends the MusicGen text-to-music generation model with a video adapter to incorporate video input. The model is evaluated on the evaluation data from OSSL, which includes both public domain and commercial film datasets.\n\nThere's a consensus among the reviewers that the OSSL dataset is a valuable contribution, addressing a gap in ethically sourced, well-annotated film-video-music data. All reviewers acknowledge the paper's readability and commitment to open-sourcing the dataset and code. However, I see several significant limitations, particularly concerning the model's evaluation and the paper's overall focus. The paper could be improved by addressing the following concerns:\n\n- Lack of Clear Research Questions and Focus: The paper lacks explicitly stated research questions, particularly for the model. As I noted, the narrative feels like \"we made a dataset, and we trained a model on it,\" without a clear hypothesis or motivation for the model's specific architecture or why it's particularly suited for this dataset. Reviewer 1 also suggests that the model work could be split into a separate follow-up paper, reinforcing the idea that the model's inclusion feels somewhat tacked on rather than central to a well-defined research inquiry.\n\n- Insufficient Model Evaluation and Comparisons: A critical weakness is the absence of a robust comparison to existing video-to-music generation systems. The baselines used are text-to-music models, making it difficult to assess the actual benefit of the video adapter. This concern was raised by myself and Reviewer 2, who explicitly requested comparisons to models like VidMuse or V2Meow. Without such comparisons, the effectiveness of the proposed model, especially its video-guided aspects, remains unconvincing.\n\n- Limited Significance of Model Results: As observed by myself and Reviewer 1, the subjective performance of the model appears to be quite poor. Reviewer 1's statement that \"the system doesn't work\" and that they had \"a difficult time perceiving any meaningful differences between the base MusicGen models vs. the text finetune vs. the image adapter\" is a strong indicator of the model's current limitations. The mixed trends in the evaluation results, as I noted, also make it hard to draw clear conclusions. While the human evaluation size is acknowledged as small by the authors, this further compounds the issue of drawing significant conclusions from the model's performance.\n\n- Discrepancy Between Title and Content: The title, \"Video-guided Text-to-Music Generation for Films,\" suggests a primary focus on the generation system. However, as noted in my review, the paper's emphasis leans heavily towards the dataset. This discrepancy, while minor in isolation, contributes to the overall impression that the model component is not as fully developed or justified as it should be for a paper with such a title.\n\n- Unclear Connection Between Dataset and Model: The motivation for connecting the OSSL dataset with this specific adapter architecture is not clearly articulated. It's not evident why this architecture is uniquely suited for the OSSL or what specific insights about video-guided generation are gained by training on this particular dataset. This raises questions about the synergy between the two main contributions.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nFirst, my subjective impression of the demo in the supplementary material was that the system doesn't work. Compared to the excellent ground truth trailers and their soundtracking, all systems were awful. Particularly, I have a difficult time perceiving any meaningful differences between the base MusicGen models vs. the text finetune vs. the image adapter.\n\nI thus wonder if it wouldn't be better to rework the paper to exclusively focus on the dataset and its creation, and leave any downstream predictive model for an isolated follow-up publication instead that builds on top of OSSL.\n\nHowever, the paper is well-written and flows great so I don't think this is a needed change but perhaps a thought for future projects.\n\nStrengths:\n\n- Introduces a valuable new dataset (OSSL) with mood annotations for film music generation.\n\n- Interesting to see concrete attempts of extending MusicGen with a video adapter for multimodal conditioning\n\n- Comprehensive evaluation using both objective and subjective metrics is appreciated\n\n- Ethical and reproducible approach, using public domain data and committing to open release\n\n- Well-considered prompt design using mood and LLM-generated captions\n\nWeaknesses:\n\n- Small human evaluation limits subjective result (fairly acknowledged in conclusion)\n\n- Model innovation is modest (not necessarily bad however), with primarily an integration of existing systems. Again, might be best to split out all model work into a follow-up paper.\n\n- Reliance on public domain data may limit stylistic diversity and realism but it's a fair limitation ofc.!\n\n- Some implementation details (e.g. training times, costs) could be added\n\n- Prompt effects are not ablated or isolated for clearer impact analysis",
      "review2": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nStrengths:\n* The OSSL fills a key gap in MIR by providing ethically sourced, well-annotated film-video-music data at scale, with mood labels.\n* The introduction of a lightweight video adapter into an autoregressive text-to-music model (MusicGen) seems efficient.\n* The paper is very readable, all major steps are well explained, and there is a strong commitment to open-sourcing the dataset, models, and evaluation metrics.\n\nSuggestions for Improvement:\n* The subjective evaluation only involved 15 participants, which is understandable given resource constraints, but future work could aim for larger user studies to strengthen conclusions. In future work, consider expanding subjective evaluations (e.g., 30\u201350 participants) to strengthen the confidence in user study results, particularly given the variability in human judgments for mood and genre.\n* It was good that the authors provided examples along with the paper. However for some of the examples the background sound was higher in volume compared to the dialogues. That might affect the subjective evaluations. \n* Additionally, the examples of nervous were more than happy or peaceful. How about sad scenes? How does the model perform for those?\n* While MusicGen baselines are appropriate, a comparison to other multimodal generative models (e.g., VidMuse, V2Meow) would further position the contribution. Even a qualitative discussion would help.\n* The negative effect of adding genre labels to prompts is an interesting observation. It would be great if future versions explored if certain genre labels are consistently harmful, neutral, or helpful.\n* It would be insightful to show a few qualitative examples where the model fails (e.g., a mismatch between video and generated mood), helping readers better understand common failure modes and future improvement directions.",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe primary contribution of the paper is the dataset. The methodology used to construct the dataset is thoughtful and detailed, and I have high hopes for its quality.\n\nIn the title, I like the framing of \"video-guided text-to-music generation\" over \"video to music generation\" which other papers use to describe similar techniques.\n\nL174: \"To achieve the highest music quality by obtaining soundtrack stems without unnecessary noise, we download soundtracks, instead of source-separated music, for each film from YouTube, guided by IMDB metadata.\" I really like this approach to ensuring high-quality audio rather than settling for youtube-encoded, source-separated audio with audio degradations and other artifacts like distortion from processing like audio ducking for speech and sound effects. I think more researchers should follow suit rather.\n\nL13: \"To demonstrate the effectiveness of our dataset in improving the performance of pre-trained models on film music generation tasks...\" Are there other tasks you can first use to show the quality of the dataset? To me there is a little bit of incongruity between the primary contribution of the paper - the dataset - and the focus of the paper - the methodology for fine-tuning and evaluating musicgen. A section analyzing the quality of the dataset and providing some statistics/distributions about the dataset compared to other SOTA datasets for the same task would go a long way in balancing things out."
    },
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Many music AI models learn a map between music content and human-defined labels. However, many annotations, such as chords, can be naturally expressed within the music modality itself, e.g., as sequences of symbolic notes. This observation enables both understanding tasks (e.g., chord recognition) and conditional generation tasks (e.g., chord-conditioned melody generation) to be unified under a music-for-music sequence modeling paradigm. In this work, we propose parameter-efficient solutions for a variety of symbolic music-for-music tasks. The high-level idea is that (1) we utilize a pretrained Language Model (LM) for both the reference and the target sequence and (2) we link these two LMs via a lightweight adapter. Experiments show that our method achieves superior performance among different tasks such as chord recognition, melody generation, and drum track generation. All demos, code and model weights are publicly available.",
      "abstract": "Many music AI models learn a map between music content and human-defined labels. However, many annotations, such as chords, can be naturally expressed within the music modality itself, e.g., as sequences of symbolic notes. This observation enables both understanding tasks (e.g., chord recognition) and conditional generation tasks (e.g., chord-conditioned melody generation) to be unified under a music-for-music sequence modeling paradigm. In this work, we propose parameter-efficient solutions for a variety of symbolic music-for-music tasks. The high-level idea is that (1) we utilize a pretrained Language Model (LM) for both the reference and the target sequence and (2) we link these two LMs via a lightweight adapter. Experiments show that our method achieves superior performance among different tasks such as chord recognition, melody generation, and drum track generation. All demos, code and model weights are publicly available.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Junyan Jiang",
        "Daniel Chin",
        "Liwei Lin",
        "Xuanjie Liu",
        "Gus Xia"
      ],
      "authors_and_affil": [
        "Junyan Jiang (New York University Shanghai)*",
        "Daniel Chin (New York University Shanghai)",
        "Liwei Lin (MBZUAI)",
        "Xuanjie Liu (MBZUAI)",
        "Gus Xia (MBZUAI)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Symbolic music processing",
        "Melody and motives",
        "Musical features and properties",
        "Music generation",
        "Harmony, chords and tonality",
        "MIR tasks",
        "Rhythm, beat, tempo",
        "MIR fundamentals and methodology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1gcm7wmZMXfuBy_ixVpwHclZZBku2G4bk/preview",
      "poster_pdf": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-9-versatile-symbolic-music",
      "title": "Versatile Symbolic Music-for-Music Modeling via Function Alignment",
      "video": ""
    },
    "forum": "25",
    "id": "25",
    "pic_id": "",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nDisagree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe clear explanation of the models can help to better use them for other tasks.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThis paper proposes to treat standard MIR tasks as music generation tasks with a music-for-music model and function alignment.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nReview_ISMIR2025_paper_25\n\nThis paper proposes to treat standard MIR tasks as music generation tasks with a music-for-music model and function alignment.\nAlthough I am not a specialist of foundations models, I found the principles and architecture of the model proposed very clearly explained.\nTwo implementations of the model are proposed (cross-attention between separately pretrained LM, self-attention on IO sequences of a single shared LM).\nThe results of those models are compared to several models/baselines (Coco-Mulla, MLP Prober, Encoder-Decoder, MelodyT5, Composers assistant V2) on three MIR tasks: chord-conditioned melody generation, drum-conditioned song generation, and song-conditioned drum track generation.\nResults have been evaluated both through a listeners survey and with objective metric computation (perplexity, L1 distance between chromagrams, CTnCTR) demonstrating the good performance of the models proposed, with slightly better results for the self-attentive implementation.\nMore details could be given on how the 8 songs for the survey were selected and on the participants.\n\nExamples are given in supplementary material and on a web page to compare output of the models.\nThe code is not given at this step but the paper indicates that code and model weights will be publicly available on a web page.\n\nThis paper appears like a solid and insightful contribution to the field.\nIt could be more self-contained by a better explanation of the notion of function alignment.\n\n\nMinor remarks: \n\np2 notations related to equation (1) -> A reordering or items for each parameter would ease the reading\nb/b_j unclear\nl136: refain -> refrain\np3 l160: corrsponds -> corresponds\np4 l241: shwon -> shown\n\nFigure 6 is too small and difficult to read, it has to be improved\n\nBibliography: I was disturbed by the high number of arxiv citations in the references: 28/55.\nThough I understand that AI improvements go very fast and it is necessary to follow the trends on arxiv, I find it excessive to rely on so many non peer-reviewed papers. (A few have been published since they were posted on arxiv and should be updated.)\nRef [17] is incomplete\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nAll reviewers appreciated the proposition to unify different MIR tasks by treating them as music generation tasks and agreed that the paper should be accepted to ISMIR.\nHowever, several weaknesses were highlighted in the reviews, notably:\n- the lack of clarity and soundness of the \"function alignment\" notion (all reviewers)\n- the need for more details on some technical aspects (conversion of generated MIDI notes to chord charts, experimental setup)\nWe advise the authors to take reviewers's remarks into account to improve the paper and increase its impact on ISMIR community.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe paper presents an interesting approach to unified music-for-music modeling. The approach is to pre-train a foundation generative model on symbolic music, then finetune on downstream tasks framed as generation tasks.\n\nThe first contribution claimed here is \"unifying a broad range of music understanding and controllable generation tasks under a shared framework\". The downstream tasks considered here are melody-to-chord, chord-to-melody, drum-to-others, others-to-drum, and chord recognition. Most of these (chord-to-melody, drum-to-others, others-to-drum) are already symbolic music generation tasks, and applying a generic pre-trained music generation model to these is certainly not a novel idea. Casting the \"music-to-chords\" tasks (melody-to-chord, chord recognition) under the same vocabulary (MIDI notes) is certainly an interesting and arguably novel idea, and is a valuable contribution. However, to make this contribution complete, I would appreciate more details on how the generated MIDI notes are converted back to a chord chart (beyond \"template matching on 16 generation\").\n\nThe second stated contribution is to introduce the idea of \"function alignment\" [7] to the field of music AI. However, this is done without a clear explanation of what \"function alignment\" actually is, and the connection of the present work to it is unclear, making this claimed contribution feel quite artificial. It only seems to be related through a vague concept of \"synergy\", which is not explained. It is suggested that this \"synergy\" is achieved by \"treating two language models (LMs) as agents\", which, in my opinion, simply isn't true. In practice, this work adds cross-attention layers between pre-trained models as in [42,43], and it is not explained how this could be seen as treating the models as agents.\n\nAlso note that the paper [7] is a very recent pre-print, is not peer-reviewed, and is not a cognitive science paper but a computer science \"position paper\", contrary to what the description \"theory of mind that attributes the emergence of intelligence to the dynamic synergy among interacting agents\" might suggest.\n\nFor these reasons, I would recommend omitting the term \"function alignment\" from the title, and possibly from the paper altogether. Consider sticking to a term from prior work, like \"zipping\" [43] or \"CALM\" [42], which this work is essentially a form of, or simply something like \"cross-attentive adapters\". If the current framing is kept, it should be explained in much more detail what \"function alignment\" actually is and why it is useful to reframe these techniques that way.\n\nFinally, the third contribution, proposing concrete training/finetuning methodology, is again framed in terms of function alignment. That aspect aside, this work presents two approaches:\n- Cross-attentive: Adapting prior work [42,43], inserting cross-attention adapters, for the symbolic music domain.\n- Self-attentive: Instead of cross-attention between two models (the conditioning one and the generating one), both the conditioning and the output are concatenated and modelled by the same model. It is shown how this effectively introduces a cross-attentive mechanism between the two representations.\n\nThe approach is validated using subjective and objective evaluation. Remarkably, the results on chord recognition seem to be close to or better than state of the art (as far as I can tell), which is a clear validation of the approach. The subjective evaluation results on the rest of the tasks are also favorable, although the sample size (2 to 4 songs) is very limited. I also appreciate the provided audio examples and think that they illustrate quite nicely the effectiveness of the approach, although I hope the authors can provide more of them.\n\nOther comments:\n- The specific framing of the proposed self-attentive technique as a form of PEFT is somewhat confusing to me. A form of PEFT used in this work is LoRA, but this is applied in *both* the cross- and self-attentive approach, and is therefore not specific to the latter. This choice also seems unrelated to the self-attentive approach (which could be just as well applied while finetuning the whole model, i.e. in a non-parameter-efficient way). Also, [35,38] are cited here as using PEFT, but it's not clear how that's relevant: [35] adds cross-attention (not self-attention) layers; [38] uses concatenation in a similar way to this work, but it is unclear how this is related to PEFT.\n- L207, L242: Instead of \"append LoRAs\", consider saying \"apply LoRA\" (referring to the technique, not the adapter itself, and avoiding the possibly misleading term \"append\").\n- The results tables and the supplementary examples contain a model called \"seq2seq\", which isn't introduced in the paper. I assume this is \"MelodyT5\"?",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\n**Summary**\nThis paper proposes a framework for symbolic music generation and analysis by modeling tasks as sequence-to-sequence mappings between symbolic sequences. Two parameter-efficient adapters are tested, namely cross-attentive between LMs and self-attentive within a shared LM. This approach proves to achieve strong performance across in both generative and few-shot symbolic music analysis tasks. While the connection to the broader \u201cfunction alignment\u201d theory remains abstract, the implementation proves effective and broadly applicable.\n\n**Strengths**\nThe paper introduces a unified modeling framework for symbolic music tasks (both generative and analytical) that are often approached with distinct, task-specific architectures (called \u201cmusic-for-music\u201d modeling in the paper). Methodologically, the work leverages existing parameter-efficient fine-tuning (PEFT) methods, integrating them into both cross-attentive and self-attentive adapter configurations. The evaluation is comprehensive, combining objective metrics with subjective listening tests, covers multiple tasks, and tests the fine-tuning capabilities on a diverse range of tasks. Experimental results demonstrate consistent improvements or competitive performance across a diverse set of tasks, including chord recognition, melody generation, and drum generation.\n\n**Weaknesses**\nA major conceptual weakness is the weak and insufficiently explained link between the proposed method and the notion of \u201cfunction alignment.\u201d This terminology risks appearing metaphorical rather than technical, especially given that the underlying adapter-based fine-tuning between pre-trained models mechanisms are well established. Several design choices\u2014e.g., adapter placement\u2014are not experimentally justified. Finally, inconsistencies in the experimental setup, mainly related to the music analysis task, might reduce the strength of the evaluation. \n\n**Presentation**\nThe paper is generally well written and easy to follow, with clear structure and adequate use of figures to illustrate architectural components and evaluation outcomes. However, the introductory section would benefit from a more transparent conceptual framing. Specifically, the reference to \u201cfunction alignment\u201d as a \u201crecently proposed theory of mind that attributes the emergence of intelligence to the dynamic synergy among interacting agents\u201d (lines 46\u201348) is not sufficiently informative. The conceptual link to this theory is not convincingly established, nor is it clear how the notion of interacting agents is operationalized in the model. This is particularly relevant in the case of the self-attentive strategy, where only a single shared LM is used. I have the feeling that the function alignment terminology risks obfuscating what is essentially a standard PEFT design pattern. The related work section is complete and includes the most relevant literature in both music foundation models and PEFT. However, the positioning of the proposed approach within the literature, especially in the \u201cMusic Foundation Models\u201d subsection, could be better clarified. A few minor issues can be found, including typographical error (\u201cshwon\u201d instead of \u201cshown,\u201d line 241) and a duplicated citation [54].\n\n**Methodology**\nThe methodology is well articulated and technically sound. Both adapter strategies are clearly described, and the use of a Roformer-based symbolic LM is explained in detail, as well as the data representation pipeline. However, the rationale behind specific design could be better explained. For example, the insertion points of the adapters, the particular LoRA parametrization, and the use of gated cross-attention mechanisms are not supported by ablation studies or empirical justification. Moreover, the description of data augmentation during pre-training (random pitch shifting within \u00b15 semitones - lines 269-270) is not quantified in terms of its prevalence across the dataset or its effect on training dynamics. Moreover, it would be helpful to quantify the added parameter count introduced by each adapter configuration, especially given that parameter-efficiency is one of the stated goals. \n\n**Evaluation**\nThe evaluation is extensive in terms of the range of tasks covered and the mix of subjective and objective analyses. In particular, the inclusion of a listening-based subjective evaluation is particularly welcome. However, the paper omits key details about the evaluation design: no information is provided on the demographics or musical expertise of the participants, nor on the level of inter-rater agreement. The objective evaluation of generative tasks is based on perplexity and task-specific metrics, and is presented with sufficient clarity. However, the music analysis task would deserve deeper inspection. The dataset is small (only 93 tracks) and homogeneous in terms of genre, annotators, and chord vocabulary used, which raises questions about generalizability. To make stronger claims, the evaluation might be extended to include widely used chord recognition benchmarks (e.g. Isophonics, Billboard, JAAH, etc.), which could provide larger and more diverse foundation to the experiments. Furthermore, the choice of evaluation metrics appears arbitrary, with the inclusion of root, majmin, and seventh metrics, but not justifying why these were selected over other ones (e.g. tetrads, MIREX, etc.). Also, the prober model is trained as a classifier, while the function alignment models and other baselines rely on sequence-to-sequence generation followed by template matching. This discrepancy might undermine the comparability of the results. Furthermore, two reference methods\u2014HMM and Chorder\u2014are only mentioned in Table 3 but are never introduced or discussed in the text. \n\n**Technical Quality/Reproducibility**\nThe technical exposition throughout the paper is clear, rigorous, and well-structured. Design choices are generally well motivated, and the implementation details are documented. The paper provide a supporting website that includes audio examples from experiments. Moreover, more supplementary is provided along with the paper submission. However, at the time of review, the source code and pre-trained models are still marked as \u201cTBA.\u201d While the intention to release them is stated, actual availability is essential for full reproducibility.",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe authors present a technique for music-to-music domain in the symbolic music domain by fine-tuning a LM and applying parameter-efficient tuning\n\nSome minor comments:\nAbstract/intro \u2014> what is an adapter?\n\nPage 1 column 2 row 52 \u2014> first approach is \u201ctreating\ntwo language models (LMs) as agents\u201d while \u201csecond approach is \u201ccreating synergy\nthrough Parameter-Efficient Fine-Tuning (PEFT).\u201d? This is not very clear from the writing.\n\nAuthors claim that they are the first to introduce Function Alignment, yet they state that it remains at the theoretical level, so what is the actual contribution? \n\nAlso, the mentioned Function alignment pape why should we consider as beneficial considering it as a \u201ctheoretical perspective?\n\nI don\u2019t understand p_t and d_t are summed? Ore flattened?\n\nPage 3 col 1 row 160 \u2014> corrsponds \u2014> corresponds\n\nPage 4 col1 line 241 shwon \u2014>shown\n\n4.5 the concept of \u201cperplexity\u201d should at least be introduced.\n\n\u201cWe\nevaluate using chord metrics (root, majmin, seventh) from\nthe mir_eval package [53]. \u201c \u2014> what do the metrics represent?"
    },
    "session": "5"
  },
  {
    "content": {
      "TLDR": "We propose See2Hear (S2H), a framework that jointly learns audio-visual representations for object detection and sound source separation from videos. Existing methods do not fully exploit the synergy between the detection and separation tasks, often relying on disjointly pre-trained visual encoders. In this paper, S2H integrates both tasks in an end-to-end trainable unified structure using transformer-based architectures. A naive combination of them, however, results in suboptimal performance. We propose a dynamic filtering mechanism that selects relevant object queries from the object detector to resolve this issue. We conduct extensive experiments to verify that our approach achieves the state-of-the-art performance in audio source separation on the MUSIC and MUSIC-21 datasets, while maintaining competitive object detection performance. Ablation studies confirm that the joint training of detection and separation is mutually beneficial for both tasks.",
      "abstract": "We propose See2Hear (S2H), a framework that jointly learns audio-visual representations for object detection and sound source separation from videos. Existing methods do not fully exploit the synergy between the detection and separation tasks, often relying on disjointly pre-trained visual encoders. In this paper, S2H integrates both tasks in an end-to-end trainable unified structure using transformer-based architectures. A naive combination of them, however, results in suboptimal performance. We propose a dynamic filtering mechanism that selects relevant object queries from the object detector to resolve this issue. We conduct extensive experiments to verify that our approach achieves the state-of-the-art performance in audio source separation on the MUSIC and MUSIC-21 datasets, while maintaining competitive object detection performance. Ablation studies confirm that the joint training of detection and separation is mutually beneficial for both tasks.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Sunyoo Kim",
        "Yunjeong Choi",
        "Doyeon Lee",
        "Seoyoung Lee",
        "Eunyi Lyou",
        "Seungju Kim",
        "Junhyug Noh",
        "Joonseok Lee"
      ],
      "authors_and_affil": [
        "Sunyoo Kim (Seoul National University)",
        "Yunjeong Choi (Seoul National University)",
        "Doyeon Lee (Seoul National University)",
        "Seoyoung Lee (The University of Texas at Austin)",
        "Eunyi Lyou (Seoul National University)",
        "Seungju Kim (Sookmyung Women's University)",
        "Junhyug Noh (Ewha Women's University)",
        "Joonseok Lee (Seoul National University)*"
      ],
      "channel_url": "",
      "day": "4",
      "keywords": [
        "Sound source separation",
        "Multimodality",
        "MIR tasks",
        "MIR fundamentals and methodology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/18mNiWr7qRykY_ENyGE9HLkfPP_ElqRP1/preview",
      "poster_pdf": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-10-joint-object-detection",
      "title": "Joint Object Detection and Sound Source Separation",
      "video": ""
    },
    "forum": "26",
    "id": "26",
    "pic_id": "",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nStrongly disagree\n\n**Q5 ( Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chosen, otherwise write \"n/a\"))**\n\nThe related work section and more generally the whole paper refers to work published in ML and computer vision conferences but only scarcely refers to work published in audio/music conferences/journals. There are a very large number of references (but many are not adequate) and clearly a significant number of relevant references are missing. \nFor instance from work by Sanjeel Parekh & al., Zhiyao Duan & al., Cynthia Liem & al., and others\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nDisagree\n\n**Q10 (Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chose, otherwise write \"n/a\"))**\n\nThe evaluation is not convincing. There are missing details on the datasets used. This does not permit to well evaluate the difficulty of the task (how many concurrent sources are played ? what are the initial SDR for a naive separator (only outputting the mix) ? how would perform a pure SoA audio separator ? The demo example provided is not explained and it is difficult to understand what is seen/heard and as such it is not a convincing demo.\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nDisagree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nStrongly disagree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nDisagree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe paper is building on previous work [14] and does not integrate new concepts. Besides, the reproducibility is rather low (no publication of code and the datasets used seems to be only partially accessible)\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nA joint object detection and audio source separation trained in an end-to-end fashion.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong reject\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper addresses the relevant problem of audio-visual music source separation and is generally well written. However, it suffers from several significant limitations.\n\nLiterature Positioning: The related work review is skewed toward computer vision literature, with insufficient coverage of prior work in the audio/music source separation domain.\n\nExperimental Validation: The experiments lack rigor. Dataset details are minimal, task complexity is not discussed, and the number of concurrent sources is unclear. There is no comparison with established music (only) source separation baselines, and the demo is neither clearly described nor convincing.\n\n Novelty: The proposed method appears to be a minor extension of existing work, possibly by the same authors.\n\nReproducibility: The absence of code and partial dataset availability significantly hinders reproducibility. The implementation detials provided in the paper are not sufficient to easily reproduce the work.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThe different reviewers had quite different opinions. But, after discussion, it was agreed that a substantial number of weaknesses pointed out by the reviewers are minor and could be improved in the final submission and that the paper has merits which could justify acceptance.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper introduces **See2Hear (S2H)**, an end-to-end transformer-based framework that jointly performs object detection and sound source separation in videos. Unlike prior methods that treat these tasks separately, S2H integrates them using shared visual and auditory representations. The authors design a dynamic filtering mechanism to prune irrelevant object queries before cross-modal fusion, enhancing both efficiency and accuracy. Evaluated on the MUSIC and MUSIC-21 datasets, S2H seems to achieve convincing results in sound separation while jointly training detection. Ablation studies validate the benefit of this joint learning.\n\n### **Core Contributions**\n\n1. **Unified End-to-End Framework:**\n\nProposes a joint architecture that simultaneously performs object detection and sound source separation using shared transformer encoders and decoders.\n\n1. **Dynamic Query Filtering Mechanism:**\n\nIntroduces a method to filter out low-confidence or redundant object detections (bounding boxes) before fusion, improving both sound separation quality and detection precision.\n\n1. **Transformer-Based Audio-Visual Fusion:**\n\nUtilizes cross-attention between visual object queries and audio tokens, allowing fine-grained association between objects and their sounds.\n\n1. **State-of-the-Art Results:**\n\nAchieves superior SDR, SIR, and SAR performance over existing baselines (e.g., iQuery, Sound-of-Pixels) on MUSIC and MUSIC-21 datasets.\n\n## **Concerns & Suggestions**\n\n1. **Dependence on pre-trained models:** The model relies on pseudo-ground truth bounding boxes generated via an external detector (`detic`). For the audio part the model relies on pre-trained weights from the AST model (likely pre-trained on noisy audioset). And for the core visual classifier the model relies on pre-trained `DETR` model. This weakens claims of full end-to-end learning and could limit the applicability in less structured domains. As the authors mentioned, they retrained \u201call baselines on the same set of currently available videos to ensure a fair comparison\u201d but to me it would have been more fair if the pre-trained models would be take out of the equation. This could mean to train all models from scratch or use the same original datasets (like AudioSet in this case).\n2. I\u2019m not deeply familiar with the MUSIC/MUSIC-21 dataset, so, I find it difficult to understand the objective of the task. Specifically if permutation plays a role in the task itself: are two instruments of the same kind (e.g. violin + viola) mixed? In that wouldn\u2019t the loss function have to be permutation invariant to produce meaningful results? I would encourage the authors to to discuss if that is the case or whether the query based visual branch make it possible to not require PIT loss functions. \n3. **Limited Evaluation Scope:** The evaluation is restricted to musical instruments in controlled settings (MUSIC/MUSIC-21). It remains unclear how well the approach generalizes to more complex scenes, diverse object categories, or real-world noise. Also as far as the evaluation and ablation study goes, it is not clear which ablation completely disables the video branch. As I understand, this kind of ablation is common is audio-visual speech separation to demonstrate how well the separation with just audio would work. If this is not easily possible in this framework I would at least suggest to do an ablation experiment where the bounding boxes are pertubated with random noise or the input image itself is destructed. \n4. **Audio separation baseline missing:** In the same direction as above: i would highly suggest to also add an audio-only baseline separation model so that the reader can understand the benefit of the audio models. This should be trained with the same data to have a fair comparison. \n5. **Computational Complexity & Scalability:** The transformer-based architecture and fusion module may be computationally heavy, especially in multi-object or high-resolution settings. Runtime and memory usage analysis would strengthen the paper.\n6. **Lack of clarity on audio analysis and synthesis:** the paper describes in paragraaph 5.1 how the masking is taking part in practice. However, I found it difficult to understand how exactly the mask is computed, is the mask complex valued or real-valued? What parameters of the STFT have been used? How does the model deal with higher sampling rates like 44k if the model was only trained on 11khz inputs? How were the video frames selected when the hop size / sampling rate of video and audio are different? Was interpolation used?\n7. **Temporal positional encoding of video frames:** the paper mentions that for each sample, 3 frames of video are sampled. From each of the 3 frames, a number of bounding boxes is estimated and features are inferred. If this is correct, I wonder how the bounding boxes were tracked over the course of the 3 frames and how the model would be able to utilize temporal video information. Imagining a violinist would move his/her bow 3 frames might be enough to already encode temporal information. Therefore I wonder if (or if not: why) temporal positional encoding was used for the video frame index.\n8. **Lack of Qualitative Detection Results:** While separation results are illustrated, visual detection outcomes are not qualitatively analyzed or benchmarked beyond mAP/mIoU, making it hard to assess detection reliability.\n9. Please use a spell checker",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe main strength of this paper is that this paper shows how audio and visual tokens could be jointly used to train in a single model, allowing gradients from both tasks, object detection and source separation, to update the shared representation space. Such update is not easy task.\nThe main weaknesses or limiations of this paper are \n- Although 2 publicly available datasets are used and there is no more challenging datasets available, such as having multiple instruments in a same video, evaluating the proposed method on such challenging dataset is necessary. To advocate the purposed method, such dataset should be created first.\n- Source code should be said to be released soon, as such joint training is not easily to be reproduced.",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe article presents a new framework to perform audio source separation from audio-visual learning. The presentation is clear and the experimental analysis is strong. Some areas are missing further exploration and clarification, which would make this article stronger.\n\nRemarks to address:\n\nLine 226 defines \\theta but it does not mention what this threshold is.\n\nIn 263 the embedding fusion of audio and video is mentioned. Figure 1 also describes it. The dimensions of the video embedding O_o are not the same as the dimensions of S_out. This has to be fixed for clarity.\n\nPlease cite the standard protocol for the data splitting on the MUSIC dataset (line 326).\n\nData processing. It is not clear what is the video sampling rate. The standard is 30fps. If 3 frames per video are sampled that is roughly a window of 100 milliseconds. However, in audio 6 seconds are sampled. This indicates that the audio and video are not time aligned. This needs to be further explained. Also the choice of the audio sampling rate at 11kHz seems arbitrary. This also needs to be explained in detail as it is critical for experiment reproducibility. \n\nResults Section: The ablations presented are very clear. However, one ablation missing is what happens if the visual branch is missing the bounding box information. Opposite to having no b-box filtering, which means there will be a lot of visual information being merged into the network, what happens if there is no b-box information at all, leaving only audio and class information being processed by the network? This should also clarify the benefits of using bounding boxes.\n\nThe supplementary video needs to be more self-explanatory. It is not clear when each sound source should be separated. A timeline of the sound events and the task would make this clearer.\n\nIs the code to reproduce experiments going to be open sourced?"
    },
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Recent years have seen many audio-domain text-to-music generation models that rely on large amounts of text-audio pairs for training. However, symbolic-domain controllable music generation has lagged behind partly due to the lack of a large-scale symbolic music dataset with extensive metadata and captions. In this work, we present MetaScore, a new dataset consisting of 963K musical scores paired with rich metadata, including free-form user-annotated tags, collected from an online music forum. To approach text-to-music generation, we leverage a pretrained large language model (LLM) to generate pseudo natural language captions from the metadata. With the LLM-enhanced MetaScore, we train a text-conditioned music generation model that learns to generate symbolic music from the pseudo captions, allowing control of instruments, genre, composer, complexity and other free-form music descriptors. In addition, we train a tag-conditioned system that supports a predefined set of tags available in MetaScore. Our experimental results show that both the proposed text-to-music and tags-to-music models outperform a baseline text-to-music model in a listening test. While a concurrent work Text2MIDI also supports free-form text input, our models achieve comparable performance. Moreover, the text-to-music system offers a more natural interface than the tags-to-music model, as it allows users to provide free-form natural language prompts.",
      "abstract": "Recent years have seen many audio-domain text-to-music generation models that rely on large amounts of text-audio pairs for training. However, symbolic-domain controllable music generation has lagged behind partly due to the lack of a large-scale symbolic music dataset with extensive metadata and captions. In this work, we present MetaScore, a new dataset consisting of 963K musical scores paired with rich metadata, including free-form user-annotated tags, collected from an online music forum. To approach text-to-music generation, we leverage a pretrained large language model (LLM) to generate pseudo natural language captions from the metadata. With the LLM-enhanced MetaScore, we train a text-conditioned music generation model that learns to generate symbolic music from the pseudo captions, allowing control of instruments, genre, composer, complexity and other free-form music descriptors. In addition, we train a tag-conditioned system that supports a predefined set of tags available in MetaScore. Our experimental results show that both the proposed text-to-music and tags-to-music models outperform a baseline text-to-music model in a listening test. While a concurrent work Text2MIDI also supports free-form text input, our models achieve comparable performance. Moreover, the text-to-music system offers a more natural interface than the tags-to-music model, as it allows users to provide free-form natural language prompts.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Weihan Xu",
        "Julian McAuley",
        "Taylor Berg-Kirkpatrick",
        "Shlomo Dubnov",
        "Hao-Wen Dong"
      ],
      "authors_and_affil": [
        "Weihan Xu (Duke University)*",
        "Julian McAuley (University of California, San Diego)",
        "Taylor Berg-Kirkpatrick (University of California, San Diego)",
        "Shlomo Dubnov (University of California, San Diego)",
        "Hao-Wen Dong (University of Michigan, Ann Arbor)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Music generation",
        "Evaluation, datasets, and reproducibility",
        "MIR tasks",
        "Novel datasets and use cases"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/16xMmlZ3wHSovGpAkM1Ck6h27B0t0um3_/preview",
      "poster_pdf": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-12-generating-symbolic-music",
      "title": "Generating Symbolic Music from Natural Language Prompts using an LLM-Enhanced Dataset",
      "video": ""
    },
    "forum": "32",
    "id": "32",
    "pic_id": "",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nDisagree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe paper offers insights into how LLMs can be effectively leveraged to enhance music datasets and improve text-to-music generation. Generating pseudo-captions from metadata and the comparative analysis of tag-based vs. text-based control provide some insights for future research in this area. \nThe dataset will also be useful for several tasks.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThis paper introduces MetaScore, a large-scale symbolic music dataset enhanced by LLM-generated captions, enabling improved text-to-music generation with versatile controls.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper introduces MetaScore, a new large-scale dataset of 963K musical scores with rich metadata. An LLM is used to generate pseudo-captions from the metadata, and text-to-music and tag-to-music models are trained on this dataset. \nThe paper is well-written and clearly structured, and has several strengths:\nIt introduces a valuable dataset, addressing a significant gap in the availability of large-scale, richly annotated symbolic music datasets.The data collection method (scraping) raises ethical considerations, which the authors attempt to address by only releasing public domain ones (while those not in public domain will be shared upon request and only for research purposes).\nBeyond the data collection, the authors also contribute through several dataset enhancements, including: the extraction and standardization of metadata, including key signature, time signature, tempo, and instrument information, with a focus on General MIDI compatibility and consistency in composer names. Then, to address missing genre information, a genre tagger is trained, enabling genre-controlled music generation. The accuracy of this tagger is evaluated through objective and subjective tests. Finally, LLMs are leveraged to generate pseudo-captions from the metadata, creating text descriptions of the music that facilitate text-to-music generation. \nThen a text-conditioned music generation model is proposed, which can be controlled using different musical properties (e.g. genre, instruments, composer, etc\u2026)\nThe paper also presents a comprehensive evaluation, including both objective and subjective measures, and compares the proposed models with relevant baselines, and the results demonstrate the effectiveness of the proposed approach for controllable symbolic music generation.\nIn order to improve the paper there are a few things which could be clarified.\nIt is mentioned that: \u201cWe also standardize the names of well-known musicians to their full names\u201d. How is this actually done? What is the reference for the composer\u2019s full name? It would be helpful to know the specific resources or authority used for this standardization (e.g., a music database, etc.).\nWhat % of the dataset was excluded due to genres with scarce presence? This would give a better understanding of how much data was deemed unusable and the potential impact on the diversity of the generated music. Would it be possible to assign these genres to broader categories?\nWhat are the details of the split size in the genre tagger? E.g. size of the validation split\nIn the objective evaluation: values closer to the ground truth indicate better performance. While these objective metrics are valuable, it would be beneficial to also assess the originality of the generated music to ensure the model is not simply reproducing segments from the training data. Metrics or techniques to measure novelty or dissimilarity from the training set could be considered.\nIn terms of formatting and grammatical errors, please consider the following:\nGrammar/Typos: \"scrapped\" \u2192 \"scraped\" (line 133); \"purpose \u2192 purpose\" (line 456)\nClarity: \"we decompose note-on events to beat and position\" (line 279) - clarify \"beat number and position within beat\"; \n\"The description of \"six special structural events\" (lines 308-314) is wordy.\"\nRepetition: \"Lines 182-184 are repeated; 332-335 was already mentioned before\"\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nDear Authors,\n\nThank you for your contribution to ISMIR 2025 with your paper titled \"Generating Symbolic Music from Natural Language Prompts using an LLM-Enhanced Dataset. Below is a summary of the reviews provided by the reviewers and the meta-reviewer, with some suggestions for improvement.\n\nThis paper introduces MetaScore, a new large-scale dataset of 963K musical scores with rich metadata. An LLM is used to generate pseudo-captions from the metadata, and text-to-music and tag-to-music models are trained on this dataset.\n\nSummary of strengths\n\nAll reviewers (R1, R2, R3, Meta) noted MetaScore as a significant dataset contribution, filling a crucial gap in richly annotated symbolic music datasets. The ethical considerations in data collection, by releasing only public domain files and sharing others for research upon request, were also commended (Meta, R2). The paper's use of LLMs to generate pseudo-captions from metadata was highlighted as a key strength, providing reusable insights for future research in leveraging LLMs for music datasets. The proposed text-conditioned music generation model, enabling controllable music generation through various musical properties, was also seen as a notable advancement (Meta, R2). Reviewers found the paper well-written and clearly structured (R1, R2, R3, Meta) and its topic highly relevant to the ISMIR community (R1, R2, R3, Meta). The inclusion of comprehensive objective and subjective evaluations, including expert musician listening tests, added credibility (Meta, R1).\n\nSummary of weaknesses\n\nNovelty concerns: mixed opinions on the paper's novelty, with the task itself being considered standard\nInsufficient technical detail: several aspects of the methodology, including data standardization, genre classification, and specific model modifications, lacked sufficient detail.\nPresentation and clarity issues: the paper contained minor formatting errors, typos, redundant phrasing, and occasional lack of clarity in descriptions.\nEvaluation nuances: while comprehensive, there were suggestions for incorporating discussions on the originality of generated music and statistical significance.\n\nHere some suggestions to address the weaknesses for the camera-ready version, but please also consider each of the individual reviews:\n\nClarify technical details:\nBriefly explain the method or reference used for standardizing composer names (e.g., specific music database, manual curation) (Meta, R2).\nState the percentage of the dataset excluded due to genres with scarce presence (Meta, R2).\nSpecify the validation split size for the genre tagger (Meta).\nProvide clearer explanations of several aspects mentioned in each of the reviews, e.g. \"beat number and position within beat\", the modifications made to the All-MiniLM-L6-v2 model for MST-Text (R1).\nBriefly acknowledge how the distinction between relative keys (e.g., G major and E minor) was handled during key signature extraction (R3).\n\n\nImprove presentation and clarity issues:\nProofread meticulously for any remaining typos and grammatical errors (e.g., \"scrapped\" to \"scraped)\nEnhance the explanation of the \"six special structural events\" (Meta).\nRemove redundant lines of text (e.g., lines 182-184 and 332-335) (Meta).\nConsider enhancing Figure 2 readability by augmenting font sizes for labels and considering the removal of tilted Y-labels where redundant (R2).\nVerify the citation for \"Text2midi\" (R3).\n\n\nAddress evaluation concerns\nIn the limitations section, briefly discuss the originality of the generated music and acknowledge this as a future research direction (Meta, R1).\nA brief mention of statistical significance for the results presented in Section 6.3 could strengthen the work (R2)\nOverall, we believe that this article will constitute a significant contribution to ISMIR 2025, and these revisions and improvements will enhance the paper's clarity, depth, and impact. \n\nBest regards,\nMeta Reviewer",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper investigates the use of a large language model (LLM), specifically the Bloom model [19], for generating captions that are then used to create music from text via the MST-Text model.\n\nStrenghts:\n-The experiment includes a listening evaluation conducted by expert musicians, which adds credibility to the results.\n-The experiments are well-described and show some improvements offered by the proposed approach.\n\nWeaknesses:\n-The experiments are well-described and clearly demonstrate the improvements offered by the proposed approach.\n\n-The generated music lacks naturalness and does not sound as convincing as other existing systems (e.g., https://musicgeneratorai.com/).\n\n-The MST-Text method seems to be a minor modification of the All-MiniLM-L6-v2 model [22], but the details provided are insufficient to fully understand the extent of the modification.\n\nRecommendation: \nDespite these weaknesses, I recommend accepting this paper as it introduces new ideas that could pave the way for future research on the application of large language models (LLMs) to Music Information Retrieval (MIR).",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe authors present MetaScore, a dataset with over 900k musical scores paired with textual metadata, collected from the MuseScore forum. A text-conditioned symbolic music generation model is proposed, controlling instruments, genre, composer, complexity and other features.\n\nThe background coverage is appropriate, although the inclusion of other relevant symbolic music datasets could have strengthen it (e.g. DadaGP dataset, GigaMIDI dataset). However, the contribution from MetaScore is clear. \n\nThe dataset collection, supported by Figure 2., seems consistent. A few doubts persist, after reading the section and inspecting the plots. For example, `Rock` seems to be, together with `Pop`, well represented in the dataset. However, the `guitar`, which I'd argue is heavily used in `Rock`, seems to be missing from the most common 10 instruments. Furthermore, the authors state that \"We also standardize the names of well-known musicians to their full\nnames; for instance, \u201cmozart\u201d is changed to \u201cwolfgang amadeus mozart.\u201d - it would be beneficial if a few sentences on why this was done could be presented. Lines 173 to 184 are redundant, as the authors explain the idea behind the MMT genre tagger twice - please correct this. Regarding the choice of the 8 genre tags, although the authors seem to be aware of this as a limitation, the inclusion of Pop with `Rock & Metal` seems to me inadequate, specially when in the `MetaScore-Genre` (presented in Table2) there seems to be a similar number of `Rock` and `Pop` songs.\n\nIn 5. the methodology seems appropriate. However, it is a bit unclear how the authors tackle the inclusion of genre, instrument, complexity and composer information. A good reference to include could be GTR-CTRL, in which the authors condition symbolic music generation with genre and instrumentation-specific tokens.\n\nRegarding 6, it would be interesting to see if the results presented in 6.3 have statistical significance - this could strengthen the work.\n\nFinally, the ethical approach of releasing only the files and metadata in the public domain is commendable. \n\nThe supporting github page is very well structured and positively impacts the paper.\n\nMinor remarks and nitpicks:\n\n- \"To approach text-to-music generation, we leverage a pre-trained large language model (LLM) to generate pseudo\nnatural language captions from the metadata.\" - it's not clear in the abstract if the authors used an LLM to get the metadata captions, or to generate music. Maybe rephrase if possible.\n- For Figure 2., despite it's relevance, I'd invite the authors to augment the fonts of the labels, in order to faciliate visualization. The tilted Y label (e.g. Copyright Level, Genre, Time Signature) could easily be dropped in order to save space, as they can be inferred from the plot name easily.\n- In 3.1. - \"We scrapped...\" - I believe the right term is \"scraped\".\n- I'm assuming in 3.3. that `MetaScore-Genre` is the subset of `MetaScore-Raw` that has native genre annotations - if that is the case, please clarify. It is specified in 4., but not at this point.\n- Footnote 5 - is \"copyright\" suppose to be there?\n- In 8 - only \"used\" for research purposes (instead of \"use\").",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper significantly advances symbolic music generation through MetaScore, a large-scale, publicly accessible musicXML dataset, and innovative text- and tag-based models. The dataset\u2019s extensive metadata and the models\u2019 controllability offer reusable insights for MIR and multimodal AI research, enhancing user-centric music creation. Despite minor technical ambiguities, the work is impactful and acceptable for its contributions and broad applicability. I strongly hope that the dataset is made publicly available prior to the paper's publication.\n\nI lowered the score because I rediscovered a significant flaw in this paper:\n-\"Similar to MMT [15], we decompose note-on events to beat and position to reduce the size of the vocabulary and to help the model learn the rhythmic structure of music.\"--- If this is the main goal of the paper, then the authors should have outperformed text2midi in terms of groove consistency which isn't the case. Also, this authors didn't report on any structure related metrics like Compression ratio as in the text2midi paper. It brings into question how this model performs structurally. \n\nRaised questions:\n- Key Signature Extraction: The paper states that key signatures were extracted from MuseScore files, yet MuseScore is known not to explicitly distinguish between relative keys (e.g., G major and E minor). How were G major and E minor separately identified in Fig. 2\u2019s data statistics? The authors are requested to provide an explanation (e.g. any preprocessing or analytical techniques applied).\n- Key Distribution Imbalance: Significant underrepresentation of common keys like C major, as observed in Fig. 2. How does this imbalance impact the generalizability of the proposed models, and were any augmentation strategies considered?\n- Citation [1]: I wasn\u2019t able to find the paper {F. Bhandari and C. Others, \u201cText2midi: Generating symbolic music,\u201d Journal of Music and AI, vol. 1, pp. 100\u2013110, 2024}. DId you mean {Bhandari, Keshav, et al. \"Text2midi: Generating Symbolic Music from Captions.\" Proceedings of the AAAI Conference on Artificial Intelligence, vol. 39, no. 22, 2025}?"
    },
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Since 2023, generative AI has rapidly advanced in the music domain. Despite significant technological advancements, music-generative models raise critical ethical challenges, including a lack of transparency and accountability, along with risks such as the replication of artists\u2019 works, which highlights the importance of fostering openness. With upcoming regulations such as the EU AI Act encouraging open models, many generative models are being released labelled as \u2018open\u2019. However, the definition of an open model remains widely debated. In this article, we adapt a recently proposed evidence-based framework for assessing openness in LLMs to the music domain. Using feedback from a survey of 110 participants from the Music Information Retrieval (MIR) community, we refine the framework into MusGO (Music-Generative Open AI), which comprises 13 openness categories: 8 essential and 5 desirable. We evaluate 16 state-of-the-art generative models and provide an openness leaderboard that is fully open to public scrutiny and community contributions. Through this work, we aim to clarify the concept of openness in music-generative AI and promote its transparent and responsible development.",
      "abstract": "Since 2023, generative AI has rapidly advanced in the music domain. Despite significant technological advancements, music-generative models raise critical ethical challenges, including a lack of transparency and accountability, along with risks such as the replication of artists\u2019 works, which highlights the importance of fostering openness. With upcoming regulations such as the EU AI Act encouraging open models, many generative models are being released labelled as \u2018open\u2019. However, the definition of an open model remains widely debated. In this article, we adapt a recently proposed evidence-based framework for assessing openness in LLMs to the music domain. Using feedback from a survey of 110 participants from the Music Information Retrieval (MIR) community, we refine the framework into MusGO (Music-Generative Open AI), which comprises 13 openness categories: 8 essential and 5 desirable. We evaluate 16 state-of-the-art generative models and provide an openness leaderboard that is fully open to public scrutiny and community contributions. Through this work, we aim to clarify the concept of openness in music-generative AI and promote its transparent and responsible development.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Roser Batlle-Roca",
        "Laura Ib\u00e1\u00f1ez-Mart\u00ednez",
        "Xavier Serra",
        "Emilia G\u00f3mez",
        "Mart\u00edn Rocamora"
      ],
      "authors_and_affil": [
        "Roser Batlle-Roca (Universitat Pompeu Fabra)*",
        "Laura Ib\u00e1\u00f1ez-Mart\u00ednez (Universitat Pompeu Fabra)",
        "Xavier Serra (Universitat Pompeu Fabra)",
        "Emilia G\u00f3mez (Joint Research Centre, European Commission & Universitat Pompeu Fabra)",
        "Mart\u00edn Rocamora (Universitat Pompeu Fabra)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Generative Tasks",
        "Music generation",
        "Philosophical and ethical discussions",
        "Qualitative evaluations",
        "Reproducibility",
        "MIR tasks",
        "User-centered evaluation",
        "Evaluation, datasets, and reproducibility",
        "Evaluation methodology",
        "Human-centered MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1rYw9aqEN-1h3nMTj-V6fkpqxn14mVfAH/preview",
      "poster_pdf": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-14-musgo-a-community",
      "title": "MusGO: A Community-Driven Framework for Assessing Openness in Music-Generative AI",
      "video": ""
    },
    "forum": "33",
    "id": "33",
    "pic_id": "",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nDisagree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nDisagree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nStrongly Disagree (Well-explored topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nDisagree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe ideas in the paper are mostly familiar already; the value comes in pulling them together in one place and allowing easy to understand comparisons between different models.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nOpenness of AI music generation research can be assessed in a tabular form and a numerical score according to the presence or absence of several key features.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper presents a space of 13 dimensions (8 rated \"essential\" and 5 \"nice to have\") for assessing the openness of a music-AI system. Essential dimensions are rated on a 3-point scale while nice-to-have dimensions are binary. From this, a numerical openness score can be generated and comparison tables can be plotted to show how different models compare in terms of openness.\n\nThe idea itself is simple and elegant, and I appreciate the way the tabular approach in particular can make it easy to glance across several models to understand their conformance with different aspects of open science. I am less convinced about whether a sum total openness score is all that meaningful, or even whether openness is a universal property at all: it seems like the better question might be \"open for what, by whom?\" For example, the presence or absence of training data might be important or not depending on is trying to use the model.\n\nThe paper discusses a survey of 110 members of the ISMIR community which helped shape the presented framework. This level of input is nice, though I have some doubts about the depth of that engagement. As far as I can tell, the authors proposed a similar framework to begin with, and the feedback was used to make minor adjustments to it and to separate dimensions into whether or not they were essential. It would have been more interesting to give respondents more space to discuss what exactly they look for in replicating or building on existing work, which might have flagged up other categories the authors did not think of.\n\nOverall, then, the paper might not be hugely groundbreaking, but it presents an important idea in a digestible way and could gain significant and beneficial traction in the community for that reason.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThe reviewers have a spread of opinions about this paper, from strong accept (R3) to weak reject (R2). The largest concern of the reviewers is whether this framework represents a novel contribution compared to the existing body of work in evaluating the openness of AI tools more generally. R2 points out that most or all of the categories in MusGO are domain-agnostic. What is domain-specific about this work, and how can readers be sure that a more general framework is appropriate for this domain? R3 similarly asks for a \"discussion of the changes that were needed to the original framework to adapt to the music domain\". The authors could improve the paper by clarifying how the framework was arrived at and giving some examples of how it can be applied.\n\nA secondary concern, articulated in detail by R1, has to do with whether it makes sense to quantify \"openness\" to begin with, or whether it might be more useful to treat it qualitatively. I share this concern and would invite the authors to address it more thoroughly, perhaps as part of explaining the beneficial applications of the framework. R1 also identifies some issues in the quantitative analysis that should be corrected.\n\nThe paper nonetheless addresses an important issue and, novelty concerns notwithstanding, the reviewers generally find the work satisfactory and potentially beneficial. My meta-review therefore leans slightly toward acceptance with an encouragement for the authors to thoroughly address the comments raised by the reviewers around novelty, applications and analytical methodology.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper identifies a lack of clarity in the labeling of music-generative AI tools as 'open' and proposes an evaluative framework for determining the types and degrees of openness present in a music-creating AI system.\n\nI think this paper does a fairly good job of explaining both why openness is so hard to define generally and why it is difficult to achieve in the specific use case of music creation. I think it would benefit from either an explicit summary/restatement of the OSAID or a brief taxonomy or chart of the main parameters of openness given in existing frameworks; the paper (in 2.1) notes the challenge of defining openness without quite giving a clear vision of what the current consensus is (even if it is a contested consensus). I feel like I don't really understand the term's boundaries until I get to the results section, which is late.\n\nOne conceptual framing I might push the authors on is to suggest that the evaluation of openness may fundamentally need to be *qualitative* in nature. The temptation to quantitatively rate categories of openness and adjudge numerically that one system is 'more open' than another is extreme, and indeed the paper eventually concludes that it must give a rank-score evaluative framework. I think the paper's qualitative orientation (methodologically, it is a survey) aligns with a view of openness as holistic and multifaceted rather than something that can ultimately be reduced to a weighted score; perhaps we should rely more on our faculty of judgment than on the end calculation.\n\nThe methodology in 3.2.1 notes that the participants were asked to rate relevance on a five-point Likert scale, and Table 1 reports both the mean and median scores for these ratings. Reporting of the median is appropriate, but reporting of the mean is not, since the participants are giving ordinal-level data and thus the mean is an inappropriate measure of central tendency. This should be removed for the paper to be scientifically correct statistically. There's also no need to take the median reporting out to two decimal places.\n\nThe evaluation carried out in 4.2 is described as using 'a structured and iterative methodology', which appears to follow a type of consensus model, which I'm fine with, but there should be reference to precedent in the form of cited authorities for the research method chosen here.\n\nThe use of the phrase 'openness diversity' in 5.1 strikes me as rather euphemistic. I'd use the phrase 'openness variation' instead.\n\nI wonder if 5.2 needs a rethink/rewrite. This subsection goes 'beyond openness' to address other large-scale ethical quandaries relating to AI, and since the first paragraph of the paper's introduction frames the work as being motivated by substantial ethical concerns, it makes perfect sense to discuss them robustly. However, I think it would make more sense for this section to discuss how knowledge about openness (and thus the MusGO openness evaluation tool) can aid researchers in evaluating the moral rectitude of, and perhaps even arguing for changes in the governance of, AI systems. The discussion of the clash between openness and IP requirements is good (though brief), but most of the discussion in this section is about ethical problems rather than about how looking for openness can help mitigate those problems. Some of the problems probably can't even be helped by evaluating openness \u2013 for instance, the discussion of harmful or inappropriate uses explicitly notes that openness has limited ability to impact the issue. The subsequent discussions of Western bias and economic fallout set them up as if they are parallel issues to openness, but it seems to me that one of the primary benefits of examining openness is that this could make clearer for users and researchers what kinds of bias or what kinds of downstream fiscal impacts these tools might have. They are thus intertwined issues, rather than parallel ones.\n\nA minor style quibble \u2013 I find the phrase 'nice-to-have' a little gratingly informal, as if these elements were desserts or luxury goods. 'Desirable' or 'preferable' would capture the meaning just as well, or even reframing 'essential' and 'nice-to-have' as 'primary' and 'secondary' factors.\n\nAnother minor pet peeve \u2013 MusGO doesn't map very neatly as an abbreviation of \u201cMusic-Generative Open AI\u201d; I wish in general that institutions and research groups chose their initialisms and reductions less for cuteness or catchiness and more to accurately represent the full given name.\n\nThe abstract has a few awkward turns of phrase and could use a light copyedit; there are other places in the paper where the language is a bit difficult to parse, and a once-over for grammar and sentence structure would be nice.\n\nWhile I see quite a few areas that need some adjustment, they are all relatively simple fixes that I think can be taken care of in the review period.",
      "review2": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nStrengths: The idea of having a centralised resource to conveniently lookup various music generation models and how open they are is useful for researchers. It will make it easier to compare with existing models and work with open sourced models.\n\nWeaknesses: My main criticisms are with respect to the novelty and practical applications of this tool.\n\nNovelty: As the paper references, there are already tools that provide information about model details. For example, Model cards (Mitchell et al.) has License, training data, paper reference, model information etc. which overlaps significantly with the categories provided by MusGo. \n\nIt is unclear what is domain specific about any of the categories in MusGo. All the 13 categories in Figure 1 seem domain agnostic to me. There is a small discussion about training data being treated differently for music in line 426-428, but I was not able to notice any other domain specific considerations. \n\nPractical applications: It is unclear how MusGo would be used in practice because Section 4.2 outlined how classifying models required an iterative effort with multiple people. How would a new model be categorised? Can anybody fill in details? Is there an official review process? If so, who selects the review committee?\n\n\nMinor comments:\n- Table 1: Insufficient details about what the different criteria are. What are data sheets? What is package?\n- line 359: Incorrectly referencing Table 1\n- line 368: Not sure how this formula was created",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nI enjoyed reading this paper, both due to its contributions and its writing. The topic is highly relevant and timely, seeing as music GenAI has become prevalent only in recent years, and regulations are still being created. I believe the paper will spark interesting discussions at ISMIR and beyond, possibly even on a regulatory level. Therefore, I suggest to accept this paper.\n\nStrengths\nWhile the openness evaluation framework is not new, as the authors mention, it is very important to validate and adapt its evaluation criteria in specific domains. The authors do both by collecting MIR researchers\u2019 input on the existing framework, and adapting the framework and guidelines to the music domain. Then, they also demonstrate how the framework can be used on 16 music GenAI models, and have set up a leaderboard.\n\nThe leaderboard as included in the paper and on the website, is clearly formatted, and it is well-argued which categories are included and in what way.\n\nThe writing and structure of this paper are also very good. \n\nWhile I will list my suggestions for improvement below, I believe the paper as is is already quite strong.\n\nWeaknesses\nIt would be interesting to see more of the qualitative insights, on which categories there were most comments, and how many participants added comments. For reproducibility, it would be good to share the exact phrasing of the questions. It would also strengthen the paper to show more details on the model categorization discussion in Section 4.2, such as which were the categories and/or models that sparked most discussion, how often discussion was needed, and how such discussions were resolved.\n\nWhile a repo is shared for which it is stated that it will allow for public scrutiny and contributions, it is not yet clear what that process would look like in practice, even though this is one of the main added values of this work.\n\nI miss a discussion of the changes that were needed to the original framework to adapt to the music domain. Even though there are some changes mentioned in section 3, there could be a more in-depth reflection.\n\nSome sentences (e.g., line 414-417) in the discussion imply that the final framework and leaderboard were validated beyond the authors, even though that is not the case.\n\nIt would be useful to more clearly refer to the supplementary material and/or the website containing the more detailed descriptions of all categories and ratings. Now, from the paper alone, it does not always become clear what would warrant a closed/partial/open judgement for each category.\n\nThe work mentions transparency as one of its main goals, but does not define this concept in the context of this work. It would be good to add such a definition.\n\nQuite some of the arXiv-references are incomplete, e.g., missing publication year or ID.\n\nWhile this work contains a user study, no information on ethical approval is given."
    },
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Vision-to-music generation, including video-to-music and image-to-music tasks, is a significant branch of multimodal artificial intelligence demonstrating vast applications like film scoring and short video creation. However, research in vision-to-music is still in its preliminary stage due to its complex internal structure and the difficulty of modeling dynamic relationships with video. Existing surveys focus on general music generation without comprehensive discussion on vision-to-music. In this paper, we systematically review the research progress in the field of vision-to-music generation. We first analyze the technical characteristics and core challenges for three input types: general videos, human movement videos, and images, as well as two output types of symbolic music and audio music. We then summarize the existing methodologies from the architecture perspective. A detailed review of common datasets and evaluation metrics is provided. Finally, we discuss current challenges and future directions. We hope our survey can inspire further innovation in vision-to-music generation and the broader field of multimodal generation in academic research and industrial applications.",
      "abstract": "Vision-to-music generation, including video-to-music and image-to-music tasks, is a significant branch of multimodal artificial intelligence demonstrating vast applications like film scoring and short video creation. However, research in vision-to-music is still in its preliminary stage due to its complex internal structure and the difficulty of modeling dynamic relationships with video. Existing surveys focus on general music generation without comprehensive discussion on vision-to-music. In this paper, we systematically review the research progress in the field of vision-to-music generation. We first analyze the technical characteristics and core challenges for three input types: general videos, human movement videos, and images, as well as two output types of symbolic music and audio music. We then summarize the existing methodologies from the architecture perspective. A detailed review of common datasets and evaluation metrics is provided. Finally, we discuss current challenges and future directions. We hope our survey can inspire further innovation in vision-to-music generation and the broader field of multimodal generation in academic research and industrial applications.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Zhaokai Wang",
        "Chenxi Bao",
        "Le Zhuo",
        "Jingrui Han",
        "Yang Yue",
        "Yihong Tang",
        "Victor Shea-Jay Huang",
        "Yue Liao"
      ],
      "authors_and_affil": [
        "Zhaokai Wang (Shanghai Jiao Tong University)*",
        "Chenxi Bao (DynamiX)",
        "Le Zhuo (Shanghai AI Laboratory)",
        "Jingrui Han (Beijing Film Academy)",
        "Yang Yue (Tsinghua University)",
        "Yihong Tang (McGill University)",
        "Victor Shea-Jay Huang (DynamiX)",
        "Yue Liao (The Chinese University of Hong Kong)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Generative Tasks",
        "Applications",
        "Music generation",
        "Multimodality",
        "Music and audio synthesis",
        "Music videos, multimodal music systems",
        "MIR tasks",
        "MIR fundamentals and methodology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1OJ9DbBPGo9BB0MKNrtXZh1WB3rrv20hV/preview",
      "poster_pdf": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-13-a-survey-on",
      "title": "A Survey on Vision-to-Music Generation: Methods, Datasets, Evaluation, and Challenges",
      "video": ""
    },
    "forum": "38",
    "id": "38",
    "pic_id": "",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nStrongly agree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nStrongly agree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nStrongly agree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nA well-written, (mostly) complete survey of vision-to-music generation works covering methods, datasets, evaluation, and challenges. The work notes that video-to-music generation works are still in early stages and have yet to fully have a complete treatment in the academic literature (I agree). This work will help advance this progress and is worth a read for anyone looking to start working on the topic.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nA well-written, (mostly) complete survey of vision-to-music generation works covering methods, datasets, evaluation, and challenges.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nSummary: \nIn this work, the authors present a survey of vision-to-music generation works covering the topics of methods, datasets, evaluation, and challenges. Multiple timelines of representative works and categorizations of methods, datasets, eval methods, and similar are provided as well as example architecture diagrams. The references section is also extensive.\nMajor/minor comments\nOverall, very nice survey and informative work. The work is well organized at a high-level and has well written sentence structure at a low level. The survey topics of methods, datasets, evaluation, and challenges are well done as well, along with the extensive references section.\nRegarding issues for improvement, I would suggest \n\u2022 Clarification on the language of rhythmic videos. In the intro, you break down the topic into three main areas 1) general videos 2) human movement videos and 3) images. Here, the focus on \u201chuman movement videos\u201d is more focused than parts of later in the work that discuss rhythmic videos where human movement videos are a subset. It could be useful broaden the focus of human movement videos to videos with rhythmic motion where human movement videos are a subset.\n\u2022 When commenting on page 2 and elsewhere \u201cHowever, audio music lacks controllability, and the generated music is typically shorter (usually under 20 seconds) due to sampling rate limitations, I would argue this is generally untrue. Over the last few years, there have been several works showing extensive controllability for audio-domain music generation (e.g. Music ControlNet) as well as long-form generation (StableAudio). The controllability of symbolic-domain music is also focused on more typical note-level control, however.\nThe issues above are minor and easily addressable.\n\nGrammatical comments:\n\u2022 Abstract: \u201cVision-to-music Generation\u201d -> \u201cVision-to-music generation\u201d\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak reject\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nSummary: In this work, the authors present a survey of vision-to-music generation works covering the topics of methods, datasets, evaluation, and challenges. Multiple timelines of representative works and categorizations of methods, datasets, eval methods, and similar are provided as well as example architecture diagrams. The references section is also extensive.\n\nInitial Scores: 2 strong accept, 2 strong rejects\n\nMetareview: Overall, initial reviews were very mixed because of the format of a survey paper.\nOverall, everyone agrees the paper has strong points\n-R1 \"excellent tables with collected systems, datasets, metrics and the structured information like durations, music length, and so on\"\n-R2 \"very ambitious article that tackles the state of the art on vision-to-music generation, looking at models, datasets and evaluation\"\n-R2 \"timely, clear, and comprehensive contribution to an emerging multimodal field and will be of immediate use to both academic and applied communities. It is presented in a way that is very useful and very clear.\"\n-R3 \"datasets and metrics tables are a helpful views of the research landscape presented in a well-formatted and highly usable format for future researchers.\"\n\nAreas for improvement\n-R1 \"I'm not really sure what the contribution of the paper is.\"\n-R1 \"The identified challenges and their importance would be a lot more convincing if they were contextualized with impact\"\n-R3 \"However, outside of compiling information, the paper does not engage with the material very deeply and is therefore not original or impactful.\"\n-R3 \"These insights are very surface level.\"\nDiscussion: The discussion focused on issues w.r.t. technical correctness and scope which were mistakenly flagged by the meta-reviewer as minor issues before the discussion. R1 and R2 confirmed concern on some of the technical descriptions as well as the idea that this could become a wonder (potentially long-form) paper if refined a little bit more.\n\nRecommendation: Reject (Weak)",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nI've had a difficult time with this paper. On the one hand, I greatly appreciate the excellent tables with collected systems, datasets, metrics and the structured information like durations, music length, and so on. On the other hand, I'm not really sure what the contribution of the paper is. The identified challenges and their importance would be a lot more convincing if they were contextualized with impact. If the intended contribution of the paper is to attempt to establish a shared taxonomy for vision-to-music generation I would like more details on the process for that.\n\nI think this would become a wonderful review article if it receives an additional pass with details ironed out, and some sections merged/split/rearranged. Additionally, some smaller technical details seem wrong to me, or at least misleading, which isn't up to the standard I would like to see for a published survey paper that people would rely on.\n\nSome potential improvements:\n\n- 344: What is Frechet Distance in this context? Frechet Inception Distance (FID) or something else? Strongly recommend not just referring to the general measures of probability distribution similarity. Typically we mean KL-divergence given some specific, suitable feature representation, and not just time-domain audio or MIDI byte sequences.\n\n- Why is FAD under \u201cmusic-only\u201d metrics while CLAP is not. Especially considering 380-381\n\n- 381: KL isn\u2019t trained\n\n- \u201cCLAP score\u201d needs clarification. Do we mean cosine similarity between text and audio embeddings? How is that a vision-music correspondence? Or do we mean adapting CLAP to CLIP (e.g. wav2clip) to get a matching still frame (image) encoder as well?\n\n- I wonder if a better term for \u201cvision-to-music generation\u201d is \u201csoundtracking\u201d or \u201csoundtrack generation\u201d. Perhaps too limiting in use case, but just a loose thought.\n\n- Nice to avoid \u201cgenerative music\u201d as term due to historical meanings, rather see \u201cmusic generation\u201d in ethics statement\n\n- 148-149 claim deserves backing reference (or remove)\n\n- I would like to see a distinction between input types for music videos (video created for the song) and soundtracking (songs selected for the video) in figure 2\n\n- Add compute resources to table 1 (e.g. GPU hours needed to train the final system)",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis is a very ambitious article that tackles the state of the art on vision-to-music generation, looking at models, datasets and evaluation. This paper makes a timely, clear, and comprehensive contribution to an emerging multimodal field and will be of immediate use to both academic and applied communities. It is presented in a way that is very useful and very clear. While clearly structure and spacing were very well thought out to allow the necessary content to fit, model listing could have been extended with additional useful information on open-sourceness, existence of demo UIs, pretrained weights, etc. There could have been quantitative summary of trends (eg. Yearly evolutions of number of models, model types, dataset sizes, etc.) possibly in graphical or tabular form, but again it had to be a compromise in available space. There are some claims that are too definitive (\u201cNo previous surveys have focused on vision-to-music generation\u201d without \u201cto the best of our knowledge\u201d; \u201cCLAP is the dominant method\u201d instead of \u201c\u201cCLAP is currently one of the most widely used models\u201d), but given the overall soundness of the paper they become admissible.",
      "review3": "- **Overall Evaluation**: Strong reject\n\n#### Main Reviews\n\nThis datasets and metrics tables are a helpful views of the research landscape presented in a well-formated and highly usable format for future researchers. However, outside of compiling information, the paper does not engage with the material very deeply and is therefore not original or impactful. I expect a good survey to point out trends, identify open questions, and reflect on the big picture of where we started and where we're going. The extent of these insights are:\n1. There is a lack of standardized datasets and benchmarks\n2. Customization and controllability are important for practical use\n3. A promising direction is to combine symbolic and audio methods\n\nThese insights are very surface level. You have laid out a list of evaluation metrics used in the literature. Are there gaps? You analyze 3 input types: general videos, human movement videos, and images. Why are these three the focus? What are the other input types? L402: \"Exploring how to align these technologies with applications offers significant commercial opportunities\". For example? The ethical statement in section 8 simply says \"we think the ethics of this work should be considered\" but does no critical thinking to further the consideration.\n\nThe paper also makes a number of claims that are unsubstantiated opinions:\n\nL132: \"we will mainly focus on general videos and images, while paying relatively less attention to human movement videos. This is because their semantic association with music is not strong\" and \"Their application scenarios are also relatively limited.\"\n\nTo me, it's clear that the semantic association with dancing videos with music is strong, and there are many application scenarios.\n\nL278: \"the diversity of content and styles in [music videos] may be limited\"\n\nTo me, this is not the case. Music videos are incredibly diverse in content and style."
    },
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Sensory dissonance (SD) quantifies the interference between partials in a mixture of simultaneously sounding tones and correlates with the perceived dissonance or unpleasantness of this mixture. While it is mainly studied in music perception, often using synthetic signals or symbolic inputs, in this paper, we focus on a practical application and investigate SD as a tool for analyzing the interactions between voices in multi-track music recordings. Using visualization and statistical analysis on an existing dataset of four-part chorales recorded with various wind instruments, we examine how timbre, tuning, and score influences SD. To do this, we introduce the notion of relative SD, which quantifies how individual voices in a multi-track recording contribute to overall SD of their polyphonic mixture. In addition to discussing practical aspects of measuring SD between and within real music signals, our case study shows potential benefits and limitations of using SD as an analysis tool in music production, for example, to inform or automate tasks like take selection or equalization.",
      "abstract": "Sensory dissonance (SD) quantifies the interference between partials in a mixture of simultaneously sounding tones and correlates with the perceived dissonance or unpleasantness of this mixture. While it is mainly studied in music perception, often using synthetic signals or symbolic inputs, in this paper, we focus on a practical application and investigate SD as a tool for analyzing the interactions between voices in multi-track music recordings. Using visualization and statistical analysis on an existing dataset of four-part chorales recorded with various wind instruments, we examine how timbre, tuning, and score influences SD. To do this, we introduce the notion of relative SD, which quantifies how individual voices in a multi-track recording contribute to overall SD of their polyphonic mixture. In addition to discussing practical aspects of measuring SD between and within real music signals, our case study shows potential benefits and limitations of using SD as an analysis tool in music production, for example, to inform or automate tasks like take selection or equalization.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Simon Schw\u00e4r",
        "Stefan Balke",
        "Meinard M\u00fcller"
      ],
      "authors_and_affil": [
        "Simon Schw\u00e4r (International Audio Laboratories Erlangen)*",
        "Stefan Balke (International Audio Laboratories Erlangen)",
        "Meinard M\u00fcller (International Audio Laboratories Erlangen)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Timbre, instrumentation, and singing voice",
        "Applications",
        "Musical features and properties",
        "Harmony, chords and tonality",
        "Music composition, performance, and production",
        "Music signal processing",
        "MIR fundamentals and methodology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/17BiprOPuEUSqYWlYrn0XWNKRSlxqXBG1/preview",
      "poster_pdf": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-14-measuring-sensory-dissonance",
      "title": "Measuring Sensory Dissonance In Multi-Track Music Recordings: A Case Study with Wind Quartets",
      "video": ""
    },
    "forum": "46",
    "id": "46",
    "pic_id": "",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe idea of exploring automatic ways to measure dissonance and use that information for different tasks is a novel and interesting one.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nSensory Dissonance can be computed automatically and initial experiments with a dataset of wind quintets shows some promise.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak reject\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThe paper is well written and clearly describes the concept proposed and the experiments conducted. The main idea is to compute automatically a measure of time-varying sensory dissonance and use visualization and statistical analysis to support/validate the approach. \n\nMinor comments: \nThere is no Figure 1e \n\n\n\nAlthough dissonance has been extensively explored using synthetic and psychological stimuli, a more automatic approach as proposed by the authors is something interesting and potentially useful.\n\nThe experiments described show some promise but are limited by the dataset and a specific metric. They basically show that some approximation of dissonance can be computed and potentially be used. In my opinion that is not enough to warrant publication. Ideally the authors should have done some listening studies to verify that what they are measuring correlates to human perception and have some concrete examples of how it could be used for take selection and/or equalization as proposed. \n\nI really like the idea but there needs to be more work for publication.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThere is considerable agreement among the reviewers and the meta-reviewers regarding the work described in this paper. They all agree that the paper is well written and the idea of automating calculating dissonance has novelty. They also raise some important questions regarding limitations of the approach such as the experiments being conducted on a limited dataset, no listener/user study validation, and limited comparison of different approaches for example the pitch detection is somewhat unusual and it is unclear what are the benefits. \n\nIt is more of a pilot proof-of concept paper that show there is promise in the approach proposed. The future work and application are potentially very interesting. \n\nBased on these considerations I recommend weak acceptance as we would have liked either more experimental evaluation of various parameters and algorithm choices or more exploration of potential applications or both.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper presents a case study of applying sensory dissonance (SD) models to multitrack audios of wind instrument recordings. The writing is clear and the exploratory findings are interesting, with careful analyses of the contributions of timbre, tuning and score to SD. Authors argue that SD would be helpful in automating studio recording tasks such as take selection and equalization, but these applications are delegated to future work. More specific comments follow.\n\nIn equation (1) there is a subscript missing in the term w(a_i,a_j).\n\nSome motivation should be given for the use of the mean frequency (f_i+f_j)/2 in adapting the pairwise dissonance curve. It is understood that for f_i close to f_j the two sinusoids may be perceived as one with the mean frequency and a beating associated to (f_i-f_j)/2, but in this adaptation, the dissonance measures for either f_i or f_j would reflect this latter halved distance, which is equivalent to a warping in the dissonance kernel, making your actual dissonance curve wider (i.e. closer to Vassilakis'). If symmetry is the goal here, then a redefined expression for d(f_i,f_j) could be given, explaining how the frequency distance |f_i-f_j| is preserved.\n\nThe dependence of the SD values on the hypothesis that all instruments have harmonic spectra is very important, and in my opinion should appear earlier than the discussion of peak picking strategies (Section 2.2). Since it is a natural consequence of the case study scenario, mentioning it in the introduction (in the context of Fig. 1) would help the reader understand that this is a requirement of the method (and not an accidental property of an otherwise illustrative example).\n\nIn Fig. 4 there is a mention to blue rectangles, but it appears that only blue markings are visible.\n\nThe terminology \"mean tuning variability\" is very confusing for the numbers discussed after Fig. 7, because these variabilities are of SD values rather than of tuning. Maybe \"mean SD variability\" would help the reader avoid this confusion.",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper introduces the concept of relative sensory dissonance (SD) as a tool for analyzing multi-track music recordings by quantifying how individual voices contribute to the dissonance. By exploring the relationship between SD and timbre, tuning, and score, the authors demonstrate that timbre is the dominant factor influencing SD, and they propose practical applications for music production.\n\nThe paper is well-written and easy to follow. The main contribution, the formalization of relative SD, is highly relevant for readers and ISMIR attendees. Additionally, the inclusion of a Python library for reproducibility significantly strengthens the contribution to the research community. Therefore, my overall recommendation is a strong accept.\n\nMy main suggestion, or perhaps a recommendation for future work, is that conducting listener studies to correlate SD with perceived dissonance would greatly enhance the proposed method.\n\nSome minor issues:\n\n- Section 1: The authors could briefly clarify whether dissonance in this context is the same as frequency masking, and if not, explain the differences.\n\n- Line 61: A brief definition and reference for salient tonal components would help non-expert readers.\n\n- Line 63: \"we can calculate the overall SD (see Section 2)\" \n\n- Line 121: What is the main motivation for modifying the kernels proposed in [15]?\n\n- Line 128: Define \"high salience\" \n\n- Line 182: Figure 1d?\n\n- Line 192: Define \"DFT\"\n\n- Line 199: The paper could benefit from a brief discussion of the consequences of assuming a quasi-harmonic overtone structure. How would SD perform with different types of overtone structures, and is dissonance still relevant for such musical sources?\n\n- Sections 2.2 and 3.1: Loudness normalization of Pv is mentioned, but it would be helpful to discuss why an audio-based loudness normalization approach would not achieve loudness invariance in the experiments. For example, Figure 6 shows that the loudness of the take has a significant effect on SD.\n\n- Figure 7: It is unclear which plots correspond to Dv,v and Dv,v_hat",
      "review3": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nThe paper presents the concept of \"sensory dissonance,\" which measures the weighted difference between a pair of tonal peaks in the spectrum. Experiments are based on ChoraleBricks dataset, which plays ten Baroque chorales using different wind instruments. \n\nIt appears that the authors' main purpose is to provide an analysis of the proposed metric on this dataset, but I found the study is limited only to this particular dataset and the proposed metric, while there could be clearer benefits of using this metric or some practical applications that can benefit from the concept. \n\n- First, the definition of the dissonance kernel (Figure 2) lacks supporting arguments. It is different from the existing methods, but it's not clearly defined, to begin with, (i.e., there's no equation), while it is not clearly explained why and how it has to be different from other methods. \n\n- Meanwhile, the comparison between two tonal peaks, f_i, and f_j, are based on the assumption that their harmonics are not involved in the comparison. For Figure 2, for example, there can be most probably another peak at 400Hz but how does the kernel capture it?\n\n- Actually, the proposed method relies heavily on peak detection algorithms proposed in Sec. 2.2, where harmonic components are taken into account separately. While the process appears to be okay-ish, it's not proven to be robust to various real-world recording environments, e.g., music with percussive instruments, where finding f_0 and tonal peaks is known to be difficult. Essentially, the ChoraleBricks dataset is a favorable one for this kind of concept but the algorithm seems to be limited only to the dataset. \n\n- There are a few hyperparameters that could greatly affect the accuracy of the algorithm, e.g., \\gamma. \n\n- Most importantly, the proposed SD concept doesn't seem to be able to represent/correlate to perceived dissonance by human listeners. Provided results compare some results by replacing instruments, but given that the results are not correlated to the perceived dissonance, it is challenging to grasp the main insight the authors want to provide. \n\nMinor issues. \n\n- Figure 1e doesn't exist, although it was referred in line 281. \n- Eq 1. w(a, a_j) -> w(a_i, a_j)"
    },
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Joint embedding spaces have significantly advanced music understanding and generation by linking text and audio through multimodal contrastive learning. However, these approaches face large memory requirement limitations due to relying on large batch sizes to effectively utilize negative samples. Further, multimodal joint embedding spaces suffer from a modality gap wherein embeddings from different modalities lie in different manifolds of the embedding space.\n\nTo address these challenges, we propose Siamese Language-Audio Pretraining (SLAP), a novel multimodal pretraining framework that allows learning powerful representations without negative samples. SLAP adapts the Bootstrap Your Own Latent (BYOL) paradigm for multimodal audio-text training, promoting scalability in training multimodal embedding spaces.\n\nWe illustrate the ability of our model to learn meaningful relationships between music and text --- specifically, we show that SLAP outperforms CLAP on tasks such as text-music retrieval and zero-shot classification. We also observe competitive downstream performance on several MIR tasks, including with larger or supervised models (genre and instrument classification, auto-tagging).\n\nAdditionally, our approach has attractive properties, such as a quantifiably reduced modality gap and improved robustness to batch size variations on retrieval performance.\nFinally, its novel formulation unlocks large-scale training on a single GPU through gradient accumulation. ",
      "abstract": "Joint embedding spaces have significantly advanced music understanding and generation by linking text and audio through multimodal contrastive learning. However, these approaches face large memory requirement limitations due to relying on large batch sizes to effectively utilize negative samples. Further, multimodal joint embedding spaces suffer from a modality gap wherein embeddings from different modalities lie in different manifolds of the embedding space.\n\nTo address these challenges, we propose Siamese Language-Audio Pretraining (SLAP), a novel multimodal pretraining framework that allows learning powerful representations without negative samples. SLAP adapts the Bootstrap Your Own Latent (BYOL) paradigm for multimodal audio-text training, promoting scalability in training multimodal embedding spaces.\n\nWe illustrate the ability of our model to learn meaningful relationships between music and text --- specifically, we show that SLAP outperforms CLAP on tasks such as text-music retrieval and zero-shot classification. We also observe competitive downstream performance on several MIR tasks, including with larger or supervised models (genre and instrument classification, auto-tagging).\n\nAdditionally, our approach has attractive properties, such as a quantifiably reduced modality gap and improved robustness to batch size variations on retrieval performance.\nFinally, its novel formulation unlocks large-scale training on a single GPU through gradient accumulation. <br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Julien Guinot",
        "Alain Riou",
        "Elio Quinton",
        "George Fazekas"
      ],
      "authors_and_affil": [
        "Julien Guinot (Queen Mary University of London)*",
        "Alain Riou (LTCI, T\u00e9l\u00e9com-Paris, Institut Polytechnique de Paris)",
        "Elio Quinton (Universal Music Group)",
        "George Fazekas (Queen Mary University of London)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Indexing and querying",
        "Musical features and properties",
        "Representations of music",
        "Multimodality",
        "MIR tasks",
        "Lyrics and other textual data",
        "MIR fundamentals and methodology"
      ],
      "long_presentation": "TRUE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1_M345mZWCo41le6WfwjV_lpCscqrNaP1/preview",
      "poster_pdf": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-2-slap-siamese-language",
      "title": "SLAP: Siamese Language-Audio Pretraining without negative samples for Music Understanding",
      "video": ""
    },
    "forum": "47",
    "id": "47",
    "pic_id": "",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nDisagree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nDisagree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nWhile the proposed method is relevant to the specific task, I am not sure the paper itself provides a lot of insights that go beyond that specific scope. This is a great application of a method from an existing computer vision paper (BYOL), but I don't think it provide a lot more insights than the original paper\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nBasically a mix of multimodal contrastive learning and BYOL methods, adapted to the music-text modalities.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis is a very interesting paper proposing to adapt the method from BYOL to the text-music multimodal case.\nThis was worth trying, and the results seem to indicate that there is value in this method.\n\nMy main comment is that the paper is at times difficult to fully understand. For instance, the concepts of \u201conline\u201d and \u201ctarget\u201d representations are key to understand the method. However they are not explained in the paper. It is necessary for the reader to go back the original BYOL paper to understand these concepts.\nSimilarly, understanding why a \u201cstop-gradient\u201d step is included requires reading the original paper. I would therefore recommend to provide summarized explanations of all key concepts, in particular those of online and target representations.\n\nAlso, one noticeable difference with BYOL is the absence of data augmentation, which seems like a core aspect of BYOL. It would be interesting to elaborate in this paper on why not using data augmentation (other than the computational gain), and ideally to measure the impact of using vs not using data augmentation (although I understand this is probably a tough ask for a 1-2 weeks work, but that may be e.g. mentioned in future work.)\n\nWhich versions of the GTZAN and MTAT datasets are used exactly? Recent literature has favored using the same processed versions (fault-filtered, top 50 tags, etc.) corrected from their original versions (https://github.com/jongpillee/music_dataset_split). Is this the case here, or do you use the original datasets? \n(The latter would be difficult to comprehend as there has been ample literature of the shortcomings of these original datasets.)\n\nAn important paper on the topic of text-music multimodal contrastive learning is missing:\nEnriched Music Representations With Multiple Cross-Modal Contrastive Learning (2021)\nhttps://ieeexplore.ieee.org/abstract/document/9395210\n\nThe closest existing work is cited in refs [40] to [43], and it would be interesting for the reader to find a more det describe\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nStrong accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThis is a relevant paper for ISMIR, and there is a consensus among the 4 reviewers on its suitability for presentation at the conference. Note that there are also a few recommendations that would certainly further improve the paper. Please do go through all reviews and consider all these recommendations.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper proposes using EMA to learn a joint embedding space for text and audio modalities. With the EMA mechanism, \nthe need for negative samples are nullified and computation costs are reduced. It is a clever way to fuse two modalities since this method does not explicitly push one embedding away from another, which may encourage the embedding spaces of the two modalities to fuse as much as possible. The authors discuss the embedding space gap in their experiments, which justifies this benefit. The paper is well written, and the figure looks intuitive and attractive. Good job on the idea and the detailed experimental results! I look forward to your open-source model and code!\n\nAdditionally, I have a question. I notice your probing attributes do not include key or chord progression, which I think a good general audio\u2013text embedding space might fail to recognize, but a good music\u2013text embedding space should. Would you consider incorporating more music-specific designs or inductive biases in your model training to see if your method can fully handle a music\u2013text embedding space?\n\nOther comments:\n1. I suggest merging all the paragraphs of the abstract into a single paragraph.\n2. See the numbered list format in past ISMIR papers and revise the list in your introduction accordingly.\n3. Add punctuation at the end of each equation.",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper presents a novel audio-text representation learning, effectively addressing key limitations of contrastive approaches like CLAP. Eliminating negative samples through EMA encoders and asymmetric predictors is particularly innovative, while the comprehensive experiments (covering retrieval, zero-shot tasks, and downstream probing) strongly validate the method's advantages. Analyzing modality gap reduction and batch-size robustness provides valuable insights for the field. The writing is generally clear, although the figures' quality/resolution could be improved.\n\nSuggestions for improvements (that do not make the paper less relevant):\n- Exploring additional encoder architectures beyond HTS-AT/RoBERTa to demonstrate generalizability;\nIncluding explicit computational efficiency comparisons with CLAP;\n- Better describing BYOL;\n- Some technical aspects could be clarified, particularly the sensitivity to \u03bb values in loss weighting and the domain shift observed in MusicCaps.",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe authors propose a new architecture SLAP for training multimodal models without having a contrastive loss. contrastive losses have the challenge that they introduce modality gap and they are GPU compute heavy because of the need to have relatively large batch sizes. They demonstrate that their technique overcomes both of these limitations well - with improved retrieval performance and comparable or better classification and tagging results. \n\nThe paper's approach tackles two important challenges in contrastive learning and maintains or improves task performance at retrieval, classification and tagging. This approach brings in the ability to train multimodal embeddings without including batchwise pairs and provides for interesting future work for other tasks and modalities too. \n\nThe paper is generally well written and is easy to read - however a few clarifications below would do well. Some rigour in statistical testing would be preferable on the results. \n\nThe authors have done good testing on different aspects of their model however I would have liked to understand the effect if any of EMA to be quantified - the numbers in Table 3 being almost identical (and not likely not statistically different) did this really do anything ? On the other hand not having L_A and L_B is leading to model collapse. This piece needs more clarity in exposition, if not experimentation.\n\nSpecific comments\n--------------------------------\n\n1. what is meant by online context encoder in line 168? In particular what do the authors imply by the adjective \"online\" ?\n2. Unable to follow what axes is the exponential moving average working on ? In particular it is taking both the raw audio (which has time dimension) as well as embedding space where the time dimension is removed (?)\n2. Line 178. Not clear what is \\bar{z} - takes some time to figure out from the diagram too.\n3. Table 2 - Stat testing between SLAP and CLAP models individually for Pretrained and non-pretrained is preferable given numbers often are quite close (e.g. 5.7 vs 5.3 for CLAP Recall@1)\n4. I cannot follow the lower 2 rows for Table 2. Especially the lower CLAP vs the CLAP in top section. Even the CLAP model in top section of Table 2 is reproduced from public github libraries? A clarification would help\n5. It would be good to do statistical testing for the two rows in Table 3. The authors have noted in line 270 that both are viable but stat testing will establish it. For simplicity even a comparison of means would be helpful\n6. Similar comments for comparing SLAP and CLAP in Table 4 and 5. \n7. One aspect is not clear - why is downstream probing tasks being done on the head before z and not after. Also train-test splits for the downstream probing has not been specified\n8. The observation from Figure 5 seems to indicate that the best result is at \\lambda=0.5. Some insight into this would be interesting - although for multi-loss frameworks loss weights are often chosen equally, even a tuning leading to that is interesting. \n\nMinor comments\n----------\n1. Though can be inferred, lines 164,165 should have the defintitions of T_A and N\n2. Line 336 MIPS is not expanded"
    },
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Multimodal contrastive models have achieved strong performance in text-audio retrieval and zero-shot settings, but improving joint embedding spaces remains an active research area. Less attention has been given to making these systems controllable and interactive for users. In text-music retrieval, the ambiguity of freeform language creates a many-to-many mapping, often resulting in inflexible or unsatisfying results.\n\nWe introduce Generative Diffusion Retriever (GDR), a novel framework that leverages diffusion models to generate queries in a retrieval-optimized latent space. This enables controllability through generative tools such as negative prompting and denoising diffusion implicit models (DDIM) inversion, opening a new direction in retrieval control. GDR improves retrieval performance over contrastive teacher models and supports retrieval in audio-only latent spaces using non-jointly trained encoders. Finally, we demonstrate that GDR enables effective post-hoc manipulation of retrieval behavior, enhancing interactive control for text-music retrieval tasks.",
      "abstract": "Multimodal contrastive models have achieved strong performance in text-audio retrieval and zero-shot settings, but improving joint embedding spaces remains an active research area. Less attention has been given to making these systems controllable and interactive for users. In text-music retrieval, the ambiguity of freeform language creates a many-to-many mapping, often resulting in inflexible or unsatisfying results.\n\nWe introduce Generative Diffusion Retriever (GDR), a novel framework that leverages diffusion models to generate queries in a retrieval-optimized latent space. This enables controllability through generative tools such as negative prompting and denoising diffusion implicit models (DDIM) inversion, opening a new direction in retrieval control. GDR improves retrieval performance over contrastive teacher models and supports retrieval in audio-only latent spaces using non-jointly trained encoders. Finally, we demonstrate that GDR enables effective post-hoc manipulation of retrieval behavior, enhancing interactive control for text-music retrieval tasks.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Julien Guinot",
        "Elio Quinton",
        "George Fazekas"
      ],
      "authors_and_affil": [
        "Julien Guinot (Queen Mary University of London)*",
        "Elio Quinton (Universal Music Group)",
        "George Fazekas (Queen Mary University of London)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Generative Tasks",
        "Music retrieval systems",
        "Applications",
        "Multimodality",
        "Interactions",
        "MIR fundamentals and methodology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1DFsjiCssCbQRMJ3oYHus9KAozY6hZcef/preview",
      "poster_pdf": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-2-gd-retriever-controllable",
      "title": "GD-Retriever: Controllable generative text-music retrieval with diffusion models",
      "video": ""
    },
    "forum": "48",
    "id": "48",
    "pic_id": "",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nStrongly agree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nStrongly agree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nStrongly agree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nStrongly Agree (Very novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nStrongly agree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe application of diffusion models for retrieval is very interesting, specially to possibility of doing negative or refined queries.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nGenerative models using diffusion can be leveraged for music retrieval from text, and have the ability to perform negative and refined queries.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nStrongly agree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper presents an approach for multimodal music retrieval that instead of training a multimodal joint model, it uses a generative diffusion-based model to translate embeddings obtained from a text encoder to the space of audio embeddings. The particularity is that instead of one embeddings, this generative approach generates several embeddings that better represent non-exact queries. The major contribution to the field of music retrieval is the possibility of adding conditioning in the generative model to perform operations like negative queries, or refine previous queries. This is very relevant in real-world retrieval systems.\nThe paper is well written and explained and the evaluation is comprehensive. I have some minor comments:\n- When the authors refers to sequence of embeddings it is not clear if they are referring to a sequence of dimensions, or a set of embeddings that correspond to different parts of a song. This should be better explained.\n- There is a missing capital letter at the beginning of the sentence in line 132.\n- The authors argue that this approach avoids the training of a multimodal model. However there is just a substitution of a contrastive learning model by this diffusion model that translates the embeddings from the text modality to the audio modality. Therefore there is no simplification, the model needs to be trained on embeddings from both modalities, in the same way contrastive learning multimodal approaches can be trained with frozen encoders. This claim should be toned down accordingly.\n- In the paragraph starting in line 160 a description of what is the difference between z and Z would be useful. Here when you say time-wise average pooling you realize that maybe sequence of embeddings are related to different parts of a track, but not before.\n- Authors uses MULE baseline, they say that the approach was reimplemented in the MTG Jamendo dataset. Does it mean they trained the unsupervised approach described in the reference paper or that they used the released model and computed the embeddings of the Jamendo dataset?\n- A little description of what is out-of-domain for the authors would be useful.\n- Table 2 would benefit of a reordering. Having the teacher model and the GD retriever one next to the other would help to visualize the discussion. I mean having CLAP and right after GDE-CLAP for example.\n- Add a short description of the CLAP scores, so the reader doesn't have to check the source paper.\n- I think the part of negative queries and query conditioning is the most interesting part of the paper, a deeper dive in this section would have been nice, and also a discussion about future work in this direction.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThe reviewers agree about the novelty and relevance of the work. The method is clearly explained, and the experimental results are convincing. The idea of leveraging pre-trained modality-specific encoders while avoiding joint training is appreciated, though as one reviewer noted, the claim that this \u201cavoids multimodal training\u201d might be too strong, since the model still learns mappings between modalities. This point should be toned down accordingly.\n\nAnother common point in the reviews concerns the controllability aspect. Several reviewers, including myself, find this to be one of the most interesting features of the paper. However, the current experiments and analysis around it are relatively limited. A deeper exploration \u2014 either through qualitative examples or user-facing use cases \u2014 would significantly strengthen this part of the work.\n\nThere were also comments about generalization, particularly regarding the reliance on the PrivateCaps dataset. While the use of private data is acceptable within ISMIR guidelines, the limited evaluation on public datasets makes it harder to assess reproducibility and broader applicability. Clarifying these points and discussing future directions to address domain mismatch would be helpful.\n\nFinally, the paper would benefit from some minor edits and clarifications, including:\n\nA clearer explanation of what is meant by \"sequence of embeddings\"\n\nMore details about how baselines like MULE were implemented\n\nBetter organization in tables (especially Table 2)\n\nMinor corrections in grammar and table labels\n\nOverall, this is a solid and timely contribution that opens up new directions in controllable music retrieval. Despite the noted limitations, I support acceptance of this paper and look forward to seeing further developments on this line of work.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper proposes Generative Diffusion Retriever (GD-RETRIEVER), which applies diffusion models to focus on the important challenge of controllability in text-to-music retrieval, making it a pioneering work in the field. The main contribution, adapting generative model control techniques such as negative prompting and DDIM inversion to retrieval, is interesting, and the idea of enabling interactive search experiences for users is commendable. Furthermore, the flexibility to utilize encoders that are not jointly trained is an advantage.\n\nHowever, there are several concerns regarding the proposed method.\nMost notably, retrieval performance varies between in-domain data (PrivateCaps) and out-of-domain data (MusicCaps). Although the paper attributes this to domain mismatch and proposes latent space alignment as a mitigation strategy, analyzing such mismatch for each model is not practical in real-world scenarios.\n\nAnother concern is that the main training results rely heavily on a private dataset (PrivateCaps). While this is permitted under ISMIR\u2019s policy, it limits the ability of other researchers to reproduce or verify the results. It also remains unclear whether the trends observed with PrivateCaps hold when evaluated solely on public datasets.\n\nAlthough the controllability of the model is evaluated, including some quantitative analyses using CLAP scores, the lack of user studies assessing how effective or intuitive these control features are from a user perspective is unfortunate. Even simply illustrating the example retrieval results that can be performed in real-world scenarios would serve as a strong validation of the usefulness of the proposed method.\n\nThese concerns, especially those regarding generalizability and reproducibility, somewhat weaken the overall impact of the paper. Nevertheless, the novel direction of controllable retrieval, the creative use of diffusion models for retrieval tasks, and the thorough analyses (on domain mismatch and query quality) make this a valuable contribution. Future work is expected to address domain mismatch more comprehensively and to strengthen evaluation on public datasets.\n\nThere are also a few typographical errors:\n* Lines 214\u2013216: punctuation (period placement)\n* Table 6: \u201cNPP\u201d should be \u201cPNP\u201d, etc.",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper proposed to use diffusion model for text-to-music retrieval task not the audio generation. The methodology is to train a diffusion model to generate audio embeddings using text conditioning. To verify the effectiveness of the proposed method, the authors first verified that whether the proposed approach is superior than CLAP like text-audio joint embedding model-based text-to-music retrieval. If we see the Table 2, we can see that the proposed method is superior than CLAP like models. However, since they utilized CLAP like joint embedding model for their text conditioning and audio embeddings, there exists some performance degrades due to incompleteness of the joint embedding models. Therefore, they used separated models for text conditioning and audio embeddings, then in Table 4, this problem was solved well. If we take a step back and look at the proposed method again, then it can be viewed as a simple regressor. So, the model is trained to predict audio embeddings using texts, and they retrieve songs based on the predicted audio embeddings. Therefore, the authors compared the proposed method with a simple regression method + simpler diffusion model. And, they verified that the proposed method outperforms regressions and simpler method. At last, since the proposed method is a diffusion model, they could apply several techniques like negative prompting and DDIM inversion for more controllable retrieval. Overall, the paper is well-written and the experiments verified me the concerns I've had while reading paper (especially regression experiment seems really nice to have). \n\nThis is an additional comments that the authors can think of. Since the model is trained on caption-audio pairs using diffusion model, I'm curious about how much this model works well on really simple tag-based retrieval cases. For example, if the user types \"hiphop\" then would the model works well? I think if the authors can evaluate the proposed model on tag-based retrieval task (using well established tag dataset) and compare the pros and cons compared to the CLAP like model, then it would give many insights to the readers further. (Even though the performance got worse in this scenario, there would be many lessons that the readers can receive from this experimentation)",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\n- Highly relevant work overall.\n\n- Well introduced and well explained approach and justification. Clear writing.\n\n- As mentioned in section 15, it's a topic that can inspire and branch out into multiple adjacent and follow up explorations and use cases.\n\n- minor observation: in your conclusion you mention \"...uses diffusion models to produce latent queries in retrieval-optimized spaces.\" While not necessarily incorrect, I'd frame this more like \"retrieval-relevant or retrieval-friendly spaces\" rather than \"optimized\". \"optimized\" sounds a bit strong or at least, a) I thought I'd see something related to the optimization of the retrieval space itself and/or b) it left me wanting to see a justification as to why these spaces are already optimal in a retrieval setting.\n\n- minor omission: \"Figure 2: GD Retriever Method: We train a model to generate text-conditioned ghost queries for retrieval. Left: A diffusion model is trained to generate audio [LATENTS] from text captions. Right: Using the frozen model, we generate audio embeddings from a caption to retrieve similar audio via ghost queries.\" -> you're missing the term latents (or something similar, maybe \"embeddings\"), otherwise it reads as if you are generating the actually audio output. \n\n- My reason for weak accept and not strong accept is mainly related to the Controllability section. The emphasis on the overall claim is centered around Controllability. While the authors did carry experiments with negative prompting and DDIM inversion and provided some results and metrics, for this to be a strong accept I would have needed more in-depth experiments and more clear evidence on controllable retrieval behavior under different settings, including examples and potentially demos. Within the current scope of the experiments, controllability in retrieval seems promising but not conclusive enough to declare it a robust or preferred approach for controllable retrieval, compared to other methods."
    },
    "session": "3"
  },
  {
    "content": {
      "TLDR": "This article describes objective measures of segment regularity for use in evaluating musical structure annotations.\nThe core idea derives from identifying simple ratio relationships between segment durations (e.g.,, 2:1 or 3:4), and can be implemented in both musical time (beats) or absolute time (seconds).\nExtensions are proposed to further quantify regularity within labeled segment groups, across hierarchical levels, and evaluate balance or uniformity of segment durations.\nThe efficacy of the proposed methods is demonstrated through an empirical study of several standard datasets for music structure analysis.\n\nThe results indicate: 1) under reasonable assumptions of tempo stability, regularity can be reliably measured in absolute time, 2) most existing datasets exhibit regularity, 3) regularity interacts meaningfully with segment labeling, 4) regularity and balance are distinct concepts, and 5) multi-level segmentations exhibit cross-level regularity.",
      "abstract": "This article describes objective measures of segment regularity for use in evaluating musical structure annotations.\nThe core idea derives from identifying simple ratio relationships between segment durations (e.g.,, 2:1 or 3:4), and can be implemented in both musical time (beats) or absolute time (seconds).\nExtensions are proposed to further quantify regularity within labeled segment groups, across hierarchical levels, and evaluate balance or uniformity of segment durations.\nThe efficacy of the proposed methods is demonstrated through an empirical study of several standard datasets for music structure analysis.\n\nThe results indicate: 1) under reasonable assumptions of tempo stability, regularity can be reliably measured in absolute time, 2) most existing datasets exhibit regularity, 3) regularity interacts meaningfully with segment labeling, 4) regularity and balance are distinct concepts, and 5) multi-level segmentations exhibit cross-level regularity.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Brian McFee"
      ],
      "authors_and_affil": [
        "Brian McFee (New York University)*"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Evaluation metrics",
        "Musical features and properties",
        "Structure, segmentation, and form",
        "Evaluation, datasets, and reproducibility",
        "Evaluation methodology",
        "MIR fundamentals and methodology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://drive.google.com/file/d/1FCAr3_OnOxDq37G0_BSAdXayNA1e9kXs/preview",
      "poster_pdf": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-4-quantifying-regularity-in",
      "title": "Quantifying regularity in music structure analysis",
      "video": ""
    },
    "forum": "50",
    "id": "50",
    "pic_id": "",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nStrongly agree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nStrongly agree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nDisagree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe topic of the paper is quite specific.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThis paper proposes a metric of regularity of structural segments' duration, on musical and absolute time.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak reject\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper proposes a metric of regularity of structural segments' duration, on musical and absolute time.\n\nThe paper is clear and well written. The proposed metrics, very simple, are evaluated on several well-known datasets (Beatles, HarmonixSet, Jazz Structure Dataset, Jazz Audio-Aligned Harmony, Real-World Computing, SALAMI).\nThis paper proposes interesting results on an under-explored topic. The metrics can help quantification of the deviation from what is expected when computing structure segmentation. \nThe computation of regularity in absolute time is more accurate than relying on beat and downbeat estimation.\nHowever the application perspectives of the results seem rather low.\n\n\nFigure 1: the caption should be improved to clarify the figure. When Fig. 1 is cited, the reader does not know what $d_1$, $d_2$ or $\\ro$ means. What are the \"patterned regions\" if they are not marked by dashed lines? It is unclear what \"multiples of this unit\" refer to.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThis paper is interesting and fills a gap in the evaluation of music structure. However some weaknesses were raised by the reviewers, that should be taken into consideration for the camera-ready version. In particular, the authors should clearly define the regularity in the beginning of the methodology (metric) section to clarify the area where the paper contributes to.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper reads well and makes good use of the allotted space -- the scope is just right.\n\nHaving a Limitations section tilts the balance towards acceptance even if the detail is in an area of research I am not familiar with.",
      "review2": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nThis paper introduces several methods, along with their basic properties and extensions, for quantifying regularity in existing music structure annotations. The evaluation on standard music structure datasets reveals findings such as the relationship between beat-based and time-based regularity, as well as regularity patterns across multi-level segmentations.\n\nBefore going into detailed comments, I would like to note that my overall assessment leans toward a borderline recommendation, meaning I would support either a weak accept or a weak reject, depending on how the final decision balances strengths and limitations.\n\nThe paper contains several strengths.\n\nFirst, the paper is well-motivated and reflects a fair degree of novelty. It addresses an under-explored area within music structure analysis evaluation: the role of regularity or duration-wise consistency. By proposing formal metrics to quantify regularity, the work brings attention to an important but often overlooked aspect of music structure analysis task. Assessing regularity in existing music structure annotations can help researchers better understand the duration-wise consistency and coherence of datasets, and may also serve as a metric for guiding segmentation models toward avoiding overly irregular segment durations.\n\nSecond, the experimental design and analysis are well-executed and solid. The authors present detailed and insightful statistical evaluations across a range of widely used music structure datasets. The comparisons span across different framing choices (beat-based vs. time-based) and factors (tempo stability). These empirical results highlight the potential value of the proposed metrics for evaluating and interpreting the quality and consistency of structural annotations in music datasets.\n\nNonetheless, there are two major drawbacks to consider.\n\nFirst, the paper's writing lacks proper structure in certain areas. For instance, although the authors mention that \"most prior work stops short of providing a formal definition of regularity,\" they also fail to propose a formal definition themselves, instead transitioning directly into the section on \"temporal divisibility\" without clarification. There is no explicit definition of \"regularity\" in the context of music structure analysis, with the only potential example-based definition appearing in Figure 1. Additionally, it would be beneficial to discuss the applications and benefits of the proposed regularity metrics in both dataset-level quality assessment and in improving the music segmentation algorithms. While a limitations section is included, an \"applications\" section might be more appropriate and should precede it.\n\nSecond, a critical aspect of the paper lies in the broader applicability of the proposed metrics, such as their potential to improve or evaluate music segmentation models. The paper demonstrates that the proposed metrics can reflect the quality of existing music structure annotations, but such evaluation of \"regularity\" in music structure analysis is somewhat limited in procedure, and is not correlated to the musical content. It is important to note that quality control for music structure analysis datasets is often conducted through detailed manual procedures, where annotators refine each sample to provide accurate structural labels. As such, it may not be necessary to develop new metrics to \"re-evaluate\" whether an existing dataset provides quality structure labels if the data collection process is already reliable. Moreover, these metrics have limitations as they only measure the consistency of duration across annotated segments, such as ensuring there is no irregular number of bars. They do not assess whether the annotations accurately reflect the correct musical phrases or boundaries. As a result, a high \"regularity\" score, as proposed in this paper, does not necessarily indicate the true quality of the data from a music structure perspective. Furthermore, in the context of evaluating or guiding music segmentation models, the limitation of these metrics becomes more apparent, as they serve primarily as a duration regularizer during evaluation, but can hardly be utilized in the training process (due to the lack of differentiability). Given these constraints, the value of the proposed metrics appears limited, as they offer a partial, non-content-based assessment of music structure data and contribute less to the improvement of music segmentation models.\n\nIn conclusion, this paper introduces a novel perspective on evaluating regularity metrics within the music structure analysis task. While the idea and the proposed metrics are interesting and reasonable, their practical value appears limited due to the scope and applicability of the proposed metrics. \n\nThat said, my expectations may extend beyond the intended goals of the paper (\"Our goal in this work is not to propose new algorithms for structure analysis, but rather to gain insights about how regularity manifests in existing structural annotations\"). Therefore, I remain open to recommending either a weak reject or a weak accept, depending on how the contribution is clarified upon the discussion phase.",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper introduces a novel framework for evaluating the regularity of segmentations in music structure analysis. The authors propose a set of quantitative metrics that defines regularity and balance by evaluating the ratios between segment durations. These metrics are designed to operate in both musical time (beats) and absolute time (seconds), making them versatile for various music contexts, and notably when beat estimation is not reliable (which was not possible before).\n\nI found the paper to be very well-written and enjoyable to read. This paper fills a hole in the current evaluation process of music structure analysis and will help future researchers integrate the regularity principle with quantitative arguments, hence improving the rigor of this principle.\n\nI have minor comments on the paper that I will detail hereafter:\n-- The authors do not mention the MIREX10 set of annotations for RWC Pop (which, unfortunately, does not contain segment labels), while it was motivated to enhance the regularity of the original (AIST) set of annotations [1]. I would have liked to see if this original motivation turned out to be quantitatively observable in practice.\n-- Line 243: it is said that \"in general,\" the balance is lower or equal to the regularity. It seems to me that this \"generality\" is, in fact, always true. Maybe the authors meant a strict inequality? Otherwise, I would suggest rephrasing the word \"in general\" to be more direct. It would also clarify the discussion about the properties of the balance metric that follows this line.\n\n[1] Bimbot, F., Sargent, G., Deruty, E., Guichaoua, C., & Vincent, E. (2014, January). Semiotic description of music structure: An introduction to the Quaero/Metiss structural annotations. In AES 53rd International Conference on Semantic Audio (pp. P1-1)."
    },
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Text-to-audio diffusion models produce high-quality and diverse music but many, if not most, of the SOTA models lack the fine-grained, time-varying controls essential for music production. ControlNet enables attaching external controls to a pre-trained generative model by cloning and fine-tuning its encoder on new conditionings. However, this approach incurs a large memory footprint and restricts users to a fixed set of controls. We propose a lightweight, modular architecture that considerably reduces parameter count while matching ControlNet in audio quality and condition adherence. Our method offers greater flexibility and significantly lower memory usage, enabling more efficient training and deployment of independent controls. We conduct extensive objective and subjective evaluations and provide numerous audio examples on the accompanying website.",
      "abstract": "Text-to-audio diffusion models produce high-quality and diverse music but many, if not most, of the SOTA models lack the fine-grained, time-varying controls essential for music production. ControlNet enables attaching external controls to a pre-trained generative model by cloning and fine-tuning its encoder on new conditionings. However, this approach incurs a large memory footprint and restricts users to a fixed set of controls. We propose a lightweight, modular architecture that considerably reduces parameter count while matching ControlNet in audio quality and condition adherence. Our method offers greater flexibility and significantly lower memory usage, enabling more efficient training and deployment of independent controls. We conduct extensive objective and subjective evaluations and provide numerous audio examples on the accompanying website.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Tom Baker",
        "Javier Nistal"
      ],
      "authors_and_affil": [
        "Tom Baker (University of Manchester)*",
        "Javier Nistal (Sony Computer Science Laboratories, Paris)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Melody and motives",
        "Generative Tasks",
        "Musical features and properties",
        "Music generation",
        "Harmony, chords and tonality",
        "Music and audio synthesis",
        "MIR tasks"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1zvlWdyX_GU-1qc7THLmfWUEJ5IG5fXVL/preview",
      "poster_pdf": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-5-lilac-a-lightweight",
      "title": "LiLAC: A Lightweight Latent ControlNet for Musical Audio Generation",
      "video": ""
    },
    "forum": "53",
    "id": "53",
    "pic_id": "",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nStrongly agree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nStrongly agree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nthe use of identity-initialized and zero-initialized conv layers for fine-tuning seems interesting\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nwe can use lightweight conv layers to fine-tune a text-to-music generation model to learn new conditions\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper presents a lightweight alternative to the ControlNet approach for fine-tuning a text-to-music generation model to handle new conditions. Figure 1 clearly illustrates the idea, which involves learning identity-init and zero-init convolutional layers instead of using cloned encoder blocks. The authors implemented their method using Diff-a-Riff (432M parameters) as the backbone and tested variants with learnable parameters ranging from 32M to 64M, all smaller than the ControlNet baseline (165M). Both objective and subjective evaluations show that the proposed method matches the performance of the ControlNet baseline, though it does not surpass it.\n\nStrengths:\n* Fresh and interesting and interesting use of zero-init and identity-init convolutional layers for fine-tuning.\n* Achieves similar performance to the Music ControlNet baseline while using about 1/4 to 1/2 of the learnable parameters.\n* Solid set of experiments, with nice use of APA and MUSHRA evaluations.\n\nWeakness:\n* The backbone, Diff-a-Riff, is not open source.\n* The reduction in trainable parameters is not substantial.\n* It\u2019s unclear if the idea of identity-init convolution is novel, as the authors don\u2019t clarify this.\n* No comparison with a relevant recent work [11] (Hou et al., ICASSP 2025), though this is understandable since [11] is new.\n* Insufficient explanation of how their method differs from an important prior work, ControlNet-XS [13].\n* Limited description of how their approach differs from another key prior work, Sketch2Sound [36], from a methodology point-of-view, although the authors did implement Sketch2Sound (labeled LiLAC* in Table 1).\n\nMinor issues:\n* References are not consistently formatted.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThe reviewers are generally positive about this submission, noting issues like the problematic abstract opening and several points needing clarification. I encourage the authors to use the reviewers' feedback constructively to enhance the paper's quality while preparing for the camera-ready version.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper proposes a lightweight, modular variation of the method proposed in controlnet [9] and evaluate the approach using an example music generation model. The evaluation focusses on time-varying control over music generation\u2014something lacking in current systems. The approach is validated through objective and subjective evaluations and demonstrated with audio examples online.\n\nGeneral remark:\n\nOverall I feel that the method is explained clearly and motivated sufficiently. Experimental results are a bit limited. In table 1 all methods achieve APA == 1, and this independently of the fact that MSE is quite different. I wonder whether the APA metric is very helpful here. \n\nSugested modifications:\n\nTable 2 appears a bit confusing. Section 5.3.4 explains the motivation of the experiment. \n\n> 306-309: achitecture is more susceptible to CLAP leakage\u2014where over-specified control signals (e.g., chroma) can dominate or obscure CLAP\u2019s condition.\n\nI am not sure to understand this. My question would be: what would one want? If you have to conflicting control signals (here CLAP and chroma), then there is a design problem. I do not think it is generally better if CLAP or chroma wins. So the arrows in table 2 do not seem justified, or at least I don't see why one would favor one over the other. One could for example say that the chroma feature is more specific and therefore it should overrule the more general specification (CLAP). It appears table 2 reflects an understanding that is the other way around. It would be helpful to understand why.\n\nI am also a bit disturbed by the discussion of the SCA results in paragraph in 509-516. There we find that comparing to SAQ we the differences for SCA are clearer. However, in the table 2 it is the other way around. the differences are more pronounced for SAQ (max diff 4.2) then in SCA (max diff 2.7). \n\nIf I understand correctly the results displayed in table 4 col SCA, it appears that the control efficiency is still somewhat weak.",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper builds off of ControlNet in introducing controllability to pre-trained generative music models. To reduce the number of parameters, the authors introduce light weight convolutions in the adaptor branch. Through the experiments, the authors show that (1) the quality of generation is not affected by their method, (2) effects of conflictive conditioning (conflicts between post-hoc conditions, i.e. chroma and pre-existing conditions, i.e. CLAP embeddings), (3) the effect of specificity of conditions, i.e. chroma and chord, and (4) the quality of the post-hoc conditions, i.e. chords and chroma. \n\nI think the paper is well organized and does a good job of presenting a compelling argument for their light-weight controllable architecture. No mention has been made of the code being made available and I hope this could be done to help the community build on it. I also appreciate the extensive samples page. Below I have listed a few points I would like clarification on:\n\n1. Fig 1: I think the figure does a good job of communicating the idea. I was confused about a couple of things. (1) In the right subfigure, what is the zero convolution (in the center of the figure, along the y-axis) doing and how is the output of the convolution integrated with the output from the identity convolution? I also don\u2019t see this zero convolution in the equation defined in line 168. Additionally, I think it would be helpful to have a legend for the colours indicating clearly which parts are trained and which aren\u2019t.\n\n2. Regarding conditioning:\n\n2 a. Training details, CFG on conditioning: Lines 229 - 234: My understanding is that CFG is being used on the conditions introduced in this paper, i.e. chroma and chords when training the adapter branch. The pre-trained model takes in CLAP embedding and the audio context as conditioning signals as well. Is CFG being used for these conditions as well during the training of the adapter branch? If so, this should be made clear. It is unclear to me what the \u201cnew c\u201d refers to in line 232 as \u201cc\u201d is defined as \u201cthe condition c\u201d (chroma and chord) that is introduced to the adapter branch in line 191. \n\n2 b. Table 1: My understanding is that the audio context is not used for all the models in table 1 except for the Diff-a-Riff + Context model. Is this correct? I have comments based on if this assumption is correct or not:\n\nIf this is the case, then I\u2019m a little confused what the relevance of APA is, since the model is not seeing the audio prompt at all. Perhaps the goal here is to show that even without the audio prompt the generated samples adhere to the context audio with just a chromagram input, if this is the case it should be explicitly stated. \nIf my assumption is incorrect, then lines 384 - 386 seem to attribute alignment to the context to the chroma conditioning which wouldn\u2019t make sense if the context is also seen as conditioning. \n\nAdditionally, I think cMSE being reported for the Diff-a-Riff models is similarly a little confusing. While I see the value of a standard metric across all models, I believe that if the models are not seeing the chroma conditioning during inference, it should be explicitly stated to avoid confusion. \n\n\n2 c. Chroma MSE as a metric: I wonder if a per-frame chroma overlap would be a more reliable metric in the context of this work, similar to that used in Music ControlNet [1]. I think MSE introduces biases by penalizing notes that are closer less than notes that are further away, which could be problematic.\n\n3. LiLAC*: LiLAC* is said to be similar to Sketch2Sound [2]. However Sketch2Sound finetunes the backbone after adding the input condition. Is that being done for this baseline? If so, that should be explicitly stated since this is not the case for the other LiLAC models. \n\n4. Samples page: I would be very interested to listen to samples from the misaligned conditions which I don\u2019t believe are on the samples website right now. Especially examples which highlight the behavior stated in lines 436-442.\n\n5. Minor corrections:\nIn section 3.2.1, the term \u2018adaptor branch\u2019 is never explicitly linked to G_l (..). The term is termed as \u2018its cloned counterpart\u2019, I would recommend making the link in the text.\nThere is a space missing between Table and 2 in line 411\n\n[1] Wu, Shih-Lun, et al. \"Music controlnet: Multiple time-varying controls for music generation.\" IEEE/ACM Transactions on Audio, Speech, and Language Processing 32 (2024): 2692-2703.\n[2] Garc\u00eda, Hugo Flores, et al. \"Sketch2sound: Controllable audio generation via time-varying signals and sonic imitations.\" ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2025.",
      "review3": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nI. Strengths\n============\nI'm highly supportive of the direction that this work explores, i.e., to shave off the resources/parameter needed to achieve fine-grained control. It is especially important for musical applications as different users might have drastically different types of controls they'd like to achieve. The lightweightness of these methods can meaningfully reduce the barrier to achieve plug-and-play controls for various use cases.\n\nI also appreciate the exploration on conflicting controls (Sec 5.3.4 and Table 2) since these use cases are potentially central to musicians' creative inquiries, e.g., making playing techniques / phrases (controlled via fine-grained signals) that are technically impossible with some instruments (controlled via text / CLAP). And, it's nice to see that LiLAC seems to have an edge over the heavier-weight ControlNet.\n\nBesides, some architectural ablations and a listening study are both conducted, which are commendable. However, I think overall the work is less than ready for publication in its current state.\n\nII. Weaknesses\n============\n(W1) Missing analyses on memory/efficiency improvements\nWhile the advantage on memory is a repeated claim in the manuscript, there isn't any comment or experiment supporting this claim. Also, Figure 1 sort of tells that the proposed design actually has more components, and hence total parameters, than ControlNet -- if I understand correctly, this would worsen inference-time speed and memory footprint (although it is likely still advantageous at training thanks to fewer trainable parameters hence fewer optimizer states).\n\nSome speed/memory stats compared to ControlNet should be reported. Besides that, the authors could consider other ways where the proposed architecture could shine more efficiency-wise -- perhaps it's combining multiple fine-grained controls, since all controls can share the same encoder backbone, or demonstrating improved sample efficiency (i.e., the required amount of training data to make controls work) which implies better applicability in cases where controls are costly to obtain (e.g., require hand labeling).\n\n(W2) Limited exploration on controls and output space\nThis work primarily explored single-instrument outputs and harmonic controls (chroma and chords), which I feel is a little narrow especially considering that prior works like Music ControlNet and DITTO have tackled multi-instrument audios and a wider range of controls (dynamics, rhythm, structure, etc.).\n\n(W3) Insufficient motivation & demonstration on additional experiments\nIt's great to see experiments that discuss the interactions/conflicts between heterogeneous controls which might contain overlapping information (Tables 2 & 3). Yet, I think more could be done to better ground these explorations to applications, through, for example, motivating why \"less bleeding\" is important (and in what use cases) and/or providing generated samples to show that the differences are qualitatively substantial. I have these comments because from the metric gaps in Tables 2 and 3, it's difficult to judge how much better LiLAC is over ControlNet as these metrics are, after all, proxies to the actual desiderata, which in turn depend on specific application goals.\n\n(W4) Problematic abstract opening\nI would strongly advise revising the first sentence in abstract (\"Text-to-audio ... lack fine-grained controls\") as this is not true anymore. Plenty of citations in this manuscript also refute this statement."
    },
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Music mastering style transfer aims to model and apply the mastering characteristics of a reference track to a target track, simulating the professional mastering process. However, existing methods apply fixed processing based on a reference track, limiting users' ability to fine-tune the results to match their artistic intent.In this paper, we introduce the ITO-Master framework, a reference-based mastering style transfer system that integrates Inference-Time Optimization (ITO) to enable finer user control over the mastering process. By optimizing the reference embedding during inference, our approach allows users to refine the output dynamically, making micro-level adjustments to achieve more precise mastering results. \nWe explore both black-box and white-box methods for modeling mastering processors and demonstrate that ITO improves mastering performance across different styles. Through objective evaluation, subjective listening tests, and qualitative analysis using text-based conditioning with CLAP embeddings, we validate that ITO enhances mastering style similarity while offering increased adaptability. Our framework provides an effective and user-controllable solution for mastering style transfer, allowing users to refine their results beyond the initial style transfer.",
      "abstract": "Music mastering style transfer aims to model and apply the mastering characteristics of a reference track to a target track, simulating the professional mastering process. However, existing methods apply fixed processing based on a reference track, limiting users' ability to fine-tune the results to match their artistic intent.In this paper, we introduce the ITO-Master framework, a reference-based mastering style transfer system that integrates Inference-Time Optimization (ITO) to enable finer user control over the mastering process. By optimizing the reference embedding during inference, our approach allows users to refine the output dynamically, making micro-level adjustments to achieve more precise mastering results. \nWe explore both black-box and white-box methods for modeling mastering processors and demonstrate that ITO improves mastering performance across different styles. Through objective evaluation, subjective listening tests, and qualitative analysis using text-based conditioning with CLAP embeddings, we validate that ITO enhances mastering style similarity while offering increased adaptability. Our framework provides an effective and user-controllable solution for mastering style transfer, allowing users to refine their results beyond the initial style transfer.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Junghyun Koo",
        "Marco Martinez-Ramirez",
        "WeiHsiang Liao",
        "Giorgio Fabbro",
        "Michele Mancusi",
        "Yuki Mitsufuji"
      ],
      "authors_and_affil": [
        "Junghyun Koo (Sony AI)*",
        "Marco Martinez-Ramirez (Sony AI)",
        "WeiHsiang Liao (Sony AI)",
        "Giorgio Fabbro (Sony Europe B.V.)",
        "Michele Mancusi (Sony Europe B.V.)",
        "Yuki Mitsufuji (Sony AI, Sony Group Corporation)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "MIR fundamentals and methodology",
        "Applications",
        "Creativity",
        "Human-ai co-creativity",
        "Music composition, performance, and production",
        "Tools for artists",
        "Music signal processing",
        "MIR tasks",
        "Music synthesis and transformation",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1lQMMfHXdwwYIobpWM-Z3yV0nIll8GTgg/preview",
      "poster_pdf": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-2-ito-master-inference",
      "title": "ITO-Master: Inference-Time Optimization for Audio Effects Modeling of Music Mastering Processors",
      "video": ""
    },
    "forum": "59",
    "id": "59",
    "pic_id": "",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nStrongly agree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThis paper focuses pretty specifically on reference-based automatic music mastering, but has some specific insights that may apply to work on ITO more broadly. Specifically, the insight to differentiably optimize z_ref instead of employing black box optimization parameters for audio FX chains\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nInference-time optimization is a promising family of methods for automatic music mastering based on a reference track\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak reject\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper proposes a novel strategy for automatic music mastering based on a reference track. Following past work on controlling audio effects, the authors explore a self-supervised strategy for training FX chain encoders, and inference-time optimization for iteratively refining the automatic mastering process based on the reference. The authors perform extensive quantitative and qualitative evaluation on their proposed approach.\n\nOverall, this is an interesting paper demonstrating promising results on a difficult task. It will be of interest to several sub-communities of researchers within ISMIR working on automatic mastering, inference-time optimization, and differentiable signal processing. However, there is a key issue around the *motivation for exploring differentiable synthesis* that limit the ability to judge the promise of the proposed method in the broader landscape of recent MIR research. Moreover, there are some areas for improvement around *missing baselines*, the *quantitative evaluation protocol*, *minor methodological novelty*, and the presentation of *incomplete follow-up work*.\n\n*Motivation for differentiable synthesis*. L58 says \"white box methods ... are often constrained by the simplicity of their differentiable processors, which may not fully replicate the complex tools in professional mastering\". However, this paper then goes on to explore slightly more sophisticated differentiable primitives, which are likely always going to have a lower performance ceiling than professional chains. An apples-to-apples comparison against an off-the-shelf black box optimization framework (e.g. ST-ITO) on the parameters of a *professional* (non-differentiable) mastering toolchain is essential here to justify the decision to go with a differentiable approach.\n\n*Additional baselines*. It would be very helpful to see a few additional simple baselines: (1) randomized z_ref for black box model, (2) randomized z_ref for white box model, and (3) randomized FX parameters for white box model. As it stands, it's unclear to what extent the benefits of the proposed approach are coming from the audio manipulation primitives vs. the encoding / optimization procedures. \n\n*Quantitative evaluation protocol*. In the white box setting, why not just directly evaluate the ability of the model to exactly reconstruct the original FX parameters? I.e., just report the error between the ground truth FX parameters and the estimated ones.\n\n*Minor methodological novelty*. The idea of differentiable optimization of z_ref is interesting, though ultimately a bit limited in its novelty and applicability to non-differentiable settings. Moreover, it is unclear if this result would hold for other audio production / effects matching scenarios. This isn't a huge issue but as it stands it's unclear how reusable this insight is outside of the specific context in this paper\n\n*Incomplete follow-up work*. Section 5.3 is somewhat interesting but currently unjustified (no experiments or evaluation). Also, why is it important to match a text prompt if an audio reference can be provided? I would much rather see that extra page devoted to more thorough investigation or analysis of the non-text-conditioned setting\n\nErrata\n- L161 xin = fnorm(f1(A)) based in Figure 1, here you wrote it the other way\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak reject\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nOverall, the recommendation for this paper is weak reject. R1/R2 and myself (MR) leaned negative on the work in our initial reviews, while R3 leaned strongly positive. During the discussion, R1 reiterated concerns from their review about the evaluation / real world practicality / limited methodological innovation, and R2 reiterated a lack of clarity in the writing. R3 eventually adjusted their score to weakly positive in light of criticisms raised by other reviewers.\n\nSummary of strengths / weaknesses from reviews:\n\nStrengths: well-motivated (R3), diverse set of experiments and baselines (R3)\nWeaknesses: Arbitrary choice of reference tracks (R1), concerns about generalization from synthetic setup to real-world setting (R1), writing clarity (R2), concerns with practical usefulness (R3/MR), insufficient evaluation protocol (MR)",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nAs with many MIR tasks, evaluating \"similarity\" between a mastered track and a reference master is tricky. The devil is usually in the details when it comes to bridging the gap between evaluation metrics in a paper and real-world applicability or impact. For example, what kind of reference tracks are conceivable choices for real-world mastering engineers to use? In some of the audio samples on the demo page, the choices of references seem difficult to justify as useful choices given the stylistic/genre differences. Perhaps a bigger concern is that the mastering FX chains that are being modeled in the data are themselves random rather than specifically targeted because they were used intentionally for mastering. This seems to me to create a significant gap between the actual task at hand (matching a dataset of randomized audio FX chains) and the real-world musical activity used to justify the technical work (mastering music with reference tracks).\n\nWhile the technical method proposed in the paper does seem to perform comparatively well next to the baselines, I don't think that the application of the method and reporting of these metrics alone are enough to justify publishing this paper in ISMIR. If this is primarily a paper about music mastering, I think that the proxy-mastering task would need to be refined or justified further; otherwise, I could imagine this being reframed to focus more on the ITO method.",
      "review2": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nThis paper describes a mastering-by-example system, created using the following components: a transfer network called the Mastering Style Converter, a differentiable effects chain, retraining of the reference encoder, and ITO, which we take to mean direct optimization of the embedding target for the transfer network.\n\nAn evaluation is described, in which white vs. black box approaches were compared, including retraining of the reference encoder, and proposed ITO procedure. The comparisons are done using a suite of objective metrics, as well as with a subjective test. A further test was conducted using embeddings from text prompts.\n\nIn this work, a complex new system was trained and many experiments were conducted. However, I had some problems understanding the main ideas behind ITO and its motivation. I feel that what ITO is was not clearly explained, neither in the introduction, nor in section 2.2. Some clear statement about what ITO is, combined with a reference, would be helpful. Similarly, a technical definition of FX normalization, and why it is used, would be helpful.\n\nAfter reading the references I eventually settled that ITO must mean something like in [16], in which ITO means optimizing a latent noise reference through a diffusion process (which additionally requires gradient checkpointing through iterative steps of the algorithm). But the use of ITO here is more like an analogy, because instead of optimizing latent noise, we are optimizing the reference embedding. Furthermore, since diffusion doesn\u2019t seem to be involved, implementation of the gradients is more straightforward than [16]. \n\nIn general, I would have appreciated greater clarity in the text and direct mathematical expression (with citations), to better understand the contributions of what seems like a unique system.\n\nAdditional questions:\nAre there any regularizers preventing that z_ref goes directly to the new reference? In Section 3.4 Inference-Time Optimization on Reference Embedding, it mentions using the Audio-Feature loss from [13] but doesn\u2019t give more details.\n\nIn the evaluation, ITO was only performed as an additional step when the reference encoder Phi was retrained. As it seemed to show little marginal benefit for some cases, was it possible to test using a fixed reference encoder?",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper proposes a novel approach to improving post-production style transfer using inference-time optimization. It presents a range of experiments with and without optimization, introduces strong baselines and novel metrics, and offers valuable insights into optimization techniques involving reference encoders and effect parameters. Overall, I really enjoyed reading the paper.\n\nStrengths:\n\nThe use of inference-time optimization for post-production style transfer is both novel and well-motivated. A diverse set of experiments and baselines help contextualize the effectiveness of the proposed approach. The comparison of various optimization techniques (reference encoder and effect parameters) provide a useful reference for future research.\n\nCriticisms and Suggestions:\nReal-world usability vs. reference alignment: While the proposed method aims to align the output with a given reference, I found that masters from E2Emastering and Matchering subjectively sounded more usable in real-world scenarios, even though they were less aligned with the reference. Future work should consider not only similarity to the reference but also the production value and usability of the output. Reference-based mastering is a curatorial task, if the reference is poorly chosen, the result may be undesirable.\n\nUncited similarity in Figure 1:\nFigure 1 appears quite similar to the one in this ISMIR 2024 LBD paper (https://ismir2024program.ismir.net/lbd_446.html), but this is not cited. Please include the citation or clarify the relationship between the figures.\n\nFx-normalization claims (Lines 151\u2013152):\nThe paper states that Fx-normalization improves model performance, but no supporting metrics or references are provided. Please clarify or provide evidence. Notably, Fx-normalization is typically used in mixing tasks involving wet stems- if this paper builds on that prior work, relevant citations should be included.\n\nClarification on distortion removal (Line 159):\nPlease elaborate on the reasoning behind the need to remove all distortion. \n\nPercentages in Line 196:\nThe methodology behind the percentage calculations is unclear. Please explain how these values were derived.\n\nSubjective listening tests (Line 414):\nBased on my listening experience, I preferred the outputs from E2Emastering and Matchering in terms of usability. However, I understand that the objective of your listening tests may have been reference similarity, not user preference. I would encourage future evaluations to include subjective preference ratings in addition to similarity, especially since mastering involves aesthetic and perceptual judgments.\nSection 5.3: The paper evaluated only one song and though the figures show how the CLAP embedding is able to drive the system to produce different masters, it is unclear if they sound usable as no audio examples for the same were shared. Further, this was not evaluated using subjective listening tests. \nAdditional Comment:\nThe introduction of new evaluation metrics for post-production style transfer is commendable, especially given the difficulty of objective evaluation in this domain. However, incorporating user preference and real-world usability into the evaluation pipeline will strengthen the practical relevance of this work."
    },
    "session": "2"
  },
  {
    "content": {
      "TLDR": "The task of text-to-music editing, which employs text queries to modify music  (e.g. by changing its style or adjusting instrumental components), presents unique challenges and opportunities for AI-assisted music creation. Previous approaches in this domain have been constrained by the necessity to train specific editing models from scratch, which is both resource-intensive and inefficient; other research uses large language models to predict edited music, resulting in imprecise audio reconstruction. In this paper, we introduce Instruct-MusicGen, a novel approach that finetunes a pretrained MusicGen model to efficiently follow editing instructions such as adding, removing, or separating stems. Our approach involves a modification of the original MusicGen architecture by incorporating a text fusion module and an audio fusion module, which allow the model to process instruction texts and audio input concurrently and yield the desired edited music. Remarkably, although Instruct-MusicGen only introduces 8% new parameters to the original MusicGen model and only trains for 5K steps, it achieves superior performance across all tasks compared to existing baselines. This advancement not only enhances the efficiency of text-to-music editing but also broadens the applicability of music language models in dynamic music production environments.",
      "abstract": "The task of text-to-music editing, which employs text queries to modify music  (e.g. by changing its style or adjusting instrumental components), presents unique challenges and opportunities for AI-assisted music creation. Previous approaches in this domain have been constrained by the necessity to train specific editing models from scratch, which is both resource-intensive and inefficient; other research uses large language models to predict edited music, resulting in imprecise audio reconstruction. In this paper, we introduce Instruct-MusicGen, a novel approach that finetunes a pretrained MusicGen model to efficiently follow editing instructions such as adding, removing, or separating stems. Our approach involves a modification of the original MusicGen architecture by incorporating a text fusion module and an audio fusion module, which allow the model to process instruction texts and audio input concurrently and yield the desired edited music. Remarkably, although Instruct-MusicGen only introduces 8% new parameters to the original MusicGen model and only trains for 5K steps, it achieves superior performance across all tasks compared to existing baselines. This advancement not only enhances the efficiency of text-to-music editing but also broadens the applicability of music language models in dynamic music production environments.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Yixiao Zhang",
        "Yukara Ikemiya",
        "Woosung Choi",
        "Naoki Murata",
        "Marco Mart\u00ednez-Ram\u00edrez",
        "Liwei Lin",
        "Gus Xia",
        "Wei-Hsiang Liao",
        "Yuki Mitsufuji",
        "Simon Dixon"
      ],
      "authors_and_affil": [
        "Yixiao Zhang (ByteDance Inc)*",
        "Yukara Ikemiya (Sony)",
        "Woosung Choi (Sony)",
        "Naoki Murata (Sony)",
        "Marco Mart\u00ednez-Ram\u00edrez (Sony)",
        "Liwei Lin (New York University)",
        "Gus Xia (MBZUAI)",
        "Wei-Hsiang Liao (Sony)",
        "Yuki Mitsufuji (Sony)",
        "Simon Dixon (Queen Mary University of London)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Music generation",
        "MIR tasks",
        "Music composition, performance, and production",
        "Applications"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/17OIvKDUinUQd1tin5MQ50h6w-Hx0T4BW/preview",
      "poster_pdf": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-10-instruct-musicgen-unlocking",
      "title": "Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning",
      "video": ""
    },
    "forum": "64",
    "id": "64",
    "pic_id": "",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nDisagree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nStrongly agree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nsee below\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nInstruction fine tuning of a pre-trained music generation system can go a long way.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThe authors present a system for extracting, removing or adding a stem from a given audio recording. The idea is to adapt a pretrained musicgen system to enable audio inputs following the llama-adapter approach and then fine-tuning on paired stem data.\n\nOverall, I am torn on this paper. On the one hand, this is novel and interesting, from both a technical but also conceptual perspective. In that sense, I think the paper is making considerable contributions and I liked reading it. On the other hand, I think the claims presented by the paper are not matching the actual contributions made by the paper. In particular, the title, abstract and introduction mention explicitely that the method enables 'text-to-music editing'. I think this is drastically overpromising what the system actually does. In particular, the system is fine tuned to understand three keywords: extract, remove, add. There is no additional text understanding enabled beyond these key words. Two of these instructions (extract, remove) are already supported by a regular stem separation system. Yet the authors do not compare to any such system - in fact, current systems for stem separation are around 2 orders of magnitude better at those two tasks. Hence the only interesting task that is actually being enabled is adding a stem. And while I like how the system is adapted and trained, it does rely on paired data. And there are not many natural dataset (i.e. beyond simplistic signal processing augmentations for which solutions already exist) that cover interesting editing operations. One could argue that this is just a matter of data collection - but if future, potential data collection is justifying the claims, almost any claim would become valid. For example, system A performs worse than system B with system A having more capacity, then one could argue that system A is useful because future data collections might improve the performance.\n\nSo, ignoring what could be in the future, and just looking what the system actually does and proofs, the proposed training and system setup seem to be restricted to the task of adding a stem (plus extract/remove as discussed above) and thus it's not clear how there could be a reasonable path forward based on this system to enable text-to-music editing as claimed. \n\nIf the paper gets accepted, I would strongly encourage the authors to rephrase what is being claimed in the paper throughout.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThis paper presents Instruct-MusicGen, a system built upon the pre-trained MusicGen architecture that enables text-guided music editing via instruction tuning. Specifically, the authors propose a lightweight fine-tuning method that allows the system to respond to simple editing commands\u2014\u201cadd,\u201d \u201cremove,\u201d and \u201cextract\u201d\u2014targeted at manipulating instrumental stems in music audio. The technical contributions lie in the integration of cross-modal adapters and a modest training budget, enabling the model to perform editing with a small subset of trainable parameters.\n\nThe reviews converged on a positive yet tempered consensus, with three reviewers awarding a \u201cweak accept\u201d and one reviewer supporting a \u201cstrong accept.\u201d The strengths of the paper are generally recognized across reviews: it tackles a relevant and increasingly impactful task in the MIR community; it proposes a technically efficient solution using adapter-based fine-tuning; and it demonstrates solid empirical performance across both synthetic (Slakh2100) and real-world (MoisesDB) datasets. The evaluation protocol is seen as fairly thorough, including a mixture of objective metrics (e.g., SI-SDR, CLAP, SSIM) and a user study.\n\nHowever, several substantive concerns were raised that limit the enthusiasm. Chief among them is a potential mismatch between the claims made in the title, abstract, and introduction\u2014suggesting general-purpose text-to-music editing\u2014and the actual capabilities demonstrated, which are limited to recognizing and executing three predefined operations. This raised concerns of overstatement, particularly given the lack of comparisons with existing stem separation systems, which can perform some of the same tasks more effectively. Reviewers also noted missed opportunities to cite closely related work (e.g., Audio Prompt Adapter, ControlNet-based approaches), and questioned whether the model meaningfully preserves the fidelity of the original audio during editing.\n\nFurther technical feedback touched on clarity issues in the method section and underwhelming visualizations. For example, key architectural figures and evaluation details could benefit from refinement and better explanation. Moreover, there was an expressed desire for deeper insight into performance trade-offs\u2014such as why \u201cremove\u201d operations underperform in some cases\u2014and more granular reporting of subjective evaluation outcomes.\n\nIn summary, Instruct-MusicGen introduces a creative and potentially impactful approach to music editing, with strong implementation and evaluation foundations. Yet, to truly meet the ambition implied by its framing, the paper must better align its claims with its current demonstrated capabilities, offer stronger comparisons with existing methods, and expand its analysis to deepen reader insight. Final recommendation: Weak Accept.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nSummary:\nThis paper presents Instruct-MusicGen, an instruction-tuned extension of the MusicGen architecture that enables text-based music editing operations such as adding, removing, or extracting instrumental stems. By incorporating lightweight audio and text fusion modules into the pretrained MusicGen model, the authors demonstrate an efficient and effective approach to music editing with minimal finetuning (~8% of parameters, 5k steps). The system is evaluated on both in-domain (Slakh2100) and out-of-domain (MoisesDB) datasets using a comprehensive suite of objective and subjective evaluations.\n\nStrengths:\n-Relevant and impactful task: Text-guided editing is an increasingly important problem, and the proposed approach provides a practical and well-scoped solution.\n- Efficiency and scalability: The method modifies only a small portion of the original MusicGen parameters, making it computationally lightweight while maintaining strong performance.\n- Generalization to real-world data: It is notable that the model, trained on synthetic Slakh2100 data, generalizes reasonably well to MoisesDB.\n- Comprehensive evaluation: The paper includes a broad set of objective metrics (FAD, CLAP, SI-SDR, etc.) and a user study, offering multiple perspectives on performance.\n\nWeaknesses:\n- Lack of clarity in method section: Section 3.2, which is central to the contribution, is dense and hard to follow. Figure 2 is visually cluttered and not well explained, and the notation (e.g., for audio/text embeddings and attention) could be clarified significantly.\n- Missing analysis of performance gaps: The model performs less well on \"remove\" tasks in the MoisesDB set, yet the authors do not discuss this drop or offer a possible explanation. Some reflection here would strengthen the evaluation.\n- Subjective evaluation could be more granular: While the listening test is appreciated, the results are aggregated across editing tasks. Breaking down instruction adherence and audio quality scores per task would offer more insight into where the model succeeds or struggles.\n- Visualizations are underwhelming: Figure 3 does not effectively show the editing impact. A spectrogram comparison of a single stem or multiple stems (e.g., before and after removal) would provide a clearer qualitative validation.\n\nOverall Assessment:\nThis is a solid and timely paper that addresses a real need in controllable music generation. While there are some issues with clarity in the method section and a few missed opportunities in evaluation analysis and visualization, the overall contribution is clear, well-motivated, and empirically supported. I believe it meets the bar for ISMIR and recommend acceptance.",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nStrengths:\n1. Low computational resources required.\n2. Reusable to other models.\n3. The objective evaluation includes comprehensive metrics.\nWeakness:\n1. Lack of demonstrating whether the CLAP model recognizes \"Add\", \"Remove\", \"Extract\". Should provide the results for \"Ground truth\" as in the subjective evaluation.\n2. Didn't compare with Audio Prompt adapter, while they also demonstrate \"add instrument\" ability in their demo.\n3. The paper emphasizes that other model often lacks the ability to precisely reconstruct the conditional audio, but after listening to the audio samples on the demo page, I consider this an unsolved problem. The fidelity of the audio input is not preserved.\n4. The samples on the demo page are in 4~5 seconds, which are relatively short, and may not fully demonstrate the model's capability.\n5. While the method seems to be inspiring in the field of music editing, the novelty is limited to \"Instruct pix2pix\". The difference is that Instruct-musicgen uses an adapter-based training method and uses simple text commands.",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper introduces Instruct-MusicGen, a text-to-music editing model focused on adding, removing, or extracting stems from music.\n\nMain strengths: A clever idea of modifying a pre-trained MusicGen model allows for the model to be trained in relatively few steps (5k). Intro and related work is relevant. The method section is written clearly and all the variables are well defined. Experiment section is extensive and elaborate. Overall, the writing is very concise and readability is good.\n\nMain weaknesses: While the scope of the work was set on adding, removing, and extracting stems, it would be good to see a broader scope with more instructions possible. Minor concerns in the evaluation section, see the points below.\n\nQuestions/clarifications/suggestions/typos:\n1. It is not clear to me why and how was CLAP used for evaluation of the Remove task. It makes sense for the Add and Extract tasks, but I am concerned it is not too valid for the Remove task, unless clarified. \n2. Lines 222 and 223 - typo in \"audio samples in Table 3\" -> you can consider rephrasing to something like \"(example spectrograms) in Figure 3\"?\n3. Figure 3 caption could be a bit longer than just \"audio samples\" - perhaps you could clarify these are spectrograms\n4. Lines 310 and 311 - \"Table 6 in the Appendix\". However, there is no Appendix and there is no Table 6. This looks like a leftover from a potential previous arxiv submission. Or are you referring to the InstructME paper's Table? It needs clarification/rewriting.\n5. Line 370 - why is it specified that on Slakh, Instruct-MusicGen achieved best CLAP and SSIM scores for the addition task, when results in Table 2 indicate that it actually achieved best scores in all tasks?\n6. Lines 378 and 379 - \"...showed improvements in CLAP and SSIM metrics for both addition and removal tasks\" - This is true as SSIM is better for both tasks, but CLAP is not better for both tasks, only for the addition task.\n7. Lines 379 to 382 - \"While it did not always lead in SI-SDR, it consistently outperformed baseline models, highlighting its efficiency and effectiveness in text-to-music editing applications.\" - However, the proposed Instruct-MusicGen does lead in SI-SDR in all the results shown in Tables 2 and 3. Why is this sentence present in the text?\n\nWith all that said, I am still inclining to strong accept because of very good readability, clear explanations, elaborate evaluation, and the elegance of the proposed idea itself."
    },
    "session": "3"
  },
  {
    "content": {
      "TLDR": "We propose a pre-trained BERT-like model for symbolic music understanding that achieves competitive performance across a wide range of downstream tasks. To achieve this target, we design two novel pre-training objectives, namely token correction and pianoroll prediction. First, we sample a portion of note tokens and corrupt them with a limited amount of noise, and then train the model to denoise the corrupted tokens; second, we also train the model to predict bar-level and local pianoroll-derived representations from the corrupted note tokens. We argue that these objectives guide the model to better learn specific musical knowledge such as pitch intervals. For evaluation, we propose a benchmark that incorporates 12 downstream tasks ranging from chord estimation to symbolic genre classification. Results confirm the effectiveness of the proposed pre-training objectives on downstream tasks.",
      "abstract": "We propose a pre-trained BERT-like model for symbolic music understanding that achieves competitive performance across a wide range of downstream tasks. To achieve this target, we design two novel pre-training objectives, namely token correction and pianoroll prediction. First, we sample a portion of note tokens and corrupt them with a limited amount of noise, and then train the model to denoise the corrupted tokens; second, we also train the model to predict bar-level and local pianoroll-derived representations from the corrupted note tokens. We argue that these objectives guide the model to better learn specific musical knowledge such as pitch intervals. For evaluation, we propose a benchmark that incorporates 12 downstream tasks ranging from chord estimation to symbolic genre classification. Results confirm the effectiveness of the proposed pre-training objectives on downstream tasks.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Jun-You Wang",
        "Li Su"
      ],
      "authors_and_affil": [
        "Jun-You Wang (Academia Sinica)*",
        "Li Su (Academia Sinica)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Symbolic music processing",
        "MIR fundamentals and methodology",
        "Evaluation metrics",
        "Musical features and properties",
        "Representations of music",
        "Automatic classification",
        "MIR tasks",
        "Evaluation, datasets, and reproducibility",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1axSJdG1-Nj1pXIFRpKyjAzmmemeDCd1j/preview",
      "poster_pdf": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-9-improving-bert-for",
      "title": "Improving BERT for symbolic music understanding using token denoising and pianoroll prediction",
      "video": ""
    },
    "forum": "66",
    "id": "66",
    "pic_id": "",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nStrongly agree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nStrongly agree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nMusically informed pre-text tasks (pre-training objectives) improve a model's learning of high-level music representations from symbolic data.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nTesting on a mixture of established and novel symbolic music understanding downstream tasks, the authors show that musically informed pre-training objectives outperform vanilla masked-language modeling in learning meaningful music representations.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis is the initial review for the paper \"Improving BERT for symbolic music understanding using token denoising and pianoroll prediction\", submitted to ISMIR 2025 in Daejeon. The paper approaches representation learning from symbolic music data in various formats (score, score-derived MIDI, performance-derived MIDI) using Transformer-based language modeling. As their main contribution, the authors propose to replace the vanilla masked language modeling (MLM) pre-training with two more musically informed pre-training objectives: correcting slightly corrupted tokens and predicting piano-roll-like pitch and chroma representations. Moreover, the authors extend an existing set of downstream tasks in symbolic music understanding with various other tasks, using this set of tasks for a comprehensive and systematic evaluation that shows the model's efficacy in the context of baselines and other models variants.\n\nIn general, this is an interesting idea. While not revolutionary and, in some respect, being an incremental adapation to previous models, the clear description, good motivation and, in particular, the comprehensive and systematic evaluation makes it an insightful contribution to ISMIR, which I recommend for acceptance. The only substantial criticism I have is the strict assumption of a 4/4 time signature. This crucially limits the model's applicability and contradicts the musically informed approach (moreover, this important information is mentioned much too late in the paper).\n\nOverall, the paper is well-structured and the writing is clear. However, there are a number of imprecise and even wrong statements/definitions that have to be corrected. Moreover, some important information is given in the paper at a later stage but should be mentioned earlier to guide the reader into the right direction. I will list these problems in the following:\n\nline (l.) 11: \"predict the [...] piano roll\" - from what (single note representation)???\nl. 51: Why is 5 tasks not comprehensive enough? Is 12 comprehensive?\nl. 64: \"infer pitch and chroma distribution from the input note sequence\" - this seems trivial as described here, does it involve the correction of corrupted notes? Then, this should be made clear\nl. 92: \"Symbolic music is an abstract form of music\" - This is not correct. There is no \"symbolic music\", maybe \"symbolic music data\" or \"representation\". Moreover, these are representations and not \"forms of music\" - please correct!\nl. 97: \"Both MIDI and sheet music are forms of symbolic music\" - This is also not correct. In particular, sheet music refers to the written artifact, which could be physical paper, or (as data) simple pixel graphics, which are *NOT* symbolic (i.e. machine-readable by explicitly encoding musical information). Please correct this\nl. 145: \"raw DB information [by retrieving] tick information (ticks per beat)\" - this is unclear, doesn't it rather require knowlege of the ticks per measure? If this is derived by assuming a constant time signature (4/4), this assumption needs to be mentioned beforehand.\nl. 164 ff: What is a \"1/4 beat\" compare to a \"32nd\" note? Are beats assumed to be equal to quarter notes? This is a crude simplification of symbolic music data, and crucially limits model performance! Moreover, please stick to one semantic description (beats or note durations) and explain why onset positions and durations are modeled in different resolutions!\nl. 187: \"15% in practice\" - 15% of what?\nl. 223: \"sampling from all tokens\" - from all possible tokens?\nl. 255: \"tatum-level prediction\": confusing, better \"local prediction\"! \nl. 264: \"one bar contains 16 tatums\" - Why??? Oversimplification!\nl. 331: \"Piano performer style classification\" - up to this point, the reader assumes only score-like music in the data. It should be mentioned much earlier that the model works with performance-like data (e.g. performance MIDI) as well!\nl. 434f.: \"while excluding all no-4/4 time signatures\" - This information is way too late! Also, this is a crude oversimplification that crucially limits the model's usefulness!\nl. 464.: \"almost the downstream tasks\" - almost all the downstream tasks?\nl. 502: \"outperforms SOTAs in six tasks\" - unscientific statement!\nl. 515: \"Due to page limitation...\" - this is not an excuse! Please adapt the paper writing to fit more of these results.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nStrong accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nSummarizing the main aspects from the four reviews, this paper constitutes a valuable and interesting contribution to ISMIR. All authors agree on this positive assessment, emphasize the interesting strategy and the insightful multi-task benchmark.\n\nThere are two substantial issues of criticism (apart from several minor writing problems), which should be addressed:\n* There is a strict assumption of a 4/4 time signature, which substantially limits the model's applicability and contradicts the musically informed approach. Please mention this earlier and bring arguments for this choice.\n* The claims from the experiments are too strong. In particular, the effect of the proposed training strategies (token denoising & piano roll prediction) are rather small, while other tweaks have a stronger effect. Please discuss this more carefully and cautiously.\n\nOverall, we congratulate the authors to this interesting submission and look forward to see this paper at ISMIR!",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper introduces M2BERT, a model for understanding symbolic music (notes). It learns by 1) fixing \"broken\" notes and 2) predicting piano rolls. It also presents the SMC benchmark (12 tests) to evaluate music understanding, aiming to improve learning beyond old methods. \nResults are limited by over-simplification of the problem (e.g. only 4/4 timings, only MIDI), and by missing tests and resources, but code is shared, aiding community work. \nOverall, a valuable ISMIR contribution.\n\nHere is the list of notes that highlights my critics.\n\n3. PROPOSED METHOD\n* Getting beat start (DB) info is poorly explained (ala \"not perfect but works\"), hurting trust in input quality and repeatability.\n* Rounding note timing for tokens might lose rhythm details, bad for time-sensitive music.\n* Tokenization for scores vs. performances isn't clearly different, despite their timing variations.\n* Choice of how much to \"break\" notes for fixing (corruption levels) isn't well justified or tested for optimality.\n\n4. THE SMC BENCHMARK\n* Tests mix score and performance data without a clear strategy. This makes it hard to know what the model learns from each distinct music type.\n* For beat-finding tests, forcing all music to one tempo/rhythm (4/4) is unrealistic and questions test validity in real case scenarios.\n\n5. EXPERIMENT SETUP\n* Excluding non-4/4 music from the \"Reduced\" learning set limits rhythm understanding. No reason given.\n* The \"Full\" dataset model (for SOTA comparison) didn't train long enough (25 epochs, still improving), making results unreliable.\n* 30% note corruption and specific breakage levels lack clear justification or ablation study.\n\n6. RESULTS \n* Conclusions from the \"Full\" dataset (including SOTA claims) are weak due to insufficient training.\n* Impact of music changes for beat-finding tests isn't discussed enough.\n* Longer music piece test (ablation) only on two tasks; too limited to generalize.",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nIn my opinion the two strongest point of this work are:\n\n1) To build an extensive downstream evaluation framework with 12 different tracks.\n2) To perform an exhaustive ablation of all the proposed contributions showing mostly consistent improvements in most of the proposed task.\n\nAs a downside, I find it a bit misleading that, while the paper's title and abstract suggest that the token denoising and the pianoroll prediction are the main contributions of the paper, it can be seen in Table that typically these are not the modifications causing the highest performance bost (except for tasks VE and OT). Most of the time, the highest performance boost is due to the improved architecture (ModernBERT, row 2) or the extended training dataset (row 6). While I believe that ISMIR's policy doesn't allow for a title change, I'll encourage the authors to highlight the impact of these modificaiton in the model performance.\n\n## Specific comments\n\nLines 192-197: I disagree with the authors statement suggesting that the proposed model accounts for domain specific knowledge. While this is something that is not proven by the proposed experiments, a simpler explanation is that providing noise that is closer to the in-distribution data (instead of a MASK token or pure random noise) is a harder tasks to solve, so it results is more robust representations which benefit the downstream tasks.",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper improves Midi-BERT by introducing a new backbone architecture, a musically informed token corruption method, and piano-roll/chroma generation pre-tasks, achieving superior performance in most of the 12 downstream tasks. \n\nOverall, the writing is clear and accessible, and the model demonstrates clear performance gains compared to Midi-BERT with similar model size and dataset.\n\nRaised question:\n- \nRaised Question: From my understanding, the relationship between pitch in CP-tokens and pitch in piano-roll representations is nearly a 1:1 mapping. Unless piano-roll prediction includes additional spatial (e.g., octave-related) mechanism, their training effect seems limited, making me partially disagree with the authors. In contrast, chroma representations could enable learning of octave relationships (e.g., C1-C2) not captured in CP-tokens. Thus, I wish Table 1 included an ablation study showing the individual contributions of chroma and piano-roll to better understand their respective impacts."
    },
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Local key estimation (LKE) is an important yet challenging task in music information retrieval since it involves a high level of musical abstraction, which entails ambiguity and low inter-annotator agreement. Relying on limited (small) datasets with a single annotation may introduce not only dataset bias but also annotator bias. To address such problems, we propose in this paper a novel, annotation-free evaluation strategy for LKE. To this end, we exploit datasets where multiple versions of the same musical work are available. We investigate the models' consistency across versions, expecting an effective and robust model to output similar predictions on different versions of the same work. In our experiments, we study the behavior of the proposed cross-version consistency measure at the example of different models and datasets, indicating a strong correlation between cross-version consistency and the models' effectiveness on in-domain data as well as their generalization to out-of-domain data. Our further studies show that, while being correlated to common evaluation metrics, cross-version consistency is also capturing different aspects of model behavior, thus serving as an additional figure of merit for evaluating LKE models.",
      "abstract": "Local key estimation (LKE) is an important yet challenging task in music information retrieval since it involves a high level of musical abstraction, which entails ambiguity and low inter-annotator agreement. Relying on limited (small) datasets with a single annotation may introduce not only dataset bias but also annotator bias. To address such problems, we propose in this paper a novel, annotation-free evaluation strategy for LKE. To this end, we exploit datasets where multiple versions of the same musical work are available. We investigate the models' consistency across versions, expecting an effective and robust model to output similar predictions on different versions of the same work. In our experiments, we study the behavior of the proposed cross-version consistency measure at the example of different models and datasets, indicating a strong correlation between cross-version consistency and the models' effectiveness on in-domain data as well as their generalization to out-of-domain data. Our further studies show that, while being correlated to common evaluation metrics, cross-version consistency is also capturing different aspects of model behavior, thus serving as an additional figure of merit for evaluating LKE models.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Yiwei Ding",
        "Yannik Venohr",
        "Christof Weiss"
      ],
      "authors_and_affil": [
        "Yiwei Ding (University of W\u00fcrzburg)*",
        "Yannik Venohr (University of W\u00fcrzburg)",
        "Christof Weiss (University of W\u00fcrzburg)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Evaluation metrics",
        "Musical features and properties",
        "Harmony, chords and tonality",
        "MIR tasks",
        "Evaluation, datasets, and reproducibility",
        "Evaluation methodology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1GDW7KTR_1atpLS_zcY7YFHiKd8kwzqIU/preview",
      "poster_pdf": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-5-an-evaluation-strategy",
      "title": "AN EVALUATION STRATEGY FOR LOCAL KEY ESTIMATION: EXPLOITING CROSS-VERSION CONSISTENCY",
      "video": ""
    },
    "forum": "67",
    "id": "67",
    "pic_id": "",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nDisagree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nWhile the proposed metric can and probably should be adopted in future papers working on local key estimation, it teaches little about the type of errors being made, nor provides a straightforward way to lead to improved estimation.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nWhen multiple versions of a composition are available, the proposed metric can give insight into the local key estimation of an algorithm without relying on annotations.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak reject\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nSummary\n-----------\nA new metric for local key estimation is proposed, which is based on the consistency of the estimated local key across different versions of a piece of music and therefore does nor rely on annotations. It is shown to be correlated to existing, annotation-based recall metrics, by performing experiments with multiple deep learning based systems. There are the differences between the proposed and existing metrics though, for instance the ranking between models is not necessarily preserved, but it is not clear what the proposed metric captures that the existing ones don't and vice versa.\n\nPositives\n----------\n- A well written text, with a clear structure and a good didactic approach.\n- A variety of models and datasets used.\n\nNegatives\n-----------\n- Narrow focus on classical music. The method relies on the availability of multiple interpretations of the same piece of music, in a way that is common in classical music. It would be interesting to see if the proposed methed transcends this narrow field and would also work with popular or jazz music. The exact defintion of and difference between version would be crucial there. Is a remaster of a pop song sufficiently different? Does the metric still work with covers? Can this be used with repeated improvisations in jazz music?\n- Creating models of varying quality by saving checkpoints every 10 epochs is the easiest, but likely not the best way to do this because of the obvious correlation between subsequent checkpoints. Better would be to do small architecture variants, such as reducing the number of neurons in a layer/number of layers/etc.\n- While the clear explanation and increasing difficulty of the experiments is much appreciated, the proposed contribution feels relatively small to spread out over 6 pages. RQ3 is the one that matters in the end, the preceding RQs are part of the metric's development, but of lesser consequence. \n- It's unclear if the new metric will have a significant impact on the field. The CVC can/will be reported in future papers on LKE, but will new methods be proposed/accepted only based on CVC performance on unlabelled data? Either further insights into the differences between CVC and recall measures or demonstration of its applicability beyond classical music should be added to ensure impact of the work. \n\nOverall\n--------\nA well executed and presented paper that is easy to read and understand. The proposed metric is interesting, but the narrow focus on classical music and the lack of insight into the differences between the proposed and existing metrics diminish the value of its contribution.\n\nPresentation\n--------------\n- The correlation matrices presented in Fig. 6 would be better presented as triangular matrix to avoid unnecessary duplication of data and visual distraction.\n- l. 197: \"mesaure\" should be \"measure\"\n- l. 237: \"architecure\" should be \"architecture\"\n- l. 403: \"shorteset\" should be \"shortest\"\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak reject\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nAll reviewers agree that the root of the ideas presented in this submission are very interesting and have great potential.\nThe excellent presentation and writing style is also much appreciated. Some questions over the interpretation and applicability of the proposed metric remain, however, which would ideally be addressed in future iterations of this work. Do have a look at the individual reviews for more details.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nSummary\n\nThis paper presents a simple yet effective idea: in the absence of ground truth labels, align multiple datasets of the same songs, and treat agreement between predictions as a proxy for recall.\n\nStrengths\n- Clearly defined problem and straightforward solution\n- Comprehensive evaluation\n- Incorporates a music-knowledge-based scoring variation\n\nWeaknesses\n- The proposed metric correlates with recall, but the rationale for prioritizing recall is not well-justified\n- Lack of analysis involving other standard metrics, such as precision or F1-score\n- Unclear whether the reported recall is micro or macro. Given that the authors align it with accuracy, it should be micro.\n\nDetailed Comments\n\nThe authors should elaborate on the choice of recall as the primary evaluation metric. From the manuscript, it appears they are using micro recall, that corresponds to overall accuracy in most settings (single label as this one).\n\nHowever, accuracy (or micro recall) is known to be biased toward the most frequent class. In highly imbalanced settings, a naive classifier that always predicts the most common label can achieve a high recall. It would be valuable to understand whether the proposed method accounts for this bias in any way.\n\nMoreover, a discussion on the trade-offs and limitations of relying solely on recall would strengthen the paper\u2019s evaluation.\n\nAnalysis\nI independently downloaded the dataset and used a quick script (with help from ChatGPT) to verify that the concerns raised above were not critical in this particular case. Nevertheless, the lack of discussion on metric selection remains a weakness, and I encourage the authors to address it.\n\nhttps://chatgpt.com/share/681fe25f-dec4-8006-b7d2-8dd3d1f048c5",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper cleverly exploits the fact that different performances of a western classical music piece should have the same local key annotations (given the same annotator) to define an annotation-free local key estimation evaluation metric (CVC) describing the cross-version model performance consistency. The key to making this happen is time-aligning the different performances of the same piece. \n\nThe paper overall is well-written, with each concept and method being clearly explained. The music theory introduced and explained is sound. I would recommend that the fact that the method is applicable to western classical music be disclosed in the abstract, given that absolutely aligned reperformances with the same structural section ordering/repetitions are not standard outside of this context, at least for existing datasets.\n\nAs mentioned before, I think the potential of annotation-free evaluation is big, and it has certainly driven progress in some areas in MIR. This particular trick, while not particularly complex, is clever. While one could think of it as simply a use of domain-shift generalization as a proxy for model evaluation, this particular application is particularly suited, given its \"domain-shift\" almost perfectly preserves non-performance-related factors of variation/content.\n\nOverall, I like the experiments conducted, particularly the idea of using the different checkpoints as a proxy to expected model performance. However, my main point of criticism is that while inter-annotator agreement is emphasized as a problem in these scenarios, and, thus, also of using recall/MIREX given they are based on annotations, the correlation experiments are still ran against them. If there is annotation bias in the annotations used, then correlating to recall based on the annotations is obviously limited and does not address the issue. Obviously I don't think this is an easy problem to solve, but I think the way it was introduced made me expect that an experiment would be conducted to address it. I think it would be useful to 1. acknowledge the limitations of using these metrics as a base of comparison in the experiment design, given the annotation limitations introduced, and 2. mention more valid uses of this metric, rather than the absolute measure of LKE performance (which currently is also hampered by the cross-model CVC inconsistencies) - one such use would be as a supervision signal.\n\nMy other criticism is that, while the figure 6 experiment is well designed, the conclusion that the CVC variants are a novel figure of merit because of it is a bit of a generous interpretation of the weaker correlation. That's particularly because of the small differences between the original and EMD variant compared to MIREX. I would have liked to see these results investigated further. Given the overall potential of the method, I would have prefered that more space is allocated to a discussion about possible directions of improvement (particularly in the direction of more musically/perceptually informed approaches), limitations, and future work.\n\nOverall, I think this is a good paper with scientific merit, and some changes within the scope of the camera ready can increase its soundness and impact, though some mentioned limitations would remain.",
      "review3": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nThe problem and the idea are both very interesting. However, the main comment is whether the results are convincing enough. In Figure 6, the MIREX and recall agree much better than do CVC_TVD and CVC_EMD, which in turn, are better than any cross combinations like Recall/MIREX and CVC_*.\n\nOn the other hand, the authors have clarified that the CVC is not a replacement for current metrics, but complements them. In that case, the applications of CVC must be brought out. What additional utility does it bring?\n\nSome other points I noticed:\nFigure 2 is not referred in the text anywhere. Presumably illustrates the description in Section 3.\nSection 3: L is not defined, hopefully it is the number of gray segments in Figure 2. Similarly for M and N can be mapped to the figure.\nPlease clarify in the text that the predictions are probabilities of the 24 keys at the introduction of p and q.\n\nI'm not an expert in the subject, but \"frame\" in the context of audio suggests 10 to 60 ms. The frame length is not in the paper anywhere. It is hard to imagine a local key every frame, unless there is a window of a few seconds ending at that frame."
    },
    "session": "2"
  },
  {
    "content": {
      "TLDR": "In this work, we introduce the Sheet Music Benchmark (SMB), a dataset of six hundred and eighty-five pages specifically designed to benchmark Optical Music Recognition (OMR) research. SMB encompasses a diverse array of musical textures, including monophony, pianoform, quartet, and others, all encoded in Common Western Modern Notation using the Humdrum **kern format. Alongside SMB, we introduce the OMR Normalized Edit Distance (OMR-NED), a new metric tailored explicitly for evaluating OMR performance. OMR-NED builds upon the widely-used Symbol Error Rate (SER), offering a fine-grained and detailed error analysis that covers individual musical elements such as note heads, beams, pitches, accidentals, and other critical notation features. The resulting numeric score provided by OMR-NED facilitates clear comparisons, enabling researchers and end-users alike to identify optimal OMR approaches. Our work thus addresses a long-standing gap in OMR evaluation, and we support our contributions with baseline experiments using standardized SMB dataset splits for training and assessing state-of-the-art methods.",
      "abstract": "In this work, we introduce the Sheet Music Benchmark (SMB), a dataset of six hundred and eighty-five pages specifically designed to benchmark Optical Music Recognition (OMR) research. SMB encompasses a diverse array of musical textures, including monophony, pianoform, quartet, and others, all encoded in Common Western Modern Notation using the Humdrum **kern format. Alongside SMB, we introduce the OMR Normalized Edit Distance (OMR-NED), a new metric tailored explicitly for evaluating OMR performance. OMR-NED builds upon the widely-used Symbol Error Rate (SER), offering a fine-grained and detailed error analysis that covers individual musical elements such as note heads, beams, pitches, accidentals, and other critical notation features. The resulting numeric score provided by OMR-NED facilitates clear comparisons, enabling researchers and end-users alike to identify optimal OMR approaches. Our work thus addresses a long-standing gap in OMR evaluation, and we support our contributions with baseline experiments using standardized SMB dataset splits for training and assessing state-of-the-art methods.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Juan C. Martinez-Sevilla",
        "Joan Cerveto-Serrano",
        "Noelia Luna-Barahona",
        "Greg Chapman",
        "Craig Sapp",
        "David Rizo",
        "Jorge Calvo-Zaragoza"
      ],
      "authors_and_affil": [
        "Juan C. Martinez-Sevilla (University of Alicante)*",
        "Joan Cerveto-Serrano (University of Alicante)",
        "Noelia Luna-Barahona (University of Alicante)",
        "Greg Chapman (-)",
        "Craig Sapp (Stanford University)",
        "David Rizo (University of Alicante)",
        "Jorge Calvo-Zaragoza (University of Alicante)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Evaluation metrics",
        "Music transcription and annotation",
        "Novel datasets and use cases",
        "Optical music recognition",
        "MIR tasks",
        "Evaluation, datasets, and reproducibility",
        "Evaluation methodology",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1vIRtBhRSleT_xnSuXXNAJQBfOEZGaydQ/preview",
      "poster_pdf": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-13-sheet-music-benchmark",
      "title": "Sheet Music Benchmark: Standardized Optical Music Recognition Evaluation",
      "video": ""
    },
    "forum": "70",
    "id": "70",
    "pic_id": "",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nStrongly agree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThe paper introduces a high-quality benchmark dataset (SMB) and a detailed evaluation metric (OMR-NED) that together offer standardized, fine-grained assessment of modern OMR systems.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper fills an important gap in OMR. offering a much-needed benchmark dataset (SMB) and a novel evaluation metric (OMR-NED) that provides fine-grained error analysis across musical symbol categories. The construction and annotation of the dataset are rigorous, and the metric builds meaningfully on existing tools like MusicDiff. The baseline experiments, while not yielding high accuracy, are appropriate and indicative of the dataset's complexity.\n\nStrengths:\n- Timely and highly relevant contributions to the MIR/OMR community\n- Solid technical methodology for both dataset and metric\n- Clear writing and thoughtful organization\n\nSuggestions for improvement:\n- Consider adding a summary table comparing SMB to previous datasets (e.g., size, scope, textures, and coverage).\n- Provide visual examples or schema for OMR-NED categories and scoring to aid understanding.\n- Ensure that links to dataset/code are well-documented and accessible post-acceptance.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nStrong accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThis paper introduces a significant and much-needed contribution to the Optical Music Recognition (OMR) research community through the development of a standardized benchmark dataset (SMB) and a novel evaluation metric (OMR-NED). The work is timely, addressing a longstanding gap in reproducible, fine-grained evaluation for OMR systems, and has clear potential to become a foundational reference for future research in the field.\n\nAcross the board, reviewers acknowledge the technical soundness, relevance, and clarity of the work. The dataset construction and annotation processes are described as thorough, and the OMR-NED metric is appreciated for its granularity and thoughtful design.\n\nSome reviewers expressed concerns about details that could be improved in the camera-ready version, including:\n\n* Clarifying the licensing terms of the dataset.\n* Providing per-category evaluation scores to highlight the variability in task difficulty.\n* Offering more clarity on the tokenization process and how OMR-NED treats substitutions.\n* Expanding the explanation of differences between SER and OMR-NED and motivating the design choices in the latter.\n* Verifying the accuracy of labels in the dataset (e.g., use of \u201cmonophonic\u201d).\n\nThese are constructive and actionable suggestions, none of which undermine the overall scholarly contribution of the work. Rather, they indicate areas for refinement that will enhance the impact and usability of the benchmark.\n\nImportantly, the paper meets ISMIR\u2019s criteria for reproducibility and openness, and it presents reusable infrastructure\u2014dataset, metric, and tools\u2014that will serve the community for years to come. It is rare to see a benchmark paper that is both technically solid and practically impactful in the way this one is.\n\nThe paper represents an exemplary effort in infrastructure building for MIR, especially in a field where robust benchmarks have been lacking. It is likely to catalyze future work, foster comparability between methods, and stimulate further discourse on evaluation methodologies in OMR.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe paper introduces a new, large and diverse benchmark for the optimal music recognition (OMR) task, with detailed dataset construction, metric definition, and baseline results. The key contributions are:\n\n- The dataset is large and diverse, and the paper provides a thorough statistical analysis\n- The process of annotation and post-processing is well-described, helping the users better understand the characteristics and any limitations of the dataset.\n- A new suggested standard metric (OMR-NED) for the benchmark and the improved tools for calculating them, with discussions on the pros and cons of each.\n\nMinor limitations that could be addressed for camera-ready:\n\n- The paper doesn't seem to clarify the license for the data, other than a mention of public-domain uploads.\n- In addition to the dataset, a reference implementation of the OMR-NED and SER could further facilitate easier and more reproducible comparison of the benchmark.\n- While having a competitive baseline is not the primary goal of a dataset paper, and the low performance is expected without a vision model as the authors note, OMR-NED often being over 90% is somewhat discouraging. Using a pretrained encoder (by using one of publicly available vision models or pre-training on GrandStaff) would have produced more competitive baseline metrics that can help better understand how difficult the benchmark is and how likely it may become saturated soon and no longer useful.\n- Also, per-category baseline metrics would've been also good to include, basically a version of Table 3 for the models/data in Table 2, which would be informative on the expected difficulty in each category and also encourage the researchers using this benchmark to report per-category details.",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper introduces a dataset and an evaluation measure to benchmark OMR systems. As such, it is a valuable contribution to the community, as there is a lack of annotated evaluation datasets in OMR. \nThe description of the dataset is suitable, however the description of the proposed evaluation metric and the baseline experiments lack in detail. To improve the paper, I suggest the authors to:\n\n- describe the limitations of the chosen **kern format - it is not a fully-fledged music notation format such as MusicXML, so you should describe what it can and can't represent\n\n- provide more details on how the proposed evaluation metric is calculated, as it is not completely clear\n\n- the baseline system's error rates are so high, that it makes me wonder if there is any use in including the results - over 90% error rate means that almost all symbols are wrong, so the output is almost random? If so, it makes no sense in including this evaluation, if not, you should make this clearer in the paper.\n\n- you should elaborate more deeply on the differences between SER and the proposed measure, as they can differ over 50%?",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe proposed dataset and evaluation metric are a meaningful contribution to OMR research. Overall the paper is written clearly, but some sections are lacking, and the experiments are not entirely convincing.\n\nStrengths:\n* New dataset for OMR of scanned classical Western music (685 pages)\n* Standardization of kern notation for consistent tokenization\n* Proposed OMR-NED evaluation gives information on type of error\n\nWeaknesses:\n* Discrepancy between experiments and stated dataset purpose: Due to the small size of the dataset, training from scratch gave bad results. Authors could have fine-tuned a model trained on more data, or used the dataset as a benchmark for existing models, which is what they proposed this dataset for in the first place.\n* Not clear whether author release all their code (Humdrum standardization, MusicDiff modifications, evaluation metric calculation). Just making sure.\n* In OMR-NED, does it really make sense to treat a substitution as insertion+deletion, counting it as two errors?\n* Tokenization scheme is not stated (is every full \u2018word\u2019 a token?), yet dataset statistics are reported on tokens rather than e.g. notes.\n\nAdditional comments:\n* Table 2: Interestingly, OMR-NED and SED give a different ordering of the rows. For example, if we look at the ekern results only, then Monophony got the worst results under OMR-NED, but placed second under SED. How do authors explain this discrepancy? Can they somehow show that OMR-NED matches human judgement better than SED, which counts substitutions differently?\n* Table 2: How come Monophony (which is supposedly the easiest task) got worse scores than Pianoform, Quartet, and Other?\n* I downloaded the dataset and noticed that many scores labeled as Monophonic (in the mono_scores folder) are in fact not monophonic (they have chord/voices). Did authors (mis-)use the term monophonic throughout the paper to mean \u2018single staff\u2019?\n* Figure 4: The lyrics seem made up and don\u2019t match the music. This is not a very good illustration of OMR-NED.\n* Authors mentioned dataset splits in the abstract, but where are they? I see only a \u2018train\u2019 split.\n* Line 211: \u2018particellas\u2019 is not a standard term in English. The term is \u2018parts\u2019."
    },
    "session": "5"
  },
  {
    "content": {
      "TLDR": "This work introduces PeakNetFP, the first neural audio fingerprinting (AFP) system designed specifically around spectral peaks. This novel system is designed to leverage the sparse spectral coordinates typically computed by traditional peak-based AFP methods. PeakNetFP performs hierarchical point feature extraction techniques similar to the computer vision model PointNet++, and is trained using contrastive learning like in the state-of-the-art deep learning AFP, NeuralFP. This combination allows PeakNetFP to outperform conventional AFP systems and achieves comparable performance to NeuralFP when handling challenging time-stretched audio data. In extensive evaluation, PeakNetFP maintains a Top-1 hit rate of over 90% for stretching factors ranging from 50% to 200%. Moreover, PeakNetFP offers significant efficiency advantages: compared to NeuralFP, it has 100 times fewer parameters and uses 11 times smaller input data. These features make PeakNetFP a lightweight and efficient solution for AFP tasks where time stretching is involved. Overall, this system represents a promising direction for future AFP technologies, as it successfully merges the lightweight nature of peak-based AFP with the adaptability and pattern recognition capabilities of neural network-based approaches, paving the way for more scalable and efficient solutions in the field.",
      "abstract": "This work introduces PeakNetFP, the first neural audio fingerprinting (AFP) system designed specifically around spectral peaks. This novel system is designed to leverage the sparse spectral coordinates typically computed by traditional peak-based AFP methods. PeakNetFP performs hierarchical point feature extraction techniques similar to the computer vision model PointNet++, and is trained using contrastive learning like in the state-of-the-art deep learning AFP, NeuralFP. This combination allows PeakNetFP to outperform conventional AFP systems and achieves comparable performance to NeuralFP when handling challenging time-stretched audio data. In extensive evaluation, PeakNetFP maintains a Top-1 hit rate of over 90% for stretching factors ranging from 50% to 200%. Moreover, PeakNetFP offers significant efficiency advantages: compared to NeuralFP, it has 100 times fewer parameters and uses 11 times smaller input data. These features make PeakNetFP a lightweight and efficient solution for AFP tasks where time stretching is involved. Overall, this system represents a promising direction for future AFP technologies, as it successfully merges the lightweight nature of peak-based AFP with the adaptability and pattern recognition capabilities of neural network-based approaches, paving the way for more scalable and efficient solutions in the field.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Guillem Cort\u00e8s-Sebasti\u00e0",
        "Benjamin Martin",
        "Emilio Molina",
        "Xavier Serra",
        "Romain Hennequin"
      ],
      "authors_and_affil": [
        "Guillem Cort\u00e8s-Sebasti\u00e0 (Universitat Pompeu Fabra, BMAT Licensing S.L.)*",
        "Benjamin Martin (Deezer)",
        "Emilio Molina (BMAT Licensing S.L.)",
        "Xavier Serra (Universitat Pompeu Fabra)",
        "Romain Hennequin (Deezer)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Indexing and querying",
        "Fingerprinting",
        "Similarity metrics",
        "MIR tasks",
        "Reproducibility",
        "Pattern matching and detection",
        "Evaluation, datasets, and reproducibility"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1kvgIZSeYEPs0dh3hMlrdH2ojrA-VQjGw/preview",
      "poster_pdf": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-11-peaknetfp-peak-based",
      "title": "PeakNetFP: Peak-based Neural Audio Fingerprinting Robust to Extreme Time Stretching",
      "video": ""
    },
    "forum": "74",
    "id": "74",
    "pic_id": "",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nStrongly agree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nStrongly agree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe idea of point cloud processing can be used for other applications in music and audio processing\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nAudio fingerprinting method using only the spectral peaks and PointNet++, leading to efficient deployment\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nContribution:\n- The paper proposes to use PointNet++ for the Audio Fingerprinting (AFP) task\n- This leads to a reduction in model size and input data size\n- Results are on par with the NeuralFP system.\n\nLimitations:\n- The experimental evaluation is very limited. \n- The pointNet++ algorithm, as used for AFP, is not described. \n - What is the feature vector at every peak? (Line 347 talks about feature aggregation)\n - Is the magnitude of the peak considered?\n - Is the displacement (distance + direction) between peaks considered for deriving the aggregated features? What is the mathematical formula used?\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nStrong accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nAll reviewers unanimously agree to accept this paper.\nThe authors could do minor edits to include the reviewer suggestions in the final version of the paper.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nGeneral comments:\n\n- This is a well-written paper, with a practical contribution and solid references. I wish the description of the model will come with better/more explanations and clearer/larger plots. The evaluation could also be a bit improved, to better showcase the performance of the proposed model. See the details comments below.\n\n\nDetailed comments:\n\n1.\n- Reference [18] does not seem to be about audio fingerprinting but codec identification. Perhaps you meant to cite the following patent from this company? R. Coover and Z. Rafii, \u201cMethods and Apparatus to Fingerprint an Audio Signal via Normalization,\u201d 16/453,654, Mar. 2020.\n\n3. \n- Figure 1 is too small.\n\n- When you say \"raw local maxima,\" do you mean that you also use the amplitude of the peaks? And how do you define a local maxima? Do you use a specific window size or maximum peak distance(s)? How do you deal with silences or segments with sparse energy? I would have liked to see some information about the spectrogram itself.\n\n- I would also make Figures 2 and 3 bigger (including the fonts). It would really help the reader to understand the system.\n\n- I am unclear what SA and MSG are in Figures 2 and 3. What kind of features gets into the MLP then? Section 3.2 is a very important section as it is meant to explain the core of the algorithm. I would (try to) make it clearer. I didn't fully understand the steps, how the features would look like at every step, and what is actually being learned.\n\n- \"NT-Xent\"\n\n- What is a \"faiss\" index?\n\n- Could you write a bit more about the dataset, the type of audio in it? Does it come with noise too?\n\n4. \n- It would make things easier for the reader if Table 1 had more information in the caption. What are j and A, B, C, for example?\n\n- Why not (also) use the more classic TP/FP/FN, and/or recall and precision as the metrics?\n\n- The fonts in Figure 4 are too small; I cannot really see the values in the plot.\n\n- Some of the results showed in section 4.2 could be turned into (perhaps more convincing) tables (or perhaps you were too short on space).\n\n6.\n- Please, fix your bibliography:\n- Be consistent in the way you list every entry\n- Avoid repetition (some entries have the name of the conference and the location listed twice)\n- Use capital letters when needed (e.g., proper nouns and acronyms)",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe authors of this paper present a novel approach to neural audio fingerprinting, combining the neural self-supervised approach such as in NeuralFP with peak-based algorithms. With substantially less parameters, their proposed model performs comparably well with an implementation of NeuralFP.\n\nThe paper is very well written and follows a clear and logical structure. The motivation is outlined well and a good overview is given over related previous works. Section 3.2 \"Hierarchical peak set feature extraction\", being a crucial part of this work could be presented in a little more detail as PointNet++ will be less familiar to the MIR community. \n\nSome minor notes on the paper:\n\nLine 100 ff: \"This method is commonly used by DJs to synchronize the tempo of different songs within a mix or to create remixes that are either slowed down or sped up [20]\". -> Yes, while traditionally DJs apply pitch-shifting which incorporates both time and freqeuncy changes of the signal. In fact, it would have been interesting to see the apporach for manipulation in both of these domains. To the best of my knowledge, time stretching and pitch shifting are often used by video creators to circumvent licensing costs for music they use in their productions. This could be added to the motivation (there might be internet sources at least).\n\nLine 389: The \"candidate pruning\" could be described in a little more detail. Also, this term does not appear in the referenced NeuralFP paper - I suggest you are referring to what they denote as \"Sequence search\"?\n\nLine 400: \"and it comes\" -> \"that comes\"\n\nLine 405: It is unclear to me where the 278 seconds average length come from. \n\nLine 408: \"in\" -> \"as in\" ?\n\nLine 433: What motivates the somehow uncommon batch size of 240?\n\nLine 502: With \"query size\", do you refer to the segments' size in seconds? It would be favorable if you would stick to a clear unified terminology here (you usually use \"query length\").\n\nI strongly suggest that this paper will be accepted for ISMIR 2025. I believe that this paper would also deserve a nomination for a best paper award. As noted at point 18:\n\nThis paper is very well written and clearly outlines the motivation. With neural audio fingerprinting, the authors tackle an important topic in MIR, which on top of artist discovery can aid digital rights management and fair artist compensation. In the era of generative music AI, such fingerprinting systems become even more crucial. The authors present a smart, signal processing based feature engineering approach for a more efficient implementation that can help save resources which is a further important factor for the paper to be outstanding. \nTo me, the combination of these points reasonably justify the nomination for a best paper award.",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nDear authors, thank you for your paper. It is clearly written, focused, and well-executed around a central idea.\n\nThe motivation is strong: the paper addresses a real-world gap by combining sparse peak-based input with neural representations. This hybrid strategy is well-motivated for low-resource or privacy-constrained environments. Your framing of the practical trade-offs (e.g., computation on client devices, data volume, dense vs. sparse representations) is excellent and grounds the paper in relevant applications.\n\nThe technical implementation is clearly explained, and the evaluation is thorough. The comparison with QuadFP and NeuralFP is carefully done, and results are clearly presented. I appreciated the validation of your own QuadFP implementation and the transparency around dataset differences.\n\nThere are a few areas for improvement:\n* While the paper focuses on robustness to time stretching, omitting pitch shifting is a limitation. Since pitch and tempo changes often co-occur in real-world transformations, I encourage you to include this in future work (which you mention at the end).\n* The \u201c11\u00d7 smaller input data\u201d claim in the abstract and conclusion is interesting, but the exact basis or formula behind this figure is not explained. Please consider clarifying how it\u2019s computed.\n* The categorization of related work into \u201clocal descriptor-based\u201d approaches could benefit from a clearer definition and concrete examples.\n* The use of \u201cfaiss\u201d should be capitalized to \u201cFaiss\u201d.\n\nOne of the strongest aspects of the paper is that you commit to sharing both code and dataset. This makes it easier to reproduce your results and build on top of the work, and I expect it will encourage follow-up research from both your team and others.\n\nWhile this is not a \u201crevolutionary\u201d paper in terms of algorithmic novelty, it is a solid contribution with strong execution and real practical relevance. I strongly support its acceptance."
    },
    "session": "2"
  },
  {
    "content": {
      "TLDR": "We propose a system to adapt a user\u2019s music to their exercise by aligning high-energy music segments with intense intervals of the workout. Listening to music during exercise can boost motivation and performance. However, the structure of the music may be different from the user\u2019s natural phases of rest and work, causing users to rest longer than needed while waiting for a motivational section, or lose motivation mid-work if the section ends too soon. To address this, our system, called RISE, automatically estimates the intense segments in music and uses cutpoint-based music rearrangement techniques to dynamically extend and shorten different segments of the user\u2019s song to fit the ongoing exercise routine. Our system takes as input the rest and work durations to guide adaptation. Currently, this is determined either via a pre-defined plan or manual input during the workout. We evaluated RISE with 12 participants who compared our system to a non-adaptive music baseline while exercising in our lab. Participants found our rearrangements seamless, intensity estimation accurate, and many recalled moments when intensity alignment helped them push through their workout.",
      "abstract": "We propose a system to adapt a user\u2019s music to their exercise by aligning high-energy music segments with intense intervals of the workout. Listening to music during exercise can boost motivation and performance. However, the structure of the music may be different from the user\u2019s natural phases of rest and work, causing users to rest longer than needed while waiting for a motivational section, or lose motivation mid-work if the section ends too soon. To address this, our system, called RISE, automatically estimates the intense segments in music and uses cutpoint-based music rearrangement techniques to dynamically extend and shorten different segments of the user\u2019s song to fit the ongoing exercise routine. Our system takes as input the rest and work durations to guide adaptation. Currently, this is determined either via a pre-defined plan or manual input during the workout. We evaluated RISE with 12 participants who compared our system to a non-adaptive music baseline while exercising in our lab. Participants found our rearrangements seamless, intensity estimation accurate, and many recalled moments when intensity alignment helped them push through their workout.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Alexander Wang",
        "Chris Donahue",
        "Dhruv Jain"
      ],
      "authors_and_affil": [
        "Alexander Wang (University of Michigan)*",
        "Chris Donahue (Carnegie Mellon University)",
        "Dhruv Jain (University of Michigan)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Human-computer interaction",
        "Human-centered MIR",
        "Music interfaces and services",
        "Applications"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1NcFjFvB-m4aS3a8tdMpuFN_OXfMx-Tjv/preview",
      "poster_pdf": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-2-rise-adaptive-music",
      "title": "RISE: Adaptive Music Playback for Realtime Intensity Synchronization with Exercise",
      "video": ""
    },
    "forum": "75",
    "id": "75",
    "pic_id": "",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly disagree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nStrongly agree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nStrongly agree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThis paper clearly presents a novel approach for adaptively controlling playback of existing music audio to better match the burst and rest phases of workouts, and discusses its effectiveness and limitations based on a user study with 12 participants.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThis paper proposes a system that loops or skips parts of music to align its high-energy segments with workout bursts and its low-energy segments with rest periods.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nStrongly agree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper describes a novel system that loops and skips existing music audio to better suit exercise activities. The paper is very well-written and makes clear contributions. Given the following strengths, I recommend \"Strong accept.\"\n\n- The processing steps in the system are well-justified and highly regarded. The system first obtains structural segments and beats using an existing method and classifies each segment as either a high-intensity or low-intensity segment by computing beat-level LUFS loudness. Although the binary labeling of segments is a simple approach, it effectively serves the intended purpose. It then estimates intra-segment cuepoints using a beat-level self-similarity matrix to achieve natural looping and skipping. Limiting jumps to within segments enhances the naturalness of transitions and is a reasonable design choice. Finally, music playback is adaptively looped or skipped in two ways: either by following a predefined workout plan of burst and rest phases or by adjusting playback in real time through manual button controls when transitioning between phases. Although relying on manual controls is a limitation and ideally the system should sense the user's physical state and make automatic decisions, the authors explicitly acknowledge this limitation in Section 6.\n\n- The user study with 12 participants clearly demonstrates the system's effectiveness through analysis of interview transcripts. Participants who were initially skeptical about the naturalness of looping and skipping audio before the experiment came to appreciate its effectiveness after experiencing it firsthand. In addition, participants reported that the adaptive playback \"helped them push through and get an extra one or two reps,\" which shows the practical benefits of the proposed system.\n\n[Suggested improvements]\n\n- The word \"Rearrangement\" in the title is misleading, as it typically refers to changes in instrumentation or harmony. Since this system only performs adaptive playback control without modifying the music audio itself, I strongly recommend rephrasing the title.\n\n- There is a typo in reference [19]: \"2006, p. 7th.\" I suggest carefully reviewing the references again.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThis paper has several strengths, such as \"This work presents a timely and compelling contribution\" (R4), \"The methodological rigor is commendable\" (R4), \"The concept is promising\" (R5), \"This is an interesting topic and application for the MIR research.\" (R7), and \"a novel system that loops and skips existing music audio to better suit exercise activities\" (meta-reviewer).\n\nHowever, as pointed out by the reviewers, there are several weaknesses that should be addressed through revision. In particular, the need for manual input to identify 'high' and 'low' intensity segments should be clearly emphasized much earlier in the paper. Furthermore, as noted in the review comments, the title is misleading and should be revised to better reflect the content of the paper.\n\nSince all reviewers gave positive ratings (two \"Strong accept\" and two \"Weak accept\"), an \"Accept\" recommendation was considered. However, based on the comments regarding the weaknesses, it was suggested during the discussion phase that the final recommendation should be \"Weak accept.\" Therefore, our final recommendation is \"Weak accept\", though it is close to an \"Accept\" assuming that the identified issues will be thoroughly addressed. If this paper is accepted, the authors are strongly expected to address the issues raised in the reviews.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis work presents a timely and compelling contribution to music information retrieval (MIR) by addressing a critical gap in adaptive music systems for exercise. The authors\u2019 core innovation\u2014real-time music rearrangement to align musical intensity with workout phases\u2014represents a significant advancement over existing approaches that operate at the song or tempo level. By integrating computational structure analysis (Section 3.1) with cutpoint-based rearrangement (Section 3.2), the authors demonstrate how adaptive music can be dynamically tailored to users\u2019 exertion states while preserving perceptual seamlessness. This dual focus on structural granularity and real-time adaptability is novel and addresses a longstanding challenge in context-aware music systems.\n\nThe methodological rigor is commendable. The preprocessing pipeline (beat detection, drum isolation, and intensity estimation) is well-justified, leveraging established MIR tools while introducing domain-specific adaptations (e.g., drum track LUFS thresholds). The two usage modes (guided/unguided) thoughtfully accommodate diverse exercise routines, expanding the system\u2019s practical utility. The quantitative evaluation (Section 4) and mixed-methods user study (Section 5) provide robust evidence of both technical efficacy (transparency of transitions) and experiential benefits (enhanced motivation). Notably, participants\u2019 reports of \u201cpushing through\u201d workouts due to aligned intensity peaks (Section 5.3) highlight the real-world impact of this work.\n\nThis paper sets a foundation for future research in adaptive music beyond exercise (e.g., gaming, productivity). While limitations like manual state input are acknowledged, the proposed solutions (sensor integration, generative audio inpainting) point to promising directions. Overall, the authors deliver a cohesive, user-centered contribution that bridges MIR techniques with behavioral science\u2014a hallmark of impactful ISMIR research. I recommend acceptance.",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper introduces a music-looping state machine aimed at enhancing workout sessions. The concept is promising, especially if the authors plan to commercialize it. However, the title and abstract is misleading: What is presented is a decent method for mixing piece sections for guided interval training: neither *rearrangement*, nor *Realtime Intensity Synchronization with Exercise*... \nOutside this overselling issue, from a scientific perspective, the paper lacks novelty and does not offer a meaningful methodological contribution. It is best categorized as an application-focused paper. The limitations in its applicability need to be stated more clearly. If accepted, the authors should explicitly mention the absence of an automated intensity detection mechanism in the abstract.\nMajor Concerns:\nManual Input of Intensity Levels:\nA major limitation is the need for manual input of 'high' and 'low' intensity segments. This significantly reduces the system\u2019s practicality. While the paper implies that detecting intensity changes is straightforward, the lack of an automated detection mechanism is a serious omission. This should be acknowledged earlier in the paper, not buried in the limitations section. Real-time, reliable detection of workout intensity is a non-trivial challenge, and the current framing is misleading.\nLack of Scientific Contribution:\nThe methods used for estimating segment intensities and determining cutpoints rely on existing techniques without any meaningful adaptation or innovation. There is no novel insight or technical advancement presented.\nGuided vs. Unguided Use:\nThe guided mode shows some promise, but the unguided mode fails to deliver reliable results. This limits the system\u2019s usefulness in real-world, unsupervised scenarios.\nI appreciate the creativity behind the work. It\u2019s borderline for acceptance. If accepted, the paper should be clearly framed as a proof-of-concept application without misleading the conference participants. The lack of automated intensity detection should be stated upfront, and the structure should be revised to avoid the overselling issue. Clarifying these limitations would improve the paper\u2019s impact and help set appropriate expectations",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper proposes a system that rearranges a given music based on the user\u2019s state during a workout. The system leverages several existing MIR techniques (e.g., music structure analysis, source separation, and beat tracking) to process a given music and synchronizes the processed music with the workout state based on the exertion state. In addition, a demonstration video regarding the use case of the proposed system is provided, which is very fascinating and helpful.\n\nThis is an interesting topic and application for the MIR research. The authors demonstrate how the existing MIR techniques can improve human activities from a different angle. One major concern is that the manuscript lacks a discussion on the task itself. How did the authors formulate the task? (Note that \u201cintensity\u201d as a key idea for the synchronization is not explicitly defined. Also, the use of loudness as a proxy of intensity is not clarified and justified.) Which parts of the task are most challenging? How did the authors deal with those parts? How did the authors make the design choices? Such kinds of discussion would provide more insights into the task and facilitate related research. The authors may consider reducing the size of the figures to make space for including more discussions.\n\nSimilarly, a thorough discussion on the key techniques used in the system (e.g., the all-in-one model and Spleeter) would be required. The authors may provide a brief introduction to each of the techniques in Section 2 for readers who might not be familiar with these methods. In addition, it would be helpful if the authors could provide an overview of the system before addressing the details.\n\nOverall, this is quite an interesting work, and I think there is enough time for the authors to revise the manuscript to address the aforementioned issues. Therefore, I evaluated it as a \"Weak Accept.\"\n\nOther comments:\n- Lines 2-4 and lines 10-12 might seem redundant.\n- Line 162: The definition of \u201ca cutpoint\u201d is somewhat weird. \u201cA cutpoint tuple/pair/set\u201d might be some alternatives.\n- Line 175: incorrect indent before \u201cwhere\u201d"
    },
    "session": "1"
  },
  {
    "content": {
      "TLDR": "As artificial intelligence (AI) continues to shape creative practices, understanding its role in human-AI songwriting remains crucial. This paper expands the Human-AI Songwriting Processes (HAISP) dataset by incorporating data from the 2024 AI Song Contest, building upon the original 2023 dataset. By analyzing new submissions, we provide further insights into AI's evolving impact on songwriting workflows, creative decision-making, and control. A comparative study of AI tool usage and participant strategies between the 2023 and 2024 contests reveals shifts in collaboration patterns and tool effectiveness. Additionally, we assess the differences between general-purpose AI systems and personalized, fine-tuned tools, highlighting their impact on creative agency. Our findings offer key design implications for AI-assisted songwriting tools, providing actionable insights for AI developers and music practitioners seeking to enhance co-creative experiences.",
      "abstract": "As artificial intelligence (AI) continues to shape creative practices, understanding its role in human-AI songwriting remains crucial. This paper expands the Human-AI Songwriting Processes (HAISP) dataset by incorporating data from the 2024 AI Song Contest, building upon the original 2023 dataset. By analyzing new submissions, we provide further insights into AI's evolving impact on songwriting workflows, creative decision-making, and control. A comparative study of AI tool usage and participant strategies between the 2023 and 2024 contests reveals shifts in collaboration patterns and tool effectiveness. Additionally, we assess the differences between general-purpose AI systems and personalized, fine-tuned tools, highlighting their impact on creative agency. Our findings offer key design implications for AI-assisted songwriting tools, providing actionable insights for AI developers and music practitioners seeking to enhance co-creative experiences.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Lidia Morris",
        "Michele Newman",
        "Xinya Tang",
        "Renee Singh",
        "Marcel V\u00e9lez V\u00e1squez",
        "Rebecca Leger",
        "Jin Ha Lee"
      ],
      "authors_and_affil": [
        "Lidia Morris (University of Washington)*",
        "Michele Newman (University of Washington)",
        "Xinya Tang (University of Washington)",
        "Renee Singh (University of Washington)",
        "Marcel V\u00e9lez V\u00e1squez (University of Amsterdam)",
        "Rebecca Leger (Fraunhofer Institute for Integrated Circuits)",
        "Jin Ha Lee (University of Washington)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Human-centered MIR",
        "Creativity",
        "Human-ai co-creativity",
        "Novel datasets and use cases",
        "Tools for artists",
        "Computational creativity",
        "Evaluation, datasets, and reproducibility",
        "Creative practice involving MIR or generative technology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/12-pcaheXMfsPhR_I4DrhfeZzJdXd22zc/preview",
      "poster_pdf": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-3-expanding-the-haisp",
      "title": "Expanding the HAISP Dataset: AI's Impact on Songwriting Across Two Contests",
      "video": ""
    },
    "forum": "77",
    "id": "77",
    "pic_id": "",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nDisagree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nDisagree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nDisagree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThere are useful insights in the paper, but they would be more reusable if the connection between the results and design principles was more clear.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nProcess reports from selected 2024 AI Song Contest participants elucidates AI usage and informs how system design could further support these users.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak reject\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis is the meta-reviewer's independent review for paper 77: Expanding the HAISP Dataset: AI\u2019s Impact on Songwriting Across Two AI Song Contests\n\nThis paper provides an update and extension of the Human-AI Songwriting Processes (HAISP) dataset with new data from 2024. The authors describe how the new data were collected and coded, discuss main themes emerging across the two years, and offer design principles. \n\nThis paper was well-written and easy to read throughout. The authors do a great job of providing high-level context in the Introduction which is elaborated upon in Background; and the relevance to ISMIR is clear. Overall I find this to be a solid paper on a timely topic, but am unsure of some aspects of the paper. Below I provide four main points of feedback, followed by minor feedback.\n\n** Main feedback 1: Concerns around incremental contributions **\nIt's exciting to see the HAISP dataset effort continue, and the reported comparisons between 2023 and 2024 make sense given the framing of the paper. However, I am not sure whether I agree with the authors' claims of interpretable changes over time given only a 1-year sampling interval. E.g., \"an evolving discourse\" (p.4 line 390) and demographic trends in submissions (such as the noted increase in non-academic teams) would be more strongly reported across a longer longitudinal timeframe. Incremental research is fine of course, but as the authors mention \"more concrete frameworks for future entries\" (line 507), I am left wondering whether the frameworks might be better concretized across less frequent, more impactful papers that span larger time frames.\n\n** Main feedback 2: Drawing stronger connections between design principles and results **\nThe design principles provided in Section 5 are all reasonable and well articulated. However, they would be reasonable even if proposed separately from this study's results, and as the paper is written it is hard to see how they arose from the present results. The authors are advised to more clearly show how these principles arose from the specific findings -- in other words, to show how the results uniquely inform these principles. For example, this could be done by referencing the derived themes or even including additional participant responses from relevant themes for each. In addition to tying the principles more clearly to the results, it could also be helpful to begin Section 5 by stating this causal relationship outright. \n\n** Main feedback 3: Seeking more information on coding process **\nI was also curious to know more about the codes and coding process. Were these codes used in the original dataset paper (it seems not), or were new codes derived this year and then applied to both years of data in a new analysis? Can some examples of codes themselves be given? Can the authors offer more detail on how the coding turned out -- is there a reliability metric or statistic that can be reported to summarize the extent of agreement before/after discussion as well as number of tiebreaks needed? Finally, how were the codes translated to the themes reported in Section 4?\n\n** Main feedback 4: Provide more context on commonality of results **\nIn Section 4, there were many qualitative mentions of how common a theme was (e.g., \"Many users\" line 255, \"teams frequently encountered\" line 278, \"much more\" line 298) but as a reader I found it hard to translate these extents to tangible quantities. Can the authors provide more grounding to readers by providing more quantitative backing to the main results? \n\nMinor\n- Line 48: Based on ref #10, is it more accurate to say HAISP was introduced in 2024?\n- Section 3: Suggest removing \"Methodology\" from the section title since Section 3.2 is more results than methodology\n- In the Background section, it could be useful to provide more context on the AI Song Contest, for readers who are learning about it for the first time. \n- In Section 3.1 I was curious at this point why Udio/Suno submissions were disqualified. The authors go into this in detail in Section 4.3 but it could be helpful to provide more context upfront (and perhaps reference back to it).\n- Related to above two points: The content in lines 360-365 could also go in a Background subsection on AI Song Contest and also help the reader understand the omission of Udio/Suno entries.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak reject\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThis is the meta-review for paper 77: Expanding the HAISP Dataset: AI\u2019s Impact on Songwriting Across Two AI Song Contests. The meta-reviewer and 3 other reviewers independently reviewed the paper. While we did not achieve consensus in our individual reviews, we reached an understanding during the discussion phase regarding the direction of the overall recommendation. \n\nAll reviewers praise the paper for its clear writing, relevance to MIR, useful information, and timeliness with regard to the topic. Across the reviews, several suggestions for improving the paper also arose. First, there was uncertainty around the paper's impact -- for example, whether the contribution provides enough depth or longitudinal value. On a related point, while the design principles made sense, their novelty and connection to results were unclear. Finally, more details on the coding process, the dataset, and the longitudinal aspect of the results could have been helpful. A longitudinal contribution spanning more years could provide heightened impact; or the authors could do a deeper dive into reporting on the present methods and data -- and connect results more strongly to the design principles -- which would serve as a faster (and hence timely) contribution to a fast-moving research topic.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\n- This is a well written paper and I think the dataset and the findings will be useful for the community. I think the trends among the participants and the year-to-year differences are really interesting to see.\n\n- The design principles emerging from analyzing this dataset don't seem to break new ground. I'm not sure whether there's more to uncover here, but one interpretation of this finding could be an updated understanding about what the AI Song Contest itself is providing to the community - this year, it offered valuable insights about the changing attitudes and backgrounds of participants, but revealed less about the usability of music AI tools themselves. Maybe this is something to consider for the contest organizers as they choose their directions for the future.",
      "review2": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nI think this paper brings up interesting (and quantitatively derived) trends regarding use of AI in songwriting, including how it is shifting out of academia to industry (now that more tools are mature), that this proliferation is leading to more pressure regarding ease of use, ethical transparency, and flexibility, and backs all this up with quotes from real-world composers and song-writers, which I appreciated.\n\nI also find the methodology of this paper to be successful, specifically:\n\n1. Clearly presenting the dataset to be analyzed and quantitative statistics regarding it\n2. Analyzing general trends (and reinforcing them with qualitative quotes from the dataset, where useful, which I very much appreciated)\n3. Taking these trends and making them into actionable guidelines for AI tools producers: including interactivity and adaptivity, enhancing transparency and explainability, including the ability to include style and genre customization, and integrating tools into existing creative workflows.\n\nThe paper is easy to read and the authors clearly convey their main takeaways, summarizing them into a set of conclusive \"Design Principles for Music Generation AI\" (see above) which could certainly be acted upon/used as design principles for GenAI software development teams and companies.",
      "review3": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nStrengths: \nThe HAISP dataset is extended with submissions from 2024 AI song contest, providing opportunity for the community to study the changes in the creative practices and participant behaviours about AI in various aspects of the song production process. \nWeaknesses: The paper could have presented in depth details about the dataset such as participant distribution in terms of skills, professional practices. They could have also listed the tools used by participants and a clear distinction of tools for different tasks. The most important results from the paper highlight increased participation from creative industry in 2024 and participants preferring assistive systems that are ethically trained. This is interesting but not interesting enough. The paper seems a bit underdeveloped. I would encourage the authors to describe the dataset in more details and present detailed and comprehensive longitudinal comparison across the years. The authors say that a journal paper is underway and I look forward to it. \nLastly, authors propose guidelines for developing AI tools which again is interesting but nothing novel. Similar content has been discussed in several papers and presentations before, some of which authors fail to mention."
    },
    "session": "1"
  },
  {
    "content": {
      "TLDR": "We propose a simple, yet effective approach to semantic song segmentation. Our model is a convolutional neural network trained to jointly predict frame-wise boundary activation functions and segment label probabilities. The input features consist of a log-magnitude log-frequency spectrogram and self-similarity lag matrices, combining modern deep learning approaches with hand-crafted features.\n\nTo evaluate our approach, we first examine commonly used datasets and find substantial overlap (up to 22%) between training and testing sets (SALAMI vs. RWC-Pop). As this overlap invalidates meaningful comparisons, we propose using the previously unexplored McGill Billboard dataset for testing. We carefully eliminate duplicate entries between McGill Billboard and other datasets through both audio fingerprinting and string-matching of song titles and artist names. Using the resulting set of 719 tracks, we demonstrate the effectiveness of our approach.\n",
      "abstract": "We propose a simple, yet effective approach to semantic song segmentation. Our model is a convolutional neural network trained to jointly predict frame-wise boundary activation functions and segment label probabilities. The input features consist of a log-magnitude log-frequency spectrogram and self-similarity lag matrices, combining modern deep learning approaches with hand-crafted features.\n\nTo evaluate our approach, we first examine commonly used datasets and find substantial overlap (up to 22%) between training and testing sets (SALAMI vs. RWC-Pop). As this overlap invalidates meaningful comparisons, we propose using the previously unexplored McGill Billboard dataset for testing. We carefully eliminate duplicate entries between McGill Billboard and other datasets through both audio fingerprinting and string-matching of song titles and artist names. Using the resulting set of 719 tracks, we demonstrate the effectiveness of our approach.\n<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Filip Korzeniowski",
        "Richard Vogl"
      ],
      "authors_and_affil": [
        "Filip Korzeniowski (Music.AI)*",
        "Richard Vogl (Music.AI)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Structure, segmentation, and form",
        "Evaluation, datasets, and reproducibility",
        "Novel datasets and use cases",
        "Musical features and properties"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1J_50_v05CoATo0d7-99Wm4MnriygBAFC/preview",
      "poster_pdf": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-13-simple-and-effective",
      "title": "Simple and Effective Semantic Song Segmentation",
      "video": ""
    },
    "forum": "79",
    "id": "79",
    "pic_id": "",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nDisagree\n\n**Q5 ( Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chosen, otherwise write \"n/a\"))**\n\nMuch previous work is cited. However, the connection between this work and previous work is not clear. Sections 1 and 2 do not hint at the need for future work on MSA. A brief contrast with this work is given in lines 143\u2013153, which states a desire to diverge from \"current trends in deep learning\". If this is the main motivation for the design of the proposed model, I think an explanation of these trends \u2014 why they are popular, what are some examples, and how the trajectory of these trends has soared or fizzled in other MIR tasks \u2014 is deserved.\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q10 (Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chose, otherwise write \"n/a\"))**\n\nThe explanations are all very clear. The critiques of earlier evaluations and recommendations for future ones are clear, although I am not sure that the evaluation conducted here resolves all the issues. While the comparison using the McGill Billboard dataset seems fair (Table 4), the comparison in Table 3 seems unfair, given that the proposed algorithm was tested using cross-validation, whereas the competing algorithms are all tested in a cross-dataset scheme. It is true that the \"train-test overlap ... could lead to inflated results\" (line 419\u201320); but it also seems true that training within a dataset could inflate results compared to a cross-dataset scenario.\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nDisagree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nStrongly Disagree (Well-explored topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe paper sets a good example for evaluation in MSA in several respects: it emphasises the importance of using 'trimmed' annotations (i.e., not including trivial 'begin' and 'end' tokens in the evaluation) and it points out the overlapping datasets compromise the cross-dataset evaluation. These are valuable recommendations that may be known by others in the field (mir_eval has a 'trimmed' setting, and the overlap between SALAMI, RWC and Isophonics is intentional) but are still worth committing to the proceedings.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nMSA evaluation should be conducted correctly and reported carefully, and cross-dataset performance of MSA is poorer than within-dataset performance.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper proposes a new algorithm for music structure analysis (MSA) with a simple architecture, combining different implementation tricks from previous work. The paper describes some previous MSA evaluations as sloppy and propose a more rigorous approach, and they also identify a new dataset for evaluation in MSA: the McGill Billboard dataset (MBD).\n\nThe paper is clear and well written. The high level of detail in the algorithm description will be appreciated by anyone wanting to replicate this work. The tips for making sure evaluations are correct and rigorous are important and valid critiques of previous work, and the MBD is a good dataset to include in future work. I think this paper merits acceptance based on this.\n\nThat said, I think each of the 3 main contributions (algorithm, evaluation, choice of dataset) from the authors could be made clearer in an updated version.\n\nRegarding the first contribution: I trust that the new pipeline was built correctly, and the evaluation with the MBD suggests it is a new state of the art. But in the explanation of proposed design (Section 3), I did not find reusable insights. This is because the connection to the existing literature is unclear, so for each chosen element in the pipeline, I am not sure what the other options were, and why any particular option was chosen. An ablation study would have been interesting, since there are a few points where it is clear that preliminary studies were used to fine-tune the process (e.g., line 202: \"we found that this does not further improve performance\", and line 243 footnote: \"we found in preliminary experiments that individual probabilities for each segment label work better in practice\").\n\nOne motivation for the proposed design is clear from the title: simplicity. The introduction says the proposal is \"a simple, yet effective approach\". If the simplicity of the algorithm is part of its value, why is it not clearly contrasted with the complexity of previous work? Also, by what standard is it \"simple\"? The level of detail in Section 3 suggests it is a sophisticated algorithm relying on many clever submodules and implementation tricks. I would have enjoyed a discussion about how other systems and training methods are needlessly complex, and in what way the proposed method is simple, and what makes simplicity desirable for a given task. For example, simplicity may come from there being a clear musical intuition that underlies a method. This paper does not discuss the musicality of the problem.\n\nRegarding the 2nd contribution, the paper sets a good example for evaluation in MSA in several respects: it emphasises the importance of using 'trimmed' annotations (i.e., not including trivial 'begin' and 'end' tokens in the evaluation) and it points out the overlapping datasets compromise the cross-dataset evaluation. These are valuable insights, worth repeating, but they are not strictly new: mir_eval has a 'trimmed' setting because it is known among MSA researchers that this makes a big impact. Different authors make different choices about the parameter, but I hope that within any paper, the authors make consistent choices so that they are comparing apples to apples. And regarding the overlap between datasets: this overlap is known \u2014 it is by design! The original SALAMI paper mentions that data from RWC and Isophonics was deliberately included for comparison. Still, it is good to point out that this affects the value of the evaluations, and this overlap is evidently not common knowledge.\n\nHowever, while the comparison using the McGill Billboard dataset seems fair (Table 4), the comparison in Table 3 seems unfair, given that the proposed algorithm was tested using cross-validation, whereas the competing algorithms are all tested in a cross-dataset scheme (albeit with small amounts of overlap between test and train sets). The paper argues earlier (Section 4.1), persuasively, that cross-dataset evaluation may lead to underperformance, since the datasets differ greatly. It sounds like training on all the datasets might lead to more robust methods. Given this, why devote Table 3 to comparing the proposed method (CV-8) with so many methods trained in a cross-dataset (CD) way? Given the CD results from other works, the apples-to-apples comparison would be to also train the proposed system in the (unrecommended) CD way and report these results. This would seem more fair, and also would not detract from SOTA findings shown in Table 3 (top section: Harmonix) and Table 4.\n\nPut another way: in an evaluation, I am most interested in the comparison of methods, not the absolute performance achieved. So, comparing apples to apples (on a dataset with 3% leakage) is better than comparing apples to oranges (on datasets with no leakage).\n\nFinally, regarding that 3rd contribution: given that this is the first use of MBD for evaluating structure, perhaps the paper should say a bit more about the creation of this dataset: how were the structure labels annotated, and how does it differ from SALAMI and Harmonix? This information is available from the respective papers about each, but since the contribution is highlighted in the introduction, I expected more commentary on it in the results section.\n\nPost-discussion comment: the paper does not discuss the provenance of the audio data. How was the audio accessed? If the MBD was not provided by the original dataset owners, how did was the alignment between audio and annotations verified?\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThe initial ratings that the reviewers gave for this paper ranged from strong reject to strong accept, with an average rating slightly favouring acceptance.\n\nSome aspects that the reviewers agreed on:\n1. The model architecture is interesting and the explanation of the method was clear (R1, R3, MR)\n2. The discussion of evaluation integrity in music structure analysis (MSA) is insightful (R1, R2, R3, MR)\n3. The evaluation makes unfair comparisons between algorithms, and does not perform any ablations on the proposed method (R2, MR, and R1 post-discussion).\n4. The title states that the method is \"simple\" (and the text says \"lightweight\"), but it is not clear by what standard this is claimed.\n\nOutside of this, the reviewers had a variety of suggestions for how to improve the paper. In particular, R2 wrote that the tone of the paper seemed unfairly dismissive of previous work. The way that point 3 undercuts point 2 (in my list above) may contribute to this.\n\nOverall, we lean towards accepting this paper for the clear contributions outlined above. However, we strongly recommend that the authors revise their work to give clearer justifications for the choices made in the algorithm design, training procedure and evaluation, and to ensure that no claims or critiques in the paper are overstated.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\n- General comment: \nThe paper introduces a convolutional neural network (CNN)-based approach for \"semantic song segmentation,\" which utilizes hand-crafted features as input (namely a log mel-like representation and self-similarity lag matrices). The authors address key challenges in music structure analysis (MSA), such as dataset overlap and inconsistent evaluation metrics, and propose a robust framework for segmenting songs into \"semantic\" sections, i.e., functionally labeled sections (intro, chorus, verse, etc.). The paper is well-written, and the problem is clearly motivated, making it a valuable contribution to the field.\n\n- Strengths:\n1. The paper is well-written, with clear explanations of the methodology, datasets, and evaluation metrics.\n2. The authors address key issues in MSA evaluation, providing a more reliable benchmark for future research.\n3. The authors propose a novel model with SOTA performance.\n4. The model in itself is quite simple, with relatively few different elements.\n\n- Weaknesses:\n1. The training routine is very complicated and seems very ad hoc.\n2. The model is said to be \"lightweight\", but does not seem lightweight to me: 9 convolutional layers as front-ends (3 layers per front-end), followed by 11 blocks composed of both convolutional and dense layers, and 2 final dense layers. It may be very few compared to the recent standard in the literature, but I feel that this is far more than the compared models, e.g., (using the references from the paper, and not limited to): [9, 13, 14, 15, 16]. *NB: I voluntarily restricted this list to conv models, because comparing convolution and attention layers does not seem fair in terms of parameters vs. data required for training.\n\n- Comments:\n-- The authors propose to process music audio signals using log-frequency log-magnitude spectrograms. How do the log-frequency triangular filters relate to mel filters?\n-- I do not understand why the authors state that \"60 channels [are] reduced to 30 using 1x1 convolution\" (lines 217-218, section 3.2): shouldn't 1x1 convolution result in the same shape as outputs?\n-- Lines 284-288: Maybe I misunderstood, but, to the best of my understanding, when several annotations are available, the authors use this data several times in training, using each annotation once. It feels to me that this compromises the principle of learning, where the same data should be associated with only one ground truth (how should the error backpropagation make sense otherwise?).\n-- Lines 318-319: The authors state that their model should work with a wide variety of music but evaluate it on Western music (mostly popular). In my opinion, a \"wide variety of music\" should include far more than the proposed datasets. I suggest the authors lower that sentence or rephrase it.\n-- The authors use the 0.5s tolerance only, while standards use both 0.5 and 3s tolerances. Why is this choice, and why not use both tolerances?\n\nOverall, I found the paper sound, and would recommend it for publication.",
      "review2": "- **Overall Evaluation**: Strong reject\n\n#### Main Reviews\n\nOverall, the major scope of this work remains unclear. On one hand, the proposed method lacks sufficient ablation studies to identify which components contribute to its performance gains. On the other hand, if the paper aims to guide the field toward more rigorous evaluation practices, it would benefit from broader comparisons with existing approaches under the proposed controlled environment to generate deeper insights.\n\nI do see value in the paper\u2019s emphasis on evaluation methodology, which is timely and useful for the community. I encourage the authors to consider reshaping the paper\u2019s focus toward evaluation and reproducibility, and to resubmit it to a more appropriate track at ISMIR or relevant conferences. For broader impact, it would be highly beneficial to open-source the experimental protocols, allowing future work to build upon and benchmark against a common, well-defined setup.\n\nAnother concern relates to the writing style of the paper. While it is both valid and important to highlight limitations or issues in previous evaluations, presenting these observations in a more constructive and collaborative tone would help foster a more positive and productive dialogue within the research community. Acknowledging the inherent challenges of working with dataset limitations in prior work could further strengthen the paper\u2019s message, framing its critique as part of a collective effort to advance reproducibility and methodological rigor in the field.\n\nOther Concerns:\n- Table 1, Convolutional Front-End: It is unclear how the dimensionality is reduced from 81 to 1 using three layers of (3, 1) max-pooling. Based on the pooling configuration, this setup appears to reduce the dimension by a factor of 27 (i.e., 3 \u00d7 3 \u00d7 3), not 81. \n- Post-Processing: The contribution of the optimized post-processing method to overall performance remains unclear. Post-processing is often dataset-dependent and, in some cases, can affect evaluation scores by more than 10%. Quantifying its impact would help readers better understand the relative contributions of the model and the post-processing step.\n- Table 2: It is not clear whether the addition of the \u201csolo\u201d label affects performance. Labels such as \"impro\", \"interlude\" and \"guitars\" may implicitly include solo sections.\n- Section 5.1, Line 409: The statement that \u201cthe effect of using additional training data is not straightforward to assess\u201d is somewhat unclear. If the benefit of extra training data is uncertain, it raises the question of why mixing datasets was used in this paper.",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe authors present a simple architecture for music structure analysis. The paper is well written and its methodology is explained with detail. Despite the approach does not present any novel idea in terms of feature implementation, model architecture or post-processing stage, it shows an elegant and efficient solution for a well-known problem. The previous work is appropriately described and referenced. In my view, the primary strength of this paper lies in its critical examination of the evaluation methodologies used in previous studies. \n\nA minor criticism of this work is the absence of the pairwise frame clustering (PFC) metric in the reported results. Given that including this metric would require minimal additional effort, I recommend its inclusion provide a more transparent evaluation."
    },
    "session": "6"
  },
  {
    "content": {
      "TLDR": "The multimodal nature of music performance has driven increasing interest in data beyond the audio domain within the music information retrieval (MIR) community. This paper introduces PianoVAM, a comprehensive piano performance dataset that includes videos, audio, MIDI, hand landmarks, fingering labels, and rich metadata. The dataset was recorded using a Disklavier piano, capturing audio and MIDI from amateur pianists during their daily practice sessions, alongside synchronized top-view videos in realistic and varied performance conditions. Hand landmarks and fingering labels were extracted using a pretrained hand pose estimation model and a semi-automated fingering annotation algorithm. We discuss the challenges encountered during data collection and the alignment process across different modalities. Additionally, we describe our fingering annotation method based on hand landmarks extracted from videos. Finally, we present benchmarking results for both audio-only and audio-visual piano transcription using the PianoVAM dataset and discuss additional potential applications.",
      "abstract": "The multimodal nature of music performance has driven increasing interest in data beyond the audio domain within the music information retrieval (MIR) community. This paper introduces PianoVAM, a comprehensive piano performance dataset that includes videos, audio, MIDI, hand landmarks, fingering labels, and rich metadata. The dataset was recorded using a Disklavier piano, capturing audio and MIDI from amateur pianists during their daily practice sessions, alongside synchronized top-view videos in realistic and varied performance conditions. Hand landmarks and fingering labels were extracted using a pretrained hand pose estimation model and a semi-automated fingering annotation algorithm. We discuss the challenges encountered during data collection and the alignment process across different modalities. Additionally, we describe our fingering annotation method based on hand landmarks extracted from videos. Finally, we present benchmarking results for both audio-only and audio-visual piano transcription using the PianoVAM dataset and discuss additional potential applications.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Yonghyun Kim",
        "Junhyung Park",
        "Joonhyung Bae",
        "Kirak Kim",
        "Taegyun Kwon",
        "Alexander Lerch",
        "Juhan Nam"
      ],
      "authors_and_affil": [
        "Yonghyun Kim (Georgia Institute of Technology)*",
        "Junhyung Park (KAIST)",
        "Joonhyung Bae (KAIST)",
        "Kirak Kim (KAIST)",
        "Taegyun Kwon (KAIST)",
        "Alexander Lerch (Georgia Institute of Technology)",
        "Juhan Nam (KAIST)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "MIR fundamentals and methodology",
        "Music transcription and annotation",
        "Multimodality",
        "Novel datasets and use cases",
        "MIR tasks",
        "Annotation protocols",
        "Evaluation, datasets, and reproducibility",
        "Metadata, tags, linked data, and semantic web",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1_7rvc2F_CVcCWTk9nGBSLXBFzkOHoZRa/preview",
      "poster_pdf": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-4-pianovam-a-multimodal",
      "title": "PianoVAM: A Multimodal Piano Performance Dataset",
      "video": ""
    },
    "forum": "87",
    "id": "87",
    "pic_id": "",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nStrongly agree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nStrongly agree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe multimodal data, documentation, and benchmark analyses of the PianoVAM dataset will facilitate future research in piano transcription and related topics.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThe authors contribute a well-constructed and well-documented multimodal piano performance dataset along with benchmark analyses.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis is the meta-reviewer's independent review for paper 87: PianoVAM: A Multimodal Piano Performance Dataset\n\nThis paper introduces the PianoVAM dataset, which is a multimodal dataset of piano performances. The paper describes how data were acquired and preprocessed, attributes of the dataset, as well as the fingering annotation algorithm and benchmark results between PianoVAM and MAESTRO and between audio-only and audio-video transcription. \n\nI commend the authors for this comprehensive and well-written paper. The dataset serves as a useful contribution to the MIR field; the paper was thorough and information-dense, while also being easy to follow. The Introduction effectively the reader through the topics of multimodal MIR and piano transcriptions to arrive at the present work; and the tables and figures do an excellent job of summarizing a lot of information in understandable ways. I also thank the authors for the ethics statement in the paper, and for providing the anonymized repo and example video for review. I have two points of main feedback and other minor suggestions for the authors. \n\nMain feedback\n- The positioning of PianoVAM-Finger within the PianoVAM dataset in the paper was a little confusing. Based on visiting the repo, it seems the fingering information is indeed part of the larger dataset, but PianoVAM-Finger is first mentioned in Section 2.3 and Table 2, and its inclusion or separation wasn't totally clear to me. The authors could clarify this, for example, by mentioning PianoVAM-Finger by name earlier (e.g., in the Introduction) and state outright that it's part of PianoVAM.\n- Please state the license of the dataset in the paper (at the start of Section 3 for example). \n\nOther minor suggestions for the authors\n- Section 2.1: There are additional audio-visual datasets not covered here; the authors could point out that those covered are just examples and not all available datasets of this kind. \n- The audio loudness normalization procedure in Section 3.2.2 makes sense. But can the non-normalized versions also be released as part of the dataset? Some users might wish to work with the original recordings or implement their own normalization procedure. If only the normalized audio is released, it is not clear how a user could get back to the original versions.\n- Section 4 paragraph 1: Can the authors elaborate a bit on how the range of works/composers and performer skill levels compares to other published datasets? \n- Section 4 paragraph 2: The difference in sustain pedal usage between MAESTRO and PianoVAM is notable, especially when pitch and velocity distributions were well matched across the datasets. Can the authors speculate as to why PianoVAM has such higher use of the sustain petal? (E.g., performer skill, emphasis on informal practice, pieces performed, hardware?)\n- Section 5: Can more information on the usage of the GUI-based fingering annotation tool be provided? Was it used only for the annotations reported in Section 5.2.1? Can more information be provided on the number of annotators, their qualifications, and the extent and nature of the annotations?\n- Minor typos/wording issues: Line 228 \"This sections\"; line 252 sentence beginning \"Floating hand\", line 450 \"which makes player to prepare\".\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThis is the meta-review for paper 87: PianoVAM: A Multimodal Piano Performance Dataset. The paper received independent reviews from the meta-reviewer as well as 4 additional reviewers, all of whom were broadly positive in their assessments of the paper. \n\nThe reviews highlight many positive aspects of the paper including the clarity of writing, the paper being well-structured and densely informative, the value of the main contribution (the multimodal dataset), and clear relevance to MIR. The reviewers have also provided suggestions for improving the paper. Examples include missing details, a need for more explanation on the limited improvement in onset detection from adding visual information, potential shortcomings of focusing on practice sessions, opportunities for more reflection, and missing license. The authors are encouraged to read all of the reviews and incorporate the reviewer feedback.",
      "publish_reviews": "TRUE",
      "review1": "#REF!",
      "review2": "",
      "review3": ""
    },
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Hierarchical planning is a powerful approach to model long sequences structurally. Aside from considering hierarchies in the temporal structure of music, this paper explores an even more important aspect: concept hierarchy, which involves generating music ideas, transforming them, and ultimately organizing them\u2014across musical time and space\u2014into a complete composition. To this end, we introduce TOMI (Transforming and Organizing Music Ideas) as a novel approach in deep music generation and develop a TOMI-based model via instruction-tuned foundation LLM. Formally, we represent a multi-track composition process via a sparse, four-dimensional space characterized by clips (short audio or MIDI segments), sections (temporal positions), tracks (instrument layers), and transformations (elaboration methods). Our model is capable of generating multi-track electronic music with full-song structure, and we further integrate the TOMI-based model with the REAPER digital audio workstation, enabling interactive human-AI co-creation. Experimental results demonstrate that our approach produces higher-quality electronic music with stronger structural coherence compared to baselines.",
      "abstract": "Hierarchical planning is a powerful approach to model long sequences structurally. Aside from considering hierarchies in the temporal structure of music, this paper explores an even more important aspect: concept hierarchy, which involves generating music ideas, transforming them, and ultimately organizing them\u2014across musical time and space\u2014into a complete composition. To this end, we introduce TOMI (Transforming and Organizing Music Ideas) as a novel approach in deep music generation and develop a TOMI-based model via instruction-tuned foundation LLM. Formally, we represent a multi-track composition process via a sparse, four-dimensional space characterized by clips (short audio or MIDI segments), sections (temporal positions), tracks (instrument layers), and transformations (elaboration methods). Our model is capable of generating multi-track electronic music with full-song structure, and we further integrate the TOMI-based model with the REAPER digital audio workstation, enabling interactive human-AI co-creation. Experimental results demonstrate that our approach produces higher-quality electronic music with stronger structural coherence compared to baselines.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Qi He",
        "Gus Xia",
        "Ziyu Wang"
      ],
      "authors_and_affil": [
        "Qi He (Music X Lab)*",
        "Gus Xia (Machine Learning Department, MBZUAI)",
        "Ziyu Wang (Computer Science Department, NYU Shanghai)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Generative Tasks",
        "Evaluation metrics",
        "Musical features and properties",
        "Music generation",
        "Creativity",
        "Human-ai co-creativity",
        "Tools for artists",
        "MIR tasks",
        "Structure, segmentation, and form",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1v9mPnARt-by31siNy5SNo0X7giTjBTD7/preview",
      "poster_pdf": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-11-tomi-transforming-and",
      "title": "TOMI: Transforming and Organizing Music Ideas for Multi-Track Compositions with Full-Song Structure",
      "video": ""
    },
    "forum": "88",
    "id": "88",
    "pic_id": "",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nStrongly agree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nDisagree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThis paper could provide inspiration to exploit in-context learning with LLMs, not only in the context of music creation but also for other tasks.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThe proposed framework structures music generation around clips, sections, tracks, and transformations, and uses an LLM (with in-context learning) to generate multi-track electronic music.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper introduces TOMI, a novel approach to music generation that models the concept hierarchy in music composition. The system structures music generation around clips, sections, tracks, and transformations, using an LLM to manipulate this structure and generate complete multi-track electronic music. The system is integrated with the REAPER digital audio workstation. \nThis paper has several interesting aspects:\nThe TOMI framework, which offers a new way to conceptualize and represent music composition, explicitly modeling the hierarchical relationships between musical ideas, their transformations, and their organization in time and instrument layers. This is a significant contribution as it moves beyond simply modeling the temporal sequence of musical events and attempts to capture the underlying conceptual structure of a composition. \nThe use of in-context learning to guide the LLM in generating the parameters of the TOMI structure is a creative and promising approach. This allows the system to leverage the LLM's language understanding capabilities to generate musically meaningful structures. \nDAW integration offers practical implications and supports human-AI co-creation, even if it\u2019s not a major scientific contribution.\nThe paper presents a comprehensive evaluation, including both objective metrics for structural consistency and a subjective listening test to assess the perceived quality of the generated music. The inclusion of ablation studies is also interesting, by isolating the impact of different components of the system. \nThe paper is generally well-written and clearly presented, and structured. The figures and tables are informative and contribute to the reader's understanding of the proposed approach, as well as the demo website\n\nAnd now some of the weaknesses.\nUsing GPT-4o with context learning is a creative and interesting choice, but it would be interesting to know how much do the generations rely on the examples given at the prompt, and also how creative the generated compositions are\u2026 What degree of variation does the system allow, given similar \u201cprompts\u201d? \nThen, the TOMI framework facilitates iterative experimentation with song arrangement and instrumentation due to its explicit representation of sections and tracks. However, the system's reliance on pre-existing musical clips limits the user's ability to iterate on the development of core musical ideas (melodies, harmonies, rhythms) within the system. This could be a significant constraint for users who may want to combine manual editing + the use of AI in order to explore musical motifs and variations during the compositional process.\nThen, some design / naming choices could be more clear: e.g. the creation of a drum sequence (e.g. kick + snare) takes place within a Drum Transformation Node, while this is basically a composition/arrangement task \u2026 Then, a \u201cFx transform\u201d, actually only decides if there is a riser or faller and the end/beginning of a section, so it seems more about arrangement than transformations\u2026\nAccording to section 3.2. \u201cWe initiate the sample retrieval process to get the actual clip materials. Then, we set a global tempo and key to unify the keys and tempos of clips.\u201d While this is a valid approach, it could be beneficial to inform the sample retrieval process with the keys and tempo so as to avoid large pitch stretching factors. Also, the sample retrieval doesn\u2019t really seem to be informed on the arrangement / style. E.g. The bridge section in the example webpage video introduces a violin which is unexpected for the style of the composition\u2026 and it is in audio format, so the virtual instrument couldn\u2019t be replaced.\nThen it mentioned that: \u201cit can also extract music stems, such as bass, chord, and melody, from the source MIDI to augment the data.\u201d. How is this done? It would be good to have a reference?\nTo finalise, there are some weaknesses in the article, but there are many positive aspects, and one could foresee interesting extensions of this work, including the control of FX, and the use for other musical (and potentially non-music related) tasks, which use hierarchy / transformations.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThe paper \"TOMI: Transforming and Organizing Music Ideas for Multi-Track Compositions with Full-Song Structure\" introduces TOMI, a novel system for multi-track music generation. It proposes a hierarchical taxonomy to describe electronic music songs, utilized with a Large Language Model (LLM) for in-context learning to guide the transformation and organization of existing music clips. The system is integrated with a Digital Audio Workstation (DAW) to facilitate human-AI co-creation. The authors evaluate TOMI through objective metrics and a crowdsourced listening test, comparing it against rule-based methods and other generative approaches.\n\nStrengths\n\n- Novel hierarchical taxonomy: The paper introduces a well-structured hierarchical taxonomy for electronic music, explicitly modeling relationships between ideas, transformations, and organization across time and instruments.\n- Innovative LLM integration: Using an LLM for in-context learning to guide the TOMI structure is a creative approach\n- Practical utility & DAW integration: The system's DAW integration demonstrates practical implications for human-AI co-creation and production workflows.\n- Comprehensive evaluation: The paper presents a thorough evaluation, including objective metrics, subjective listening tests, and ablation studies.\n- Strong performance: TOMI-LLM is shown to generate more coherent music than baselines, suggesting the LLM learns musical structure, with non-LLM components handling lower-level details effectively (R4).\n- Readability and presentation: The paper is well-written, clearly presented, and logically structured, aided by informative figures and a demo website\n- Reusable insights: The TOMI data structure is extensible, offering potential applications beyond the paper's scope (R1, R4, Meta).\n\nSummary of weaknesses\n\n- Limited content generation: The system primarily organizes existing clips, which limits its ability to generate truly novel musical content (R2, Meta).\n- Reproducibility: Details needed to fully reproduce the LLM's in-context learning behavior are unclear (R1, Meta).\n- Incomplete related work (R4).\n- Musical coherence: There are concerns about harmonic consistency within tracks and also on how MIDI instrument presets are assigned (R2).\n- Design and logic clarity: Some framework design choices and aspects of the clip retrieval process need clearer justification (R1, Meta).\n- Missing technical details regarding the extraction of stems and some quantitative metrics (e.g. track count/duration) (R2).\n- Minor presentation flaws (phrasing issues and inconsistent reference formatting).\n\nHere are some specific recommendations for the camera-ready version. To enhance the paper's clarity, depth, and impact with feasible effort, consider these changes (and consider each of the individual reviews as well for more detail):\n\n- Clarify LLM prompts: Provide examples of complete TOMI prompt structures and standalone LLM prompts on the companion website (R1).\n- Address clip search: Clarify how clip search operates (R1, Meta).\n- Cite relevant work: Include and discuss the \"SymPAC: Scalable Symbolic Music Generation With Prompts And Constraints (ISMIR 2024)\" paper in the related work (R4).\n- Acknowledge the need for better solutions for MIDI instrument assignment than random selection (R2).\n- Clarify design choices: Explain the reasoning behind naming conventions (e.g., \"Drum Transformation Node,\" \"Fx transform (Meta).\n- Add missing technical details: Briefly explain stems extraction from MIDI. Provide metrics or discussions on the number of tracks and duration TOMI can generate (R2).\n- Refine language and references (see each of the reviews)\n\nTo conclude, the paper presents a significant contribution with the TOMI framework and its innovative integration of LLMs for structured music composition, including valuable DAW integration. Despite some minor concerns, the overall strengths and potential for future work are substantial. We recommend acceptance, believing the suggested revisions will further enhance the paper's clarity, depth, and impact.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper presents a hierarchical taxonomy to describe an electronic music song, which can then be used for in-context learning when provided to an LLM. In general the writing is clear and the experiments are clearly described and thorough including a crowdsourced listening test. \n\nStrengths:\n- Nice hierarchical taxonomy\n- Good experimental validation\n\nWeaknesses:\n- The companion website gives some prompt details, but I'm still unsure how in-context learning happens. How many examples are provided to the LLM? A full prompt that is actually fed to the LLM would be helpful here\n- It wasn't clear to me what a user actually provides the LLM, again full LLM prompts should be provided on the companion website.\n-I was unclear how clip search happens",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe authors propose a new architecture to accomplish full-song-level music generation by leveraging the idea of transforming and organizing existing music clips, and utilize an LLM to guide the generation process. (Although, in my opinion, merely organizing existing music clips from the dataset without creating new content can hardly be defined as \"composition.\")\nThe audio samples provided are of high quality, and the authors also integrate the system with a DAW, enabling human-AI co-creation, which I believe is highly meaningful.\nThe methodology is explained in detail, and they compare their method with other approaches in the literature using both objective metrics and subjective tests.\n\nThe topics discussed in this paper are of interest to the ISMIR community, and the writing is clear and thorough. I recommend it for publication.\n\nFor weaknesses, there are some points that need to be addressed in future work:\n\n1. The harmonic coherence within the musical content of each track is not confirmed.\n\n2. For MIDI tracks, randomly assigning instrument presets is not a suitable choice. I believe a better solution should be explored.\n\n3. You emphasize that you achieve multi-track compositions with full-song-level structure. Can you provide a metric to evaluate how many tracks and what duration your method can generate?",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nStrengths:\nThis is a well-written paper that proposes a system (TOMI) for transforming and organizing musical ideas into complete compositions. The system is capable of handling both audio and MIDI clips, and organizes them into songs with a process that I imagine is roughly similar to the actual process of many electronic music producers. \n\nTOMI has several potential uses beyond what is explored in the paper. For instance, the system can work with an artist's self-developed database of musical ideas. The system might also be useful for companies like Splice (I could imagine a \"help me make a song\" link on their homepage). The system could also be used as an interface between an artist and their DAW, with an LLM filling in part of the TOMI data structure and the artist filling in the rest. The artist could swap in or out specific clips that are chosen by the system within this interface, and have the system build (or re-build) the song from the modified data.\n\nThe system integration with a DAW is nice, and shows that once the TOMI data structure is complete, the song it encodes can be automatically built inside a DAW for further tweaking by the end user. \n\nThe paper finds that an LLM using the full TOMI system generates more coherent music than (a) the LLM using the system without composition links, (b) a rule-based method using the full TOMI system, and (c) MusicGen. These findings, in particular finding (b), are quite interesting to me. Based on these findings and the strengths above, I recommend strong acceptance of this paper. In my experience, current LLMs struggle with music theory, but (b) suggests that they have learned something about the structure of music that is useful for creating new works, and the non-LLM portion of the TOMI system effectively handles the lower-level detail portions of the music-making process that LLMs are weaker at. \n\nWhile it is clear from the demos that the current implementation of TOMI is not capable of producing music at the level of humans by itself, it is a good step forward, it outperforms reasonable baselines, and it can be used to augment human creativity.\n\nWeaknesses/specific suggestions for improvement:\n-In the companion website, I would like to see an example or two of a *complete* TOMI prompt structure. Right now there are just ...'s in many places. The authors promise to open-source their code after acceptance, and while I'm sure the prompts would be in that code, I don't think readers should have to go digging to find it.\n\n-I would also like to see an example or two of the standalone LLM prompts to ensure that the baseline comparison is reasonable. \n\n-Are >, -, and = the only three symbols allowed in general transforms? Please make this clear somewhere on the companion website.\n\n-It seems that general and drum transforms are limited to 16th note patterns in this specific implementation. Again, just make this clear somewhere.\n\n-Line 148: \"defined by a set of features\" The clips have a set of features. They are not \"defined\" by these features. \n\n-Line 155: \"they can query the databases\" Maybe change this to \"we can query the databases,\" since the LLM supplies f doesn't directly assist in the query.\n\n-Line 204: \"is reused twice\" -> \"is used twice\"\n\n-Line 260-264: I hope this portion of the code will be released.\n\n-Line 272: \"If no matches are found, the clip and its associated composition links are discarded\" I'm a little worried about this, as it could cause important parts of the composition to be missing. Would it be reasonable to modify your error-catching script to catch cases like this and ask the LLM to try again?\n\n-Line 358: What exactly does \"size\" mean here?\n\n-The references need to be cleaned up. (Many citations point to the arxiv rather than the official published versions of papers.)"
    },
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Automatic transcription of guitar strumming is an underrepresented and challenging task in Music Information Retrieval (MIR), particularly for extracting both strumming directions and chord progressions from audio signals. While existing methods show promise, their effectiveness is often hindered by limited datasets. In this work, we extend a multimodal approach to guitar strumming transcription by introducing a novel dataset and a deep learning-based transcription model. We collect 90 minutes of real-world guitar recordings using an ESP32 smartwatch motion sensor and a structured recording protocol, complemented by a synthetic dataset of 4 hours of labeled strumming audio. A Convolutional Recurrent Neural Network (CRNN) model is trained to detect strumming events, classify their direction, and identify the corresponding chords using only microphone audio. Our evaluation demonstrates significant improvements over baseline onset detection algorithms, with a hybrid method combining synthetic and real-world data achieving the highest accuracy for both strumming action detection and chord classification. These results highlight the potential of deep learning for robust guitar strumming transcription and open new avenues for automatic rhythm guitar analysis.",
      "abstract": "Automatic transcription of guitar strumming is an underrepresented and challenging task in Music Information Retrieval (MIR), particularly for extracting both strumming directions and chord progressions from audio signals. While existing methods show promise, their effectiveness is often hindered by limited datasets. In this work, we extend a multimodal approach to guitar strumming transcription by introducing a novel dataset and a deep learning-based transcription model. We collect 90 minutes of real-world guitar recordings using an ESP32 smartwatch motion sensor and a structured recording protocol, complemented by a synthetic dataset of 4 hours of labeled strumming audio. A Convolutional Recurrent Neural Network (CRNN) model is trained to detect strumming events, classify their direction, and identify the corresponding chords using only microphone audio. Our evaluation demonstrates significant improvements over baseline onset detection algorithms, with a hybrid method combining synthetic and real-world data achieving the highest accuracy for both strumming action detection and chord classification. These results highlight the potential of deep learning for robust guitar strumming transcription and open new avenues for automatic rhythm guitar analysis.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Sebastian Murgul",
        "Johannes Schimper",
        "Michael Heizmann"
      ],
      "authors_and_affil": [
        "Sebastian Murgul (Klangio GmbH)*",
        "Johannes Schimper (Karlsruhe Institute of Technology)",
        "Michael Heizmann (Karlsruhe Institute of Technology)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Applications",
        "Music transcription and annotation",
        "Musical features and properties",
        "Multimodality",
        "Harmony, chords and tonality",
        "Novel datasets and use cases",
        "Automatic classification",
        "Music training and education",
        "MIR tasks",
        "Evaluation, datasets, and reproducibility",
        "MIR fundamentals and methodology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1hKEQpL3CMsehgkYmHjv3PDaeWRbus_OW/preview",
      "poster_pdf": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-13-joint-transcription-of",
      "title": "Joint Transcription of Acoustic Guitar Strumming Directions and Chords",
      "video": ""
    },
    "forum": "89",
    "id": "89",
    "pic_id": "",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Real-time music alignment, also known as score following, is a fundamental MIR task with a long history and is essential for many interactive applications. Despite its importance, there has not been a unified open framework for comparing models, largely due to the inherent complexity of real-time processing and the language- or system-dependent implementations. In addition, low compatibility with the existing MIR environment has made it difficult to develop benchmarks using large datasets available in recent years. While new studies based on established methods (e.g., dynamic programming, probabilistic models) have emerged, most evaluations compare models only within the same family or on small sets of test data. This paper introduces Matchmaker, an open-source Python library for real-time music alignment that is easy to use and compatible with modern MIR libraries. Using this, we systematically compare methods along two dimensions: music representations and alignment methods. We evaluated our approach on a large test set of solo piano music from the (n)ASAP, Batik, and Vienna4x22 datasets with a comprehensive set of metrics to ensure robust assessment. Our work aims to establish a benchmark framework for score-following research while providing a practical tool that developers can easily integrate into their applications.",
      "abstract": "Real-time music alignment, also known as score following, is a fundamental MIR task with a long history and is essential for many interactive applications. Despite its importance, there has not been a unified open framework for comparing models, largely due to the inherent complexity of real-time processing and the language- or system-dependent implementations. In addition, low compatibility with the existing MIR environment has made it difficult to develop benchmarks using large datasets available in recent years. While new studies based on established methods (e.g., dynamic programming, probabilistic models) have emerged, most evaluations compare models only within the same family or on small sets of test data. This paper introduces Matchmaker, an open-source Python library for real-time music alignment that is easy to use and compatible with modern MIR libraries. Using this, we systematically compare methods along two dimensions: music representations and alignment methods. We evaluated our approach on a large test set of solo piano music from the (n)ASAP, Batik, and Vienna4x22 datasets with a comprehensive set of metrics to ensure robust assessment. Our work aims to establish a benchmark framework for score-following research while providing a practical tool that developers can easily integrate into their applications.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Jiyun Park",
        "Carlos Eduardo Cancino-Chac\u00f3n",
        "Suhit Chiruthapudi",
        "Juhan Nam"
      ],
      "authors_and_affil": [
        "Jiyun Park (KAIST)*",
        "Carlos Eduardo Cancino-Chac\u00f3n (JKU)",
        "Suhit Chiruthapudi (JKU)",
        "Juhan Nam (KAIST)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Generative Tasks",
        "Musical features and properties",
        "Alignment, synchronization, and score following",
        "MIR tasks",
        "Reproducibility",
        "Expression and performative aspects of music",
        "Real-time considerations",
        "Evaluation, datasets, and reproducibility",
        "Evaluation methodology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1CBBK74gzATCkY_ZQ2dv4jpqsR_qYyKjd/preview",
      "poster_pdf": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-11-matchmaker-an-open",
      "title": "Matchmaker: An Open-Source Library for Real-Time Piano Score Following and Systematic Evaluation",
      "video": ""
    },
    "forum": "92",
    "id": "92",
    "pic_id": "",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe package makes score following algorithms easily available to the community, which in turn allows for a deeper analysis and understanding of these algorithms, as well as baselines for future experiments.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nA Python package for real-time score following algorithms.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper presents an open-source Python package designed for real-time audio-based score following. The authors provide and evaluate several algorithms and features on three public piano datasets.\n\nMain Strengths:\n\n- Main contribution: an open framework for real-time score following is provided, comparable to available offline tools like the Sync Toolbox or Match. This makes score following algorithms easily available for everyone.\n- The provided algorithms work well on piano music. Additionally the design of the packages suggests that it should be easy to add additional algorithms in the future.\n- Standardising the evaluation of these algorithms is important, thus providing the evaluation measures directly with this Python package is very valuable for future research.\n\nMain Weaknesses:\n\n- Limited Scope: Only three algorithms are included and evaluated (two OLTW and one HMM), excluding some well-known older approaches (like Antescofo) as well as more recent approaches based on learned features. One problem is that most of the approaches are not publicly available, but these would be worthwhile additions to this package (and to the comparison of approaches in the paper). Additionally, the algorithms are only evaluated on piano music, while in general they should work on any kind of music. It would be very useful to include at least some results on other genres.\n\nFurther Comments:\n\n- It would be interesting to look into the relationship between quantitative evaluations and qualitative feedback to identify which evaluation measures show a high correlation with the perceived quality. This is probably highly dependent on the application (automatic accompaniment vs. visualisations vs ...), but it is something I am missing in the literature and it would provide some grounding of the evaluation results.\n\n- I was a bit confused by ASAP vs (n)ASAP, e.g. in the table it is named ASAP, in the text it is referred to as (n)ASAP. Would be good to unify/clarify this.\n\n- I do not fully understand Table 4. How are these delays measured? Is this the time it takes to compute one step of the algorithm? This would then be highly dependent on the implementation details of each of the algorithms. This makes sense in the context of the package (on certain hardware), but it is not a general guideline regarding the properties of the algorithms. Also, how is the MAE for the different features types computed? This only makes sense in combination with an alignment algorithm. Or is this some average over all algorithms?\n\n- Regarding features, there is also another version of the LSE features in Arzt, Widmer, Dixon: \"Adaptive distance normalization for real-time music tracking\" (EUSIPCO 2012), where they were combined with chroma features, which led to further improvements. \n\n\nTypos, Grammar, Style:\n- Line 113: othen -> often\n- Line 142: The usage of the package is mainly divided into two scenarios -> The package supports two main usage scenarios\n- Line 145: with default setting -> with the default setting\n- Line 220: We only included performance -> We only included performances\n- Line 239: which we name it log-spectral energy -> which we name log-spectral energy\n- Line 430: ourperforms -> outperforms\n- Figure 6: OTLWArzt -> OLTWArzt\n\nSummary:\n\nThis paper presents a much-needed easily usable Python package for real-time score following. While there is room for improvements (e.g. providing more algorithms and considering more music genres, not only classical piano music) it is as a valuable contribution to the field of MIR.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThis paper presents an open-source Python framework for the evaluation and benchmarking of real-time audio-based score following algorithms. The framework includes implementations of multiple alignment algorithms, standardised evaluation metrics, and public datasets. Reviewers broadly agree that this is a useful contribution that addresses a significant gap in reproducibility and standardisation in the score following community.\n\nHowever, there are a number of concerns with this manuscript, with the main one being the strong focus on piano music. It would be good to see other music genres included, also to make sure that the focus on a specific genre does not lead to design decisions that make it hard to use in a more general case.\n\nFurther concerns include a limited discussion of real-world problems of score following and their evaluation (e.g. trills, skips, repeats), a lack of deeper error analysis (why does the HMM underperform?), some needed clarifications (e.g. how is the delay measured?), and the writing style (incl. typos). Please see the individual reviews for details.\n\nOverall, despite these concerns, this is a useful contribution which fits well into ISMIR. Before a potential publication, the authors are strongly advised to improve the writing of this paper and to try to address/discuss some of the concerns discussed in the reviews.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nI believe the team has thoroughly examines state of the art score following methods and the manuscript demonstrates their passion not only to make fair comparisons but also to make the job easier for fellow researchers and developers. However, I recommend border line reject due to the writing style. At times, the style of this manuscript is more like a combination of review article (quick mention of 58 citations) + user manual (Sec. 3). It contains a lot of useful information but sometimes this makes it hard for a reader to find out the emphasis and identify where the novelty truly is. I think the paper might generate discourse as a demo but I am not sure if it is ready to be accepted as a technical paper. Below are some additional comments and inquiries for the authors to consider.\n\n1. in Sec. 2.2 we see some empty citations \"[]\".\n2. I suppose that AE (absolute error) would remove an important aspect of score following -- whether the follower is lagging behind or rushing forward. Since I am not fully aware of the current progress on this topic, I am curious whether anybody has discussed this and use median and average error with a sign (+/-) to supplement for the lost information due to taking the absolute value?\n\n3. In Sec. 5.1: please define $\\theta_e$ -- I suppose it is the tolerance of inaccuracy?\n\n4. In line 316-323, the symbols t_i, t_j are performance \"time\" but t_k is performance \"beat\". Does that mean t_k is in the unit of beat while t_i and t_j are in seconds? I am puzzled and thus the equation looks vague to me.\n\n5. Line 390: I cannot see where the \"horizontal segments\" are. Perhaps they can be manually marked to enhance visibility?\n\n6. Sec. 8 is great and I very much look forward to playing with the Web demo.\n\n7. The performance of HMM looks far below the other methods in Table II and III and the authors revealed that it might have been under-evaluated (Line 434). This might weaken the overall acceptability of the manuscript, but I agree that this is worth future investigation.",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper provide a unified, open-source Python framework for evaluating and benchmarking score-following algorithms. This is important since the lack of such a framework has hindered progress in the field due to issues with reproducibility, comparability, and generalizability of research findings. This paper makes a valuable contribution to the field of music information retrieval, specifically in the area of real-time audio score following. \n\nThe strengths of the paper lie in:\n\nAddressing a Real Need: The paper tackles the problem of fragmented implementations and the difficulty of comparing score-following methods, which is a significant bottleneck in the field.\n\nOpen-Source Framework: The development and release of an open-source Python package provide a valuable tool for the MIR community.\n\nSystematic Evaluation: The authors conduct a systematic evaluation of different music representations and alignment methods using multiple datasets. The use of diverse datasets (ASAP, Batik, and Vienna4x22) and comprehensive evaluation metrics strengthens the validity of their findings.\n\nDespite the strengths, I have several concerns and questions:\n\nThe paper focuses on alignment accuracy but does not consider how systems handle large deviations like repeats, skips, or performer errors. These are common in live piano practice. Could the framework take these factors in to accound beyond local timing errors?\n\nTable 4 reports extremely low alignment delay values (e.g., 0.07 ms for OLTWArzt). However the performance MIDI files from ASAP only have 3 ms resolution. Can the authors clarify how latency was measured and whether reporting sub-millisecond values is meaningful under these resolution limits?\n\nWhile the focus is real-time alignment, offline score following remains important for batch evaluation or annotation purposes. Does the current framework support an offline evaluation mode using non-causal alignment (e.g., full-sequence DTW)? If not, could it be extended for that purpose?",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper proposes a new framework for score following evaluation. This tool could be of great use for the scientific community. An open source tool that is well-maintained could also evolve for wider use, integrating new metrics and model benchmarking as new research comes to the field. While the article is well-written and proposes a novel evaluation framework, there are some areas for improvement:\n\n- The experimental analysis is clear and covers a decent amount of baselines. However, there is no clear explanation to the errors seen on models like HMM. The experimental analysis would benefit from having more examples of error analysis. This would inform why some methods struggle more than others. \n\n- In scores, there is often musical ornamentation, such as \"trills\". Trills are a good example that add notes to the musical performance while they do not show on the score. This is an area that requires to be discussed as it can improve the error correction for the proposed metrics.\n\n- Does the framework support other instruments and polyphonic performance? It sounds like the tool applies to other instruments as well. If that is the case, the experimental analysis would benefit from discussion on other instruments.\n\n- Missing citations in line 102, 107"
    },
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Current approaches to music emotion annotation remain heavily reliant on manual labelling, a process that imposes significant resource and labour burdens, severely limiting the scale of available annotated data. This study examines the feasibility and reliability of employing a large language model (GPT-4o) for music emotion annotation. In this study, we annotated GiantMIDI-Piano, a classical MIDI piano music dataset, in a four-quadrant valence-arousal framework using GPT-4o, and compared against annotations provided by three human experts. We conducted extensive evaluations to assess the performance and reliability of GPT-generated music emotion annotations, including standard accuracy, weighted accuracy that accounts for inter-expert agreement, inter-annotator agreement metrics, and distributional similarity of the generated labels. \n\nWhile GPT's annotation performance fell short of human experts in overall accuracy and exhibited less nuance in categorizing specific emotional states, inter-rater reliability metrics indicate that GPT's variability remains within the range of natural disagreement among experts. These findings underscore both the limitations and potential of GPT-based annotation: despite its current shortcomings relative to human performance, its cost-effectiveness and efficiency render it a promising scalable alternative for music emotion annotation.",
      "abstract": "Current approaches to music emotion annotation remain heavily reliant on manual labelling, a process that imposes significant resource and labour burdens, severely limiting the scale of available annotated data. This study examines the feasibility and reliability of employing a large language model (GPT-4o) for music emotion annotation. In this study, we annotated GiantMIDI-Piano, a classical MIDI piano music dataset, in a four-quadrant valence-arousal framework using GPT-4o, and compared against annotations provided by three human experts. We conducted extensive evaluations to assess the performance and reliability of GPT-generated music emotion annotations, including standard accuracy, weighted accuracy that accounts for inter-expert agreement, inter-annotator agreement metrics, and distributional similarity of the generated labels. \n\nWhile GPT's annotation performance fell short of human experts in overall accuracy and exhibited less nuance in categorizing specific emotional states, inter-rater reliability metrics indicate that GPT's variability remains within the range of natural disagreement among experts. These findings underscore both the limitations and potential of GPT-based annotation: despite its current shortcomings relative to human performance, its cost-effectiveness and efficiency render it a promising scalable alternative for music emotion annotation.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Meng Yang",
        "Jon McCormack",
        "Maria Teresa Llano",
        "Wanchao Su"
      ],
      "authors_and_affil": [
        "Meng Yang (Monash University)*",
        "Jon McCormack (Monash University)",
        "Maria Teresa Llano (University of Sussex)",
        "Wanchao Su (Monash University)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Evaluation metrics",
        "Musical features and properties",
        "Annotation protocols",
        "Evaluation, datasets, and reproducibility",
        "Musical affect, emotion and mood"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1tlfl29uRGrb0fr9kfeqhZ4qYajSNX4rj/preview",
      "poster_pdf": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-4-exploring-the-feasibility",
      "title": "Exploring the Feasibility of LLMs for Automated Music Emotion Annotation",
      "video": ""
    },
    "forum": "101",
    "id": "101",
    "pic_id": "",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "2"
  },
  {
    "content": {
      "TLDR": "The evolution of the music industry has introduced multimedia elements\u2014such as video, text, and images\u2014into music consumption. However, current Music Recommender Systems (MRSs) remain predominantly audio-focused, requiring explicit user interaction to access additional media. This study explores the integration of multimedia content into MRSs, considering the role of contextual activities and the Uses and Gratifications (U&Gs) framework in enhancing personalization and engagement. A diary study with 26 participants over one week identified nine key activities, with Household Chores, Workout, and Focusing being the most relevant. These activities revealed novel U&Gs such as \"For Preference\", \"For Convenience\", \"For Discovery\", and \"To Get Distracted\". A subsequent user study compared a Basic Music App (audio-only) with a Modified Music App (multimedia-enhanced). Results showed that participants preferred the Modified Music App across five constructs: novelty, ease of use, usefulness, satisfaction, and intention to use. These findings suggest that multimedia-enhanced recommendations can improve user experience by aligning with activity-specific preferences. The study contributes to research on personalized MRSs and offers insights for developing context-aware, multimedia-driven recommendations.",
      "abstract": "The evolution of the music industry has introduced multimedia elements\u2014such as video, text, and images\u2014into music consumption. However, current Music Recommender Systems (MRSs) remain predominantly audio-focused, requiring explicit user interaction to access additional media. This study explores the integration of multimedia content into MRSs, considering the role of contextual activities and the Uses and Gratifications (U&Gs) framework in enhancing personalization and engagement. A diary study with 26 participants over one week identified nine key activities, with Household Chores, Workout, and Focusing being the most relevant. These activities revealed novel U&Gs such as \"For Preference\", \"For Convenience\", \"For Discovery\", and \"To Get Distracted\". A subsequent user study compared a Basic Music App (audio-only) with a Modified Music App (multimedia-enhanced). Results showed that participants preferred the Modified Music App across five constructs: novelty, ease of use, usefulness, satisfaction, and intention to use. These findings suggest that multimedia-enhanced recommendations can improve user experience by aligning with activity-specific preferences. The study contributes to research on personalized MRSs and offers insights for developing context-aware, multimedia-driven recommendations.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Oleg Lesota",
        "Veronica Clavijo",
        "Attia Rizwani",
        "Markus Schedl",
        "Bruce Ferwerda"
      ],
      "authors_and_affil": [
        "Oleg Lesota (Johannes Kepler University)*",
        "Veronica Clavijo (J\u00f6nk\u00f6ping University)",
        "Attia Rizwani (J\u00f6nk\u00f6ping University)",
        "Markus Schedl (Johannes Kepler University)",
        "Bruce Ferwerda (J\u00f6nk\u00f6ping University)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Personalization",
        "Applications",
        "Music recommendation and playlist generation",
        "Music videos, multimodal music systems",
        "Music interfaces and services",
        "Human-centered MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1vTi_NPPAHS-L5WxjUMDs0g9QMFbQ_H26/preview",
      "poster_pdf": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-6-enhancing-music-recommender",
      "title": "Enhancing Music Recommender Systems with Multimedia Content: A Context-Aware Approach",
      "video": ""
    },
    "forum": "103",
    "id": "103",
    "pic_id": "",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nDisagree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nDisagree\n\n**Q5 ( Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chosen, otherwise write \"n/a\"))**\n\nThere are several related work on using contextual features in recommender systems. Some that were presented at previous ISMIR conferences include:\n- Hu, Y., and Ogihara, M.. \"NextOne Player: A Music Recommendation System Based on User Behavior.\" ISMIR. Vol. 11. 2011.\n- Schedl, M., and Flexer, A. \"Putting the User in the Center of Music Information Retrieval.\" ISMIR. 2012.\n- Vigliensoni, G., and Fujinaga, I. \"Automatic Music Recommendation Systems: Do Demographic, Profiling, and Contextual Features Improve Their Performance?.\" ISMIR. 2016.\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nDisagree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\n- The ability to combine qualitative studies (diary study) with quantitative studies (user study) provides a framework to better understand user preferences, and can be applied to other user-based studies.\n- The use of UGT in a case study of music recommender systems can identify newer motivations that user might have while interacting with them such as \"preference\", \"convenience\", \"discovery\", \"distraction\"\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nUsers of music recommender systems might enjoy additional multimedia (such as images, videos or lyrics) while listening to music, depending on their context.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThe paper presents a two-step case study on the effects of context awareness and multimedia content (namely video, text and image) on a music recommendation system. In the first study, the authors attempt to capture participants\u2019 choice of the combination of an activity (according to the Uses and Gratifications Theory or UGT) and a music-related media, and the reasons for choosing such media. In the second study, the authors use the output of the previous study to inform the development of two music prototypes, one with audio-only and one with different types of media and measure different aspects of the user experience via a survey through two frameworks (ResQue, and UEQs).\n\nStrengths:\n- Comprehensive two-step study of the relationship between contextual aspects (described through activities and Uses and Gratifications) and the choice of multimedia content in the user experience of a music recommender system.\n- Statistical analysis of the results of both studies\n\nWeaknesses:\n- Repetition of content in different sections. For example, the activities in the Diary Study described both in Section 4.1 and Section 5.1.\n- Missing results in Section 4.1.1. It would have been very useful to see the results of the MANOVA tests instead of cherry picking a couple of them, especially since there is still space left in the paper.\n- Incomplete details in the User Study. Things like the participant distribution (section 4.2), the specific questions asked to the participants (section 3.2), how did the listeners interact with the app, especially the modified app? Did they need to switch between media types? Were they able to listen to any of the songs? 10 minute sessions sound pretty short. Why and how was that decided? What activities were analyzed in the UEQ-S findings?\n\nOther aspects:\n- The authors claim in Section 2 that: \u201cDespite these added functionalities, platforms generally leave it to listeners to initiate any deeper engagement, such as clicking on the lyrics tab or opting to watch a video. Consequently, the service does not dynamically recommend multimedia formats that might enhance an individual\u2019s specific context or motivation at the moment. \u201d. In order to be able to dynamically recommend multimedia, music streaming services need to capture the listener\u2019s intent in some way or another. If the focus is on a music streaming app, these streaming services can use information such as the time of day, location and whether the listener is actively interacting with the app vs. listening in the background, which are aspects that the paper did not address (although it was mentioned in the Limitations section). Location information would also pose some legal hurdles.\nLater in the discussion the authors also claim \u201cThese responses reinforce the notion that while additional media can be helpful or entertaining, contextual appropriateness remains vital, and users often want the freedom to choose how much visual or textual content accompanies the audio.\u201d\n\nComments and typos:\n- Section 1, 2nd paragraph. Since MTV\u2019s launch in 1981 \u2192 Since the launch of MTV in 1981.\n- Section 1, 2nd paragraph. Links to the music streaming - services are unnecessary. \n- Section 2, 1st paragraph. I would not necessarily say that there is a shift in Music recsys research, but more of an increase or emphasis.\n- Section 3.2.1. Did any of the participants in the user study overlap with the diary study?\n- Section 4.2.1 - 1. What were the idle chores in Household chores?\n- Section 4.2.1 - 2. What about the findings in the relaxing activities?\n- Remove \u201cCiteseer\u201d from Reference [25].\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThis is a meta-review. The reviewers have provided detailed reviews and I encourage the authors to carefully read them and address any comments, questions or suggestions. Below is a summarization of the feedback provided by the reviews.\n\nThe reviewers agreed on the following strengths of the paper:\n1) Well motivated and robust methodology, grounded in sociological theories (UGT).\n2) Clear and well organized (except for some repetitions in Section 5, and dispersion of future work across several sections (Reviewer #3))\n3) Authors thoughtfully recognize the limitations of their work (Reviewer #2)\n\nThe reviewers also pointed out a list of weaknesses, including:\n1) Lack of details in the Diary study results section, specifically subsection 4.1.1 about the co-occurrence matrix of the activities and uses and gratifications (Reviewer #2).\n2) Lack of details as well in the User study, specifically the demographic details (Reviewer #1), the task description and questions asked to the participants, how they interacted with the modified app and whether they were able to listen to the songs (Reviewer #3). The design choices for the music apps were also not described (Reviewer #3).\n3) Repetition of content, especially the activities in the Diary Study described both in Section 4.1 and Section 5.1. By removing duplicate content, authors could address the other weaknesses by adding more details\n4) Gender disparity in the User study might lead to bias (Reviewer #2)\n5) Ethical considerations not addressed, both ethical approval and ethical implications of the work's suggestions (Reviewers #1 & #3).\n6) Some incongruence in the author\u2019s claims. On one hand, they state the following:\n\u201c...Despite these added functionalities, platforms generally leave it to listeners to initiate any deeper engagement, such as clicking on the lyrics tab or opting to watch a video. Consequently, the service does not dynamically recommend multimedia formats that might enhance an individual\u2019s specific context or motivation at the moment.\u201d\n\nOn the other hand, they also recognize that:\n\u201cOpen-ended responses illustrated the desire for control over media formats, with participants expressing interest in turning off extra content if it became distracting or did not fit their ongoing activity. These responses reinforce the notion that while additional media can be helpful or entertaining, contextual appropriateness remains vital, and users often want the freedom to choose how much visual or textual content accompanies the audio.\u201d\n\nStreaming services already provide these additional media formats, and users are already able to choose which one to use, when they feel like it is appropriate. So it seems like the only benefit here is that these streaming services could anticipate the intent of the user and show the additional media when needed. In other words, instead of opting in, users would opt out if they did not feel it is appropriate. Furthermore, predicting the user intent requires additional signals, some of which can be \u201ceasily\u201d obtained, such as time or whether users are actively using the device or leaning back, but there are others, such as the location of the listener (or using camera features) that raise concerns about privacy and tracking, as mentioned by Reviewer #3 as well. This \u201ctension\u201d, this two-sides-of-a-coin between human agency and surveillance is an important and common topic in Recommender systems user research and authors should at least provide some reflection on it, perhaps in the ethical considerations section at the end of the paper.\n\nAfter a discussion among the reviewers, we came to the conclusion that this paper, despite the number of weaknesses that were reflected in the reviews, and summarized/expanded on above, deserves to be accepted to the conference. Accepted with the condition that the authors carefully read and follow all the feedback provided by the reviewers, and make the required corrections/modifications to improve the manuscript.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nStrengths\n1. Clear structure and writing\n2. Thoughtful methodology\n3. Theoretical framing\nWeaknesses\n1. Privacy and ethical considerations not addressed\n2. Redundant or unnecessary Information\n3. Lack of sample demographic details\n\nThis paper presents a two-stage study, consisting of a diary study followed by a user study, aimed at understanding the relationship between engagement with music-related media and contextual activities. The study is grounded in the Uses and Gratifications Theory (UGT), which is used to identify the motivations behind why certain activities or media types may be preferred in specific contexts. The second phase involves a controlled experiment in which participants interact with two music application prototypes, one audio-only and one enriched with multimedia information. The results suggest that different activities can influence the type of content users engage with.\n\nThe paper is well-written, provides sufficient detail to understand each step of the experimental procedure, and is structured in a way that is easy to follow. I particularly appreciate the decision to build the user study upon the initial diary study, which adds coherence and depth to the research design. While the findings are exploratory, I believe the paper offers a nuanced understanding of the phenomenon under investigation and am therefore inclined to recommend its acceptance. Below, I outline a few minor comments:\n\n- Section 1, 2nd paragraph: The URLs of the platforms could be omitted or moved to a footnote.\n- Section 2, 3rd paragraph: The list of streaming services (\u201cSpotify, Apple Music, and YouTube Music\u201d) may be unnecessary, as similar examples are already provided in the previous paragraph.\n- Ethical approval: Please specify whether the study received IRB or ethical board approval.\n- Section 3.1.1, 1st paragraph: In addition to reporting gender (\u201c19 female, 7 male\u201d), please include the average age of participants.\n- Figure 1: Images (c) and (d) appear blurred; consider improving their resolution.\n- Section 5.1: There is no need to repeat the list of activities and motivations, as they are already presented in the previous section.\n- Discussion: I would be interested to see a reflection on the implications of context-aware recommender systems. While they may offer more relevant suggestions, they also raise concerns regarding privacy, data tracking, and potential misuse of user surveillance, even within music streaming platforms.",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\n**Brief Summary**\nIn this paper, the authors use Uses and Gratifications Theory (UGT) to explore users\u2019 motivations for engaging with various music-related media formats (e.g., text, video, images) and how different contextual activities (such as doing household chores, working out, or relaxing) influence these choices. To investigate this, they conducted two studies. The first, a diary study, examined how individuals\u2019 U&G motivations align with their daily activities. Building on these findings, the second user study assessed the impact of recommending supplementary media formats (video, text, or images) tailored to users\u2019 context and motivations. The results indicate that context-aware, multimedia-enhanced music recommendations can significantly enhance user experience, as long as they offer flexibility and allow users to control or opt out of additional media content.\n\n**Strengths**\n1. The research is grounded in a compelling motivation.\n2. The methodology is robust, framed effectively through the lens of Uses and Gratifications Theory (UGT).\n3. The manuscript is well-organized and clearly written.\n4. The authors thoughtfully acknowledge the limitations of their study and propose several avenues for future exploration that build on their findings.\n\n**Minor concerns**\n1. The authors could include the co-occurrence matrix of the nine activities and thirteen uses and gratifications (U\\&G) in Section 4.1 to provide a more detailed view of the distribution of Diary Study entries.\n2. Similarly, the authors could include the distribution of activities for the 59 valid data points in the User Study.\n3. The Diary Study had a predominantly female participant group (19 out of 26), but the gender distribution for the 63 participants in the User Study is not reported. Was the male/female distribution similar across both studies? If not, could this discrepancy introduce bias into the results?",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nIn this paper, the authors explore the impact that user activities have on their preference for multimedia content types in a music recommender system context. They conducted two user studies: 1) a diary study in which users wrote down their music-listening behavior during one week, and noted their activity, type of music-related media consumed, and reasons why, and 2) a user study in which two music applications were evaluated, one with and one without additional media types beyond audio. The authors (unsurprisingly) find that there is indeed a connection between user activities and their preference for additional media types. \n\nStrong points of this work are the following:\n- The motivation for this work is presented very clearly in the introduction, and previous works are outlined so that the novelty of this work is evident. The insights help progress the research field of music recommendation and personalization by taking into account contextual factors (activities) and media types (audio, image, text, video). Therefore, it is a good fit for the ISMIR conference.\n- The two user studies fit the research questions and nicely complement each other. Results are clearly described and give some new insights into user preferences. \n- The paper is generally well-written and easily followable, with a clear structure and very few grammatical errors.\n\nStill, there is room for improvement in the following aspects, with my main point of criticism concerning the clarity and presentation of the method.\n\nMethod\n- While the diary study procedure is clear enough, for the second user study, information is missing that would be needed for full reproducibility. For example, the song selection process is only described at a very high level, and the final song selection is not shared. Moreover, the design choices for the two music apps are not described. How could users interact with the recommendations and media types? Were the recommended songs the same in both apps and how were they shown (Figure 1 prominently shows recommended artists but not different songs)? Could the participants listen to the songs, and why (not)? Also, the task description users received and the exact questions are not shared (were they 100% the same as in ref 26 and 27?), and the code for the apps is not shared. It is therefore not clear how participants were encouraged to consider the activity context, and how they were invited to share feedback after interacting with the apps. As these details are not available, it is difficult to gauge the impact of the method on the results.\n- Even though this is a user study, the authors do not mention any ethical considerations or approval of an ethical review board.\n\nResults\n- Some of the screenshots in Figure 1 are of low quality.\n- Section 3.2 mentions that participants were informed their task was to assess alignment between content and scenario, but in Section 4.2.2, some points (e.g., accuracy and novelty) do seem to be focused more on the evaluation of the song recommendations. This would be highly influenced by song preference which was not accounted for in the study design, and therefore I wonder how useful these insights are.\n- If a selection criterion is that participants needed to listen to music every day, and they had to note all music consumption episodes, what could explain the discrepancy with some participants having only recorded 3 diary entries total?\n\nDiscussion & conclusion\n- The majority of Section 5 consists of additional insights rather than interpretation of insights, and would therefore fit better in the Results section. Perhaps more discussion could be dedicated to why users might consume more music in certain situations or consume it differently, and how this can be better accounted for in the streaming services they use.\n- It would be worthwhile to mention the limitation of the second user study that the interaction environment was artificial, and participants could (and likely would) behave and/or respond differently in a real-life setting. Impact of the user study design choices on the results is insufficiently described.\n\nOverall\n- There is some repetition across sections, like repetition of the method in the results section. The manuscript could be condensed somewhat.\n- Future work is now spread out over the last 3 sections. It would be better to collect all of it in one place.\n- The first sentence of the last page\u2019s right column is incomplete."
    },
    "session": "5"
  },
  {
    "content": {
      "TLDR": "In ensemble performances, musicians use gesture and breath cues to synchronize their initial notes at the beginning of a piece, but the precise relationship between these cues and onset timing remains under-explored. This study investigates how flutists' gesture and breath cues encode the timing information for the initial note onset.\nThis research consists of four components: (1) Collection of a cue dataset containing synchronized video and audio recordings of flute-piano duets, (2) Identification of cue candidate points through facial movement curves and breath onset-offset analysis, (3) Verification of predicted onset accuracy using linear regression on these cues compared to human onset asynchronies and (4) Introduction and exploration of a `trigger' concept, defined as immediate, clearly perceivable gestures (such as stopping or raising the head) indicating the precise moment of onset.\nOur findings suggest a dual-cue system: preparatory cues broadly predict onset timing, while precise triggers refine the exact onset. We compared the time difference between the predicted and piano onsets with the flute\u2013piano asynchronies and verified the concepts of cue and trigger through expert interviews. This research contributes to a deeper understanding of the complex phenomena of musical cues during performance through multimodal analysis. This paper provides an open-access cue dataset, which can be found on the accompanying website.",
      "abstract": "In ensemble performances, musicians use gesture and breath cues to synchronize their initial notes at the beginning of a piece, but the precise relationship between these cues and onset timing remains under-explored. This study investigates how flutists' gesture and breath cues encode the timing information for the initial note onset.\nThis research consists of four components: (1) Collection of a cue dataset containing synchronized video and audio recordings of flute-piano duets, (2) Identification of cue candidate points through facial movement curves and breath onset-offset analysis, (3) Verification of predicted onset accuracy using linear regression on these cues compared to human onset asynchronies and (4) Introduction and exploration of a `trigger' concept, defined as immediate, clearly perceivable gestures (such as stopping or raising the head) indicating the precise moment of onset.\nOur findings suggest a dual-cue system: preparatory cues broadly predict onset timing, while precise triggers refine the exact onset. We compared the time difference between the predicted and piano onsets with the flute\u2013piano asynchronies and verified the concepts of cue and trigger through expert interviews. This research contributes to a deeper understanding of the complex phenomena of musical cues during performance through multimodal analysis. This paper provides an open-access cue dataset, which can be found on the accompanying website.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Jaeran Choi",
        "Taegyun Kwon",
        "Juhan Nam"
      ],
      "authors_and_affil": [
        "Jaeran Choi (KAIST)*",
        "Taegyun Kwon (KAIST)",
        "Juhan Nam (KAIST)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Human-computer interaction",
        "Applications",
        "Knowledge-driven approaches to MIR",
        "Cognitive MIR",
        "Novel datasets and use cases",
        "Music composition, performance, and production",
        "Alignment, synchronization, and score following",
        "Music videos, multimodal music systems",
        "MIR tasks",
        "Evaluation, datasets, and reproducibility",
        "Human-centered MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1ryvoBTfdjafNVfGPjfLiKvd0rSTCz9HK/preview",
      "poster_pdf": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-12-predicting-flutist-onset",
      "title": "Predicting Flutist Onset Timing in Duet Performance: A Multimodal Analysis of Gesture and Breath Cues",
      "video": ""
    },
    "forum": "105",
    "id": "105",
    "pic_id": "",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "1"
  },
  {
    "content": {
      "TLDR": "We study the capabilities of generative autoregressive transformer models trained on large amounts of symbolic solo-piano transcriptions. After first pretraining on approximately 60,000 hours of music, we use a comparatively smaller, high-quality subset, to finetune models to produce musical continuations, perform symbolic classification tasks, and produce general-purpose contrastive MIDI embeddings by adapting the SimCLR framework to symbolic music. When evaluating piano continuation coherence, our generative model outperforms leading symbolic generation techniques and remains competitive with proprietary audio generation models. On MIR classification benchmarks, frozen representations from our contrastive model achieve state-of-the-art results in linear probe experiments, while direct finetuning demonstrates the generalizability of pretrained representations, often requiring only a few hundred labeled examples to specialize to downstream tasks. ",
      "abstract": "We study the capabilities of generative autoregressive transformer models trained on large amounts of symbolic solo-piano transcriptions. After first pretraining on approximately 60,000 hours of music, we use a comparatively smaller, high-quality subset, to finetune models to produce musical continuations, perform symbolic classification tasks, and produce general-purpose contrastive MIDI embeddings by adapting the SimCLR framework to symbolic music. When evaluating piano continuation coherence, our generative model outperforms leading symbolic generation techniques and remains competitive with proprietary audio generation models. On MIR classification benchmarks, frozen representations from our contrastive model achieve state-of-the-art results in linear probe experiments, while direct finetuning demonstrates the generalizability of pretrained representations, often requiring only a few hundred labeled examples to specialize to downstream tasks. <br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Louis Bradshaw",
        "Alexander Spangher",
        "Honglu Fan",
        "Stella Biderman",
        "Simon Colton"
      ],
      "authors_and_affil": [
        "Louis Bradshaw (Queen Mary University of London)*",
        "Alexander Spangher (University of Southern California)",
        "Honglu Fan (University of Geneva)",
        "Stella Biderman (EleutherAI)",
        "Simon Colton (Queen Mary University of London)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Symbolic music processing",
        "Musical features and properties",
        "Representations of music",
        "Music generation",
        "Automatic classification",
        "MIR tasks",
        "Machine learning/artificial intelligence for music",
        "MIR fundamentals and methodology",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1JBN_KfXLRkxovRgX7ach8WYXhWElts0N/preview",
      "poster_pdf": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-10-scaling-self-supervised",
      "title": "Scaling Self-Supervised Representation Learning for Symbolic Piano Performance",
      "video": ""
    },
    "forum": "111",
    "id": "111",
    "pic_id": "",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Performers convey musical meaning not only through pitch and dynamics but also through micro-timing deviations. This study examines performance analysis and timing in Georg Philipp Telemann\u2019s 12 Fantasias for Solo Flute, focusing on how musical elements, such as implied polyphony, onset positions, and meter, influence musical performance. We release a corpus with annotations on interleaved voices gathering 11 musicological sources. We first evaluated how simple rules may detect such interleaved voices from the scores. We then analyzed six complete recordings of the fantasias, comparing their timing deviations against a metronomic interpretation. Results reveal significant timing deviations influenced not only by note position within rhythmic groupings, but also by the presence of interleaved melodic voices, in particular when these interleaved voices are notated with opposing stems.",
      "abstract": "Performers convey musical meaning not only through pitch and dynamics but also through micro-timing deviations. This study examines performance analysis and timing in Georg Philipp Telemann\u2019s 12 Fantasias for Solo Flute, focusing on how musical elements, such as implied polyphony, onset positions, and meter, influence musical performance. We release a corpus with annotations on interleaved voices gathering 11 musicological sources. We first evaluated how simple rules may detect such interleaved voices from the scores. We then analyzed six complete recordings of the fantasias, comparing their timing deviations against a metronomic interpretation. Results reveal significant timing deviations influenced not only by note position within rhythmic groupings, but also by the presence of interleaved melodic voices, in particular when these interleaved voices are notated with opposing stems.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Patrice Thibaud",
        "Mathieu Giraud",
        "Yann Teytaut"
      ],
      "authors_and_affil": [
        "Patrice Thibaud (Univ. Lille, CNRS, Inria, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France)*",
        "Mathieu Giraud (Univ. Lille, CNRS, Inria, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France)",
        "Yann Teytaut (Univ. Lille, CNRS, Inria, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Digital musicology",
        "Musical features and properties",
        "Computational musicology",
        "Alignment, synchronization, and score following",
        "MIR tasks",
        "Expression and performative aspects of music",
        "Structure, segmentation, and form",
        "Rhythm, beat, tempo",
        "Computational music theory and musicology",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/10jJjXIq-RXmTnMn5OCl_-yjzioHq6qZk/preview",
      "poster_pdf": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-14-when-voices-interleave",
      "title": "When Voices Interleave: Timing Deviations in Six Performances of Telemann's Fantasias for Solo Flute",
      "video": ""
    },
    "forum": "112",
    "id": "112",
    "pic_id": "",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nStrongly agree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nStrongly agree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nTiming deviations in flutist playing are influenced by the interleaving of voices. It is unclear how reusable this insight is as the analysis is based on a single piece of a particular time period and performance practice.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nTiming deviations in flutist playing are influenced by the interleaving of voices in the Teleman fantasia for solo flute.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThe authors analyze micro-timing deviations among different flute players in the Teleman fantasia for solo flute. The work is interesting but is leaning more towards musicology with some limited use of computational techniques. It is definitely within the scope of ISMIR but there are other venues that would be more appropriate. \n\nThe writing and placing the work in context is good. Some aspects could be improved. For example the alignment procedure needs to be described in more detail as there is not enough information for reproducibility. \n\nThe musicological sources and how they were used need to be clarified. \n\nThe approach is interesting and there is some novelty in the voice rule-based heuristic. \n\nThe main weakness is that this is limited to one particular piece and the conclusions could be specific to baroque performance practice. One of the advantages of computational approaches is that they can scale beyond the analysis of limited repertoire typically done in musicology but in this case this has not happened. \n\nI think the paper has value for the ISMIR community especially musicologists looking into performance analysis but it is not a strong contribution of more general interest.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThere is considerable agreement among the reviewers that this is an interesting paper connecting audio analysis, performance practice, and symbolic information. The idea is interesting and well executed and the paper is well written. The main criticisms arise from the limited analysis making it unclear if the proposed methodology could be used more generally as well as various other smaller criticisms provided by the reviewers.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper presents an analysis of unaccompanied flute recordings, focusing on the relationship between timing and the voicing inside the implied polyphony. The paper analyzes Teleman's 12 Fantasias played by six flautists. \nThe paper first presents a rule-based method for detecting the change of voice, incorporating a new feature: alternation in intervals. \nThe paper uses a ground-truth voice annotation to analyze the performances. It confirms the practice of notes inegales for groups of notes and differences in playing when playing different voices.\n\nThe work is interesting and valuable for computationally understanding performance practices. Having the dataset available for other researchers to analyze is also nice.\nThe analysis of the paper, however, raises more questions than it answers, due to the dataset focusing exclusively on Teleman's Fantasia and the analysis being done at the performer level. This first of all makes it difficult to say whether the results arise due to Baroque playing practices in general or are specific to the Fantasia; since implied voicing is a compositional technique used outside of unaccompanied pieces, it might make sense to incorporate other Baroque pieces (with accompaniments). Second, the results presented are also specific to the flautist. How much of the results are general performative practice of the Baroque flute as opposed to artist-specific? Which of these results are similar between each artist and different? \n\nThe dataset seems important for researchers interested in Baroque flute playing, but the analysis is a bit lacking; therefore, I would suggest a minor accept of the paper. \n\nSome other comments.\nI am not an expert in Baroque playing practices, but I would have liked to see more musical treatment of section 5.2, and uncover what kind of different IOI strategies can be taken to express change of voicings. That would have made the paper more valuable to the music performance community. I say this because Baroque practices, especially after the HIP movement, have evolved by interpreting past treaties (e.g., that of Quantz for flute), so there are many unwritten performative practices that we can expect performers to abide by. For example, the results in 5.1 are somewhat \"expected\" given many treatises dealing with the note inegales to a great extent. On the other hand, Section 5.2 in my opinion is the gem of using computational analysis, as there seem to be only a few treatises on executing voicings. This means it is up to the performer to devise a way to deliver it effectively, meaning there is a greater expressive freedom to make it work musically. I would have liked to see more IOI-based analysis on similar strategies taken by the flautists, for example, by clustering or categorizing some of the IOI patterns.",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nStrengths\nThe paper presents an original study focused on Telemann\u2019s 12 Fantasias for Solo Flute, a repertoire rarely explored computationally within the MIR community. The investigation of micro-timing deviations in relation to implied polyphony (interleaved voices) is musically meaningful and technically underexplored.\nThe authors compile a symbolic corpus annotated with implied polyphony from 11 scholarly sources.\n\nThe paper builds upon prior theoretical work and contributes additional symbolic features for detecting voice interleaving.\n\nThe authors release their dataset, alignment outputs, and annotations under open licenses, and provide an interactive Streamlit web application for browsing scores, performances, and timing metrics. This supports reproducibility and encourages further research.\nWeaknesses\n\nAlignment procedure lacks clarity and evaluation:\nThe description of the alignment method (Section 4.3) is incomplete. For instance, in line 231, the frame size for audio analysis is unspecified. Given that the entire \u2206IOI/\u2206o-based analysis depends on these alignments, the absence of alignment accuracy evaluation is a major concern. The paper could benefit from basic alignment diagnostics (e.g., confidence scores, visual inspection tools, or comparison to manually annotated excerpts).\n\nNo handling of rests and ornamentation in alignment:\nThe alignment process does not account for ornamentation or rests, even though these are prevalent in Baroque performance and can lead to onset mismatches. This raises concerns about the validity of timing deviation metrics, especially in slow or ornamented passages.\nWhile DTW is a standard and reasonable alignment approach for monophonic music, the paper would benefit from a broader discussion of alternative alignment tools. For instance, SyncToolbox\n\nInsufficient detail on musicological resources:\n Although the paper claims to incorporate annotations from 11 musicological sources, it does not adequately describe:\nHow these sources were selected and weighted.\n\n\nWhether conflicting annotations were reconciled.\n\n\nHow movement boundaries, tempo indications, or voice labels were standardized.\n More transparency is needed here, especially since these annotations form the basis for the voice detection and timing analysis.\n\n\nFormatting and minor presentation issues:\nThere are a few formatting and typographical issues that should be corrected:\n\nLine 227: \u201calignment\u201d is misspelled.\n\nLine 257: unmatched parenthesis (\u201c[\u201d) causes confusion in formula readability.",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper analyzes the micro-timing deviations in solo flute performances, investigates their relationship with musical elements such as implied polyphony and interleaved voices, and assesses the consistency across performers. To achieve this, the authors build a corpus with annotations and propose rule-based methods based on Davis's work [32]. The ideas and methods are well presented, and the figures and tables are easy to understand. Moreover, the web application and embedded audio links in the paper are also valuable resources.\n\nThe main reasons I did not give this paper a strong accept are as follows:\n\nI am not very familiar with flute music or performance analysis. While I learned some solid methods for analyzing micro-timing from this paper, I am not able to judge how novel, robust, or limited they are.\n\nSimilarly, the statistical analyses and results (e.g., Tables 5 and 6) make sense to me, but I could not see a broader picture or clear objective arising from them\u2014perhaps due to my limited background in performance analysis. For instance, after identifying consistency or inconsistency across performers, what motivates further investigation? Would the observed patterns or conclusions hold with a larger pool of performers?"
    },
    "session": "3"
  },
  {
    "content": {
      "TLDR": "In this paper, we address the challenge of Optical Music Recognition (OMR) for handwritten jazz lead sheets, a widely used musical score type that encodes melody and chords. The task is challenging due to the presence of chords, a score component not handled by existing OMR systems, and the high variability and quality issues associated with handwritten images. Our contribution is two-fold. We present a novel dataset consisting of 293 handwritten jazz lead sheets of 163 unique pieces, amounting to 2021 total staves aligned with Humdrum **kern and MusicXML ground truth scores. We also supply synthetic score images generated from the ground truth. The second contribution is the development of an OMR model for jazz lead sheets. We discuss specific tokenisation choices related to our kind of data, and the advantages of using synthetic scores and pretrained models. We publicly release all code, data, and models.",
      "abstract": "In this paper, we address the challenge of Optical Music Recognition (OMR) for handwritten jazz lead sheets, a widely used musical score type that encodes melody and chords. The task is challenging due to the presence of chords, a score component not handled by existing OMR systems, and the high variability and quality issues associated with handwritten images. Our contribution is two-fold. We present a novel dataset consisting of 293 handwritten jazz lead sheets of 163 unique pieces, amounting to 2021 total staves aligned with Humdrum **kern and MusicXML ground truth scores. We also supply synthetic score images generated from the ground truth. The second contribution is the development of an OMR model for jazz lead sheets. We discuss specific tokenisation choices related to our kind of data, and the advantages of using synthetic scores and pretrained models. We publicly release all code, data, and models.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Juan Carlos Martinez-Sevilla",
        "Francesco Foscarin",
        "Patricia Garcia-Iasci",
        "David Rizo",
        "Jorge Calvo-Zaragoza",
        "Gerhard Widmer"
      ],
      "authors_and_affil": [
        "Juan Carlos Martinez-Sevilla (University of Alicante)*",
        "Francesco Foscarin (Johannes Kepler University Linz)",
        "Patricia Garcia-Iasci (University of Alicante)",
        "David Rizo (University of Alicante)",
        "Jorge Calvo-Zaragoza (University of Alicante)",
        "Gerhard Widmer (Johannes Kepler University Linz)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Symbolic music processing",
        "MIR fundamentals and methodology",
        "Music retrieval systems",
        "Applications",
        "Musical features and properties",
        "Harmony, chords and tonality",
        "Novel datasets and use cases",
        "Optical music recognition",
        "MIR tasks",
        "Evaluation, datasets, and reproducibility",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1v-z1EvHPbtTcJZq2exc-zB1WIfy1PcJD/preview",
      "poster_pdf": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-10-optical-music-recognition",
      "title": "Optical Music Recognition of Jazz Lead Sheets",
      "video": ""
    },
    "forum": "113",
    "id": "113",
    "pic_id": "",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nStrongly agree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe dataset and model could easily be reused in other music domains requiring chord recognition or handwritten score processing. The tokenization strategies and dataset alignment procedures are generalizable and valuable.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThis paper introduces a novel dataset and model for OMR of handwritten jazz lead sheets, incorporating chord recognition and showing the benefits of synthetic data, pretraining, and symbolic-aware tokenization.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper is an important step forward in the development of OMR systems that can handle handwritten lead sheets, particularly in the jazz domain. The introduction of chord recognition into the OMR pipeline is a novel and valuable contribution, especially given the diversity and inconsistency of chord symbol notations. The dataset of 293 handwritten lead sheets and the aligned symbolic formats provide a much-needed resource for the community. The exploration of different tokenization strategies and the demonstration of how synthetic data and pretraining influence performance are well-executed and informative.\n\nStrengths:\n- Novel problem (handwritten jazz lead sheet OMR)\n- Carefully constructed dataset with region-level annotations\n- Thoughtful tokenization and error metric discussions\n- Solid experimental framework with reproducibility in mind\n\nSuggestions for improvement:\n- A summary figure/table comparing datasets (e.g., CoCoPops, ChoCo, this dataset) would help emphasize uniqueness.\n- Some technical sections (e.g., tokenization and metric rationale) would benefit from additional diagrams or summaries.\n- The discussion on chord equivalence is insightful; incorporating this more explicitly into training/evaluation might be a next step.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nStrong accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThis paper presents a compelling and timely contribution to the field of Optical Music Recognition (OMR) by addressing the particularly challenging domain of handwritten jazz lead sheets. It introduces a novel dataset of 293 annotated lead sheets, a thoughtful evaluation pipeline, and an adapted transformer-based model trained with synthetic pretraining and symbolic-aware tokenization.\n\nThe reviews converge on the significance of the dataset and the careful attention to tokenization and alignment strategies. While some reviewers expressed reservations due to the model architecture being previously established and the dataset's modest size, these factors do not diminish the impact of the paper\u2019s contributions.\n\nAll reviewers acknowledged the relevance of the topic, its novelty in scope and application, and the scientific soundness of the methodology. The primary value of this paper lies not in architectural innovation, but in advancing OMR research by addressing a real-world, underexplored, and musically important use case. The dataset and methods are reusable across music genres, and the experiments are thorough and informative, especially in their treatment of chord symbol challenges.\n\nThe reviewers provided useful suggestions for enhancement:\n\n* Clarifying methodological details (e.g., YOLO fine-tuning, decoding strategies, vocabulary scale).\n* Quantitatively distinguishing between melody and chord recognition errors.\n* Providing additional analysis or augmentations for image data.\n* Addressing minor inconsistencies between images and ground truth representations.\n\nThese are constructive comments that can be easily addressed in the camera-ready version and do not detract from the overall contribution.\n\nThis paper sets a strong precedent for research at the intersection of OMR and jazz studies. Its dataset, methods, and findings will undoubtedly stimulate discourse, inspire follow-up work, and contribute foundational tools and benchmarks to the community.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper presents an Optical Music Recognition (OMR) system for handwritten jazz lead sheets \u2014 a particularly challenging domain due to the diverse handwriting styles and complex musical elements, especially chord symbols. The authors introduce a novel dataset of jazz lead sheets, providing ground truth annotations in both Humdrum **kern and MusicXML formats, and they develop a model tailored for this data.\n\nIt is clear that the authors have put significant effort into constructing the dataset. Handwritten jazz lead sheets exhibit a high degree of variation and unique characteristics, such as inconsistent chord notations, as discussed in Section 3.4. The resulting dataset includes 2,021 staff regions from 293 handwritten sheets and 2,208 regions from 326 synthetic scores. The authors employed a region identification system using Ultralytics YOLOv8, combined with manual verification to ensure data quality. For the OMR model, they adopted the previously proposed Sheet Music Transformer by Rios-Fila et al., applying only minimal modifications.\n\nThis work appears to be an important first step in addressing the specific challenges of handwritten jazz lead sheets, though there remains significant room for improvement. As the authors acknowledge, the transformer-based model may require more substantial adaptation to handle the peculiarities of this dataset effectively. Additionally, I believe the OMR performance \u2014 particularly the recognition of chord symbols \u2014 could benefit greatly from integrating techniques from the field of Optical Character Recognition (OCR), especially systems designed for complex or cursive handwriting.\n\nAlthough the model used is not novel and is only lightly modified, this paper holds significance as the first to focus specifically on handwritten jazz lead sheets. The dataset it provides lays the foundation for future research in this area. For these reasons, I believe the work is worthy of presentation at the conference.",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper presents the first dedicated OMR dataset of handwritten jazz lead sheets, addressing a relevant and previously underexplored area within the OMR community. Lead sheet handwriting is still common among jazz players and students who train their skills.\n\nIn general, the paper is adequatly written and easy to follow. The work is methodologically thorough, the preprocessing and evaluation design are in their way novel, considering existing OMR tasks. The chord symbol alignment and tokenization approaches are also interesting. On the other hand, the used model has already been previously proposed by (possibly different) authors. However, the paper includes two significant contributions (dataset and methodology), which I deem satisfactory, considering the page limit of the conference.\nOverall, the paper represents a foundational step toward practical OMR, including currently used hand written scores, and including a new genre (in comparison to traditional OMR tasks).",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis is a good paper. It is written clearly and explains everything from the background to the details of the method in a way that is both approachable and interesting. While it does not present a novel model architecture, it gives a good example of collecting a dataset for a new OMR variant (handwritten jazz lead sheets) and fine-tuning an existing model on it.\n\nStrengths:\n* New open dataset for a challenging OMR variant\n* Important details (e.g. standardization of chord spellings) are taken care of in the dataset cleaning process and discussed thoroughly\n* Explored alternative Humdrum tokenization schemes\n* Comprehensive evaluations, both quantitative and qualitative\n* Clear explanations with graphical examples\n* Code (including new Humdrum tokenizers) and model weights are released\n\nWeaknesses:\n* Dataset is rather small\n* The model is not novel\n\nAdditional comments:\n* Line 185: Unclear, did authors fine tune YOLOv8 for region identification? Is this work published anywhere, or can authors add some detail on this?\n* Line 313: This described greedy decoding, is this really the sampling method used (as opposed to beam search)?\n* Line 339: Vocab size of 1762 is not \u201cvery large\u201d compared to popular transformer models (e.g. BERT has 30,522, GPT2 has 50,257). Anyway, since the music sequences here are very short, sequence length is less of a problem.\n* Line 525: Data augmentation can (and should) also be applied to the scanned lead sheets (e.g. adding noise, rotating, cropping).\n* Figure 3: Why does the example\u2019s ground truth include clef, time signature and key signature, while the image does not?\n* Section 5.3: Considering the qualitative observation that most problems are in chord symbols, I\u2019d be interested in a quantitative evaluation of melody errors vs chord errors.\n* Line 493: How can the model know the key of the piece, when it only receives the region in Figure 3, which doesn\u2019t include the key signature? (Or is the image cropped?) This might point at a discrepancy in the dataset between the image and the ground truth."
    },
    "session": "6"
  },
  {
    "content": {
      "TLDR": "The guitar is one of the most popular musical instruments, and numerous pedagogical tools have been developed to support learners. They rely on vast collections of songs, sheet music, and tablatures, making it challenging for guitarists to navigate and identify pieces that are both pedagogically relevant and aligned with their musical interests. \n\nWe introduce a simple multi-criteria rule-based model to assess both the difficulty of learning a piece and the skill level of a guitarist, taking into account musical and technical criteria. The models provides personalized recommendations that help learners progress efficiently, considering parts within songs, but also multiple versions of the same part, accounting for simplified adaptations or different playing styles, and finally exercises used to progressively learn each part version. \n\nWe implement and evaluate this approach in the context of rhythm guitar in popular music, using a dataset designed for the proprietary application Guitar Social Club. Expert evaluation of 77 recommendations for 8 user profiles of varying levels indicate that in 82% of cases, the model provides relevant recommendations. While the full dataset remains proprietary, we release under open licenses the code along with a sub-corpus containing annotated difficulties for 319 versions of 110 parts from 40 songs.",
      "abstract": "The guitar is one of the most popular musical instruments, and numerous pedagogical tools have been developed to support learners. They rely on vast collections of songs, sheet music, and tablatures, making it challenging for guitarists to navigate and identify pieces that are both pedagogically relevant and aligned with their musical interests. \n\nWe introduce a simple multi-criteria rule-based model to assess both the difficulty of learning a piece and the skill level of a guitarist, taking into account musical and technical criteria. The models provides personalized recommendations that help learners progress efficiently, considering parts within songs, but also multiple versions of the same part, accounting for simplified adaptations or different playing styles, and finally exercises used to progressively learn each part version. \n\nWe implement and evaluate this approach in the context of rhythm guitar in popular music, using a dataset designed for the proprietary application Guitar Social Club. Expert evaluation of 77 recommendations for 8 user profiles of varying levels indicate that in 82% of cases, the model provides relevant recommendations. While the full dataset remains proprietary, we release under open licenses the code along with a sub-corpus containing annotated difficulties for 319 versions of 110 parts from 40 songs.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Zakaria Hassein-Bey",
        "Yohann Abbou",
        "Alexandre d'Hooge",
        "Mathieu Giraud",
        "Gilles Guillemain",
        "Aur\u00e9lien Jeanneau"
      ],
      "authors_and_affil": [
        "Zakaria Hassein-Bey (Universit\u00e9 de Lille)",
        "Yohann Abbou (Guitar Social Club)",
        "Alexandre d'Hooge (Universit\u00e9 de Lille)",
        "Mathieu Giraud (CNRS, Universit\u00e9 de Lille)*",
        "Gilles Guillemain (Guitar Social Club)",
        "Aur\u00e9lien Jeanneau (Universit\u00e9 de Lille)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Personalization",
        "Applications",
        "Musical features and properties",
        "Music training and education",
        "User-centered evaluation",
        "Human-centered MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1uEZlF6cyf7W6vY24WDxT_Wn5YlkPdH1U/preview",
      "poster_pdf": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-6-what-song-now",
      "title": "What song now? Personalized Rhythm Guitar Learning in Western Popular Music",
      "video": ""
    },
    "forum": "116",
    "id": "116",
    "pic_id": "",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Automatic Music Transcription (AMT) is a central task\nwithin MIR, enabling various subsequent applications. De-\nspite advancements thanks to deep learning, improving\nAMT remains challenging due to the scarcity of large,\nhigh-quality annotated datasets. Recognizing pitches in\nmulti-instrument settings beyond solo piano is particularly\ndifficult, as models struggle to generalize across domains\ndue to dataset biases and overfitting. AMT research ap-\npears to have hit a glass ceiling, where further progress is\ndifficult to achieve and to measure. To address this, we\npropose cross-version consistency (CVC)---an annotation-\nfree evaluation framework that measures a model\u2019s tran-\nscription consistency across different recordings of the\nsame musical work. We formalize this concept and sys-\ntematically analyze its relationship with standard evalua-\ntion metrics on the AMT subtask of multi-pitch estimation.\nOur results show that CVC is closely tied to standard evalu-\nation metrics and enables model assessment using only un-\nlabeled multi-version datasets, making it particularly valu-\nable in domains where annotated data is scarce but multi-\nversion recordings are easy to obtain, such as orchestral\nmusic. Beyond this, we argue that CVC is, by design, a\ndesirable property for transcription models and our results\nindicate that it can provide insights into a model\u2019s robust-\nness, i. e., its ability to generalize to out-of-domain data.",
      "abstract": "Automatic Music Transcription (AMT) is a central task\nwithin MIR, enabling various subsequent applications. De-\nspite advancements thanks to deep learning, improving\nAMT remains challenging due to the scarcity of large,\nhigh-quality annotated datasets. Recognizing pitches in\nmulti-instrument settings beyond solo piano is particularly\ndifficult, as models struggle to generalize across domains\ndue to dataset biases and overfitting. AMT research ap-\npears to have hit a glass ceiling, where further progress is\ndifficult to achieve and to measure. To address this, we\npropose cross-version consistency (CVC)---an annotation-\nfree evaluation framework that measures a model\u2019s tran-\nscription consistency across different recordings of the\nsame musical work. We formalize this concept and sys-\ntematically analyze its relationship with standard evalua-\ntion metrics on the AMT subtask of multi-pitch estimation.\nOur results show that CVC is closely tied to standard evalu-\nation metrics and enables model assessment using only un-\nlabeled multi-version datasets, making it particularly valu-\nable in domains where annotated data is scarce but multi-\nversion recordings are easy to obtain, such as orchestral\nmusic. Beyond this, we argue that CVC is, by design, a\ndesirable property for transcription models and our results\nindicate that it can provide insights into a model\u2019s robust-\nness, i. e., its ability to generalize to out-of-domain data.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Yannik Venohr",
        "Yiwei Ding",
        "Christof Weiss"
      ],
      "authors_and_affil": [
        "Yannik Venohr (University of W\u00fcrzburg)*",
        "Yiwei Ding (University of W\u00fcrzburg)",
        "Christof Weiss (University of W\u00fcrzburg)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Evaluation metrics",
        "Music transcription and annotation",
        "MIR tasks",
        "Evaluation, datasets, and reproducibility",
        "Evaluation methodology",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1XGsASoZQ4j6RugwT2gvIHShqYePB2NMN/preview",
      "poster_pdf": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-3-towards-robust-music",
      "title": "TOWARDS ROBUST MUSIC TRANSCRIPTION BY MEASURING CROSS-VERSION CONSISTENCY IN WESTERN CLASSICAL MUSIC",
      "video": ""
    },
    "forum": "122",
    "id": "122",
    "pic_id": "",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Richard Wagner's four-opera cycle \"Der Ring des Nibelungen\" presents unique challenges for music analysis due to its scale, structural complexity, and intricate relationship between music and drama. While harmony plays a central role, additional factors such as tempo, instrumentation, and leitmotifs significantly contribute to formal organization. This study combines computational and musicological approaches to analyze \"Siegfried\", the third opera in the Ring cycle, focusing on Act III. By integrating symbolic score data, annotated recordings, and libretto information, we visualize harmonic progressions, tempo variations, and dramatic interactions to reveal large-scale structural developments in Wagner's music. Our interdisciplinary analysis highlights the role of tonal stability and instability, tempo contrasts, and character interactions in shaping form. More broadly, this study demonstrates how computational methods can complement traditional musicological analysis, offering a structured framework for studying complex operatic works. Our findings contribute to a deeper understanding of Wagner's compositional techniques and pave the way for further research integrating computational tools into opera analysis.",
      "abstract": "Richard Wagner's four-opera cycle \"Der Ring des Nibelungen\" presents unique challenges for music analysis due to its scale, structural complexity, and intricate relationship between music and drama. While harmony plays a central role, additional factors such as tempo, instrumentation, and leitmotifs significantly contribute to formal organization. This study combines computational and musicological approaches to analyze \"Siegfried\", the third opera in the Ring cycle, focusing on Act III. By integrating symbolic score data, annotated recordings, and libretto information, we visualize harmonic progressions, tempo variations, and dramatic interactions to reveal large-scale structural developments in Wagner's music. Our interdisciplinary analysis highlights the role of tonal stability and instability, tempo contrasts, and character interactions in shaping form. More broadly, this study demonstrates how computational methods can complement traditional musicological analysis, offering a structured framework for studying complex operatic works. Our findings contribute to a deeper understanding of Wagner's compositional techniques and pave the way for further research integrating computational tools into opera analysis.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Pascal Schmolenzky",
        "Stephanie Klauk",
        "Rainer Kleinertz",
        "Christof Wei\u00df",
        "Meinard M\u00fcller"
      ],
      "authors_and_affil": [
        "Pascal Schmolenzky (Universit\u00e4t des Saarlands)*",
        "Stephanie Klauk (Institut f\u00fcr Musikwissenschaft, Universit\u00e4t des Saarlandes)",
        "Rainer Kleinertz (Institut f\u00fcr Musikwissenschaft, Universit\u00e4t des Saarlandes)",
        "Christof Wei\u00df (Center for Artificial Intelligence and Data Science, Universit\u00e4t W\u00fcrzburg)",
        "Meinard M\u00fcller (International Audio Laboratories Erlangen)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Digital musicology",
        "Musical features and properties",
        "Harmony, chords and tonality",
        "Computational musicology",
        "Systematic musicology",
        "Music signal processing",
        "Rhythm, beat, tempo",
        "MIR fundamentals and methodology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1CnIaSWDqmx8A3lvXuKmeSuipcWzn0LOx/preview",
      "poster_pdf": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-3-a-multidimensional-approach",
      "title": "A Multidimensional Approach to Opera Analysis: Harmony, Tempo, and Dramatic Interaction in Wagner's Siegfried",
      "video": ""
    },
    "forum": "125",
    "id": "125",
    "pic_id": "",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "2"
  },
  {
    "content": {
      "TLDR": "The study of piano rehearsals can offer interesting insights into the strategies adopted by a pianist in order to learn, interpret and eventually perform musical pieces. The analysis of rehearsal processes requires computational methods that differ from those used for piano performance, due to challenges like mistakes, repetitions of musical segments, or forward and backward skips to sections in the piece. The scarcity of publicly available rehearsal data limits the empirical understanding of these challenges. We release the Rach3 MIDI Dataset, an openly available collection of MIDI files containing more than 750 hours of recordings of piano rehearsals and corresponding MusicXML scores by four pianists (3 advanced, 1 beginner), collected over a period of more than 4 years. This dataset records the progression of pianists learning new repertoire, as well as practicing familiar pieces, all in the Western Classical tradition. We describe the rehearsal piece identification process used for automatically labeling a portion of the data in this release. Furthermore, we use the Rach3 data to highlight several challenges and future research directions pertaining to the computational analysis of piano rehearsals, specifically symbolic rehearsal-to-score alignment, rehearsal structure analysis, and automatic mistake\nidentification.",
      "abstract": "The study of piano rehearsals can offer interesting insights into the strategies adopted by a pianist in order to learn, interpret and eventually perform musical pieces. The analysis of rehearsal processes requires computational methods that differ from those used for piano performance, due to challenges like mistakes, repetitions of musical segments, or forward and backward skips to sections in the piece. The scarcity of publicly available rehearsal data limits the empirical understanding of these challenges. We release the Rach3 MIDI Dataset, an openly available collection of MIDI files containing more than 750 hours of recordings of piano rehearsals and corresponding MusicXML scores by four pianists (3 advanced, 1 beginner), collected over a period of more than 4 years. This dataset records the progression of pianists learning new repertoire, as well as practicing familiar pieces, all in the Western Classical tradition. We describe the rehearsal piece identification process used for automatically labeling a portion of the data in this release. Furthermore, we use the Rach3 data to highlight several challenges and future research directions pertaining to the computational analysis of piano rehearsals, specifically symbolic rehearsal-to-score alignment, rehearsal structure analysis, and automatic mistake\nidentification.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Alia Morsi",
        "Suhit Chiruthapudi",
        "Silvan Peter",
        "Ivan Pilkov",
        "Laura Bishop",
        "Akira Maezawa",
        "Xavier Serra",
        "Carlos Eduardo Cancino-Chac\u00f3n"
      ],
      "authors_and_affil": [
        "Alia Morsi (MTG)",
        "Suhit Chiruthapudi (Johannes Kepler University Linz)",
        "Silvan Peter (Johannes Kepler University Linz)",
        "Ivan Pilkov (Johannes Kepler University Linz)",
        "Laura Bishop (University of Oslo)",
        "Akira Maezawa (Yamaha Corporation)",
        "Xavier Serra (Music Technology Group)",
        "Carlos Eduardo Cancino-Chac\u00f3n (Johannes Kepler University Linz)*"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Symbolic music processing",
        "Fingerprinting",
        "Musical features and properties",
        "Similarity metrics",
        "Novel datasets and use cases",
        "MIR tasks",
        "Expression and performative aspects of music",
        "Evaluation, datasets, and reproducibility",
        "MIR fundamentals and methodology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1Y1UF8G110PPJQcXGJM_W9nxfuSW0Cfre/preview",
      "poster_pdf": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-14-enabling-empirical-analysis",
      "title": "Enabling Empirical Analysis of Piano Performance Rehearsal with the Rach3 MIDI Dataset",
      "video": ""
    },
    "forum": "127",
    "id": "127",
    "pic_id": "",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nDisagree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nStrongly agree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\n- use fingerprinting methods for matching original score and rehearsal notes.\n- ways to collect such dataset (the need of a big time span as well as the large range of the format types of the data signals -e.g. video/audio/symbolic domain).\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nA large piano rehearsal dataset with a large time-span is made publicly available and it highlights challenges and further research directions in practice analysis.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThe paper introduces a new dataset which includes MIDI files associated with piano rehearsals. It is claimed that it is the largest open-source dataset of its kind and an expansion plan is highlighted. The authors provide a solution for rehearsal piece identification since a part of the dataset missed the exact match of original score and rehearsal excerpt. Also, they explore how to group repeated or related musical fragments and finally, they showcase the ways such a dataset can be used in mistake identification, providing some initial results using published algorithms.\n\nThe paper is well-structured in general and provides evidence of the importance of such dataset in various research domains. Also, the supplementary material is very informative. Hence, I suggest an acceptance. However, I found Section 5 being weak, which I explain in-detail below. Also, I provide some additional comments that should be addressed in the revised version.\n \n- In Section 5, the reader would expect a preliminary computational analysis of rehearsal structure, as it is stated in Introduction. Instead, the section was mainly focused on the limitations the authors found of the technique they designed to identify fragments. It would be more informative and impactful if in Section 5 you presented some preliminary results, given the aforementioned limitations, for example by comparing a file being rehearsed by an expert versus a file being rehearsed by a beginner. If this is not possible, then you would need to adjust the text in Introduction of what exactly Section 5 presents. \n\n- For clarity, in line 222, I would suggest you present the percentage of the dataset that needed the fingerprinting step. \n\n- Line 336: need more details about how a \u201cchord\u201d bin is defined.\n\n- Figure 3 font size is very small. I would suggest you reduce the size of the square by including only the non-matching rows, since this part is more interesting.\n\nFurther suggestions:\n- Line 66: I would add GigaMIDI Dataset as well (https://arxiv.org/pdf/2502.17726)\n\n- In the background section, I would add the following study for completeness: A Novel Interface for the Graphical Analysis of Music Practice Behaviors J Sokolovskis, D Herremans, E Chew - Frontiers in Psychology, 2018 \n\n- It's not very common to separate the abstract to multiple paragraphs.I would merge them to one.\n\nMinor comments:\n- Lines 76, 340: missing \u201c.\u201d\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThe paper presents a large piano rehearsal dataset with a large time-span and it highlights challenges and further research directions in practice analysis. All reviewers have decided to accept the submission and we can see potential on collaborations across disciplines with researchers in music perception and cognition, exploring this data further.\n\nBelow, I\u2019m highlighting some key points, however please see the individual reviews for details:\n\n- In Section 5, the reader would expect a preliminary computational analysis of rehearsal structure, as it is stated in Introduction, however this is not the case. Please adjust the text in introduction of what Section 5 actually presents, or edit Section 5, including some preliminary results, given the mentioned limitations.\n\n- For more clarity, provide the insights on the peculiarities of rehearsal (versus the more commonly studied performance) situations in a separate dedicated section. \n\n- Elaborate on definitions and decisions in lines 328-340. \n- Elaborate on annotation procedures (for fragments or mistakes).",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nMost of the manuscript is well-prepared. I am inspired by the down-to-earth manner this manuscript shows in terms of data collection, repertoire selection, and ways to conquer unique challenges. I am also convinced that the future work directions in Sec. 8 are worth exploration, as an amateur classical pianist who has been practicing over 40+ years.\n\nThe quality can be further improved in a few small places, listed below.\n\n1. Line 314: what does TEC stand for?\n2. In general, line 328-340 could benefit from further elaboration. I have difficulty understanding what exactly have been done to generate the self-similarity matrix in Fig. 4. First, is Nbin the length of the fragment divided by 100 ms? Secondly, what exactly does it mean to convolve each \"chord\" vector with a small kernel? Is the convolution kernel 1D or 2D? I suppose it's 1D, and adding an equation may help. Third, in Eq. (1), please add an index to indicate the range of multiplication in the product $\\Pi$. In the same equation, what does the exponent pitch_i mean? I suppose it's not a pitch and the naming of the variable confuses me. \n\nAlso, in Fig. 1b, I am curious how come the music score could look so different to the instances shown below. The plot of the instances contains much more notes than the score, for example. It might be good to explain this in the caption so readers are not puzzled early during their read.",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\n### **Review Summary:**\n\nThis paper presents the <Anonymized> MIDI Dataset\u2014an extensive collection (750+ hours) of piano rehearsals\u2014which fills a major gap in symbolic MIR resources. It includes efforts in piece identification via symbolic fingerprinting, rehearsal structure analysis, and initial steps toward mistake detection. The paper also discusses the substantial challenges of applying performance-score alignment methods to rehearsal data.\n\n---\n\n### **Strengths:**\n\n- Valuable and original dataset focused on real rehearsal data, not just polished performances.\n \n- Timely discussion of limitations in applying alignment techniques to noisy, fragmented rehearsal data.\n \n- Opens new research directions in performance analysis, pedagogy, and mistake modeling.\n \n\n---\n\n### **Weaknesses:**\n\n- The rehearsal structure analysis lacks formal evaluation and is sensitive to parameters.\n \n- Performance-score alignment is discussed clearly, but no solution is proposed\u2014only limitations.\n \n- Annotation procedures (for fragments or mistakes) are not fully explained.",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nWhile several projects have attempted to implement applications that track pianist's performances, the rehearsal context with its non-linearity, non-score-prescribed repetition, and proneness to error is particularly challenging to manage. This new dataset provides opportunities for improving algorithms and software targeting this use-case. \n\nThe paper is well-structured, well-written, and easy to follow. The effort taken in making the set-up and analytical pipeline reproducible is much appreciated, and promises potential future expansion in the availability of this sort of data. \n\nThat said, while the present dataset provides very rich, deep, valuable reflection of the rehearsal behaviour of the four pianists over a long period, the fact that it is \"only\" four participants should be more clearly acknowledged as a limitation in the paper. Note that this is not intended as a criticism of the dataset -- there are good reasons why vastly expanding the set of participants would be unfeasible -- but this should still be spelled out in the paper. In particular, the generalisability of insights on specific rehearsal behaviours is likely limited by this small sample frame, and this should be explicitly stated.\n\nThe first paragraph of Section 4 mentions a switch in practice from recording rehearsal sessions as single takes in the first two years of the project, to recording on a more granular level later. It is stated that this decision was taken \"for practical reasons\" - could you expand on this slightly? It seems like convenience-to-pianist (pressing record once per rehearsal session) may have been sacrificed for convience-to-researcher (having salient segmentation of the recorded data already be performed in-situ, reducing post-processing needs). In particular, it would be important to know how much intervention was required on the part of the pianists in order to reset the recording set-up, as this could have implications regarding ecological validity; ideally one would interfere with the normal rehearsal environment and workflow as little as possible. \n\nThese concerns are all minor and could readily be addressed before camera-ready. I congratulate the authors on this interesting paper and am happy to recommend strong acceptance. \n\nTwo more minor nitpicks to finish: \n* there are some minor errors and formatting (capitalization) inconsistencies in your references -- please do another edit pass over all of these. Particularly, I have spotted typos in [6], [21], [31]; a problem in the URL in [20]; and some issues with format in [17].\n* in following up your references I noticed that the Vienna piano dataset's webpage lists a DOI: 10.21939/4X22. Indeed, resolving this DOI (https://doi.org/10.21939/4X22) forwards to a different version of the website than the one listed in your submission: \nhttps://datasets.mdw.ac.at/datasets/dataset/98ea25fa-2468-43ff-929b-3c926e163583. \nRather than URLs, please always cite the DOI when one is available; it should be assumed to provide the canonical reference."
    },
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Research on music retrieval and recommendation often neglects the fact that a user\u2019s response to a music track depends on contextual factors, such as the composition of the results list, the design of the user interface or the additional media displayed.\nHowever, a body of psychological research suggests that human perception and decision making can be strongly influenced by contextual factors. In particular, an initial positive aesthetic impression of a product may influence a buyer's perception of its features unrelated to appearance, such as utility or reliability, which is a manifestation of a cognitive bias called the halo effect. The work at hand investigates whether an album cover shown to the listener during playback can create a halo effect, influencing the listener's liking of the track. We approach this question by means of a two-stage user study. In the first stage, participants individually rated a series of album covers and music snippets. In the second stage, they were presented with music tracks and album covers (from those they indicated as unfamiliar to them at the first stage) arranged in pairs, such that their least liked tracks were shown with their most liked album covers and vice versa. The results show that displaying an appealing album cover while playing a music track may result in a higher rating of the track and vice versa. We also observe an indication of halo effect created by the perceived degree of matching between the music tracks and the album cover shown.",
      "abstract": "Research on music retrieval and recommendation often neglects the fact that a user\u2019s response to a music track depends on contextual factors, such as the composition of the results list, the design of the user interface or the additional media displayed.\nHowever, a body of psychological research suggests that human perception and decision making can be strongly influenced by contextual factors. In particular, an initial positive aesthetic impression of a product may influence a buyer's perception of its features unrelated to appearance, such as utility or reliability, which is a manifestation of a cognitive bias called the halo effect. The work at hand investigates whether an album cover shown to the listener during playback can create a halo effect, influencing the listener's liking of the track. We approach this question by means of a two-stage user study. In the first stage, participants individually rated a series of album covers and music snippets. In the second stage, they were presented with music tracks and album covers (from those they indicated as unfamiliar to them at the first stage) arranged in pairs, such that their least liked tracks were shown with their most liked album covers and vice versa. The results show that displaying an appealing album cover while playing a music track may result in a higher rating of the track and vice versa. We also observe an indication of halo effect created by the perceived degree of matching between the music tracks and the album cover shown.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Oleg Lesota",
        "Anna Hausberger",
        "Ivanna Pshenychna",
        "Oleksandr Shvydanenko",
        "Olha Yehorova",
        "Markus Schedl"
      ],
      "authors_and_affil": [
        "Oleg Lesota (Johannes Kepler University)*",
        "Anna Hausberger (Johannes Kepler University)",
        "Ivanna Pshenychna (Johannes Kepler University)",
        "Oleksandr Shvydanenko (Johannes Kepler University)",
        "Olha Yehorova (Johannes Kepler University)",
        "Markus Schedl (Johannes Kepler University)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Music retrieval systems",
        "Applications",
        "User behavior analysis and mining, user modeling",
        "Music recommendation and playlist generation",
        "Knowledge-driven approaches to MIR",
        "Cognitive MIR",
        "Music interfaces and services",
        "Human-centered MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1to5r7XN2NFTztW3ziSGS8znWMJlaLrQd/preview",
      "poster_pdf": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-8-investigating-music-track",
      "title": "Investigating Music Track Liking in the Halo of Album Covers",
      "video": ""
    },
    "forum": "128",
    "id": "128",
    "pic_id": "",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nDisagree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nDisagree\n\n**Q5 ( Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chosen, otherwise write \"n/a\"))**\n\nThe work could include machine learning (ML) works that employ album cover image and audio information tackling downstream tasks such as music classification in the multi-modal setup. For instance, following two works can be under such category:\n\nSergio Oramas, Francesco Barbieri, Oriol Nieto, Xavier Serra:\nMultimodal Deep Learning for Music Genre Classification. Trans. Int. Soc. Music. Inf. Retr. 1(1): 4-21 (2018)\nIgor Vatolkin, Cory McKay:\nMulti-Objective Investigation of Six Feature Source Types for Multi-Modal Music Classification. Trans. Int. Soc. Music. Inf. Retr. 5(1): 1-19 (2022)\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nDisagree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nAs contextual material, the album cover can affect the perception/appreciation of the music while listening. This insight opens up a number of research directions, both in music psychology and MIR applications such as recommender systems.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nAlbum cover art co-presented during music listening can affect listeners' perception of music.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\n# Summary\n\nThe work investigates the halo/horn effect of the presence of album cover art as a contextual material in music preference (liking) when it is presented during music listening. An two-stage experiment is designed, where the first phase collect each participants' preference on music and album cover art independently as a baseline, and in the second phase participants indicate the preference on each music with which are paired with album covers of the most significant preference difference to the music in the first round. The result indicates that the delta between the preference scores of music on the two stages positively correlates with the album covers, which implies the halo/horn effect of album covers on the music when presented. Additionally, the matching between audio and cover is recorded in the second phase, which shows the lesser, but positive effect on the delta of preference, might suggest that the matching can be contributed as the latent factor.\n\n\n# Major Comments\n\n## Strengths\n- The work investigates a novel topic. The effectiveness of album cover art has already been studied considerably, but mainly on machine learning (ML) downstream applications in the MIR field, which generally positively affects the automatic (machine) music understanding. To my knowledge, psychological studies involving human perception study, which are relatively less studied in MIR, could not only explain the positive effect in applications, but also build fundamental understanding of the phenomenon.\nThe experimental design is reasonably sound, and the result shows the effect's existence (even with some noise concern that will be followed in the next subsection). This has several interesting implications for applications and opens some future research directions.\n- The work is well-read in general.\n\n## Weaknesses\n- The experimental design could be improved:\n 1. The primary material can be more representative:\n * The study album covers are heavily sampled, but in a skewed way that filters out ones containing any text. This results in about 1% of the covers from the study population, potentially implying that the remaining samples might be atypical. An alternative approach might be uniform sampling followed by anonymization (i.e., anonymization of title/artist/album name text), which might be better for the distribution of images.\n * Music data might also not be the most representative, as it includes the least popular songs from each popular genre. Popularity might be confounded with qualities of musical/production aspects of the song (i.e., songs can be unpopular due to their poor production, or musically too atypical, etc.). This could make the remaining study materials less representative.\n 2. The experimental procedure could be less noisy by controlling a few factors:\n * The participants are allowed to listen to music songs for 5 seconds to 15 seconds, which might add variability in the result as the stimuli vary arbitrarily per participant.\n * As it is not indicated that the participants are from a US representative population (which one could choose in the prolific platform), I assume that there is a chance that the participants' distribution is less representative.\n\n\n# Minor Comments\n- p3.l219 \"In the second stage, ... a song snippet.\": Is this randomized? Or with sorting applied to the previous paragraph?\n- p3.l258 \"The agreement score ... computed analogously.\": This seems to be an identical definition to precision@K, which is popularly used in recommender systems literature\n- p3.l264 \"To ensure that ... over the 100 runs.\": The variance of these individual bootstrap values would also be interesting.\n- p3.l270 \"... Spearman's rank correlation ... second stage ($a_s^1$)\": Why Spearman's rho is used? It may be due to the standardization, which transforms the integer scale to something closer to a Gaussian distribution. However, as Kendall's Tau is non-parametric, I assume the result would be almost identical.\n- p4.l328 \"This suggests that ... affected track ranking.\": There is no control group (i.e., coming to the second stage and doing the same thing as the first round, without being presented with album covers). There might be a baseline fluctuation because people can forget things.\n- p4.l332 \"... (with the threshold 0.05) ...\": I assume it is the threshold on p-value, but it could be better clarified.\n- p5.l440 \"... to knew ...\": It seems \"new\" is intended here.\n- p6. \"5. LIMITATIONS & FUTURE WORK\": it could be interesting to check (as a side study) to see how the participants generally care about the album cover in their daily lives. Some might be keen to check the album cover if they are music fans, while others might not even see it while using streaming services. Controlling this factor might get a more precise result\n- p6.l474 \"The study at hand ... streaming platforms.\": Is it a representative US population (from Prolific option)?\n- p6.l489 \"Future work could ... first listening session.\": Also, the experimental design could add a third session, to see if the effect decreases?\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\n# Summary of the reviews\n\n## Strengths:\n- The topic is novel, relevant, and interesting, and opens new research directions\n- The experimental design overall is sound and reproducible to some extent\n- The presentation of the work is generally good and easy to follow\n\n## For improvement:\n- A few methodological limitations can be found:\n * Experimental materials (e.g., selection of songs and album covers) need more elaboration and justification\n * Participants study population can be better representative\n- It would benefit from a more in-depth introduction of related work surveys, connected to methodological choices\n\nOther notable points brought by reviewers are:\n- Explicit mention of the ethics approval procedure seems to be missing\n- The work could offer the experimental design as a product of the work, which can be reused for future works in an extended context\n- Presentation of the result section could be improved (e.g., using a correlation matrix)\n\n\n# Overall comment on the decision\n\nThe reviewers all agreed that the work conducted a novel and interesting study, opening future research directions. The experimental design is overall sound and well-presented. Reviewers also found a few points to be improved: 1) methodological limitations such as representativeness of participants and study materials (i.e., study samples of covers and songs) and 2) related work, including engineering-centered MIR papers involving the album covers as part of data. The overall decision was unanimous, indicating that all reviewers agreed that the work is a valuable contribution to the conference, especially with these improvements.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\ni love the idea of this submission - the research question is really interesting and well-supported by background literature. I had some questions about the method: \nSelecting 40 album covers from 38746 - i'd like to know more about the choices. And 5 most pop + 8 least pop x 5 genres seems to be more than 40 tracks, so again, i'd love to know the details of the media selected for the study (also what 15 sec of 30 were selected). and if people are getting individualised study media in the second step, is the sample large enough (power) for the analyses?\n\ni found the results section hard to easily engage with. but i think this piece of research will create lively discussion amongst the community.",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nStrengths of this paper:\n\n>> A very interesting topic that is worth the exploration.\n\n>> The study has opened up further integration of psychological perspectives in to MIR research and practices.\n\n>> The research questions are clearly presented.\n\n>> Various research design decisions are fairly well-justified, e.g., 5 seconds as the minimum duration of each snipper, participation in the second part no earlier than 36 hours, etc.\n\n>> Results and their implications are clearly presented.\n\n>> There is potential for further research on\n\nIssues/potential improvement of this paper:\n\n>> As mentioned in the abstract, there could be other contextual factors affecting users\u2019 preferences of music tracks. If space allows, perhaps the literature review can briefly cover what other of these contextual factors are, and also how the study\u2019s design attempted to minimize the confounding influences of these other factors.\n\n>> Perhaps more details about the music tracks can be supplied, e.g., instrumental vs. vocal music, if with lyrics then what are the languages, etc.\n\n>> What are the demographics of the participants? (It has been briefly mentioned in the Limitations section, though more demographic information should have been described earlier in the paper.) Many factors could come into play, e.g., their music listening habits/experience, the preferences over multimedia materials (e.g., visual vs. auditory)\n\n>> As pinpointed in the Conclusion, the album covers were supposed to be \u201cunfamiliar\u201d to the users, though it is possible that some visual elements in an album cover art might be seen by the users before \u2014 even though the entire album cover is \u201cnew\u201d to the users.\n\n>> While the current way of presenting the results seem clear, authors can consider using a correlation matrix, which would help fast readers understand the results better.\n\n>> In addition to practical implications, authors can think about what methodological contributions this study (and/or their further endeavours) can make, such as the experimental design for research in similar directions (e.g., matching between contextual media and listeners\u2019 preferences of tracks) and/or the modalities of data (e.g., incorporating multimodal data for the evaluation), etc.",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper describes a user study looking into context of music recommendations - specifically, album art. My overall evaluation is to weakly accept this paper, as it has added value to the ISMIR community, but there is room for improvement in several aspects.\n\nStrengths:\n\nGeneral topic: The investigation of the halo effect in this specific domain and context has not been done before (though is not ground-breaking), and the authors discuss how results from this study may impact future work in a broader application area.\n\nWriting and structure: The paper is written very well and has a clear structure. It is useful that key findings are outlined separately, though it sometimes does lead to some duplicate information.\n\nIntroduction: The context and research gaps are provided well.\n\nMethod: I appreciate that this is a user study, focusing on measuring the halo/horn effect in a novel context. It is clearly described how songs and album covers were selected, and familiarity is taken into account and controlled for (though it still may be that participants recognize an artist, but not the specific song). The evaluation metrics seem appropriate.\n\nResults: The tables and graphs are helpful in understanding the insights, which are clearly described.\n\nReproducibility: The paper provides all questions asked in the study, and the final dataset that was used. \n\n\nWeaknesses:\n\nAbstract: The last sentence is put somewhat too strong based on the presented evidence, it would be good to introduce some of the nuance that is descibed in the paper.\n\nResearch questions: It could be made more clear what the difference is between the two RQs and the approach to answer them. The questions are separately answered as described in the method section, with RQ1 seemingly looking into changes in ranking due to having an album cover present yes/no, and RQ2 looking into changes in ranking due to the appeal of the album cover. However, as both influence each other, it could be clarified that they are not completely independent. Also, the phrasing implies only RQ2 is measuring the halo/horn effect, but is RQ1 effectively not also describing this effect? \n\nMethod: While some choices in the study are supported (e.g., number and length of samples), it is not clear whether the method as a whole was based on any previous work. The decisions on the song-album matching, as well as the survey questions, should be validated in order to know whether they actually measure what is intended, but it is unclear if they were. How do other works measure halo/horn effect?\nSecondly, as the study contains several steps and data/participant filtering mechanisms, it would be helpful to include a flowchart of the process to visualize it.\nThirdly, as the specific UI might have slightly influenced the study (as also mentioned in limitations), it would have been good to also include one or several screenshots.\n\nEthical considerations: Even though this is a user study, the authors do not mention any ethical considerations or approval of an ethical review board.\n\nResults: Perceived matching between album and song is described in the context of RQ2 as a control question, but in the results, it is mentioned as being relevant to halo/horn effects. The introduction and RQs do not mention song/album matching in the context of these effects, so it is unclear how this survey question came about and whether it helps answering RQ2. \n\nDiscussion: There is no comparison with insights from previous work, which makes it more difficult to properly assess the contribution of the current work. \n\nImplications: In a real-life music listening setting, listeners will likely see the album cover before actually hearing the music, as opposed to receiving both at the same time. Also, users are likely to receive recommendations in the context of a playlist, possibly also mixed in with songs/album covers that are familiar to them. It would be good if expected changes in outcome compared to this study\u2019s, were discussed."
    },
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Solo piano music, despite being a single-instrument medium, possesses significant expressive capabilities, conveying rich semantic information across genres, moods, and styles. However, current general-purpose music representation models, predominantly trained on large-scale datasets, often struggle to captures subtle semantic distinctions within homogeneous solo piano music. Furthermore, existing piano-specific representation models are typically unimodal, failing to capture the inherently multimodal nature of piano music, expressed through audio, symbolic, and textual modalities. To address these limitations, we propose PianoBind, a piano-specific multimodal joint embedding model. We systematically investigate strategies for multi-source training and modality utilization within a joint embedding framework optimized for capturing fine-grained semantic distinctions in (1) small-scale and (2) homogeneous piano datasets. Our experimental results demonstrate that PianoBind learns multimodal representations that effectively capture subtle nuances of piano music, achieving superior text-to-music retrieval performance on in-domain and out-of-domain piano datasets compared to general-purpose music joint embedding models. Moreover, our design choices offer reusable insights for multimodal representation learning with homogeneous datasets beyond piano music.",
      "abstract": "Solo piano music, despite being a single-instrument medium, possesses significant expressive capabilities, conveying rich semantic information across genres, moods, and styles. However, current general-purpose music representation models, predominantly trained on large-scale datasets, often struggle to captures subtle semantic distinctions within homogeneous solo piano music. Furthermore, existing piano-specific representation models are typically unimodal, failing to capture the inherently multimodal nature of piano music, expressed through audio, symbolic, and textual modalities. To address these limitations, we propose PianoBind, a piano-specific multimodal joint embedding model. We systematically investigate strategies for multi-source training and modality utilization within a joint embedding framework optimized for capturing fine-grained semantic distinctions in (1) small-scale and (2) homogeneous piano datasets. Our experimental results demonstrate that PianoBind learns multimodal representations that effectively capture subtle nuances of piano music, achieving superior text-to-music retrieval performance on in-domain and out-of-domain piano datasets compared to general-purpose music joint embedding models. Moreover, our design choices offer reusable insights for multimodal representation learning with homogeneous datasets beyond piano music.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Hayeon Bang",
        "Eunjin Choi",
        "Seungheon Doh",
        "Juhan Nam"
      ],
      "authors_and_affil": [
        "Hayeon Bang (KAIST)*",
        "Eunjin Choi (KAIST)",
        "Seungheon Doh (KAIST)",
        "Juhan Nam (KAIST)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Representations of music",
        "Knowledge-driven approaches to MIR",
        "Music retrieval systems",
        "Applications"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1GigyVtI1MlCGC3wM_7wv58VNj27xXme9/preview",
      "poster_pdf": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-3-pianobind-a-multimodal",
      "title": "PianoBind: A Multimodal Joint Embedding Model for Pop-piano Music",
      "video": ""
    },
    "forum": "129",
    "id": "129",
    "pic_id": "",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nDisagree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nDisagree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nStrongly agree\n\n**Q5 ( Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chosen, otherwise write \"n/a\"))**\n\nThe abstract states that their \"design choices offer reusable insights\" but I did not see clear discussion of insights beyond using more modalities can be helpful in an embedding problem.\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nDisagree\n\n**Q10 (Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chose, otherwise write \"n/a\"))**\n\nWhile overall the work appears to be sound, there is one sentence that causes me some pause. In section 5.1.1, the authors state that \"the results demonstrate that the pre-training and fine-tuning approach consistently outperforms the combined training across all modality configurations and metrics.\" This is not true as shown in Table 1, where Audio for In-domain at R@1 does better in combined training and Symbolic for out-of-domain at R@5 and R@10 does better in combined training. In fact, when I saw these results in the table, I was hoping that the authors would address these cases and hypothesize why in these cases there would be results that are against our intuition.\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nStrongly agree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nDisagree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe abstract states that their \"design choices offer reusable insights\" but I did not see explicit discussion of insights beyond using more modalities can be helpful in an embedding problem. \n\nI believe that the paper may obliquely offer the reusable insights 1) that more modalities improve results and 2) that pre-training + fine-tuning also improve results, but both of these feel intuitive.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nAn embedding system that makes use of 3 modalities instead of 2 and makes use of both generalized and task-specific training sets will provide more meaningful embeddings than other systems.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper presents PianoBind that leverages symbolic (ie. MIDI), audio, and text to create an effective embedding space for pop-piano music. I appreciated that the authors spent considerable time in the introduction and related works section to make the careful point that piano music, while based on a single instrument, is a more nuanced instrument than others, offering complex polyphonic pieces. This point is an important one, especially for those that seek to treat piano music as \"just a single instrument.\" \n\nThe paper has many strengths, especially in how the authors contextualize their proposed method on existing work. The authors also offer a clear vision for how the PianoBind system addresses weaknesses in previously deployed systems. The depth and quantity of their evaluation studies are also impressive. \n\nThere are a few missed opportunities in this work, namely in the presentation of the work itself. As noted above, the authors could be more explicit about their reusable insights.\n\nI also appreciated that the authors have made a demo and made their code available to the community.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThe reviewers found many strengths in this work, including the well-articulated motivation, well-supported claims, and clear writing. Reviewers 1 and 2 as well as the meta-reviewer were moved by the specialized nature of the work that reflects the multi-modality nature of piano. \n\nThe strengths and weaknesses of the evaluation methods for this work are discussed across the reviews. The difference in the opinions between Reviewer 3 and the rest of the reviewers could potentially be due to the presentation of the work (and accompanying evaluation section). Both Reviewers 1 and 2 offer suggestions and questions about the evaluation section that could help the authors adjust their paper. In thinking about the details in review 1, I would recommend that the authors be a bit more forward about the limitations of their data, but in context of the task they are doing. \n\nThere are a number of further details in the individual reviews on the above points as well as other suggestions to strengthen the paper.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper explores training a multi-modal representation learning model for text-based music retrieval in a specific context, Pop piano music. The authors claim that SoTA models trained over large and diverse datasets of music can miss the subtleties that distinguish performances of piano music. \n\nThis claim is supported by the results in Section 5, as the retrieval performance is best with the model trained on a piano specific dataset. Additionally, there is a significant difference in retrieval performance between the two types of training strategies, with pre-training & fine-tuning the clear winner. Not all researchers have access to industrial-sized datasets for their specific domain. However, these results suggest that collecting a smaller/expertly annotated dataset to fine-tune a larger/noisier dataset could be a successful strategy.\n\nOne thing that could be added to the results section is an example of a query from the evaluation set paired with results from PianoBind and one of the broadly-trained CLAMP models. It could be useful to see what sorts of qualitative improvements PianoBind provides. Are there words that are rare outside of Pop piano music, but PianoBind has learned what content they refer to? I think some examples would help give readers confidence that this research could apply to their sub-genre as well.\n\nFor the final training objective, I am curious if the authors tried anything other than equal weighting of audio-text and MIDI-text contrastive losses. It makes intuitive sense to give these equal importance, but it would have been interesting to see an ablation study to explore this.\n\nThis paper provides some interesting insights for researchers working with highly specialized types of music. The authors make reasonable design choices and compare their results to SoTA models in this problem space. I think the main drawbacks here are the limited size of the evaluation datasets and the lack of evidence to support the effectiveness of this strategy on other genres. Nonetheless, I think this would be a useful contribution to the conference and I recommend acceptance.",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nSummary\n\nThis paper presents PianoBind, a trimodal joint embedding model specifically designed for solo piano music. It effectively integrates audio, symbolic (MIDI), and textual modalities to capture nuanced semantic attributes such as genre, mood, and style. PianoBind is trained using a multi-source strategy that combines large-scale weakly aligned data with small-scale expert-annotated data through pre-training and fine-tuning processes. Experiments conducted on both in-domain (PIAST-AT) and out-of-domain (EMOPIA-Caps) datasets demonstrate that PianoBind significantly outperforms general-purpose models in text-to-music retrieval, with trimodal learning proving to be more effective than bimodal approaches.\n\nStrengths\n\n- The paper addresses a timely and relevant problem in music representation: capturing fine-grained semantics in solo piano music through multi-modal modeling.\n\n- The evaluation framework is sound, with both in-domain and out-of-domain tests, and clearly demonstrates the advantages of the proposed trimodal approach.\n\n- Although focused on piano, the methods\u2014particularly the staged multi-source training strategy\u2014are broadly applicable to other homogeneous or low-resource musical genres.\n\nLimitations\n\n- Line 333 mentions involvement of a \u201chuman music expert\u201d in refining GPT-generated captions, but lacks detail on their qualifications or the verification protocol used, reducing transparency in evaluation.\n\nOverall Assessment\n\nThis paper presents a well-motivated and technically sound contribution to music information retrieval, particularly within the under-explored space of domain-specific multi-modal modeling for solo piano. While some reproducibility and methodological transparency concerns remain, the model design, training strategy, and results offer valuable and transferable insights for MIR researchers working with low-resource or stylistically narrow domains.",
      "review3": "- **Overall Evaluation**: Strong reject\n\n#### Main Reviews\n\nThe paper reads well and the logic is clear. The main strengths are its structure and clarity. However, the core idea feels quite familiar, especially with models like CLaMP already doing similar multimodal things. The bigger issue is the experiment setup for retrieval \u2013 testing on just a couple hundred tracks makes the task seem too easy and doesn't really convince me the model works well in a more realistic scenario."
    },
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Recent advancements in Automatic Drum Transcription (ADT) have improved overall transcription performance. However, state-of-the-art (SOTA) models still struggle with certain drum classes, particularly toms and cymbals, and the specific factors limiting their performance remain unclear. This paper addresses this gap by leveraging the Separate-Tracks-Annotate-Resynthesize Drums (STAR Drums) dataset to create multiple dataset versions that systematically eliminate potential performance constraints. We conduct experiments using three common ADT deep neural network (DNN) architectures to identify and quantify these limitations. For drum transcription in the presence of melodic instruments (DTM), the primary limiting factor is interference from melodic instruments and singing. Aside from this, performance improves by approximately five percent when training and testing use the same single drum kit, only strong onsets are present, or notes are not played simultaneously. For drum transcription of drum-only recordings (DTD), nearly error-free transcription is achieved when simultaneous onsets are removed. This confirms that overlapping drum hits are the main performance constraint. By identifying key ADT challenges, we provide insights to enhance SOTA models and improve overall transcription accuracy.",
      "abstract": "Recent advancements in Automatic Drum Transcription (ADT) have improved overall transcription performance. However, state-of-the-art (SOTA) models still struggle with certain drum classes, particularly toms and cymbals, and the specific factors limiting their performance remain unclear. This paper addresses this gap by leveraging the Separate-Tracks-Annotate-Resynthesize Drums (STAR Drums) dataset to create multiple dataset versions that systematically eliminate potential performance constraints. We conduct experiments using three common ADT deep neural network (DNN) architectures to identify and quantify these limitations. For drum transcription in the presence of melodic instruments (DTM), the primary limiting factor is interference from melodic instruments and singing. Aside from this, performance improves by approximately five percent when training and testing use the same single drum kit, only strong onsets are present, or notes are not played simultaneously. For drum transcription of drum-only recordings (DTD), nearly error-free transcription is achieved when simultaneous onsets are removed. This confirms that overlapping drum hits are the main performance constraint. By identifying key ADT challenges, we provide insights to enhance SOTA models and improve overall transcription accuracy.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Philipp Weyers",
        "Christian Uhle",
        "Meinard M\u00fcller",
        "Matthias Lang"
      ],
      "authors_and_affil": [
        "Philipp Weyers (Fraunhofer IIS)*",
        "Christian Uhle (Fraunhofer IIS)",
        "Meinard M\u00fcller (International Audio Laboratories Erlangen/Fraunhofer IIS)",
        "Matthias Lang (Fraunhofer IIS)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Musical features and properties",
        "Music transcription and annotation",
        "Novel datasets and use cases",
        "Automatic classification",
        "MIR tasks",
        "Evaluation, datasets, and reproducibility",
        "Rhythm, beat, tempo",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1PtYX9wyP-0mzZuWiMcy_AM0BaNIOuL95/preview",
      "poster_pdf": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-10-understanding-performance-limitations",
      "title": "Understanding Performance Limitations in Automatic Drum Transcription",
      "video": ""
    },
    "forum": "130",
    "id": "130",
    "pic_id": "",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nStrongly agree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nStrongly agree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nStrongly agree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe insights into training/test drum kit mismatch, simultaneous onset challenges, and weak onset impact are highly reusable for future ADT system design. The methodology could serve as a template for analyzing bottlenecks in other MIR tasks.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nPerformance limitations in ADT systems can be systematically analyzed and mitigated by controlling drum kit variation, onset overlap, and signal complexity using variants of the STAR Drums dataset.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper offers a detailed and well-structured investigation of the factors that limit the performance of current ADT systems. By systematically altering the STAR Drums dataset along key dimensions\u2014number of kits, presence of weak/simultaneous onsets, and accompaniment context\u2014it succeeds in quantifying the contribution of each constraint to the overall performance ceiling.\n\nStrengths:\n- Methodologically rigorous and clearly explained.\n- Explores three model architectures, increasing generality of results.\n- Performance metrics are consistent and grounded in prior work.\n\nSuggestions for improvement:\n- Some tables (especially Table 3) are dense and could benefit from clearer formatting or graphical summaries.\n- It would be useful to discuss the potential implications of these findings for realworld ADT applications (e.g., how these bottlenecks affect user-facing systems).\n- The discussion on model failure modes could be deepened with a few more qualitative examples.\n\nOverall, this is a valuable contribution that helps the MIR community better understand how and why ADT systems fail\u2014and where gains can still be made.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThis paper conducts a thorough investigation into the performance limitations of automatic drum transcription (ADT) systems by using a well-designed experimental setup based on variants of the STAR Drums dataset. Rather than introducing a novel model, the work takes a diagnostic approach, aiming to isolate and measure the contribution of various known challenges\u2014such as drum kit variability, overlapping onsets, and melodic interference\u2014on ADT performance.\n\nThe core value of this paper lies in its methodological clarity and diagnostic utility. Its key contribution is not architectural or algorithmic, but rather analytical: a reusable blueprint for benchmarking ADT bottlenecks. Multiple reviewers praised the clarity of the research questions and the strength of the experimental controls. The paper offers a useful \u201cperformance budget\u201d that can help guide future efforts toward the most impactful areas of improvement.\n\nThat said, some limitations temper the impact:\n\n* Heavy reliance on re-synthesized data raises questions of external validity and real-world applicability.\n* The evaluation on real recordings is modest in scale and genre diversity.\n* The paper stops short of proposing or testing solutions to the identified issues, which reduces its immediate applicability to system development.\n\nThis paper offers a valuable and reusable diagnostic framework for understanding performance limits in ADT systems. While it lacks immediate practical solutions and broader validation, its analytical contributions may justify its inclusion in the ISMIR program.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper investigates the limiting factors of ADT through a systematic evaluation using synthetic data. Specifically, different variants of STAR Drums are synthesized by removing different limiting factors, which allows for the assessment of their individual impacts. Through the analysis of the evaluation results, this paper addresses the research questions with quantitative outcomes, shedding light on the future direction of ADT research. The paper is well-structured and nicely written. The literature survey is comprehensive, and the research questions are well-motivated. \n\nOne aspect I really appreciate about this work is the simplicity of the idea. By controlling variables such as number of drum kits, strong/weak onsets, and the presence of simultaneous sounds, one can study their impacts in a consistent environment with clear results. The core idea is straightforward, and the execution appears to be thorough. Generally speaking, I find the answers to the research questions convincing and well-supported. \n\nThe main frustration while reading this paper, however, lies in the presentation of the evaluation results (i.e., Table 3). With three CNN architectures and two scenarios (i.e., DTD and DTM), there are many numbers in the same table without labels. The discussion section attempts to guide readers through this table by referring to the changes in numbers, however, it is still very difficult to parse. It took me a while to memorize the order of things, and I still found myself going back and forth just to ensure I was reading the correct numbers. The occasional typo in the numbers (see minor comments below) also makes it harder to follow. In my opinion, this paper could really benefit from some simplification or aggregation in the evaluation section in order to increase clarity. \n\nTo summarize, I think this paper provides interesting insights for both DTD and DTM based on a simple yet effective idea. The evaluation is thorough, and the discussion is generally satisfactory. Overall, I believe this paper is a good contribution to the ADT community, and my recommendation is a weak accept. For further improvements, I would definitely encourage the authors to reconsider the presentation of the results. \n\n=============\nMinor comments: \nLine 61, \u201cSTAR Drums\u201d \u2192 missing reference to [6]?\n\nLine 116-117, \u201c... with global F-measures above 0.8 \u2026\u201d \u2192 on which evaluation set? Any reference?\n\nLine 125, \u201c... especially cymbals are often played with alternating weak and strong onsets\u2026\u201d \u2192 HiHat and Ride cymbals, to be more specific\n\nLine 284, \u201c... using a peak picking with\u201d \u2192 a peak picking algorithm?\n\nLine 331 \u2192 should be 0.79 to 0.73?\n\nLine 334, \u201c... 0.73 to 0.78 and 0.78 to 0.82\u2026\u201d \u2192 it is a bit confusing since the previous paragraph was talking about 10Kits. It took me a while to realize this is comparing 20Kits to 1Kits. \n\nLine 350-352, \u201c... suggests that even a relatively low number of virtual drum kits is sufficient to develop well-generalizing models for DTM\u201d \u2192 this is a very dangerous statement, especially given the size of MDB Drums. Maybe the drum sounds in MDB are not diverse enough? Maybe the 10Kits somehow match the drum sounds in the MDB samples? In any case, I am a bit skeptical about this statement. \n\nLine 407, \u201c... and decreases for the other two architectures for DTM.\u201d \u2192 why? Isn\u2019t 20KitsNoSim an easier test set compared to 20Kits? Any insight/explanation?",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nStrengths\n - The idea, the purpose, and the setup of this study are sound and clearly presented\n - The work draws from and builds nicely on the previous efforts made by the MIR community\n - The results are clearly interpreted\n\n Weaknesses\n - It would have been great if the dataset used for testing had been kept entirely segregated, i.e., avoiding using it even for validation. \n - The testing is only performed on tracks that are re-synthesized. It will be interesting to see how results generalize to \"actual\" real music. \n\n - [minor] One of the conclusions is formulated badly: \"Ensuring that drum sounds between training and testing are identical ...\". It is clearly impossible to \"ensure\" that. The conclusion should be formulated as \"If the drum sounds in testing is identical to ..., then ...\".\n - [minor] Some more details about the Star datasets would be appreciated. For instance, genre coverage.\n\n Questions\n - It is argued that drum source separation could help addressing the challenge of overlapping onsets. How? How is that task different (easier) than ADT? If source separation is effective, then the same techniques could be applied to ATD, couldn't they?\n - Does it make sense to average the F-measure of experiments with different number of T/F positives and T/F negatives? Could/should the F-measure be weighted?",
      "review3": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nStrengths (1) Diagnostic clarity: Instead of yet another SOTA system, the paper isolates four hypothesized bottlenecks in ADT and quantifies their individual cost\u2014e.g., melodic masking and simultaneous hits\u2014providing a \u201cperformance budget\u201d for future work. (2) Well-controlled dataset design: The five STAR-Drums variants (20 Kits, 10 Kits, 1 Kit, NoWeak, NoSim) form a clean, switch-off-one-factor benchmark that others can replicate or extend with minimal effort. \n\nWeaknesses (1) External validity: Results rely largely on re-synthesised audio; the single real-recording test set (MDB, \u224822 min) is stylistically narrow, leaving domain-gap questions unresolved. (2) While the paper presents an analysis of limiting factors affecting ADT performance, it does not offer concrete solutions to the identified bottlenecks, thereby reducing its immediate applicability and practical impact in real-world scenarios.\n\nJustification: The work offers a valuable analysis toolbox that will help the community allocate effort and funding more intelligently. However, the reliance on synthetic data, modest breadth of external evaluation, and absence of modelling advances limit immediate practical impact. Balancing methodological novelty against these drawbacks yields a review of Weak reject."
    },
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Recent advances in latent diffusion models have demonstrated state-of-the-art performance in high-dimensional time-series data synthesis while providing flexible control through conditioning and guidance. However, existing methodologies primarily rely on musical context or natural language as the main modality of interacting with the generative process, which may not be ideal for expert users who seek precise fader-like control over specific musical attributes. In this work, we explore the application of denoising diffusion processes as plug-and-play latent constraints for unconditional symbolic music generation models. We focus on a framework that leverages a library of small conditional diffusion models operating as implicit probabilistic priors on the latents of a frozen unconditional backbone. While previous studies have explored domain-specific use cases, this work, to the best of our knowledge, is the first to demonstrate the versatility of such an approach across a diverse array of musical attributes, such as note density, pitch range, contour, and rhythm complexity. Our experiments show that diffusion-driven constraints outperform traditional attribute regularization and other latent constraints architectures, achieving significantly stronger correlations between target and generated attributes while maintaining high perceptual quality and diversity.",
      "abstract": "Recent advances in latent diffusion models have demonstrated state-of-the-art performance in high-dimensional time-series data synthesis while providing flexible control through conditioning and guidance. However, existing methodologies primarily rely on musical context or natural language as the main modality of interacting with the generative process, which may not be ideal for expert users who seek precise fader-like control over specific musical attributes. In this work, we explore the application of denoising diffusion processes as plug-and-play latent constraints for unconditional symbolic music generation models. We focus on a framework that leverages a library of small conditional diffusion models operating as implicit probabilistic priors on the latents of a frozen unconditional backbone. While previous studies have explored domain-specific use cases, this work, to the best of our knowledge, is the first to demonstrate the versatility of such an approach across a diverse array of musical attributes, such as note density, pitch range, contour, and rhythm complexity. Our experiments show that diffusion-driven constraints outperform traditional attribute regularization and other latent constraints architectures, achieving significantly stronger correlations between target and generated attributes while maintaining high perceptual quality and diversity.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Matteo Petten\u00f2",
        "Alessandro Mezza",
        "Alberto Bernardini"
      ],
      "authors_and_affil": [
        "Matteo Petten\u00f2 (Politecnico di Milano)",
        "Alessandro Mezza (Politecnico di Milano )*",
        "Alberto Bernardini (Politecnico di Milano)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Symbolic music processing",
        "Generative Tasks",
        "Musical features and properties",
        "Music generation",
        "Music and audio synthesis",
        "MIR tasks",
        "Machine learning/artificial intelligence for music",
        "MIR fundamentals and methodology",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1bNCKpXBSI_swf1Bnj89uw8BlObgToHJ0/preview",
      "poster_pdf": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-6-conditional-diffusion-as",
      "title": "Conditional Diffusion as Latent Constraints for Controllable Symbolic Music Generation",
      "video": ""
    },
    "forum": "133",
    "id": "133",
    "pic_id": "",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Predominant Local Pulse (PLP) estimation is a key technique in rhythmic analysis of music recordings, designed to identify the most salient pulse in an audio signal while adapting to local tempo variations. Unlike global tempo estimation, which assumes a fixed tempo, PLP dynamically adjusts to changes in tempo and rhythm, making it particularly effective as a post-processing strategy to enhance the locally periodic structure of a given input novelty or activity function. Traditional PLP estimation relies on a max operation to select the most prominent periodicity, limiting its use in differentiable learning frameworks. In this paper, we introduce dPLP, a differentiable version of PLP estimation that replaces the max operation when selecting a locally optimal periodicity kernel with a softmax-based weighting scheme. This modification ensures good gradient flow, allowing PLP to be seamlessly integrated into deep learning pipelines as an intermediate layer or as part of the loss function. We provide technical insights into its differentiable formulation and present experiments comparing it to the original non-differentiable PLP approach. Additionally, case studies in\nbeat tracking highlight the advantages of dPLP in improving periodicity-aware representations within neural network architectures.",
      "abstract": "Predominant Local Pulse (PLP) estimation is a key technique in rhythmic analysis of music recordings, designed to identify the most salient pulse in an audio signal while adapting to local tempo variations. Unlike global tempo estimation, which assumes a fixed tempo, PLP dynamically adjusts to changes in tempo and rhythm, making it particularly effective as a post-processing strategy to enhance the locally periodic structure of a given input novelty or activity function. Traditional PLP estimation relies on a max operation to select the most prominent periodicity, limiting its use in differentiable learning frameworks. In this paper, we introduce dPLP, a differentiable version of PLP estimation that replaces the max operation when selecting a locally optimal periodicity kernel with a softmax-based weighting scheme. This modification ensures good gradient flow, allowing PLP to be seamlessly integrated into deep learning pipelines as an intermediate layer or as part of the loss function. We provide technical insights into its differentiable formulation and present experiments comparing it to the original non-differentiable PLP approach. Additionally, case studies in\nbeat tracking highlight the advantages of dPLP in improving periodicity-aware representations within neural network architectures.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Ching-Yu Chiu",
        "Sebastian Strahl",
        "Meinard M\u00fcller"
      ],
      "authors_and_affil": [
        "Ching-Yu Chiu (International Audio Laboratories Erlangen, Germany)*",
        "Sebastian Strahl (\tInternational Audio Laboratories Erlangen, Germany)",
        "Meinard M\u00fcller (International Audio Laboratories Erlangen, Germany)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "MIR fundamentals and methodology",
        "Musical features and properties",
        "Music signal processing",
        "Rhythm, beat, tempo",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1wRmxoveHiJSD2WQOWuUkBogDZrFwVZ7T/preview",
      "poster_pdf": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-10-dplp-a-differentiable",
      "title": "dPLP: A Differentiable Version of Predominant Local Pulse Estimation",
      "video": ""
    },
    "forum": "135",
    "id": "135",
    "pic_id": "",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nStrongly agree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nStrongly agree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nPredominant local pulse (PLP) can be integrated into a DNN.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nA differentiable version of predominant local pulse (PLP) can be integrated for end-to-end DNN training in periodicity-aware music analysis tasks.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper proposes a dPLP, a differentiable version of the predominant local pulse (PLP) estimator, in the spirit of differentiable digital signal processing (DDSP). By replacing the max operation with the softmax function, dPLP can be integrated seamlessly into a deep learning pipeline for periodicity-aware music analysis like beat tracking, achieving end-to-end training and better performance.\n\nPros:\n- Has a potential of improving the performance of periodicity-aware DNNs with dPLP.\n- Enables end-to-end training of rhythm analysis systems without isolated post-processing.\n- Provides a model-based, interpretable, and flexible module for periodicity enhancement.\n\nCons:\n- The generalizability or usefulness across diverse musical genres have not been fully investigated.\n- Peak picking post-processing is still required and has a strong impact on the performance.\n- Performance gains might be partly due to increased model size in integrated architectures.\n- dPLP may have a bias towards faster tempi.\n\nThis is a well-motivated work that could potentially have a large impact on the MIR community. The periodicity is the most basic nature of music. The simplicity of the proposed dPLP would be a key because it is easy to implement and integrate in a wide variety of periodicity-aware music analysis tasks. One of the main concerns is that the influence of the softmax temperature parameter was fixed to 1 and has not been investigated. The sensitivity of dPLP (how much dPLP can differ from PLP) is unclear.\n\nPlease insert a table showing the configurations of the methods listed in Table 1.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nStrong accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThis work proposes a differentiable version of predominant local pulse (PLP), a classical signal processing method used in various tasks. The reviewers confirmed the novelty and effectiveness of the proposed method and positively supported the acceptance of the paper.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe paper proposes a differentiable version of Predominant Local Pulse (PLP) estimation, enabling rhythmic salience modeling to be fully integrated into end-to-end neural pipelines\u2014something not achievable with classical PLP due to its non-differentiable nature. This work advances rhythmic analysis in MIR by making both onset detection and periodicity modeling trainable. Specifically, the authors implement a trainable spectral flux module and a softmax-based PLP mechanism, forming a lightweight yet effective architecture for beat tracking. The modular design is clearly motivated, and the experiments\u2014especially the ablation comparisons (M1\u2013M3) and visualizations (Figure 3)\u2014make it easy for readers to understand how each component contributes to performance improvements. This makes the work not only technically sound but also highly reusable for future rhythm-aware models.\n\nAlthough I believe the paper stands well on its own, I would like to share a few points that could be clarified or expanded, along with some broader suggestions that may go beyond the current scope.\n\nFor the first part \u2013 things that can be clarified or strengthened:\n\n(1.) Choice of softmax for differentiable PLP:\nThe authors replace argmax with softmax for periodicity selection, which is a valid and widely used approach. However, I recommend discussing why softmax was chosen over other common differentiable relaxations (e.g., sparsemax, entmax, Gumbel-softmax), especially since this choice is central to dPLP\u2019s formulation and performance characteristics.\n\n(2.) Related work on differentiable argmax replacements:\nSince the core mechanism of dPLP relies on replacing the non-differentiable argmax with a softmax relaxation, I suggest referencing relevant literature on differentiable approximations to argmax, such as Gumbel-softmax, sparsemax, or entmax. Even a brief citation and comparison would help ground the technique in broader machine learning literature and clarify whether softmax was chosen for theoretical or empirical reasons.\n\n(3.) Related work on differentiable DSP for analysis:\nI suggest citing Kim et al., \u201cSelf-supervised Pitch Detection by Inverse Audio Synthesis\u201d [1], which also applies a differentiable DSP-inspired module for an analysis task (pitch tracking). This would help contextualize dPLP within the emerging body of work focused on DDSP for analysis, rather than generation.\n\n(4.) Evaluation on unstable tempo genres (recommended extension):\nThe paper mentions that PLP is well-suited for expressive or non-steady tempo music (e.g., jazz, classical), but does not evaluate dPLP on such material. Including an ablation or case study in this context would provide stronger evidence of dPLP\u2019s robustness and generality.\n\n(5.) Limited downstream task evaluation:\nWhile the beat tracking results are convincing, it would strengthen the work to apply dPLP to at least one additional rhythmic analysis task, such as downbeat tracking or meter estimation. This would better demonstrate its general-purpose utility.\n\n(6.) High-level musical interpretation:\nBeyond improving quantitative scores, what does dPLP reveal about musical rhythm? A brief discussion on how the soft salience output reflects rhythmic structure, listener perception, or interpretability\u2014especially in expressive or syncopated music\u2014would enhance the conceptual depth of the contribution.\n\nFor the second part \u2013 suggestions beyond the current scope (optional but valuable):\n\n(1.) Self-supervised training potential:\nGiven that dPLP produces a continuous rhythmic salience representation, it has strong potential for self-supervised learning, such as contrastive learning based on rhythmic consistency or alignment-based pretext tasks. I encourage the authors to consider this direction, particularly for under-annotated genres or low-resource scenarios.\n\n(2.) Comparison with SOTA and integration into modern architectures: While the paper intentionally focuses on a lightweight, modular architecture, I believe it would be valuable to explore how dPLP performs when integrated into larger, modern neural systems, such as CRNNs or Transformers for multi-task learning. For example, dPLP could be tested as a component within All-In-One [2], a recent state-of-the-art metrical and functional structure analysis model. This would help clarify whether dPLP\u2019s inductive bias provides additive benefit beyond controlled settings.\n\nOverall, I believe the paper makes a meaningful contribution by enabling differentiable rhythmic structure modeling and validating its effectiveness through well-structured experiments. It opens up several promising research directions and should be included in ISMIR this year. As the field continues to explore rhythm-aware learning systems, this work represents a timely and well-executed step forward.\n\nReferences:\n\n[1] Self-supervised Pitch Detection by Inverse Audio Synthesis. Kim et al., NeurIPS 2021. https://openreview.net/forum?id=RlVTYWhsky7\n\n[2] All-In-One Metrical and Functional Structure Analysis With Neighborhood Attentions on Demixed Audio. Chou et al., arXiv 2023. https://arxiv.org/abs/2307.16425",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe paper proposed a differentiable version of the classic PLP method (dPLP) for beat tracking. The method is simple: it applies a softmax function to the tempogram along the tempi dimension. The resulting weights are used to weight-sum all the possible sinusoidal kernels at each frame. A small-scale but carefully designed, proof-of-concept experiment shows the benefits of dPLP, supported by some further analysis. However, the paper doesn't give much details on the proposed differentiable spectral flux module, which I found quite important. The computational benefit of log-scale BPMs is not fully explained. In addition, the paper could be more impactful by scaling up the model and benchmarking it against more recent beat tracking methods.\n\nOne question that bothers me the most is in Section 2.3, the authors mention that the log-scale BPMs can reduce computational cost. Which part of the computation is reduced? This is not clear from the text. The same claim is mentioned again in Section 4.1. Moreover, the connection between sensitivity to relative tempo changes and log-scale tempos is unclear to me. Some reference is needed.\n\nThe other suggestion is that the authors should provide more details on the differentiable spectral flux module, such as what kind of parameterisation is going on inside the module, the size of the kernel, etc. I would also like to see what kind of convolution kernels are learnt at the end and compare them with the first-order differentiation kernel.\n\nI understand that M2-S is the same as SFX-T, but it would still be better if the authors could mention this specifically and highlight their equality in Table 1. I also recommend adding the scores of recent beat tracking methods to Table 1 for comparison.\n\nI appreciate the author's efforts to make their work scientifically sound. Below are some personal suggestions on the notation:\n- Section 2.1, \"This window, ...., and zero outside.\" can be omitted or simplified.\n- Is \\omega and \\phi(n, \\tau) in the range [0, 1)? I infer this from the following text. It's better to clarify it since people usually assume it's in [0, 2\\pi).\n- I feel \\mathbb{R}_+ is more common than \\mathbb{R}_{>0}.\n- The author could consider writing equations 5 and 8 as something like `max(*, 0)`, which is much easier to understand, but I understand the current notation is from the FMP textbook. It's just a personal preference.\n- Section 4.2, \"standalone PLPs\" => did you mean softmax PLPs?\n\nPlease avoid putting long texts in the footnote (specifically, the number 5 footnote). The author can shorten the explanation or move it to the main text.\n\nAlthough the authors said they will try more advanced architectures in the future, the work would have been better if they had done this or provided some initial results. Still, great work overall.",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe paper presents a differentiable implementation of the predominant local pulse estimation function (dPLP) that can be used in deep-learning-based methods. For this, it replaces the argmax operation that selects the optimal periodicity kernel for a softmax-based weighting scheme that merges multiple sinusoidal kernels.\n\nThe paper is very clear and well-written, and I have only minor suggestions and comments, which are presented in the following:\n\nIntroduction\n=========\n* Instead of describing \"tempo\" as \"the speed of those beats\", it would be perhaps more precise to say it is \"the rate at which those beats occur\".\n* The reference for footnote 1 in the text came before the full stop, where in other instances references are added after it (as they probably should).\n\nSection 3.2.1\n==========\n* The trainable spectral flux function is a very interesting idea! However, the paper does not expand much on it (I understand it is not the main object). For instance, it is not clear how the differentiation the training of the convolution kernel works. I understand this could be remedied by the git repo, but if space is available, it could be interesting to include some more information.\n* I had to reread the last paragraph in this section a few times, thinking that this training had to somehow involve the dPLP. I believe I only really got it after reaching Section 4. The issue is that it is not usual to apply peak-picking directly to an onset detection function in order to find beats. I don't think there is a reference for this, since beat tracking methods usually transform the onset detection function into some form of periodicity representation. If there is such reference, maybe it could be added here. If not, please considering rephrasing it, making it clear that despite not being orthodox, this will be performed as a kind of baseline.\n\nSection 3.2.3\n==========\n* \"To assess the complementarity between Delta_S and the PLP curves\" should be written as \"To assess the complementarity between Delta_S and the dPLP curves\" (dPLP).\n\nSection 4.1\n=========\n* I am curious about beat tracking results on a few configurations that could serve as baselines as well and were not included in Table 1. First is the combination of SFX-I and A-* or S-*. Second, a version of M2, where S = SFX-I, instead of SFX-T. These would also allow to see the impact of training the fuser module over the GTZAN dataset with a non-optimal spectral flux module. If there is space, it could be interesting to add this discussion.\n\nSection 4.2\n=========\n* \"and L-correct metrics (from below 0.220 to 0.360)\" should be written as \"and L-correct metrics (from below 0.220 to above 0.360)\" (above).\n\nSection 4.3\n=========\n* When discussing the differences between fuser false-positives, it is argued that these can be attribute to the \"differentiability of dPLP (M1 vs. M2)\". However, if I understood correctly, in both M1 and M2, the dPLP is used, which is differentiable. The difference is that in M2 it is not backpropagated.\n* \"In contrast, when the novelty functions from S modules do not align with the PLP curves (e.g., green regions), M1-F and M2-F, which have access to the dPLP outputs, avoid making a false-positive error\" -> this is an interesting comparison, but doesn't seem correct. First, I assume the \"PLP curves\" are SFX-T and SFX-I, since M*-S will be discussed in the following lines. Second, there is barely any activation in SFX-T in the green region, only in SFX-I. Third, M1 doesn't use either SFX-T or SFX-I, since S is also trained. Perhaps this discussion should have been done using M1-S instead (where the activation is more pronouced)?"
    },
    "session": "2"
  },
  {
    "content": {
      "TLDR": "This paper explores the relationship between music and the color palettes used for designing their corresponding music cover images, providing a comprehensive analysis that bridges auditory and visual expression. Our findings reveal a relationship between musical pieces and certain colors, suggesting that the color palettes used in cover image design are carefully selected to reflect the auditory experience. Building on these findings, we propose a framework that estimates appropriate color palettes for musical pieces to support selecting colors for cover image design. Using a large private dataset of 582,894 pairs of a musical piece and its corresponding cover image from various music genres, our framework leverages deep learning techniques to train our color palette estimator. We demonstrate the effectiveness of our proposed framework in graphic design by showcasing an application that generates cover images using the estimated color palettes from given musical pieces.",
      "abstract": "This paper explores the relationship between music and the color palettes used for designing their corresponding music cover images, providing a comprehensive analysis that bridges auditory and visual expression. Our findings reveal a relationship between musical pieces and certain colors, suggesting that the color palettes used in cover image design are carefully selected to reflect the auditory experience. Building on these findings, we propose a framework that estimates appropriate color palettes for musical pieces to support selecting colors for cover image design. Using a large private dataset of 582,894 pairs of a musical piece and its corresponding cover image from various music genres, our framework leverages deep learning techniques to train our color palette estimator. We demonstrate the effectiveness of our proposed framework in graphic design by showcasing an application that generates cover images using the estimated color palettes from given musical pieces.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Takayuki Nakatsuka",
        "Masahiro Hamasaki",
        "Masataka Goto"
      ],
      "authors_and_affil": [
        "Takayuki Nakatsuka (National Institute of Advanced Industrial Science and Technology (AIST))*",
        "Masahiro Hamasaki (National Institute of Advanced Industrial Science and Technology (AIST))",
        "Masataka Goto (National Institute of Advanced Industrial Science and Technology (AIST))"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "",
        "Multimodality",
        "MIR fundamentals and methodology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/18C4MUDq97zcjSIzwn3A1OPivJiZOWBcp/preview",
      "poster_pdf": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-9-coloring-music-bridging",
      "title": "Coloring Music: Bridging Music and Color Palettes for Graphic Design",
      "video": ""
    },
    "forum": "137",
    "id": "137",
    "pic_id": "",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Non-metric music forms the core of the repertoire in\nIranian classical music. Dastg\u00afahi music serves as the un-\nderlying theoretical system for both Iranian art music and\ncertain folk traditions. At the heart of Iranian classical mu-\nsic lies the radif, a foundational repertoire that organizes\nmelodic material central to performance and pedagogy.\n\nIn this study, we introduce the first digital corpus rep-\nresenting the complete non-metrical radif repertoire, cov-\nering all 13 existing components of this repertoire. We\nprovide MIDI files (about 281 minutes in total) and data\nspreadsheets describing notes, note durations, intervals,\nand hierarchical structures for 228 pieces of music. We\nfaithfully represent the tonality including quarter-tones,\nand the non-metric aspect. Furthermore, we provide sup-\nporting basic statistics, and measures of complexity and\nsimilarity over the corpus.\n\nOur corpus provides a platform for computational stud-\nies of Iranian classical music. Researchers might employ it\nin studying melodic patterns, investigating improvisational\nstyles, or for other tasks in music information retrieval,\nmusic theory, and computational (ethno)musicology.",
      "abstract": "Non-metric music forms the core of the repertoire in\nIranian classical music. Dastg\u00afahi music serves as the un-\nderlying theoretical system for both Iranian art music and\ncertain folk traditions. At the heart of Iranian classical mu-\nsic lies the radif, a foundational repertoire that organizes\nmelodic material central to performance and pedagogy.\n\nIn this study, we introduce the first digital corpus rep-\nresenting the complete non-metrical radif repertoire, cov-\nering all 13 existing components of this repertoire. We\nprovide MIDI files (about 281 minutes in total) and data\nspreadsheets describing notes, note durations, intervals,\nand hierarchical structures for 228 pieces of music. We\nfaithfully represent the tonality including quarter-tones,\nand the non-metric aspect. Furthermore, we provide sup-\nporting basic statistics, and measures of complexity and\nsimilarity over the corpus.\n\nOur corpus provides a platform for computational stud-\nies of Iranian classical music. Researchers might employ it\nin studying melodic patterns, investigating improvisational\nstyles, or for other tasks in music information retrieval,\nmusic theory, and computational (ethno)musicology.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Maziar Kanani",
        "Se\u00e1n O\u2019Leary",
        "James McDermott"
      ],
      "authors_and_affil": [
        "Maziar Kanani (University of Galway)*",
        "Se\u00e1n O\u2019Leary (TU Dublin)",
        "James McDermott (University of Galway)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Digital musicology",
        "Computational ethnomusicology",
        "Symbolic music processing",
        "Applications",
        "Music heritage and sustainability",
        "Computational musicology",
        "Evaluation, datasets, and reproducibility",
        "MIR fundamentals and methodology",
        "Computational music theory and musicology",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/19BHBgNWLY_NxL6MKxgpFjXv1eo5Sl1XU/preview",
      "poster_pdf": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-7-radif-corpus-symbolic",
      "title": "Radif Corpus: Symbolic Dataset for Non-metric Iranian Classical Music",
      "video": ""
    },
    "forum": "138",
    "id": "138",
    "pic_id": "",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "1"
  },
  {
    "content": {
      "TLDR": "This paper describes the Interactive Digital Transcription and Analysis Platform (IDTAP): The IDTAP is a web-based application designed to enable users to digitally transcribe, archive, share, and analyze audio recordings of oral melodic traditions. The platform\u2019s underlying music-theoretical premises and corresponding data architecture have initially been developed to align with the idiomatic features of Hindustani music (i.e., North Indian classical music). These features necessitate a flexible array of pitch-contour curves; adjustable tuning systems that allow for the representation of a range of microtonal configurations found in both historical and contemporary practice; expressive microtonal pitch inflections between tuning system pitches; and a highly precise rhythmic representation that captures subtle micro-timing nuances, expressive tempo variations, and both metric and non-metric rhythmic structures exactly as performed. The IDTAP\u2019s archive, transcription editor, and analysis suite jointly are designed for future expansion to include a range of musical traditions, opening multiple sound collections and archives to digital preservation, pedagogy, and appreciation, as well as statistical, quantitative, and interpretive analysis. The platform and corresponding data architecture equips scholars from a range of disciplinary backgrounds to apply the power of twenty-first-century computational methodologies and large datasets to humanistic and creative endeavors.",
      "abstract": "This paper describes the Interactive Digital Transcription and Analysis Platform (IDTAP): The IDTAP is a web-based application designed to enable users to digitally transcribe, archive, share, and analyze audio recordings of oral melodic traditions. The platform\u2019s underlying music-theoretical premises and corresponding data architecture have initially been developed to align with the idiomatic features of Hindustani music (i.e., North Indian classical music). These features necessitate a flexible array of pitch-contour curves; adjustable tuning systems that allow for the representation of a range of microtonal configurations found in both historical and contemporary practice; expressive microtonal pitch inflections between tuning system pitches; and a highly precise rhythmic representation that captures subtle micro-timing nuances, expressive tempo variations, and both metric and non-metric rhythmic structures exactly as performed. The IDTAP\u2019s archive, transcription editor, and analysis suite jointly are designed for future expansion to include a range of musical traditions, opening multiple sound collections and archives to digital preservation, pedagogy, and appreciation, as well as statistical, quantitative, and interpretive analysis. The platform and corresponding data architecture equips scholars from a range of disciplinary backgrounds to apply the power of twenty-first-century computational methodologies and large datasets to humanistic and creative endeavors.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Jonathan Myers",
        "Dard Neuman"
      ],
      "authors_and_affil": [
        "Jonathan Myers (UC Santa Cruz)*",
        "Dard Neuman (UC Santa Cruz)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Human-computer interaction",
        "Musical features and properties",
        "Computational musicology",
        "Systematic musicology",
        "MIR tasks",
        "Structure, segmentation, and form",
        "MIR fundamentals and methodology",
        "Human-centered MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1m8Vk5Z3bix1BCaHD63UQHY0VMOVR-H-l/preview",
      "poster_pdf": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-5-beyond-notation-a",
      "title": "Beyond Notation: A Digital Platform for Transcribing and Analyzing Oral Melodic Traditions",
      "video": ""
    },
    "forum": "140",
    "id": "140",
    "pic_id": "",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Efficiently representing audio signals in a compressed latent space is critical for latent generative modelling. However, existing autoencoders often force a choice between continuous embeddings and discrete tokens. Furthermore, achieving high compression ratios while maintaining audio fidelity remains a challenge.  We introduce CoDiCodec, a novel audio autoencoder that overcomes these limitations by both efficiently encoding global features via summary embeddings, and by producing both compressed continuous embeddings at ~11 Hz and discrete tokens at a rate of 2.38 kbps from the same trained model, offering unprecedented flexibility for different downstream generative tasks. This is achieved through Finite Scalar Quantization (FSQ) and a novel FSQ-dropout technique, and does not require additional loss terms beyond the single consistency loss used for end-to-end training. CoDiCodec supports both autoregressive decoding and a novel parallel decoding strategy, with the latter achieving superior audio quality and faster decoding. CoDiCodec outperforms existing continuous and discrete autoencoders at similar bitrates in terms of reconstruction audio quality. Our work enables a unified approach to audio compression, bridging the gap between continuous and discrete generative modelling paradigms.",
      "abstract": "Efficiently representing audio signals in a compressed latent space is critical for latent generative modelling. However, existing autoencoders often force a choice between continuous embeddings and discrete tokens. Furthermore, achieving high compression ratios while maintaining audio fidelity remains a challenge.  We introduce CoDiCodec, a novel audio autoencoder that overcomes these limitations by both efficiently encoding global features via summary embeddings, and by producing both compressed continuous embeddings at ~11 Hz and discrete tokens at a rate of 2.38 kbps from the same trained model, offering unprecedented flexibility for different downstream generative tasks. This is achieved through Finite Scalar Quantization (FSQ) and a novel FSQ-dropout technique, and does not require additional loss terms beyond the single consistency loss used for end-to-end training. CoDiCodec supports both autoregressive decoding and a novel parallel decoding strategy, with the latter achieving superior audio quality and faster decoding. CoDiCodec outperforms existing continuous and discrete autoencoders at similar bitrates in terms of reconstruction audio quality. Our work enables a unified approach to audio compression, bridging the gap between continuous and discrete generative modelling paradigms.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Marco Pasini",
        "Stefan Lattner",
        "George Fazekas"
      ],
      "authors_and_affil": [
        "Marco Pasini (Queen Mary University of London)*",
        "Stefan Lattner (Sony)",
        "George Fazekas (Queen Mary University of London)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Generative Tasks",
        "Musical features and properties",
        "Music generation",
        "Representations of music",
        "Music and audio synthesis",
        "Transformations",
        "MIR tasks"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/19FJvtyYjJhSpsm-Tu2yguh7X_qhJECAg/preview",
      "poster_pdf": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-8-codicodec-unifying-continuous",
      "title": "CoDiCodec: Unifying Continuous and Discrete Compressed Representations of Audio",
      "video": ""
    },
    "forum": "141",
    "id": "141",
    "pic_id": "",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nStrongly agree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nStrongly agree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nSee below\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nUsing quantization as dropout works and works well.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThe authors present extentions for the Music2Latent2 representation which is typically used in systems for generating music. A main contribution is the addition of a quantizer dropout bottleneck, which enables parallel training for continous and discrete representations, as well as a parallel decoding mechanism, which accelerates the previous autoregressive decoding scheme. Evaluation results indicate competitive or stronger results than competing solutions.\n\nOverall, a strong paper. A lot of detail, well written, concise, comprehensive, gives insights and shoes clearly that the authors know what they are talking about.\n\nYet, there are also issues:\n\nMy main concern is that little attention is given to the modelability, sometimes called diffusibility, of the representation in comparison to others. Latent representations face a tradeoff between compression rate, reconstruction quality and modelability, where the latter means that we often increase the capacity in the latent network, but pay for it by the fact that a music generator model needs to 'unpack' the resulting non-linearity and complexity from doing so, requiring either additional capacity there or leading to difficulties training such a system. Ignoring modelability it is much easier to obtain very competitive compression rates. While there is an experiment that looks into training a generative downstream model, the results are only compared between the quantized and non-quantized version of the proposed system. That does not indicate how the proposed system compares to Music2Latent2 or other competing latents with respect to quality of generation results given a certain capacity in the network. This is especially important as the new version focused much more on transformer layers in the latent and dials back on convolutive layers, while most image and video compression systems employ convolutive layers only to limit the non-linearity of the network to increase modelability.\n\nThe second biggest issue is the lack of a listening test. Maybe FAD-clap improves upon standard FAD, but FAD's correlations with listening tests is so low that I don't expect FAD_clap to be much more informative.\n\nAnother issue is that Stable Audio Open, prepresenting the most standard VAE based approach, which is pretty much the standard in almost all image and video generation systems, was not trained on vocals at all. Hence any test material using vocals will naturally be significantly worse, as vocals are unlike any other instrument. Hence a version of Stable Audio Open trained on material using vocals should create significantly better metrics than what is shown here.\n\nSome minor comments:\n\nLine 167: \"Our model uses CT, allowing for training in isolation without a pretrained teacher model.\" But line 177 clearly states that a teacher model is used. This should be clarified, including where the teacher model is coming from, and how and for what it was trained. \nLine 177: While it's clear that you don't want to update the teacher, the notation used here does not make that clear to someone who does not that know yet. \nLine 363: Why did you use a single, fixed value for N? This looks like a clear candidate for an ablation. Space issues?\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThis paper introduces a technically sophisticated extension of the Music2Latent2 system, proposing a unified framework that supports both continuous and discrete latent representations through the novel use of FSQ-dropout. The authors further enhance the model by implementing a parallel decoding strategy to speed up generation, all while achieving competitive performance metrics in terms of reconstruction quality and compression rates.\n\nThe submission has received a strong consensus from reviewers, with three assigning a \u201cstrong accept\u201d and one offering a \u201cweak accept.\u201d All reviewers acknowledge the paper\u2019s clarity, soundness, and relevance to the ISMIR community. The writing is well-structured and the methodology is well-grounded, reflecting the authors\u2019 expertise and the maturity of the research.\n\nStrengths\nTechnical Contributions: The introduction of FSQ-dropout is seen as an impactful and broadly applicable innovation. The approach enables the simultaneous training of models with both continuous and discrete representations, a practical feature with wide utility across audio synthesis and compression.\n\nParallel Decoding: The paper\u2019s proposal to replace autoregressive decoding with a faster, parallel alternative is well-received, offering significant improvements in generation speed, though with minor trade-offs.\n\nReusable Insights: All reviewers highlight that the paper provides valuable, generalizable insights \u2014 including detailed ablations for multiple architectural components \u2014 which could serve as a foundation for future research in MIR and related fields.\n\nComprehensive Evaluation: Although there are some caveats, the experimental section convincingly demonstrates the efficacy of the proposed model. Audio examples and ablations further support the validity of the results.\n\nAreas for Improvement\nModelability of Latent Representations: The meta-reviewer and Reviewer #1 raise concerns about the paper\u2019s limited attention to modelability \u2014 a critical trade-off in latent design. There is a need for deeper comparative analysis on how the proposed representations fare in terms of generative ease and network complexity relative to alternatives like Music2Latent2 or VAE-based systems.\n\nPerceptual Evaluation: The absence of listening tests is repeatedly noted. While the authors rely on FAD and its derivative metrics, several reviewers express skepticism regarding their correlation with human perception. Alternative metrics like KAD or actual listening tests would strengthen the evaluation.\n\nReproducibility Concerns: Two reviewers highlight that the paper does not fully meet reproducibility standards, particularly due to the use of proprietary datasets or unclear documentation regarding training procedures.\n\nClarifications and Minor Issues: There are a few inconsistencies and unclear points \u2014 such as the role of the teacher model, fixed hyperparameter choices, and citation formatting \u2014 that should be addressed in the camera-ready version.\n\nFinal Recommendation\nGiven the innovative methodological contributions, clear presentation, and strong practical implications, this paper stands out as a valuable addition to ISMIR 2025. While there are areas that warrant further clarification or empirical rigor, they are outweighed by the strengths and the potential impact of the work. Therefore, the final recommendation is Accept.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper presents an evolution of a recent autoencoder Music2Latent2 that uses drop out of the quantizer stage of a Finite Scalar Quantizer to allow producing a continuous and discrete latent representation with a single model. Addtionally the paper introduces a new parallel decoding strategy for the Music2Latent2. \n\nOverall I consider the presentation clear and the argumentation convincing. The experimental evaluation demonstrates improvement compared to the Music2Latent2 model, which besides the quantization strategy appears to be similar in the overall structure.\n\nI have a problem with the presentation of the novelties, which appear a little bit distorted\n\nThe introduction fails to mention [36], which is later mentioned under FSQ-dropout. I think [36] needs to appear in the introduction. They report to be able to trade of quality and code size during inference. This then slightly changes the perception of the novelties concerning FSD. It can probably be said the at the drop out stragey is new, and switching between discrete and continuous codes has never been investigated before. The sue of summary embeddings with Music2Latent2 have been introduced in [18] and should therefore not appear under \"introduce a new model\" in the introduction.\n\nAnother problem for me is that the authors somewhat misleadingly claim to improve over existing continuous and discrete autoencoders in terms of reconstruction quality measured by FAD (line 88). \nI would caution that FAD does not measure reconstruction quality. It measures similarity in an embedding-space. It is sometimes used as a proxy for perceptual quality, but it is not appropriate to say that this measures reconstruction quality. Indeed it frequently happens that a model with a lower FAD is perceived to have lower quality than a model with a higher FAD. \n\nIn the present case without percetopual tests, SISDR appears to be a better metric for quality. When compared to Stable Audio Open the proposed model does not perform that well. On one hand the proposed model compresses stronger, so I think we cannot conclude anything here. On the other hand there is alos a difference in the training losses. While the present study uses a single loss for consistency training, stable audio open uses reconstruction loss and adversarial loss. Given reconstruction loss is part of the training of stable audio this might favor lower SISDR.",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper combines many techniques to train a strong audio codec. They test the following:\n- FSQ dropout\n- scaling transformer architectures\n- Consistency loss\n- Parallel decoding\n- Random mixing\n\nThe results are impressive and the audio samples are appreciated. The promised code release would benefit the ISMIR community.\n\nI appreciate the ablations for each technique, providing general knowledge others can utilize.",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nOverall, the paper is well-organized and clearly written. It introduces a novel approach to unify the generation of both continuous and discrete representations within a single model using a technique called FSQ-dropout (which closely resembles an approach in [36]). Additionally, similar to Music2Latent2, the method incorporates summary embeddings to enhance the system's compression capabilities while improving its architectural design. The paper also proposes an alternative parallel decoding strategy to replace standard autoregressive decoding, offering improved decoding speed (albeit with the trade-off of requiring multiple generation steps rather than just one). The proposed model achieves a higher compression ratio than baseline methods while maintaining strong reconstruction fidelity.\n\nThe model is trained on a mixture of music, speech, and general audio data. However, the evaluation focuses solely on music reconstruction, leaving open the question of the model's robustness on speech and other types of audio. Furthermore, it is unclear whether the full MusicCaps dataset was used for evaluation or just a subset. The authors state that they manually verified none of the evaluation samples overlap with the training data, an approach that may be difficult to scale. As noted in the paper \"KAD: No More FAD! An Effective and Efficient Evaluation Metric for Audio Generation\", FAD is sensitive to sample size. It would also be valuable to see evaluation results using KAD, which has demonstrated stronger alignment with human perception and is not sensitive to sample size.\n\n* Minor grammar issue: Line 498: \"inevitable\" should be \"inevitably\"\n\n* References:\n- Several references are cited as arXiv preprints instead of their corresponding conference proceedings. For example: [2] TMLR 2023, [3] & [4] NeurIPS 2023, [9] ICLR 2021, [14] ICML 2024, [15] ISMIR 2024, [16] ICASSP 2025...\n- Citation [35]: The conference name is missing (should be ICLR 2025).\n\nOverall, I recommend this paper for acceptance."
    },
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Music source separation (MSS) aims to extract individual instrument sources from their mixture. While most existing methods focus on the widely adopted four-stem separation setup (vocals, bass, drums, and other instruments), this approach lacks the flexibility needed for real-world applications. To address this, we propose GuideSep, a diffusion-based MSS model capable of instrument-agnostic separation beyond the four-stem setup. GuideSep is conditioned on multiple inputs: a waveform mimicry condition, which can be easily provided by humming or playing the target melody, and mel-spectrogram domain masks, which offer additional guidance for separation.  Unlike prior approaches that relied on fixed class labels or sound queries, our conditioning scheme, coupled with the generative approach, provides greater flexibility and applicability. Additionally, we design a mask-prediction baseline using the same model architecture to systematically compare predictive and generative approaches. Our objective and subjective evaluations demonstrate that GuideSep achieves high-quality separation while enabling more versatile instrument extraction, highlighting the potential of user participation in the diffusion-based generative process for MSS. Our code and demo page are available at https://yutongwen.github.io/GuideSep/.\n",
      "abstract": "Music source separation (MSS) aims to extract individual instrument sources from their mixture. While most existing methods focus on the widely adopted four-stem separation setup (vocals, bass, drums, and other instruments), this approach lacks the flexibility needed for real-world applications. To address this, we propose GuideSep, a diffusion-based MSS model capable of instrument-agnostic separation beyond the four-stem setup. GuideSep is conditioned on multiple inputs: a waveform mimicry condition, which can be easily provided by humming or playing the target melody, and mel-spectrogram domain masks, which offer additional guidance for separation.  Unlike prior approaches that relied on fixed class labels or sound queries, our conditioning scheme, coupled with the generative approach, provides greater flexibility and applicability. Additionally, we design a mask-prediction baseline using the same model architecture to systematically compare predictive and generative approaches. Our objective and subjective evaluations demonstrate that GuideSep achieves high-quality separation while enabling more versatile instrument extraction, highlighting the potential of user participation in the diffusion-based generative process for MSS. Our code and demo page are available at https://yutongwen.github.io/GuideSep/.\n<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Yutong Wen",
        "Minje Kim",
        "Paris Smaragdis"
      ],
      "authors_and_affil": [
        "Yutong Wen (University of Illinois Urbana-Champaign)*",
        "Minje Kim (University of Illinois Urbana-Champaign)",
        "Paris  Smaragdis (University of Illinois Urbana-Champaign)"
      ],
      "channel_url": "",
      "day": "4",
      "keywords": [
        "",
        "Sound source separation",
        "MIR tasks"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1RWAwOWQ1vhBxELkJoMQPqfjI-NLS5bjy/preview",
      "poster_pdf": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-11-user-guided-generative",
      "title": "User-Guided Generative Source Separation",
      "video": ""
    },
    "forum": "147",
    "id": "147",
    "pic_id": "",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nStrongly agree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nStrongly agree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nStrongly agree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe analysis of differing behaviour between generative and discriminative/predictive source separation approaches can be used to guide future work\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nDiffusion-based music source separation can perform well when controlled by a mimicry audio condition of the target instrument and/or from a spectrogram mask locating the target instrument\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThe paper successfully lays the groundwork for music source separation methods that can be controlled by users in a more intuitive way, with the mimicry condition being a novel conditioning method.\n\nThe paper is overall very well written and scientifically rigorous. The ablation studies prove that both proposed conditioning methods contribute to separation performance and that the diffusion-based generative approach performs better than a mask-based one (although I am not convinced this finding is entirely novel, so I encourage some more literature review and to rephrase the paper's novelty claims in that regard if needed).\n\nThere are a couple of limitations, which are not discussed enough in the paper:\n- The humming is assumed to be closely time-aligned to the target source, by way of how the system is trained. This means that users need to hum along to the music and humming without listening along to the mixture would likely fail. The paper introduces some data augmentation, but distorts timing only slightly, so more variation here would be needed\n- The audio input duration is quite short (4s), and it's unclear how the model performs with longer inputs. This also seems to be a limiting factor in case more time-warped humming inputs are to be supported\n- Authors find including a synthesis task (generate from mimicry condition) in training helps, which is surprising, but don't provide an ablation result for that\n- The paper does not train on the actual task (real humming inputs) due to unavailable data, and only evaluates in a very synthetic setting, making it difficult to assess how useful the model actually is. For future research, it would be crucial to collect some data for this novel task\n\n\nSmall corrections:\n- L275 refers to the mimicry condition suddenly as melody - would be better to use one name consistently throughout the paper\n- L277 delete \u201cin\u201d\n- L431 - the diffusion-based approach in particular?\n- L433 - could you find a more precise word than \u201cclean\u201d? Do you mean free of artifacts?\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nReview summary\n\nThis paper introduces a generative source separation approach that allows user guidance via two modalities: time-synchronized humming and mel-spectrogram masks. The method is implemented using a diffusion-based model and aims to make the separation process more interactive and intuitive. The reviewers generally found the work to be original, well-executed, and relevant to the ISMIR community.\n\nTwo reviewers rated the paper as Strong Accept, citing clear exposition, a well-motivated problem, and a promising approach with reproducible implementation. A third reviewer gave a Weak Accept, while a fourth initially gave a Strong Reject due to concerns about the evaluation setup, lack of real user input or UI, and fairness of model comparisons. However, this reviewer later acknowledged the paper\u2019s novelty and agreed to revise their stance to a \"weak accept\", provided the subjective listening test results are more rigorously evaluated.\n\nThe core idea of guided source separation through user input is novel and an interesting foundation for further research. While the evaluation and experimental setup have weaknesses, some of these can be addressed as part of a camera-ready version by the authors. Additionally, the contribution of source separation by humming using a diffusion model is convincing.\n\nRecommendation: Weak accept\n\nNote to the Authors\nTo improve the final version of the paper, please address the following key points:\n\nClarify the Subjective Evaluation: Provide more details on MUSHRA test design, participant screening, and whether remixing affected perceptual quality judgments.\n\nAcknowledge Evaluation Limitations: Clearly state the limitations of your comparisons between generative and predictive models, including the lack of computational parity.\n\nDiscuss Practicality of Inputs: Expand on how mel-mask inputs would be obtained in real-world scenarios, and briefly discuss UI considerations or potential deployment contexts.\n\nImprove Consistency and Clarity: Fix terminology inconsistencies (e.g., \u201cmimicry\u201d vs. \u201cmelody\u201d) and minor typos identified by reviewers.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe paper \"User-guided generative source separation\" introduces GuiseSep, a source separation system conditioned on mel spectrograms and mimicry input. The work is very well structured and written and clearly puts into context and communicates its contributions.\n\n\nL. 252: Typo \"noramlized\" -> \"normlaized\"\nTable 1: Typo: \"psuedo\" -> \"pseudo\"",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nGeneral Comments\n----------------\nThe proposed approach is novel in its user-guided controllability and its integration of diffusion models with guided priors. The work is both timely and relevant, addressing a growing interest in interactive music source separation. The accompanying audio examples are convincing and engaging. However, several conceptual, methodological, and evaluative aspects of the paper would benefit from further clarification and development.\n\n\nDetailed Comments\n-----------------\nL.177: The statement \u201cExtraction using non-polyphonic instruments: We restrict the condition melody to be monophonic to reflect real-world limitations of many instruments\u201d is a somewhat confisuing. The authors should clarify what is meant by \u201creal-world limitations,\u201d particularly since many real-world instruments (e.g., piano, guitar, harp) are inherently polyphonic.\n\nL.243\u2013L.252 (Notation clarity): The mathematical notation could be improved for precision and readability. Specifically:\n* The variable c is overloaded\u2014used both as a complex spectrogram and as an instrument label.\n* c_mix refers to the STFT domain, while c_mask is defined in the mel-spectrogram domain. As these are different time-frequency representations, the distinction should be made more explicit. \n* In the subsequent section, the authors mention a projection of the mel-axis using a 1-hidden-layer neural network. It would help readers if the dimensions of each matrix were provided in this earlier section, along with a note that architectural details will follow.\n* Mel vs. STFT Masking: The decision to apply masking in the mel domain instead of the STFT domain is not discussed. This design choice could have significant implications (e.g., resolution trade-offs, perceptual smoothness), and the rationale should be clarified.\n\nL.288 (Terminology): The acronym ODE should be defined as Ordinary Differential Equation when first introduced, for readers unfamiliar with this terminology.\n\nL.297\u2013L.299 (Evaluation metrics): The evaluation focuses on objective metrics like SDR, but perceptual quality is critical in music separation tasks. The authors are encouraged to include perceptual metrics or refer to established evaluation frameworks. For a comparative overview of metrics, see:\nM. Torcoli, T. Kastner and J. Herre, \"Objective Measures of Perceptual Audio Quality Reviewed: An Evaluation of Their Application Domain Dependence,\" in IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 1530-1541, 2021, doi: 10.1109/TASLP.2021.3069302. \n\nSection 3.1.1 (Dataset selection): If the model is designed to be source-agnostic, it would be valuable to test it on a wider range of musical contexts, especially multitrack classical music datasets. This would provide insights into how well the model generalizes across diverse instrumentation and structural complexity. For example, datasets like PHENICX-SMM or PCD could be considered:\n* M. Schedl, D. Hauger, M. Tkal\u010di\u010d, M. Melenhorst, and C. C. S. Liem, \u201cA dataset of multimedia material about classical music: PHENICX-SMM,\u201d in Proc. Int. Workshop on Content-Based Multimedia Indexing (CBMI), Bucharest, Romania, 2016, pp. 1\u20134, doi: 10.1109/CBMI.2016.7500240.\n* Y. \u00d6zer, S. Schw\u00e4r, V. Arifi-M\u00fcller, J. Lawrence, E. Sen, and M. M\u00fcller, \u201cPiano Concerto Dataset (PCD): A multitrack dataset of piano concertos,\u201d Transactions of the International Society for Music Information Retrieval, vol. 6, no. 1, pp. 75\u201388, 2023.\n\nSection 3.1.2 (Dropout strategies): The dropout-based sampling strategy is conceptually interesting and appears effective. However, an ablation study comparing performance with and without dropout would strengthen the empirical justification for this approach.\n\nSection 4.1 (Subjective listening tests): \n* MUSHRA methodology recommends a post-screening of the participants stating that participants should be excluded from the listening test if they\nassign the hidden reference to a score lower than 90 for more. In Figure 3a, the average score for the ground truth (GT) signal is exactly 90, raising questions about listener behavior and post-screening. It is unclear whether such screening was applied.\n* The paper does not report whether the subjective differences are statistically significant.\n* The experience level of the 13 participants is not discussed, and 13 listeners is a relatively small sample for subjective testing.",
      "review3": "- **Overall Evaluation**: Strong reject\n\n#### Main Reviews\n\nThe proposed method employs mimicry (melodically similar audio) and mel-masks as query inputs for source separation. Using humming and mel-masks as user-generated queries is an interesting and differentiated approach compared to existing methods that rely on simpler queries, such as textual labels (e.g. instrument classes) or audio examples with similar timbres. Nevertheless, the paper is rejected due to the following concerns:\n\nReasons for Rejection:\n1. Limited Practicality and Lack of UI Details The proposed approach appears less practical compared to existing methods because the required user inputs are relatively complex. Given such complexity, a carefully designed user interface (UI) would be essential; however, the paper does not provide any detailed description or demonstration of the proposed UI.\n2. Fundamental Limitations for Polyphonic and Transient Sources The mimicry condition assumes a monophonic melody to simulate humming, imposing inherent limitations. This assumption restricts the model\u2019s applicability to polyphonic or harmonically complex sources (e.g., unison or polyphonic instruments). Moreover, it fundamentally lacks suitability for transient-rich sounds like drums.\n3. Limited Contribution Considering Computational Cost and Audio Quality Although the proposed method is not intended for real-time processing, it still operates at a relatively low sampling rate of 16 kHz. This limitation, combined with the comparatively low subjective audio quality demonstrated in the provided demos\u2014particularly in comparison to existing commercial software\u2014significantly restricts the overall contribution and potential impact of the paper.\n4. Unfair Computational Comparison A major contribution highlighted in the paper is the systematic comparison between predictive and generative models. However, this comparison is not fair from a computational standpoint. To fairly evaluate performance, the generative model should use single-step inference or at least be evaluated under equivalent computational costs compared to the predictive model.\n5. Low Reliability of Subjective Evaluation The MUSHRA test produced questionable results, with the ground truth (hidden reference) scoring approximately 90 points. Such a low score strongly suggests problems with experiment design, listener instruction, or overall experiment reliability, undermining confidence in all reported subjective evaluation outcomes.\n\nMinor Comment:\n* When performing data augmentation for mel-spectral masks, randomization of frequency and time offsets should also be considered for robustness."
    },
    "session": "7"
  },
  {
    "content": {
      "TLDR": "General-purpose audio representations have proven effective across diverse music information retrieval applications, yet their utility in intelligent music production remains limited by insufficient understanding of audio effects (Fx). Although previous approaches have emphasized audio effects analysis at the mixture level, this focus falls short for tasks demanding instrument-wise audio effects understanding, such as automatic mixing. In this work, we present Fx-Encoder++, a novel model designed to extract instrument-wise audio effects representations from music mixtures. Our approach leverages a contrastive learning framework and introduces an ``extractor'' mechanism that, when provided with instrument queries (audio or text), transforms mixture-level audio effect embeddings into instrument-wise audio effect embeddings. We evaluated our model across retrieval and audio effect parameter matching tasks, testing its performance across a diverse range of instruments. The results demonstrate that Fx-Encoder++ outperforms previous approaches at mixture level and show a novel ability to extract effects representation instrument-wise, addressing a critical capability gap in intelligent music production systems.",
      "abstract": "General-purpose audio representations have proven effective across diverse music information retrieval applications, yet their utility in intelligent music production remains limited by insufficient understanding of audio effects (Fx). Although previous approaches have emphasized audio effects analysis at the mixture level, this focus falls short for tasks demanding instrument-wise audio effects understanding, such as automatic mixing. In this work, we present Fx-Encoder++, a novel model designed to extract instrument-wise audio effects representations from music mixtures. Our approach leverages a contrastive learning framework and introduces an ``extractor'' mechanism that, when provided with instrument queries (audio or text), transforms mixture-level audio effect embeddings into instrument-wise audio effect embeddings. We evaluated our model across retrieval and audio effect parameter matching tasks, testing its performance across a diverse range of instruments. The results demonstrate that Fx-Encoder++ outperforms previous approaches at mixture level and show a novel ability to extract effects representation instrument-wise, addressing a critical capability gap in intelligent music production systems.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Yen-Tung Yeh",
        "Junghyun Koo",
        "Marco Mart\u00ednez-Ram\u00edrez",
        "Wei-Hsiang Liao",
        "Yi-Hsuan Yang",
        "Yuki Mitsufuji"
      ],
      "authors_and_affil": [
        "Yen-Tung Yeh (National Taiwan University)*",
        "Junghyun Koo (Sony AI)",
        "Marco Mart\u00ednez-Ram\u00edrez  (Sony AI)",
        "Wei-Hsiang Liao (Sony AI)",
        "Yi-Hsuan  Yang (National Taiwan University)",
        "Yuki Mitsufuji (Sony AI, Sony Group Corporation)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Applications",
        "Musical features and properties",
        "Representations of music",
        "Music composition, performance, and production",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1KFjtKnn8F5aJ6BNu0rQmA1qZALlsYcMz/preview",
      "poster_pdf": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-14-fx-encoder-extracting",
      "title": "Fx-Encoder++: Extracting Instrument-Wise Audio Effects Representations from Mixtures",
      "video": ""
    },
    "forum": "148",
    "id": "148",
    "pic_id": "",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nDisagree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe paper has some resuable insights, e.g., that the overall approach of combining mixture-level and instrument-level (through querying) effects representation learning can yield better results on both levels, work well on complex effects chains, but suffer in the presence of novel timbral characteristics.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nContrastive FX representation learning augmented with query-based extraction results in improved representation learning at both the instrument and mix level.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nSummary\n-------\nThis paper presents an evolution of FX-Encoder to extract audio effects representations from both mixtures and queried instruments from within mixtures. They evaluate this task on both effect parameter retrieval, as well as effect parameter estimation via inference time optimization. They find that the proposed method \n\n\nComments\n-------- \nOverall, this paper presents an advancement of audio effects representation learning models. While prior models operated at either the mixture or instrument level, this work proposes a solution that works at both levels. The proposed methods in the paper seem sound and perform better than prior work in almost all scenarios, but there is still much room for improvement. Unfortunately, the paper has minimal error analysis, providing little insight into how such models can be improved in the future. Furthermore, many details about the evaluation are missing, including those about the evaluation dataset as well as a whole table of results related to Section 5.2. Such omissions reduce both the reproducibility of the paper and its reusable insights.\n\n\nSpecific Comments\n-----------------\nSection 4.1 - More detail is needed about the construction of the evaluation dataset, e.g., how the effects and parameters are sampled. How were the mixtures constructed (are there ever multiple instances of the same class)? Can there be single instruments? Is the effect parameter evaluation set distinct from the training set?\n\nSection 5.1 - The statement 'Notably, even when using high-quality source separation, we observe a clear gap between the \"Target Instrument\" and \"MSS(m)\"' seems to be incorrect. ST-ITO w/ MSS(m) performs better than target extraction.\n\nSection 5.2 - The table referred to in this section seems to be completely absent. The results are not present.\n\nSection 6 - What is 'VA modeling'? This has not been defined in this paper.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThe majority of reviewers agreed that this paper is addressing an important topic and is generally well-written. Some noted that the evaluation could be stronger, e.g., with an ablation study, but the majority agreed that this should be included in the ISMIR program. That said, reviewers noted aspects of the writing that could be improved before a final submission. For example, while reviewers could envision the application and utility of this work, they noted that the authors should more clearly communicate the motivation and potential applications of the work, particularly regarding the instrument-specific embeddings. Furthermore, reviewers said that authors should provide a clearer interpretation of the instrument-specific embedding evaluation.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nThis paper presents FX-Encoder++, an innovative model addressing a key challenge in music technology: extracting instrument-specific audio effect representations directly from full music mixtures without the need for source separation. The method leverages a pretrained CLAP encoder alongside an \u201cextractor\u201d mechanism, enabling both audio and text queries. A carefully designed contrastive learning framework, underpins the training process, (FX-Normalization, consistent instrument composition, and hand-crafted hard negatives ensures robust learning, the dual-objective). However, I have several concerns and questions regarding the manuscript:\n\nMotivation and Problem Definition\nThe main concern is: why is it important to extract instrument-specific embeddings from mixtures? \n\nThe authors state in lines [39\u201345]:\n\u201cApplications in this domain require..., how they shape the overall sound of a complete mixture (\u2018mixture level\u2019) and how they transform individual instruments within that mixture (\u2018instrument level\u2019).\u201d\nThis statement reads more as a definition constructed for the purpose of this work rather than a widely accepted requirement of FX-specific representation learning. Based on my understanding, the goal of FX-specific representation learning is to embeddings that are specific to audio effect transformations (rather than invariant to them, as in general-purpose embeddings). \nTherefore, the motivation for why versatile understanding of both mixture and single-instrument content is necessary should be clarified and better grounded in prior literature. From the citations in the introduction [16,17], it seems that only guitar FX classification requires instrument-wise embeddings, which are not directly evaluated in this work. (also this task not extract embedding from mixture)\nIn summary, how is the instrument embedding extracted from a mixture used in music production? \n\nInstrument-Specific Conditioning\n- Line [275]: How is conditioning performed via the MLP layer? and How to attend to effect-related features?\n\nAre the mixture-level embedding and CLAP query embedding concatenated? Is Adaptive Layer Normalization used? What specific operations are involved?\n\n\nWriting and Presentation\n- Line [160]: The phrase \u201cis generate\u201d is inappropriate here. Since your model is not a generative model or does not involve stochastic sampling, please revise this terminology.\n- Figure 1: The figure and caption are confusing. It\u2019s unclear whether \"splitting source tracks\" refers to time-based segmentation or instrument-wise source separation. The use of inconsistent icons (e.g., guitar, drums, mixture) adds to the confusion. \n- Lines [239\u2013240]: CLAP supports both audio and text queries during training and inference. Why does the paper only mention audio in line 239-240? And the biggest advantage of using CLAP is that 1) it can handle multi-modal input (audio and text) and 2) it works even with zeroshot (unseen) words. Other than that, it is no different from using random text embedding (one-hot instruments class embedding) or any audio embedding. I think it would be good to write down the purpose of using CALP Encoder.\n\nEvaluation\n- Lines [361\u2013362]: Are there any overlaps between the 8 instrument queries and MoisesDB?. Please clarify whether this evaluation setup is in-domain or out-of-domain.\n- Line [372]: What are the queries and targets in the mixture-level retrieval evaluation? The phrase \u201ctesting effect identification in complete mixtures or isolated recordings\u201d is ambiguous. Are these the query types, the retrieval targets?\n\n\nMinor Comments\n- Line [357]: The task of query-by-audio retrieval should be supported by references in audio retrieval [ref1] or audio fingerprinting [ref2], not solely CLAP. CLAP primarily addresses text-to-audio retrieval. (If I were to explain metrics rather than retrieval tasks in this part, I would state that Recall and Precision are not proposed in CLAP paper.)\n[ref1] Disentangled multidimensional metric learning for music similarity, ICASSP 2020\n[ref2] Neural Audio Fingerprint for High-specific Audio Retrieval based on Contrastive Learning, ICASSP 2021\n\n- Line [376]: The phrase \u201cdirectly from mixtures\u201d is vague and should be rephrased for clarity.\n\nI believe this paper presents impressive experiments and result tables, and it is a valuable contribution. However, due to (1) the insufficient discussion on the necessity of instrument-specific audio effects and (2) the relatively weak writing quality in the methods and evaluation sections, I recommend a weak reject.\n\nTo strengthen the work, I suggest the following for reframing the paper:\n1. Clearly articulate how fine-grained, instrument-wise information can improve the quality of mixture embeddings.\n2. Consider adding an instrument-specific music production task as a downstream application.",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe authors propose Fx-Encoder++, a novel model that extracts instrument-wise audio effects representations from music mixtures using a contrastive learning framework and an extractor mechanism guided by audio or text queries. They demonstrate that their approach outperforms previous methods on retrieval and parameter matching tasks, effectively advancing intelligent music production through improved understanding of audio effects at the instrument level.\n\nI find the work very interesting and well-written, however there are some points that are not clear. I have some comments that I would like the authors to address in order to improve the quality of the manuscript.\n\n- Introduction, line 55: what do you mean with \u201cHowever, they focus only on modeling the aggregate result, rather than identifying how effects have been applied to each instrument\u201d? Do you mean they model only the full Fx-chain and not each single Fx? \n- Sec. 3.1, line 177: \u201ciff\u201d -> \u201cif\u201d.\n- Equation (2): please, define the operator \\sim{.} and N.\n- Sec. 3.1, line 200: what do you mean with \u201cnegative\u201d and \u201cpositive\u201d pairs?\n- Sec. 3.1, line 211: Is there a requirement for the effects to be differentiable? \n- Sec. 3.1, line 290: can you please tell more about the scheduling you used to introduce the instrument-level loss? Is \\lambda_{inst} increasing linearly? Or following cosine raising? Can you also tell more about the results you obtained instead without introducing such a scheduling or by swapping the paradigm?\n- I do not know if I missed it, but can you give information on how the audio query is supposed to be?\n- Sec. 5.1, line 421: please, define USS. You should also explicitly define q_{text} and q_{audio}. To be consistent and precise, define also MSS.\n- Sec. 5.2, line 453: I do not find any definition for L_d, which you use to quantify the performance of the methods as far as matching the effect parameters is concerned. Please, add more information.\n- I really suggest to add a github page with audio examples in order to clarify even better the performance of the models.\n- Can you also provide applications of such instrument-specific embeddings?",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper tackles the important and novel problem of extracting instrument-wise audio effect representations from mixtures, an emerging challenge in the field of intelligent music production, through an original approach that includes mechanisms such as the extractor module. The demonstrated high performance on the audio effect retrieval task mark a great advancement in the field. Particularly noteworthy is the finding illustrated in Figure 2, where retrieval performance improves as the number of effects increases, a compelling result. Although there are some limitations, such as parameter matching performance and understanding of single effects, these are challenges that could be addressed in future work. Given the novelty and potential impact of the proposed approach, I consider this paper a valuable contribution to the ISMIR community.\n\nThat said, one point of concern is the lack of ablation studies for the proposed method. As a result, it is unclear which techniques contribute to the observed performance improvements. For instance, in the contrastive learning setup, techniques like Fx-Normalization and Hand-Crafted Hard Negative Samples are employed, but the individual effects of these components on retrieval and classification performance have not been evaluated. Investigating these aspects is important to validate the soundness of the method. Therefore, I do not believe this paper is suitable for an award recommendation."
    },
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Melody reduction, as an abstract representation of musical compositions, serves not only as a tool for music analysis but also as an intermediate representation for structured music generation. Prior computational theories, such as the Generative Theory of Tonal Music, provide insightful interpretations of music, but they are not fully automatic and usually limited to the classical genre. In this paper, we propose a novel computational method for melody reduction using a graph-based representation inspired by principles from computational music theories, where the reduction process is formulated as finding the shortest path. We evaluate our algorithm on pop, folk, and classical genres, and experimental results show that the algorithm produces melody reductions that are more faithful to the original melody and more musically coherent than other common melody downsampling methods. As a downstream task, we use melody reductions to generate symbolic music variations. Experiments show that our method achieves higher quality than state-of-the-art style transfer methods.",
      "abstract": "Melody reduction, as an abstract representation of musical compositions, serves not only as a tool for music analysis but also as an intermediate representation for structured music generation. Prior computational theories, such as the Generative Theory of Tonal Music, provide insightful interpretations of music, but they are not fully automatic and usually limited to the classical genre. In this paper, we propose a novel computational method for melody reduction using a graph-based representation inspired by principles from computational music theories, where the reduction process is formulated as finding the shortest path. We evaluate our algorithm on pop, folk, and classical genres, and experimental results show that the algorithm produces melody reductions that are more faithful to the original melody and more musically coherent than other common melody downsampling methods. As a downstream task, we use melody reductions to generate symbolic music variations. Experiments show that our method achieves higher quality than state-of-the-art style transfer methods.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Ziyu Wang",
        "Yuxuan Wu",
        "Roger Dannenberg",
        "Gus Xia"
      ],
      "authors_and_affil": [
        "Ziyu Wang (NYU Shanghai)*",
        "Yuxuan Wu (MBZUAI)",
        "Roger Dannenberg (Carnegie Mellon University)",
        "Gus Xia (MBZUAI)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Symbolic music processing",
        "Musical features and properties",
        "Music generation",
        "Representations of music",
        "Creativity",
        "MIR tasks",
        "Computational creativity",
        "MIR fundamentals and methodology",
        "Computational music theory and musicology",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1u3CJ280LdRPATXbA2nsCcNSw6cFzeqSm/preview",
      "poster_pdf": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-12-automatic-melody-reduction",
      "title": "Automatic Melody Reduction via Shortest Path Finding",
      "video": ""
    },
    "forum": "150",
    "id": "150",
    "pic_id": "",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Japanese idol groups, comprising performers known as \"idols,\" are an indispensable part of Japanese pop culture. They frequently appear in live concerts and television programs, entertaining audiences with their singing and dancing. Similar to other J-pop songs, idol group music covers a wide range of styles, with various types of chord progressions and instrumental arrangements. These tracks often feature numerous instruments and employ complex mastering techniques, resulting in high signal loudness. Additionally, most songs include a song division (utawari) structure, in which members alternate between singing solos and performing together. Hence, these songs are well-suited for benchmarking various music information processing techniques such as singer diarization, music source separation, and automatic chord estimation under challenging conditions. Focusing on these characteristics, we constructed a song corpus titled IdolSongsJp by commissioning professional composers to create 15 tracks in the style of Japanese idol groups. This corpus includes not only mastered audio tracks but also stems for music source separation, dry vocal tracks, and chord annotations. This paper provides a detailed description of the corpus, demonstrates its diversity through comparisons with real-world idol group songs, and presents its application in evaluating several music information processing techniques.",
      "abstract": "Japanese idol groups, comprising performers known as \"idols,\" are an indispensable part of Japanese pop culture. They frequently appear in live concerts and television programs, entertaining audiences with their singing and dancing. Similar to other J-pop songs, idol group music covers a wide range of styles, with various types of chord progressions and instrumental arrangements. These tracks often feature numerous instruments and employ complex mastering techniques, resulting in high signal loudness. Additionally, most songs include a song division (utawari) structure, in which members alternate between singing solos and performing together. Hence, these songs are well-suited for benchmarking various music information processing techniques such as singer diarization, music source separation, and automatic chord estimation under challenging conditions. Focusing on these characteristics, we constructed a song corpus titled IdolSongsJp by commissioning professional composers to create 15 tracks in the style of Japanese idol groups. This corpus includes not only mastered audio tracks but also stems for music source separation, dry vocal tracks, and chord annotations. This paper provides a detailed description of the corpus, demonstrates its diversity through comparisons with real-world idol group songs, and presents its application in evaluating several music information processing techniques.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Hitoshi Suda",
        "Junya Koguchi",
        "Shunsuke Yoshida",
        "Tomohiko Nakamura",
        "Satoru Fukayama",
        "Jun Ogata"
      ],
      "authors_and_affil": [
        "Hitoshi Suda (National Institute of Advanced Industrial Science and Technology (AIST))*",
        "Junya Koguchi (Meiji University)",
        "Shunsuke Yoshida (The University of Tokyo)",
        "Tomohiko Nakamura (National Institute of Advanced Industrial Science and Technology (AIST))",
        "Satoru Fukayama (National Institute of Advanced Industrial Science and Technology (AIST))",
        "Jun Ogata (National Institute of Advanced Industrial Science and Technology (AIST))"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Timbre, instrumentation, and singing voice",
        "Musical features and properties",
        "Novel datasets and use cases",
        "MIR tasks",
        "Evaluation, datasets, and reproducibility",
        "Sound source separation"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1tT2nLOZoda5AqZ-aRwZmZo17BHMkXLao/preview",
      "poster_pdf": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-4-idolsongsjp-corpus-a",
      "title": "IdolSongsJp Corpus: A Multi-Singer Song Corpus in the Style of Japanese Idol Groups",
      "video": ""
    },
    "forum": "159",
    "id": "159",
    "pic_id": "",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q5 ( Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chosen, otherwise write \"n/a\"))**\n\nA few other items might be appropriate to discuss here, especially since the analysis section dovetails into computational musicology analysis of the chord distributions. These are not well known and so it is not surprising the authors would not know them, but still useful:\n\n[Note first one is in Japanese and requires translation].\n[1] \u6a2a\u5c71\u771f\u7537, \u6589\u85e4\u52c7\u4e5f, Y. Masao, and S. Youya, \u201c\u30d2\u30c3\u30c8\u30c1\u30e3\u30fc\u30c8\u30e9\u30f3\u30ad\u30f3\u30b0\u4e0a\u4f4d\u306b\u5165\u308b\u697d\u66f2\u306e\u7279\u5fb4\u5206\u6790,\u201d \u7814\u7a76\u5831\u544a\u97f3\u697d\u60c5\u5831\u79d1\u5b66\uff08MUS\uff09, vol. 2015-MUS-106, no. 22, pp. 1\u20136, Feb. 2015.\n\n[2] J. Lim, \"Gendered Voices in Japanese Popular Music: A Data-Driven Analysis.\" Order No. 28769325, University of Toronto (Canada), Canada -- Ontario, CA, 2021.\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nDisagree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe corpus, while technically small, is very clean and well curated, fostering a fair amount of tasks if one considers data augmentation. For computational musicological analysis it is less useful on account of the small number of songs, but still a valuable contribution to the community for the other tasks mentioned by the authors; many of which are biased by reliance on primarily Western materials.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThe authors release a small but extremely high quality corpus of musical material specifically composed to be in the style of JPOP idol music including audio stems, annotations, lyrics, as well as dry and produced & mastered audio.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nNote: While I don't think that ISMIR has had any size restrictions, 3.2Gb for supplementary materials is very large and does not work with Microsoft cmt. For the future, please simply upload one or two examples for illustrative purposes. (FYI reviewers are not obligated to view supplementary materials).\n\nReview: \n\nThe authors put together a well-written paper outlining a novel (albeit modest) corpus of original music in the style of J-POP idol music. The corpus is small but of extremely high quality on account of the methods for creating it. Not only did they provide great detail about the corpus and how it was assembled, but they went beyond hypothetical use cases to actually provide several demonstrations of the performance of their corpus with stem separation, automatic chord estimation, and lyric transcription. Overall, this is a great paper and I commend the authors on their contribution. I mostly have minor comments on things that would improve the readability of the paper and figures, and the utility of the corpus as well, in particular for computational musicology use. \n\n\nMinor details: \n\nThe first paragraph of the introduction is quite weak and \"zooms out\" too far to basically describe all of MIR. I suggest some restructuring of the intro. In fact, I recommend removing the whole first paragraph -- it seems that the second paragraph could work better as the opening paragraph with some revision.\n\nLInes 76-77: I assume this is a video technique? What does this sentence add, exactly? Is it relevant?\n\nLines 77-79: it's not clear to me what \"specialized methods\" is referring to.\n\nLine 99: In psychoacoustics we like to distinguish between auditory (perceptual) phenomena and acoustic (measurable) phenomena. Here \"loudness\" is an auditory phenomena and I wonder if the authors could consider replacing this with whatever the primary parameter being modulated actually is? (E.g., compression?)\n\nLine 107-111: Consider removing. (Perhaps I am mistaken but I don't think the reading audience should need additional \"proof\" of the cultural and musical relevance of JPOP.)\n\nLine 116: Suggest replacing with \"realistic song structure\" (since the \"division\" part is ambiguous at this point. Or else put the \"utawari\" in parentheses.)\n\nLines 123-127: these are 100% cloned from the abstract (or vice-versa). I would suggest some (at least subtle) rewriting.\n\nFigure 1: It is unclear why only the drums and \"other instruments\" (guitar, fx, piano) go into the \"stems for music source separation\"? Obviously the paper talks about the vocals for stem separation but this doesn't seem to be reflected in the figure? (vocals only go to 'master bus w/o limiter' in the diagram).\n\nLines 225-228: Could we have a bit more info on the chord annotations? Most papers that work in chord labels or have provided expert labels will note problems of ambiguity, etc. How many annotators per song? Were at least some songs done in duplicate to examine differences, etc.? Also, in my professional experience there is no such thing as a \"professional annotator\"? So I assume the authors are referring to someone who got paid to annotate (and who would be qualified to do so)? Some clarification on this methodology would be very helpful for people hoping to use the chord labels with confidence.\n\nLines 229-238: Very nice to put this right in the paper! Often people forget to even put it in their github repos! However, I would suggest moving it to the end of the paper under a \"License/Usage\" subsection?\n\nFigure 5: Could the authors kindly clarify the difference between the chords being evaluated in \"MIREX\" versus \"MIREX4\"? (I presumed the latter was a subset of the former but then the outcome would be odd.) Likewise, a \"tetrad\" is merely any 4-note chord, which is not defined here but presumably would overlap with both \"sevenths\" and \"MIREX4\" yet the outcomes are quite different. Please clarify.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nStrong accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nAll reviewers were in agreement about the strength of the contribution of this paper to our community, as well as the novelty and quality of the work itself. Two reviewers pointed out some flaws related to the experiments conducted as \"tests\" of the dataset. Specifically, Reviewers #1 and #2 both independently pointed out the problem with many mastering effects being non-linear, and that applying the same parameters to individual stems may be technically inaccurate. \n\nWe strongly encourage the authors to include a statement acknowledging the issues related to separation of mastered music, as pointed out by Reviewer 3, and to amend to the paper how a possible solution to this issue would be \"future work\". This would strengthen the paper significantly.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nIn this paper, the authors propose the IdolSongsJP corpus, a collection of songs in the style of Japanese idol groups. Overall, the paper is well-written, easy to read, and presents a much-needed contribution to the academic community. I think there is a flaw in the experiment regarding the separation of mastered music but since this paper's contribution is much more impactful to the community, I recommend this paper as 'weak accept'. My comments are as follows:\n\nStrengths:\n\n1. Curating and publicly releasing datasets is a time-consuming and costly endeavor, so I greatly appreciate the authors\u2019 efforts. This is a tremendous asset for the MIR community. \n2. While similar to MedleyDB [1], this corpus is distinctive in that it focuses on idol songs. Also, it takes mastering effects into account. This better reflects the characteristics of real-world commercial music. Additionally, for the vocal tracks, raw stems before the application of effects are included, making it potentially valuable for future multi-singer separation research.\n3. The authors go beyond simply providing data by conducting benchmark experiments for several applications such as music source separation, automatic chord estimation, and lyrics transcription. Especially in Section 6, they offer detailed performance analysis, such as pointing out hallucination issues with Whisper.\n\nComments:\n\n1. While the introduction is generally well written, there are some parts that could be misleading. For example, in lines 52\u201353, musdb is described as lacking numerous tracks of both instruments and vocals. This is not entirely accurate for all tracks in musdb. A better phrasing might be to emphasize that the IdolSongsJP corpus more clearly reflects the characteristics of modern commercial idol music, where all songs consist of numerous tracks.\n2. In lines 61\u201362, the authors mention overlapping vocal separation [2] and the jaCappella corpus [3], but they should not omit MedleyVox [4], which also addresses multi-singer separation in popular music. Furthermore, its original version, MedleyDB [1], should also be mentioned since it is another representative multi-track corpus consisting of the multiple singer tracks with the individual instrumental tracks.\n3. Lines 107\u2013111 (e.g., \"received the Best Lyrics Award\") seem unnecessary. The idea that idol group songs are an indispensable part of Japanese pop culture is sufficiently convincing without this example.\n4. Regarding lines 166\u2013169, I believe it's quite rare for creators to prepare a low-loudness version specifically for online platforms. Mastering at lower loudness is not as simple as reducing the limiter; it often involves rebalancing via EQ and other processing on the master bus\u2014 it is a second round of mastering. Therefore, professionals usually finish mastering targeting high loudness levels and simply submit those tracks to streaming platforms. Platforms like YouTube and others perform loudness normalization (e.g., to -14 to -16 LUFS) by reducing gain. Of course, some artists release less-compressed version for CD release by the way. \n5. Related to the previous point, in lines 293\u2013299, it is crucial to clarify the target LUFS level used for mastering in this corpus. If tracks were originally mastered at -9 LUFS and then simply made louder by adjusting the gain, the balance between instruments would change unless EQ and dynamics parameters were also adjusted. Saying the tracks were made louder merely by tweaking gain parameters could be misleading. A more technically accurate description would be: the tracks were mastered at -9 LUFS, and additional loudness was achieved by gain adjustment, not by full re-mastering.\n6. In lines 300\u2013302, the authors should specify exactly what kinds of mastering effects were applied\u2014e.g., limiter, EQ, imager, distortion, compressor, etc. To my understanding, parameter settings for these effects are not included in the corpus. Assuming the project files for each song still exist, a future extension of the corpus (possibly via a separate paper or journal extension?) including such information could contribute to research on automatic mixing and mastering.\n7. In lines 303\u2013307, it is stated that the same mastering effects were applied to individual stems. However, many mastering effects (except EQ) are non-linear, so applying the same parameters to individual stems may be technically inaccurate. In the case of limiters or compressors, [5] already proposed a method to calculate the sample-wise gain ratio between the input mixture (summation of stems before applying limiter) and the limiter-applied mixture, which can then be used to derive ground-truth stems. If such considerations were included, this should be clearly stated. If not, they should be addressed for the camera-ready version. I understand that the schedule for additional experiments would be very tight so at least I STRONGLY recommend that at least the authors include the limitation of the current experimental setting. For imagers, a different method may be required, and I am unsure if such a method or related research currently exists.\n\n\n[1] Bittner, R. M., Salamon, J., Tierney, M., Mauch, M., Cannam, C., & Bello, J. P. (2014, October). Medleydb: A multitrack dataset for annotation-intensive mir research. In Ismir (Vol. 14, pp. 155-160).\n[2] D. Petermann, P. Chandna, H. Cuesta, J. Bonada, and E. Gomez, \u201cDeep learning based source separation ap- plied to choir ensembles,\u201d in Proc. 21st International Society for Music Information Retrieval Conference (ISMIR 2020), 2020. \n[3] T. Nakamura, S. Takamichi, N. Tanji, S. Fukayama, and H. Saruwatari, \u201cjaCappella Corpus: A Japanese a cappella vocal ensemble corpus,\u201d in Proc. 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023, pp. 1\u20135. \n[4] Jeon, C. B., Moon, H., Choi, K., Chon, B. S., & Lee, K. (2023, June). Medleyvox: An evaluation dataset for multiple singing voices separation. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 1-5). IEEE.\n[5] Jeon, C. B., & Lee, K. (2022, December). Towards robust music source separation on loud commercial music. In Ismir 2022 Hybrid Conference.",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nMain Comments:\n1. The paper was well written and easy to follow.\n2. I think the authors could have emphasized the novelty of their corpus much more, especially in comparison to other multi-singer and J-pop corpora. I know that a key characteristic of this corpus is that it is free to share without copyright restrictions and open for non-commercial use, but beyond that, what improvements does it make upon prior work? Is it the loudness criteria and mastering chain? More details about very similar previously published corpora would strengthen the value added by this dataset to the MIR research community.\n3. The dataset is described in detail, but I am confused by some of the design choices:\n- Why are the audio signals being saved as 32-bit float? I believe that 24-bit should be sufficient and would consume a lot less memory. Do you foresee any advantage in using a higher bit depth in terms of the results of the various MIR applications?\n- For clarification, are all tracks stereo? Was there any artificial stereoizer used in the FX chain? What kinds of bus FX were used in the stems?\n- I found the description of solo version tracks very confusing (lines 218-224). Was it the same singer throughout the entire track? This part should be revised to be much clearer.\n4. I did not understand what exactly was meant by lines 236-238 regarding the instrumental tracks. Does this mean that only the full tracks or vocals only can be used for training ML models? Why is there a restriction on using the instrumentals?\n5. In Section 4, the \"same mastering effects\" (line 304) were applied to the individual stems to match the \"acoustic characteristics\" of the final mastered version (input mixture), so were those used as the reference stems for computing the SDR? If so, I don't think this is a fair comparison. Some of the FX in the mastering chain, e.g. limiters, are not LTI (specifically, linear) systems, so applying the same FX to the original stems does not directly replicate the separated versions of the input mixture (the mastered tracks). Audibly, the difference will be minor, but in terms of computing SDR, this would affect the accuracy of the reported results. I encourage the authors to look into this further and justify why they think this is the correct approach for this experiment.\n\nMinor Comments:\n- Lines 28-39: This is a very generic overview of MIR and could be more concise and/or more specific to the topic of the paper.\n- Lines 107-111: I would remove this sentence.\n- Line 180: As mentioned in Comment #3 above, justify the use of 32-bit.\n- Line 262-263: I didn't understand the justification of only using female group songs due to \"Japanese trends\"; this claim requires more information to support it.\n- Line 309: State that you are evaluating the separation results using SDR, even if it's obvious from the figure.\n- Lines 317-323: HT Demucs was trained on MUSDB + other songs, which contains different styles of music. I don't think we can necessarily conclude that the poor SDR results are due to the loudness of the tracks. If none of the tracks in the training set were mastered, I think these results are very much expected. Maybe this part could be written more convincingly.\n- Lines 370-373: What do these chord estimation results say about this corpus specifically? Chord estimation is an open topic, so I think this section should focus more on how including the chord annotations is beneficial to future research.\n- Line 383: I am assuming that ASR methods were used due to the existing ALT models processing only English? If this is the case, identifying this gap would be important as this is something this corpus could address.",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper introduces a very high-quality, Japanese idol songs dataset for various MIR tasks such as source separation, lyrics transcription, chord recognition, and so on. By listening to the provided samples, the dataset looks well-formatted and has great potential to apply to tasks beyond those mentioned in the paper. In addition, since it's well-known that J-pop/Anisong tends to favour chord progressions that are less common in mainstream Western music, which is somehow verified in the paper, the addition of IdolSongsJp can significantly increase the diversity of already Western-dominant MIR datasets (with extra icing of Idol sub-culture research on the cake). I enjoy reviewing this great work while listening to the provided idol songs.\n\nAlthough the current writing is fine, the introduction section can be structured better. The current structure reads like keeping going from this task to the other task (first source separation, then singer diarization, active music listening, etc.), which the readers could easily lose focus on. A better format could be 1) focusing on one task/topic per paragraph (and state it clearly in the beginning!), and 2) when introducing a new task/topic, show how the proposed dataset is connected to it. Tasks that will not be the paper's primary focus can be merged into one paragraph. In this way, the dataset is always at the centre of discussion and gets full attention from the readers.\n\nWhen discussing source separation on singing voice, the author should mention and compare the MedleyVox dataset. IdolSongsJp could be very useful in the choir/unison voice separation task, which I suggest the author emphasise more. Especially unison separation, since each song has a solo version sung by more than five singers, with all the available combinations, it will be the largest public dataset for this task. I recommend the author compute the total length of vocal tracks and compare it to MedleyVox, where I think the difference will be huge.\n\nSince the dataset provides versions at different stages of the mixing process, the paired dry and wet tracks could be used for blind audio effects/mixing graph inversion and estimation. As far as I know, this will probably be the first publicly available dataset with paired data to this level of detail. I recommend briefly mentioning this and citing relevant papers (e.g., GRAFx) to have more impact.\n\nMinor nit suggestions and questions:\n\n- \"f_o\" => \"f_0\". Just like Gundam 00 is written with zeros, not \"OO\".\n- What are the dots in Figure 3? Please explain it in the caption.\n- It would be better if the author uploaded listening materials to other websites and provided an anonymous link instead of putting them on CMT, which has a very slow download speed and wasn't designed to host large files.\n- Could add a section at the end discussing ethical considerations related to the dataset, like: what license was agreed on when recruiting the singers, any ethical issues if somebody misuses the data, etc.\n\nRegarding reference entries format:\nPlease add the DOI of the MUSDB18 dataset [21, 22], as recommended on their websites. Avoid citing pre-prints if they have a corresponding published version, e.g., MoisesDB was published in ISMIR 2023 proceedings.\n\nReferences:\nJeon, Chang-Bin, et al. \"Medleyvox: An evaluation dataset for multiple singing voices separation.\" ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023.\nLee, Sungho, et al. \"Searching for music mixing graphs: A pruning approach.\" arXiv preprint arXiv:2406.01049 (2024)."
    },
    "session": "6"
  },
  {
    "content": {
      "TLDR": "This paper presents an attempt to study the aesthetics of khayal music with reference to the flexibility exercised by artists in performing well-known compositions. We study expressive timing and pitch variations of the given lyrical content within and across performances and propose computational representations that can discriminate between performances of the same song in terms of expression. We employ a dataset of two songs in two ragas each rendered by several prominent artists.",
      "abstract": "This paper presents an attempt to study the aesthetics of khayal music with reference to the flexibility exercised by artists in performing well-known compositions. We study expressive timing and pitch variations of the given lyrical content within and across performances and propose computational representations that can discriminate between performances of the same song in terms of expression. We employ a dataset of two songs in two ragas each rendered by several prominent artists.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Yash Bhake",
        "Ankit Anand",
        "Preeti Rao"
      ],
      "authors_and_affil": [
        "Yash Bhake (IIT Bombay)",
        "Ankit Anand (IIT Bombay)",
        "Preeti Rao (Indian Institute of Technology  Bombay)*"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Computational ethnomusicology",
        "Applications",
        "Musical features and properties",
        "Music composition, performance, and production",
        "Expression and performative aspects of music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1PF3JGm6lZ1MNZnRLtfnQPYF8pBQRMCss/preview",
      "poster_pdf": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-8-melodic-and-metrical",
      "title": "Melodic and Metrical Elements of Expressiveness in Hindustani Vocal Music",
      "video": ""
    },
    "forum": "163",
    "id": "163",
    "pic_id": "",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nDisagree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nDisagree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nI say \"agree' because the methods and dataset itself theoretically offer the capacity to gain accurate and deep understanding of both inter and intra-performer performance analysis (e.g., micro pitch and timing deviations from experts to novice performers). While the authors are indeed pressed for time and space in the article, I'm not sure they presented the most insightful material in the paper itself.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThrough a combination of MIR tools and techniques (stem separation, F0 extraction, onset detection, etc.) the authors assemble a small but useful dataset for analyzing performance practices in Hindustani bandish performances, offering note-level timings, syllables, and F0 estimates.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis is a lovely paper. Well written and sits right in between music performance analysis and computational (ethno/)musicology. While I think the dataset is indeed useful, it remains rather small (only 2 songs x 10 artists). I believe that the reason it is small is that despite the many automated methods for information retrieval used, there obviously was a lot of manual researcher intervention and clean up (which is time and labor intensive). In addition, the authors do not report the initial error rates for f0 detection nor onset detection. Onset detection for voice, in particular, is notoriously challenging and error prone. In fact, the authors had a rather clever approach to this problem, but we have no idea what the original error rate was (for any methodology, in fact). The authors simply mention two points as justification for this omission (1) \"Recently, a small comparative study of two performances of the same bandish,...served as a preliminary validation for methodology using automatically detected onsets...\" and (2) \"The resulting alignments are checked and corrected for the occasional errors that arise mainly due to the presence of long vowels in singing.\" My main issue with the paper is the lack of clarity and transparency in the methodology (see review & metareview). For future researchers who may want to take advantage of similar methodologies to more fully automate many of these processes, especially for non-Western music, I encourage the authors to include an evaluation and discussion of the automated methods. This will bring more value to the paper than merely justification of the 'trustworthiness' of manually-cleaned data.\n\nMinor comments:\nLines 21-24: This is a very specific and odd \"motivation\" presented in extremely passive (contorted even) language. Suggest changing to something like \"A motivation for this work was to develop a computational representation of bandish performance practice for computational musicology research. Analyzing within and across performers can provide insights into this understudied performance practice.\"\n\nLines 34-36: This is an odd end to this paragraph. I think the authors are just trying to introduce the ensuing paragraphs. Suggest removing.\n\nLines 49-52: Another work investigating ornamentation in alap you may wish to query is that of Jain et al. \u201cAn Algorithmic Approach to Automated Symbolic Transcription of Hindustani Vocals\u201d (2023)\n\nLines 171-172: The \"suitable grouping of phones\" is hardly a simplistic process for creating words/lyrics. I strongly suggest the authors elaborate here.\n\nLines 178-179: It would be appreciated if the authors could clarify the signed relationship. I would have thought that -1 would be a lag and +1 would be ahead of the beat. However, based on the language and the plot it seems the authors mean the inverse. If it is the inverse then specifying clearly would certainly be helpful.\n\nFigure 3 caption: I suggest changing \"as a fraction of beat duration\" to \"as measured in units of beat duration\" since the amount can and often is a multiple and not only a fraction. (In addition, mentioning somewhere in the text the actual millisecond distance of 1 beat for this particular example might be helpful, since being ahead or behind an entire beat seems quite extreme, but perhaps \"beats\" in this style is more akin to subdivisions of a beat in Western music.)\n\nTable 1: The \"Swar\" row wraps because the authors are listing the full collection. I might suggest replacing this with the raga instead, since the set of swara (I believe) are more or less fixed with a given raga.\n\nLines 219-224: This description or explanation is difficult to follow in consultation with Figure 3, since the prose uses anglicized syllables, while the figure using the native language. (In other words, I have no idea which of those syllables are the 'Man' syllables). I think it is fine to just pick one, but be consistent.\n\nLines 238-242: I don't understand this, I think because there are some dashes and commas missing. E.g., I think the authors mean \"variable-dimension F0 contour\" (and is it really dimension that is changing or just length?) But what is meant by assigning a \"lower\" and \"fixed\" dimensional vector when it is inherently variable? The word \"piece\" here may be contributing to the ambiguity, since in Western music this word commonly refers to an entire work, which I don't believe makes any sense in this context. Could the authors kindly clarify?\n\nLines 246-247: Any rationale for the choice of 10 uniform intervals per beat duration? Seems a bit arbitrary.\n\nFigure 4: This image is too small to read the axes, even blown up. Again, the text (\"Ja1\" in English does not align with the lyrics that are typed in the original language. Kindly add the original language syllable text to the caption for clarity.) In addition, the figure unnecessarily shows the ensuing 2-3 syllables as well. Why is this? It is unclear what the blue dotted vertical lines represent, nor why in the 3rd subfigure these lines are compressed. Please align all 3 such that the time durations are comparable (I would suggest not using time in seconds but duration of the beats given the tempo such that beats align; or else do some modest time normalization -- e.g., line 261 talks about time alignment).\n\nLines 257-263: While Levenshtein distance can be used on vectors as well as strings, the authors do say \"strings\" here, but it is unclear how the pitch contours became strings when they were clearly numeric. Is there some kind of tying together of the syllable and the pitch value? Some elaboration there would be helpful.\n\nLine 350: I think should say \"near the end OF the word or the cycle\"\n\nLine 351: The \"should\" here in this context I presume comes from the authors' domain knowledge? (This would be helpful to clarify).\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThis is a well written and fascinating paper, but the details of the methodology are at times unclear or ambiguous, making the overall conclusions of the paper rather weak. For example, the main way the authors compare expressiveness in the pitch differences across syllable instances is \"by computing the\nsimilarity of syllable pitch contours for pairs drawn from\nthe set of repetitions...[using] Levenshtein distance between string...\" However, this is the last paragraph of the Methods (\"Measuring Expressiveness\") section with no elaboration at all. It is unclear how a syllable marked over a region of f0 contours becomes a string character to perform LD on, nor which LD method they used, since of course the strings would be different overall lengths. I presume that the methodology (while still slightly unclear) is hinted at 244-254 where the authors imply that they are slicing the duration of a single syllable as a function of 10ths of a beat. I would presume, then if the full beat rested on the swar \"Sa\" that you would have 10 strings of \"S\" in a row? But this is never explained explicitly in connection to the LD similarity scoring, making steps of the methodology unclear.\nIn addition, there was no mention of the binning of the pitch and how the frequency was mapped to the swara (the authors imply using cents but don't mention the intonation. Other papers have used just tuning for this and not equal temperament). The authors mention time alignment 'optimal time alignment' however this is also non-trivial; are they aligned to the start? To the peak? Etc. More detail is needed in order to be able to understand and potentially replicate this methodology. I strongly encourage the authors to add as much detail as possible should this work be accepted for publication by the committee.\n\nReviewers (especially myself) have left suggestions for edits to improve the paper. We thank you for your contribution!",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper investigates how artists inject expressiveness in music performances, and proposes computational methods to identify and analyze these variations within and across performances of the same piece. The study found that expressive gestures, particularly in timing and pitch, are more prominent at the beginning and sometimes the end, with less variation occurring near the boundary. \n\nStrengths\nThe study utilizes a significantly larger dataset compared to previous computational studies on this topic. \n\nIt introduces the analysis of pitch-based expressiveness in addition to timing variations. \n\nThe paper proposes computational measures capable of discriminating performances based on expressiveness. \n\nWeaknesses\nThe analysis presented in the paper focuses primarily a small part of a bandish.\n\nThe chosen modes of variation may not capture all ways a performed could inject expression.\n\nThe methods do no work across a piece and need performers to have performed one same section of a piece",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nSection 2: The summary sentences of the first paragraph mention \"ornamentation\", but Figure 1 does not seem to have it. Perhaps another smaller figure with such an example will help.\n\nSection 2: The second paragraph can add a short note that these variations are akin to Sangatis in Carnatic music and compare with Sridharan 2015 (see Lit Survey comments).\nSection 2: Please mention whether these variations are completely extempore, somewhat practiced, or completely crystallized within a lineage.\nSection 2: At first use of lineage, please add \"(gharana, for those familiar with Indian music)\"\n\nSection 3: Please mention the percentage of \"the occasional errors that arise mainly due to the presence of long vowels\"\nSection 3: \"We observe from Figure 2 how the realised onsets lag the canonical locations most of the time.\" Very interesting!\n\nSection 4.2: Looking at Figure 4, it may be a good idea to identify only more or less constant pitch segments first (Viraraghavan 2019 in Lit. Survey Section) and then find the PAA in between. That way, redundant PAAs (2nd, 4th, 10th, 12th values), which are really forced to a PAA by the algorithm, can be eliminated. This is a suggestion that can be added for future work. No need to change the data in the paper.\n\nSection 4.2: Last paragraph should define the \"optimal time alignment\" and quantify the \"minor temporal shifts\".\n\nEditorial\nFigures 9 and 10 should be together together, but both should appear and be referred before Figures 5 to 8.\n\nSection 5: The first paragraph needs is starting with within-artist variation, but in the next sentence mentions \"across artists\". Please rewrite with a little more clarity and in the context of the comment regarding placement of Figures 9 and 10.",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe work is an extension of the cited work https://arxiv.org/abs/2503.21142\n\nStrengths of the paper: \nThe paper uses the timing analysis from the previous work, but adds a newer dimension to the work by introducing melodic expressivity by analysing how pitch and rhythm contribute to expressivity. \n\nThe findings paves the way to more computational musicology on improvisational aspects. \n\nWeaknesses:\nThe introduced dataset is limited to 2 ragas, Yaman and Bhimpalasi out of which Yaman was already part of the previous work. \n\nBoth these bandishes dont have variability in tala (Both of them are in Teental in madhya and drut laya). Analysing different taals could give better insight on expressivity."
    },
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Many evaluation metrics in Music Information Retrieval (MIR) rely on uniform time sampling of phenomena that unfold over time. While uniform sampling is suitable for continuously varying concepts such as pitch or dynamic envelope, it is suboptimal for inherently discrete or piecewise constant events, such as labeled segments. Current Music Structure Analysis (MSA) metrics for label evaluation are all implemented with time sampling, which can be inexact and inefficient. In this work, we propose event-based implementations of the three most widely used MSA metrics. Our approach yields evaluations that are more accurate, more computationally efficient, and more reproducible, streamlining MSA research workflows.",
      "abstract": "Many evaluation metrics in Music Information Retrieval (MIR) rely on uniform time sampling of phenomena that unfold over time. While uniform sampling is suitable for continuously varying concepts such as pitch or dynamic envelope, it is suboptimal for inherently discrete or piecewise constant events, such as labeled segments. Current Music Structure Analysis (MSA) metrics for label evaluation are all implemented with time sampling, which can be inexact and inefficient. In this work, we propose event-based implementations of the three most widely used MSA metrics. Our approach yields evaluations that are more accurate, more computationally efficient, and more reproducible, streamlining MSA research workflows.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Qingyang Xi",
        "biran mcfee"
      ],
      "authors_and_affil": [
        "Qingyang Xi (NYU)*",
        "Biran mcfee (nyu)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Evaluation metrics",
        "Musical features and properties",
        "Reproducibility",
        "Structure, segmentation, and form",
        "Evaluation, datasets, and reproducibility"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1C5yCZQkabuN4SQlqbyAWe51oTmxtvn2L/preview",
      "poster_pdf": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-7-lose-the-frames",
      "title": "Lose the Frames: Event-Based Metrics for Efficient Music Structure Analysis Evaluations",
      "video": ""
    },
    "forum": "167",
    "id": "167",
    "pic_id": "",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nDisagree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nDisagree\n\n**Q5 ( Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chosen, otherwise write \"n/a\"))**\n\nThe related work discussion seems somewhat unbalanced - I was surprised to see the reference to Holzapfel et al. in there, while questions of sustainability and responsibility seem useful extra features, rather than the true experimental core of the work (for this same reason, I feel the claim of 'more responsible' evaluation in the title is a bit overblown, and the title would be more concise and on-point when not adding that). At the same time, the related work section very sparsely relates to work on music structure analysis, which is the actual task of focus in the work. As such, a more proper introduction of this task and common evaluation strategies or challenges would have been appropriate; instead, the section discusses another task (SED) that also would suffer from frame-based evaluation issues, but again generalizability to different tasks is not the core focus of the paper at this moment in time, and as such that part of the discussion seems to better fit in relation to a future work discussion. Also note some inconsistencies in wording: here, 'music segmentation' is referred to as a task, where elsewhere in the article this rather seems to be referred to as 'music structure analysis'.\n\nIf the authors ever wish to make a broader argument on works in the MIR field that raise issues with audio (in)stability or ambiguity of signal handling, several broader relevant references include:\n- Urbano et al. 'What is the Effect of Audio Quality on the\nRobustness of MFCCs and Chroma Features?', ISMIR 2014\n- Sturm 'A Simple Method to Determine if a Music Information Retrieval System is a \u201cHorse\"', TMM 2014\n- McFee et al. 'Open-Source Practices for Music Signal Processing Research: Recommendations for Transparent, Sustainable, and Reproducible Audio Research', IEEE SPM 2019\n- Liem & Mostert 'Can\u2019t trust the feeling? How open data reveals unexpected behavior of high-level music descriptors', ISMIR 2020\n\nI do not insist these works are cited now, but they may be useful as part of a larger review on current evaluation issues and the importance of implementation transparency.\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nDisagree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nthe paper raises awareness to numerical errors/instabilities in sampled audio evaluation for musical structure analysis. I can imagine this is something that may inspire others working with and evaluating on audio data and the authors already give some first directions into this\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nExact implementations of music structure analysis evaluation measures are more efficient to compute, while giving more robust evaluation results.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper proposes to take a continuous perspective on several commonly used evaluation metrics in music structure analysis evaluation, and compares the proposed take and implementation to performance obtained by the corresponding implementations in mir_eval, which perform frame-based sampling.\n\nAt points, the authors seem somewhat inconsistent in wording; apart from the earlier-mentioned 'music segmentation' vs. 'music structure analysis', there are mixed references to 'uniform sampling' and 'frame-based sampling' where these seem to connect to the same concept.\n\nOverall the authors do come up with an original idea and empirically show their method is more computationally efficient and more robust. I must say that the instability as achieved by frame-based methods seems to yield a reasonable consistent and correctable bias, that in terms of time offset seem quite manageable.\n\nFurthermore, I wonder whether the 'exact evaluation' claim may be a bit grand from a bigger-picture perspective. As is known on the SALAMI data, when multiple annotators annotated the same songs, they were not always fully in agreement on how they annotated. Furthermore, each annotator physically had to interact with Sonic Visualiser, which may additionally cause possible latency or inaccuracy in the annotations. The original SALAMI paper (reference [16]) proposes to consider that boundaries of annotations are a match if they are within a broader time window, and propose 0.5 and 3 seconds there. This is much coarser than the worst deviations found for the frame-based mir_eval implementation for a 2-second frame size. As such, I wonder whether it makes sense to extremely precisely try to match what a human used to annotate, as there will be some degree of instability in the ground-truthing process. Thus, the suggestion that frame-based implementations are problematically unstable may be less of a convincing argument given the nature of the parti (although the work still is convincing in its computational efficiency).\n\nAs a final tip: when submitting implementation/code base anonymously, consider using the anonymous GitHub service that can host such repositories for peer review purposes.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nOn this paper, reviewers are in consensus that a nice idea is presented in this paper. While some questions remain on how large the contribution and its benefits really will be, the insights are reusable and offer a new and refreshing perspective on evaluation. As such, while the average of the reviews would lean towards a weak accept, the work clearly seems above the acceptance bar, and as such I recommend to accept the paper.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nSummary\n\nIn this paper, the authors examine three evaluation metrics for semantic segmentation: Pairwise Frame Clustering, V-measure, and L-measure. These metrics are commonly implemented using discrete sampling methods in libraries such as mir_eval.\n\nThe main contribution of the paper is to introduce continuous formulations of these metrics, leveraging the fact that each relies on piecewise linear functions, which are straightforward to integrate analytically. When the number of segments is smaller than the number of uniform samples used in the discrete approximation, the proposed continuous versions offer improved runtime performance.\n\nOverall, being drop-in replacements, the metrics are reusable.\n\nStrengths\n\n- Very simple idea, drop in replacements of common scores\n- Paper is easy to follow\n- Three scores are considered\n\nWeaknesses\n\n- The runtime is already 10^-1. Great, we are seeing a 3x improvement, but over someone that is already very small.\n- No evaluation on how exact and sampled scores deviate is presented.\n\nReasons for my score\n\nOverall, this paper is quite nice and an easy read. However, the gains are very small and not that well justified. From a statistical perspective, I would like to see two or three things:\n\n1. How does this new implementation correlate with the original ones, this should be close to 1. Very correlated.\n\n2. This approach does appear to be more accurate, thus what were the errors of approximations from the original implementations? These errors may be derived based on the errors of trapezoidal integration, after all, this is what original scores do in a sense.\n\n3. Now that we have exact formulations of the scores, can we explore that fact that these are averages and derive confidence bands/statistical tests?\n\nOverall, I'm leaning towards acceptance given that nothing is strictly wrong with the paper. However, I am of the opinion that, as was presented, the gains of the new approach are yet small.",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper produces more precise implementations of popular structure analysis metrics that are used in contemporary literature. Primarily, the focus is on PFC, NCE, V-Measure and L-measure, all of which exist in the mir_eval toolkit. The results show an impressive improvement in speed over the toolkit implementations.\n\nLooking at the mathematical definitions, it raises a few interesting ideas that require some clarification. The mathematical explanation of PFC makes sense, and the notion that areas of rectangles can be computed easily is valid and tracks quite well. Here, for the benefit of the reader, I think you need to introduce what the role of $ u, v $ actually are more clearly.\n\nFor NCE, V-measure and L-measure I referred to the definitions from the McFee journal article and the original V-Measure paper. It's especially interesting to look at these metrics as attempting to improve upon the framework of F-measures in the context of pairwise comparisons. Optimization using a table to speed up the process is clever!\n\nFor the L-measure section, you specify the definition of a d-level label mapping, without ever referncing what the $d $ might indicate here (is it depth?).\n\nI think much of this paper is reasonably well written, if not a little terse because it places a decent amount of burden on the reader to be intimately familiar with these metrics to begin with (which I was not). I'd like to see two improvements: \n\n1) Figure descriptors could better explain what the plots represent might be. As it is right now, it just reads like a \"here are the things in the plot\" type of caption. It is difficult to understand what the plots might be indicating, especially to someone who might not be familiar with segmentation metrics.\n2) Derivation of the big O improvements or the complexity of some of these algorithms can be a little more explicit, if nothing, with some additional citations.\n\nI think the discussion on frame sizes is perhaps the most interesting bit of this paper. It is rather curious that the original metrics appear to be sensitive to frame size differences while the proposed ones aren't to the same degree. This might be interesting to evaluate further through more experiments.\n\nBeyond this, I think it would be see if it is possible to apply these metrics to more datasets beyond SALAMI. I understand that SALAMI is the de-facto standard for segmentation evaluation, so it's not something that necessarily harms this paper, but it's something I think would be worth exploring in future work. \n\nThis sort of improvement on existing metrics is certainly important and exciting to read about. I think it's a good contribution that deserves to be accepted - if nothing else, because it prompts conversations about the metrics we use and how we might: a) improve their efficiency and b) critically evaluate their behaviors in context.",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper discusses a set of novel algorithms that implement three evaluation metrics for music structure analysis efficiently, by replacing the traditional sample-based algorithms with continuous algorithms.\n\nThe contribution of this paper is clear: a faster algorithm that aligns well with the original purpose of the three evaluation metrics. The efficiency of the proposed algorithms is also verified both theoretically (complexity analysis) and empirically (the actual runtime reported in Fig. 3).\n\nIn my opinion, although the contribution of this work seems marginal (just some faster reimplementations of existing evaluation metrics), in practice, it could foster large-scale evaluation of music structure analysis. Therefore, I tend to accept this paper.\n\nMinor writing issues:\n- Would be great to explicitly mention which one is the ground-truth and which one is the prediction: S or \\hat{S}?\n- What does the x-axis of Fig. 4 mean? I don't really get it."
    },
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Optical Music Recognition (OMR) systems rely on accurate layout analysis (LA) to segment different information layers in music score images. While deep learning approaches have improved performance, they remain heavily dependent on large amounts of annotated data. In this work, we propose the integration of a Few-Shot Learning (FSL) architecture into an active learning framework for LA. This enables interactive and iterative training, allowing the model to progressively improve from minimal annotated data. We evaluate how this approach enhances recognition accuracy and reduces annotation effort, and we study the impact of different sample selection criteria within this framework, comparing data selected by five expert annotators against four automated strategies: random, sequential, ink density-based, and entropy-based. Experiments across three diverse music score datasets show that entropy-based selection consistently outperforms human choices, achieving an F1-score of 81.1% with only 8 labeled patches, while humans required at least 16 to reach similar performance. Our method improves over existing FSL approaches by up to 21.6% and substantially reduces annotation time. These results suggest that automated strategies can offer more efficient alternatives to human selection in OMR annotation workflows.",
      "abstract": "Optical Music Recognition (OMR) systems rely on accurate layout analysis (LA) to segment different information layers in music score images. While deep learning approaches have improved performance, they remain heavily dependent on large amounts of annotated data. In this work, we propose the integration of a Few-Shot Learning (FSL) architecture into an active learning framework for LA. This enables interactive and iterative training, allowing the model to progressively improve from minimal annotated data. We evaluate how this approach enhances recognition accuracy and reduces annotation effort, and we study the impact of different sample selection criteria within this framework, comparing data selected by five expert annotators against four automated strategies: random, sequential, ink density-based, and entropy-based. Experiments across three diverse music score datasets show that entropy-based selection consistently outperforms human choices, achieving an F1-score of 81.1% with only 8 labeled patches, while humans required at least 16 to reach similar performance. Our method improves over existing FSL approaches by up to 21.6% and substantially reduces annotation time. These results suggest that automated strategies can offer more efficient alternatives to human selection in OMR annotation workflows.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Juan Pedro Martinez-Esteso",
        "Alejandro Galan-Cuenca",
        "Carlos P\u00e9rez-Sancho",
        "Francisco J. Castellanos",
        "Antonio Javier Gallego"
      ],
      "authors_and_affil": [
        "Juan Pedro Martinez-Esteso (Universidad de Alicante)*",
        "Alejandro  Galan-Cuenca (Universidad de Alicante)",
        "Carlos  P\u00e9rez-Sancho (Universidad de Alicante)",
        "Francisco J.  Castellanos (Universidad de Alicante)",
        "Antonio Javier  Gallego (Universidad de Alicante)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Human-computer interaction",
        "Knowledge-driven approaches to MIR",
        "Optical music recognition",
        "MIR tasks",
        "Pattern matching and detection",
        "Machine learning/artificial intelligence for music",
        "Human-centered MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1zkTtIodC_mwEVHS6UNxseMe8wbTfsbaE/preview",
      "poster_pdf": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-11-human-vs-machine",
      "title": "Human vs. Machine: Comparing Selection Strategies in Active Learning for Optical Music Recognition",
      "video": ""
    },
    "forum": "177",
    "id": "177",
    "pic_id": "",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nDisagree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nDisagree\n\n**Q5 ( Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chosen, otherwise write \"n/a\"))**\n\nThe reference on \"entropy-based\" selection is too loose to just cite a book on information theory.\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nDisagree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe experiments comparing different selection methods is informative for future OMR research, as well as the baseline measurements of human annotation time.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nBeing aware of the selection method in active learning is beneficial for the layout analysis in OMR.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThe paper is well written and structured in general. From motivation to how the experiments were organized are clear. Given that certain details are clarified and provided, this paper provided informative experiment results on how different selection method in active learning would impact a few shot learning training scheme in layout analysis for OMR. \n\nThe details that needed to be provided and/or clarified.\n- How does entropy-based selection method work? how is entropy calculated? Either provide a more specific/precise reference, or provide the technical definitions in the paper.\n- Instructions/criteria for human annotaters.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nMain revisions that the reviewers suggested. \n- Clarify how the entropy is calculated for the entropy-based method. \n- Make it clear what information or criteria human annotators used when selecting patches. For instance, did they have access to the current model's segmentation results to identify areas with errors, or were they instructed to select diverse data? Understanding the human baseline is important.\n\nPlease also address the comments by all reviewers.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper proposes the adaptation of a Few-Shot Learning (FSL) architecture to an active learning setting for layout analysis (LA). Specifically, the work explores several patch selection methods, demonstrating that some automated approaches can outperform human selection as iterations progress.\n\nThe base model is based on the few-shot learning framework previously proposed by Castellanos et al. (ISMIR 2023). The authors enhance this model through the use of active learning strategies and achieve up to a 21.6% performance improvement using different patch selection techniques. One of the key contributions of this paper is the comparative analysis between human-driven and automated sample selection strategies, showing that a well-designed selection method can significantly impact the overall performance of the OMR (Optical Music Recognition) pipeline.\n\nThe paper is clearly written, and the experiments are well explained. However, I recommend that the authors provide more detailed explanations of the patch selection strategies, especially the entropy-based method, which ultimately achieved the best results. Additionally, the paper specifies a fixed patch size of 256\u00d7256 pixels. It would be helpful to include a discussion about how different patch sizes or shapes might affect the performance or selection quality.\n\nAlthough this work does not introduce a novel architecture or fundamentally new topic, it is significant in demonstrating how the integration of active learning strategies and intelligent sample selection can enhance the effectiveness of OMR systems. I believe the insights and results presented in this paper are valuable and merit presentation at the conference.",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper is well-motivated, addressing a realistic problem in OMR, the annotation scarcity. The paper is clearly written and easy to follow.\n\nThere are two main contributions: the proposed adjusted approach and the experimental setup with the comparison of human vs. algorithmic annotators and thev tracked time spent. In addition, the finding that entropy-based selection surpasses human annotation in both efficiency and performance is a bit surprising, but supported by the provided data. In a more philosophical aspect (or maybe even economic in a different way), the paper does open a significant question of considering systems to replace the annotators. However, there are still several challenges open before achieving this goal in a form of an automatised pipeline for annotation.\nNevertheless, the paper includes a sufficiently significant contribution of the proposed approach to be considered for publication.",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper applies active learning for deep learning-based layout analysis. Starting with little labeled data, an initial model is trained. Afterwards, further annotations can be requested from an \"oracle\", where the regions to be annotated are either selected by a human or using automated strategies. The model is then re-trained with the extended labeled data and this process is repeated. The central question addressed in this paper is whether the data to be annotated should be selected by humans or using automated strategies. \n\nThe paper is well-written and generally easy to follow. However, a few details regarding the selection process remain unclear to me:\n\n1) Entropy-based selection: As far as I understand, the model outputs for each pixel 4 values between 0 and 1, corresponding to the 4 considered layers. Is the entropy calculated for each layer separately, or is the entropy calculated after jointly normalizing the 4 values?\n\n2) Human selection: Based on which information or criteria do the human annotators select patches to be annotated? Do they have access to the segmentation of the current model so that they can specifically select a region with many errors? Are they instructed to select a diverse set of labeled data? It would be important for this to be made clear.\n\nSome minor issues:\n- l. 114: The introduction of sigma seems unnecessary, as the performance-based stopping criterion is not used in this paper.\n- l. 131ff: It is unclear how sequential selection works; is the goal to distribute the patches uniformly across all images? How are patches selected within an image? While this can be read up on in the given reference, the paper would benefit from a bit more detail.\n- l. 159ff: Sounds like the patch order would be predefined for entropy-based selection.\n- l. 316ff: I would suggest indicating annotation time in person-hours, which would allow for a better understanding of the annotation effort.\n\nOverall, I suggest to accept the paper, as it appears to be one of the first applications of active learning to optical music recognition, showing the potential of this research direction."
    },
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Audio fingerprinting (AFP) allows the identification of unknown audio content by extracting compact representations, termed audio fingerprints, that are designed to remain robust against common audio degradations. Neural AFP methods often employ metric learning, where representation quality is influenced by the nature of the supervision and the utilized loss function. However, recent work unrealistically simulates real-life audio degradation during training, resulting in sub-optimal supervision. Additionally, although several modern metric learning approaches have been proposed, current neural AFP methods continue to rely on the NT\u2011Xent loss without exploring the recent advances or classical alternatives. In this work, we propose a series of best practices to enhance the self-supervision by leveraging musical signal properties and realistic room acoustics. We then present the first systematic evaluation of various metric learning approaches in the context of AFP, demonstrating that a self\u2011supervised adaptation of the triplet loss yields superior performance. Our results also reveal that training with multiple positive samples per anchor has critically different effects across loss functions. Our approach is built upon these insights and achieves state-of-the-art performance on both a large, synthetically degraded dataset and a real-world dataset recorded using microphones in diverse music venues.",
      "abstract": "Audio fingerprinting (AFP) allows the identification of unknown audio content by extracting compact representations, termed audio fingerprints, that are designed to remain robust against common audio degradations. Neural AFP methods often employ metric learning, where representation quality is influenced by the nature of the supervision and the utilized loss function. However, recent work unrealistically simulates real-life audio degradation during training, resulting in sub-optimal supervision. Additionally, although several modern metric learning approaches have been proposed, current neural AFP methods continue to rely on the NT\u2011Xent loss without exploring the recent advances or classical alternatives. In this work, we propose a series of best practices to enhance the self-supervision by leveraging musical signal properties and realistic room acoustics. We then present the first systematic evaluation of various metric learning approaches in the context of AFP, demonstrating that a self\u2011supervised adaptation of the triplet loss yields superior performance. Our results also reveal that training with multiple positive samples per anchor has critically different effects across loss functions. Our approach is built upon these insights and achieves state-of-the-art performance on both a large, synthetically degraded dataset and a real-world dataset recorded using microphones in diverse music venues.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Recep Oguz Araz",
        "Guillem Cort\u00e8s-Sebasti\u00e0",
        "Emilio Molina",
        "Joan Serra",
        "Xavier Serra",
        "Yuhki Mitsufuji",
        "Dmitry Bogdanov"
      ],
      "authors_and_affil": [
        "Recep Oguz Araz (Universitat Pompeu Fabra)*",
        "Guillem Cort\u00e8s-Sebasti\u00e0 (BMAT Licensing S.L.)",
        "Emilio Molina (BMAT Licensing S.L.)",
        "Joan Serra (Sony AI)",
        "Xavier Serra (Universitat Pompeu Fabra)",
        "Yuhki Mitsufuji (Sony AI)",
        "Dmitry Bogdanov (Universitat Pompeu Fabra)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "MIR fundamentals and methodology",
        "Music retrieval systems",
        "Applications",
        "Representations of music",
        "Music signal processing",
        "Digital libraries and archives",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1aj7_R_eIDuSbo-lwEKoSDwMXkA3i9F5-/preview",
      "poster_pdf": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-4-enhancing-neural-audio",
      "title": "Enhancing Neural Audio Fingerprint Robustness to Audio Degradation for Music Identification",
      "video": ""
    },
    "forum": "186",
    "id": "186",
    "pic_id": "",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nDisagree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nStrongly agree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nStrongly agree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe paper explores different ideas -- often developed in computer vision applications -- in the context of music fingerprinting. The results are sometimes surprising, which should make our community reconsider some assumptions about best practices.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThis paper significantly improves the performance of neural music fingerprinting techniques through systematic exploration of loss functions, data handling, and hyperparameter selection.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nSome of the paper\u2019s strengths:\n+ One significant contribution of the paper is a larger, more comprehensive, and more realistic benchmark for evaluating music fingerprinting. The paper significantly expands the database size compared to previous works, considers a wider range of additive background noise and room impulse responses and microphone responses, and corrects erroneous (or at least poorly chosen) data pre-processing techniques. This will be a valuable resource to the community moving forward.\n+ The results show large gains over a previously proposed method (NAFP, GraFP). The cumulative effect of careful attention to the data preparation, training configuration, and architecture design led to very large improvements in overall performance.\n+ The paper is very thorough and systematic in presenting a detailed analysis of the effect of many different design choices: the degradation dataset used, the selection of examples in each batch, the effect of impulse responses & reverberation, the loss function, the number of anchors per batch, etc. The experimental results are very systematic and well organized, and offer insights into the importance of these important parts of the pipeline.\n+ The writing is very clear, well organized, and easy to follow. \n\nSome of the paper\u2019s weaknesses:\n- The title is somewhat misleading in two senses: (a) it suggests that the paper proposes a new fingerprinting method, whereas this paper is more of an analysis paper, and (b) the results don\u2019t really study the scalability aspect other than running all experiments on a larger dataset. To claim that a method is more scalable, I would expect there to be some comparison of the size of the fingerprint databases, experimental runtimes, comparison of results vs database size, etc. I think a title like \u201cImproved Neural Music Fingerprinting\u201d or \u201cImproving the Robustness of Neural Music Fingerprinting\u201d would be a more accurate title.\n- The paper is missing an explanation of what NAFP actually does (section 2.2). Once it computes the log (or power) mel spectrogram, how does it convert it into a real-valued fingerprint? What architecture does it use? This could be just a few brief sentences, but I think it is important for completeness.\n- There is little novelty in the way of new ideas, though the experimental results and insights are novel.\n\n\nOther feedback:\n- Are both the synthetic and industrial datasets being released to the community? It was not clear to me, and often industrial datasets are not released due to copyright issues.\n- In Table 7 and the corresponding discussion in section 5, it was not explained very clearly what \u201cexact match\u201d and \u201cnear match\u201d meant.\n- L481: Could you explain what \u201ccompleting in a reasonable time\u201d means? This is hard to interpret because there is no discussion in this paper of runtimes.\n\nOverall, I appreciate the thoroughness of the experimental results and feel like it will be a valuable contribution to the community.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThe reviewers discussed the strengths and weaknesses of the paper, which we summarize below.\n\nStrengths\n\u2022 The paper proposes a more comprehensive and realistic benchmark for evaluation of music fingerprinting systems. In constructing this benchmark, the authors addresses multiple weaknesses in previous evaluation protocols, including insufficient diversity of room acoustics, background noise and unrealistic audio degradations. This is a valuable contribution to facilitate future research on this topic. \n\u2022 The experimental results are very systematic and thorough. The authors clearly document the steps in improving the baseline fingerprinting system. The ablation studies are done systematically and provide useful insights to other practitioners on practical issues like loss functions, batch construction, data preprocessing (filtering out higher frequencies), etc. \n\u2022 The experimental results offer clear recommendations to researchers on best practices. Careful selection of these practices led to significant improvements in overall performance of the fingerprinting system. Of particular note, some of the findings contradict some of the recommendations in the Computer Vision community (from which several of the techniques originated), so these insights are particularly helpful to the MIR community.\n\u2022 The paper provides a critical assessment of earlier work and benchmarks. The review of related work is comprehensive and current.\n\u2022 The authors plan to share their model and code with the community, which will facilitate open and reproducible research.\n\nWeaknesses\n\u2022 The biggest criticism raised by reviewers was that the \u201cmarketing\u201d of the paper was misleading. For example, the title makes it sound like a new fingerprinting method is being proposed, whereas in actuality the paper is making lots of small improvements to an existing approach. Another reviewer felt that the naming of the sections is misleading, and recommends renaming section 3 to make it clear that it describes the baseline system. The emphasis on \u201cscalable\u201d in the title and abstract is somewhat misleading since scalability is not really studied (other than evaluating on a larger dataset). The reviewers request the authors to make it clear in the title and abstract that this paper is about improving an existing system. This is itself a valuable contribution that does not need to be \u201coversold\u201d.\n\u2022 Some parts of the writing could be improved. One reviewer suggests that a figure (or at least a brief textual summary) giving an overview of the system and training/evaluation pipelines would be very helpful to the reader. The introduction could be improved by mentioning earlier unsupervised approaches (like Shazam) and convincing the reader that the newer \u201cneural\u201d fingerprinting approaches have been shown to be better. A brief description of the NAFP method and architecture would be helpful for completeness.\n\u2022 One reviewer points out that the training was done on fma_medium and evaluation was done on a part of fma_full. This may indicate overlap between songs in the training and evaluation datasets. This should be clarified if there is no overlap, or acknowledged if there is overlap.\n\nThe paper has no major novelty in terms of new ideas, but the reviewers nonetheless agree that the paper presents a very systematic and thorough set of experimental results that suggests best practices and can help guide other researchers in the field.",
      "publish_reviews": "TRUE",
      "review1": "#ERROR!",
      "review2": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nGeneral comments:\n- I feel that the title, abstract, and listed contributions are a bit misleading. Essentially, the authors are proposing various ways to improve an existing method. While incremental works are not proscribed, I would clarify this from the very beginning. Besides that point, I am a bit on the fence, as this works feels more like a technical report to me, but the reported increases in performance are significant. I would have perhaps framed this work differently and re-organized some of the parts. See my detailed comments below.\n \n\nDetailed comments:\n\n1. \n- I feel that the introduction could be improved. You kind of directly start talking about machine learning-based systems without mentioning the successful unsupervised methods which came first, such as scalable peak-based approaches like in the Shazam algorithm. It would be nice to first convince the reader that these new \"neural\" fingerprinting systems are \"better,\" or at least more promising.\n\n2. \n- More accurately, triplet loss is a function not an approach.\n\n- \"Representation quality is improved by each anchor sample in a batch having multiple positive samples.\" Is this claim backed up by reference [15]?\n\n- I would briefly explain what the NT-Xent loss is.\n\n- \"A&U [17] proposes two metrics that good representations should obtain...\" I am not sure to understand this. Could you perhaps rephrase?\n\n- You mentioned the number of anchors and positives per anchor in a batch. At this point, I am unclear if you are going to use negatives as well.\n\n- While higher sampling frequencies may not be necessary for music identification, they could definitely help, as you are getting more information which can possibly survive noise, for example. And I don't think that it's uncommon for audio to be transmitted at higher rates (8k is fairly low). \n\n- I am not sure that you really need Table 1. You are basically proposing cheaper parameters compared to other systems.\n\n3. \n- I would just explicitly mention the problems of NAFP rather than giving a link to GitHub open issue.\n\n- What does it mean that some query tracks were represented many times? Are you saying that some queries were used multiple times and some not at all in the final evaluation? How come??\n\n- Instead of \"industrial evaluation,\" I would rather talk about \"real-world evaluation.\"\n\n4.\n- I feel the organization is a bit confusing. Perhaps Sections 3, 4, and 5 could be re-organized better, combining some parts?",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nDear authors, thank you for this well-executed paper.\n\nThe paper demonstrates strong scientific quality and clarity, combining thoughtful engineering with rigorous experimentation. The related work is comprehensive and current, and the methodological contributions are clearly motivated. Your evaluation setup addresses multiple weaknesses of previous works, and your ablation studies are among the most useful I\u2019ve seen in neural AFP literature.\n\nYour \"best practices\" approach is highly pragmatic and well-executed. Each step, from refining the degradation pipeline, to resolving false negatives in batches, to restoring low frequencies, is clearly justified and shown to contribute meaningfully to the final performance. The improvements in Table 2 are especially illustrative. I also appreciate the decision to lower the frequency threshold from 300 Hz to 160 Hz, which makes practical sense for music recorded in noisy real-world environments.\n\nYour exploration of metric learning losses is another strong point. I found the analysis of NT-Xent\u2019s degradation with more than one positive per anchor particularly insightful. As you suggest, this behavior likely stems from the softmax denominator not being able to simultaneously assign high similarity to multiple positives. It would be interesting to explore modified loss formulations that either decouple positives or replace the softmax entirely. This direction could lead to more robust representations in AFP and beyond.\n\nA few suggestions for improvement:\n* Including pitch and tempo changes in your degradation pipeline would significantly strengthen your robustness claims, as these are common in music identification use cases.\n* Consider adding Top-3 or Top-5 hit rate in future work; these are often relevant in practical systems where near matches matter.\n* Provide analysis of false positives, i.e. when the right result is not the first one found. What would be the main reason(s) to see such false positive matches?\n\nFinally, it\u2019s excellent that you plan to release the code, models, and curated data. This greatly boosts the paper's impact and will support reproducibility and further research in the field."
    },
    "session": "4"
  },
  {
    "content": {
      "TLDR": "A large-scale dataset is essential for training a well-generalized deep learning model. Most such datasets are collected via scraping from various internet sources, inevitably introducing duplicated data. In the symbolic music domain, these duplicates often come from multiple user arrangements and metadata changes after simple editing. However, despite critical issues such as unreliable training evaluation from data leakage during random splitting, dataset duplication has not been extensively addressed in the MIR community. This study investigates the dataset duplication issues regarding Lakh MIDI Dataset (LMD), one of the largest publicly available sources in the symbolic music domain. To find and evaluate the best retrieval method for duplicated data, we employed the Clean MIDI subset of the LMD as a benchmark test set, in which different versions of the same songs are grouped together. We first evaluated rule-based approaches and previous symbolic music retrieval models for de-duplication and also investigated with a contrastive learning-based BERT model with various augmentations to find duplicate files. As a result, we propose three different versions of filtered list of LMD, which filters out at least 38,134 samples in most conservative settings among 178,561 files.",
      "abstract": "A large-scale dataset is essential for training a well-generalized deep learning model. Most such datasets are collected via scraping from various internet sources, inevitably introducing duplicated data. In the symbolic music domain, these duplicates often come from multiple user arrangements and metadata changes after simple editing. However, despite critical issues such as unreliable training evaluation from data leakage during random splitting, dataset duplication has not been extensively addressed in the MIR community. This study investigates the dataset duplication issues regarding Lakh MIDI Dataset (LMD), one of the largest publicly available sources in the symbolic music domain. To find and evaluate the best retrieval method for duplicated data, we employed the Clean MIDI subset of the LMD as a benchmark test set, in which different versions of the same songs are grouped together. We first evaluated rule-based approaches and previous symbolic music retrieval models for de-duplication and also investigated with a contrastive learning-based BERT model with various augmentations to find duplicate files. As a result, we propose three different versions of filtered list of LMD, which filters out at least 38,134 samples in most conservative settings among 178,561 files.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Eunjin Choi",
        "Hyerin Kim",
        "Jiwoo Ryu",
        "Juhan Nam",
        "Dasaem Jeong"
      ],
      "authors_and_affil": [
        "Eunjin Choi (KAIST)*",
        "Hyerin Kim (Sogang University)",
        "Jiwoo Ryu (Sogang University)",
        "Juhan Nam (KAIST)",
        "Dasaem Jeong (Sogang University)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Symbolic music processing",
        "Music retrieval systems",
        "Applications",
        "Similarity metrics",
        "Representations of music",
        "Philosophical and ethical discussions",
        "MIR tasks",
        "Evaluation, datasets, and reproducibility",
        "MIR fundamentals and methodology",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1jzV-qqr3e5VlgSMQ8BjWj3UQNubInGaF/preview",
      "poster_pdf": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-5-on-the-de",
      "title": "On the de-duplication of the Lakh MIDI dataset",
      "video": ""
    },
    "forum": "188",
    "id": "188",
    "pic_id": "",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nDisagree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nStrongly agree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe methodologies are proposed are directly and usefully applicable to existing and new datasets (assuming they are released on publication as promised).\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThis paper proposes and evaluates methods for removing duplicates in large symbolic data sets, with the primary goal of preventing these duplicates from corrupting validation and testing music generation models.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nOverall, this paper presents a highly useful evaluation of different methodologies for removing duplicates from large symbolic datasets. It could be usefully applied to a wide range of MIR-related domains, beyond even those highlighted by the authors. The Lakh MIDI Dataset (and the Clean version of it) are good choices for carrying out the research, and the methodology and evaluation are generally good (although the language used should perhaps be moderated, as noted below). A nice variety of methods are evaluated, both together and in combination, and expert sampling of the final results when applied to the full LMD was an essential step that was properly followed.\n\nOne possible area of improvement is a more nuanced and musically relevant discussion of what makes two symbolic music files \u201cthe same\u201d or \u201csimilar\u201d to the extent that they can be considered duplicates for the purposes discussed here. Section 3 does present a useful distinction between \u201chard\u201d and \u201csoft\u201d duplication, but more detail and justification is needed, as this issue is foundational to the entire task, and these are very short sub-sections. Ultimately, thresholds based on precision on Clean LMD were used, which to me is a very dataset and application-specific approach that sidesteps fundamentally important underlying musical questions. Also, certain assertions perhaps need more justification (e.g., why are ornamentations or chord tones under the purview of hard duplication, but not transpositions to different keys?). There are many related underlying issues that should ideally be discussed, and which are methodologically relevant; for example, one common issue that comes up in duplication detection of MIDI is the difference between the \u201csame\u201d music when encoded by a score editor vs. a different score editor vs. a MIDI instrument played by a human vs. a music generator vs. etc.\n\nCertain assertions should perhaps be reconsidered. For example, the abstract states that \u201cdataset duplication has yet to be discussed seriously in the MIR community,\u201d a statement that I would disagree strongly with, and the paper itself later cites an ISMIR paper that does in fact seriously discuss it ([30]), and there are certainly other examples as well in the MIR literature.\n\nAnother issue that could be discussed a little more is how well models or algorithms trained or tuned on LMD Clean can generalize to LMD in general, or more importantly, to entirely different datasets. LMD clean isn\u2019t that big, it is not stylistically representative and it is not in fact itself so clean (as noted by the authors in lines 403 to 408).\n\nIt would also be useful to know what the processing times are of the various approaches (especially the combined approaches), in order to have a sense of how scalable they are.\n\nThe results in Table 3 (and the thresholds adopted intentionally by the authors) result in decent precision but very low recall, meaning that many duplicates will be missed. The authors say that they \u201cprioritize high precision to minimize unintended data loss during the final de-duplication process,\u201d which is a fair point, but on the other hand the entire motivational premise of this paper is that duplicates can pose a huge problem, which seems at odds with this sentiment. Perhaps this seeming contradiction could be resolved in the text?\n\nAlso, even the high precision may not in reality be that high in practice, as shown by the listening results described in Section 7. If only 72.9% of the sampled detected LMD duplicates are in fact true hard or soft duplicates, that means that the precision found on clean LMD did not in fact generalize. Furthermore, if many false negatives occurred even on clean LMD, this suggest the potential that perhaps an even greater fraction of duplicates were missed on LMD, and potentially even more would be if the approach were applied to an entirely different dataset.\n\nNone of this is to say that this paper doesn\u2019t offer a useful contribution; it is, as being able to remove even a subset of duplicates, hopefully at a small loss of non-duplicates, is certainly a useful contribution. But statements like \u201cwe found that the 38 134 files in the LMD-full (21.4%) are considered as duplication with very high confidence\u201d are perhaps not appropriate; if the listening sampling found that only 72.9% of detected duplicates were actually duplicates, I\u2019m not sure that I would express \u201cvery high confidence\u201d in those 38 134 files actually being duplicates. It is OK if results are not perfect, and it is better to be direct about their limitations.\n\nThe paper is generally well-written, but there are a number of grammatical errors that could be corrected by additional editing.\n\nCertain aspects should be expanded on in order to improve clarity. In particular, more details are needed on LMD Clean, particularly given the fundamental role it plays in this research. More information is also needed on Beat Position Entropy and Chroma-DTW than what is provided in Sections 4.2.1 and 4.2.3. For the latter in particular, are the authors saying that only basic pitch histograms (which presumably lost all sequence information) were used for all but 250 of the files, rather than actual Chroma-DTW?\n\nSeveral of the references in the bibliography are missing dates.\n\nThe paper states that the code, metadata, training data, and evaluation results will all be distributed after publication. However, no anonymized supplementary material was submitted to make it possible to verify this.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nStrong accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nAll four reviewers unanimously agreed that this paper offers insights that will be of value to the MIR community, and we are delighted to recommend it for acceptance at ISMIR 2025.\n\nDataset duplication is an important but understudied issue in MIR, and we believe that this paper will both bring important greater awareness of the issue and provide a directly usable methodology for at least reducing the number of duplicates in symbolic datasets. Useful quantitative insight is also offered with respect to this issue on the Lakh MIDI Dataset (LMD), something that is in itself a useful contribution, given the wide use of the LMD.\n\nThere are, however, certain issues with respect to both methodology and clarity that have been highlighted in the individual reviews, particularly with respect to how some of the empirical results are interpreted in the text. If this paper is accepted at ISMIR, we strongly encourage the author(s) to take these suggestions into account when preparing their camera-ready version of the paper.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper addresses a crucial and often overlooked issue in the Music Information Retrieval (MIR) community: dataset duplication, specifically within the widely used Lakh MIDI Dataset (LMD). The authors provide a thorough investigation into the scale of this problem and evaluate various approaches for de-duplication, culminating in the proposal of a combined method.\n\nStrengths:\nThe paper tackles a highly relevant problem that directly impacts the reliability of research using LMD for tasks like music generation. Highlighting this issue is a significant contribution to the field.\nThe authors present and evaluate a performance pipeline for identifying duplicates, suggesting a methodology that could be applied to other datasets or related problems. This systematic approach has potential for future research.\nThe authors quantify the extent of duplication in LMD, providing concrete numbers that underscore the importance of the problem.\nA small but reliable listening test is performed to help validate the detection process qualitatively.\n\nWeaknesses:\nA primary limitation, as acknowledged by the authors, is the difficulty in effectively detecting \"soft duplicates\" \u2013 different arrangements of the same song.\nApplying this methodology to new data would likely require significant tuning, as the algorithmic results may not be as directly transferable or durable as the specific de-duplicated list provided for LMD. The main, highly practical outcome of this work for the community is the released filtered list for LMD.\n\nOverall:\nThis is a timely paper for the ISMIR community. The proposed methods and the released filtered lists offer practical tools for researchers. While the relatively low recall across all methods prevents it from being a definitive, perfectly complete solution to the duplication problem in LMD, I believe that identifying the scope and challenge of this significant duplication problem is a large and valuable contribution on its own.",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nStrengths:\n\nThe paper explores several different approaches to the problem of data deduplication in large MIDI datasets, including rule-based approaches, the use of deep learning models released in previous papers, and the use of a new model (CAugBERT) designed and trained by the authors. The paper finds that using CLaMP + CAugBERT to perform deduplication outperforms using either individually.\n\nThe paper focuses on the Lakh MIDI dataset, a widely used dataset in the field of symbolic music processing. While other large datasets (including but not limited to MetaMIDI, GigaMIDI, SymphonyNet, and PDMX) are now available, the approach taken in the paper should generalize to those datasets. \n\nThis is timely work that addresses a core issue in the symbolic music processing community.\n\nFor these reasons, I recommend strong acceptance of this paper.\n\nWeaknesses/suggestions for improvement:\n\nLine 124: \"the rule-based approach\" -> \"rule-based approaches\" (the paper evaluates more than one rule-based approach)\n\nLine 230: \"and the hash values of all MIDI files\" I'm not sure what this means. Please make this clear.\n\nLine 233: \"that has exactly\" -> \"that have exactly\"\n\nLine 235: \"applied simple method\" -> \"applied a simple method\"\n\nLine 236: Does the MIDI encoding scheme of [36] do something unusual with note position? (Why are you using this encoding scheme in particular?)\n\nLines 232-239: Are you using note position within measure (that would be my guess) or note position within the whole piece?\n\nLine 264: \"MIDI as MTF\" -> \"MIDI to MTF\"\n\nLine 280: What is the point of the 98:1:1 split of LMD-filtered? Please say exactly what the 1 and 1 portions of the split used for. (I am a little confused because I understand that LMD-clean, not LMD-filtered, is used for evaluation)\n\nLine 296: \"The model implementation\" -> \"The implementation of CAugBERT\" \n\nLine 338: \"following the provided code\" -> \"following the code provided in [citations]\". Also, did you have to train this projection layer, or were the weights for this projection layer provided in the CLaMP releases?\n\nLine 399: \"We note\" -> \"We also note\"\n\nLine 417: \"with the best-performing CAugBERT\" -> \"with CAugBERT\" \n\nTable 5 caption: When you refer to the threshold >= 0.99, do you mean the threshold that results in precision on LMD-clean of 0.99, or do you mean the threshold on the model's output probability itself?\n\nTable 5 caption: I think \"# Clusters refers to the number of clusters and # Duplicates refers to the number of samples to be filtered\" can be removed if you need space elsewhere for revisions.\n\nLine 439: In the language of graph theory, the \"clusters of duplication\" are the connected components of the graph.\n\nLine 459: \"underrepresented while training on LMD-clean\" LMD-clean was not \"trained on,\" right? (I understand it was used for thresholding.)\n\nLines 448-458 and Figure 1: \nDid the author compare every file in each cluster to every other file in the cluster, or did they only compare files connected by an edge? (I imagine that not every cluster you found is a complete graph.) \n\nThe percentages here and the bars in Figure 1 are hard to understand as written, because without additional definition, labels like \"hard duplicate\" and \"soft duplicate\" apply to *pairs* of files, not individual files. To illustrate, there may be a file A, which is a soft duplicate of file B, which is a soft duplicate of file C, but A and C are not soft duplicates of one another. \n\nHow is Figure 1 meant to be interpreted? My guess is that the bar heights represent counts of files (not counts of file pairs), a file falls into the \"hard duplicate\" category if it is a hard duplicate of some other file in its cluster, a file falls into the \"soft duplicate\" category if it is not a hard duplicate of any file in its cluster and is a soft duplicate of at least one other file in its cluster, a file is in the \"similar\" category if it is not a hard duplicate of any file in its cluster, it is not a soft duplicate of any file in its cluster, and it is similar to at least one other file in its cluster, and a file is \"irrelevant\" if it is none of the above. I'm confused why the unhatched bars don't add up to 506, and I am also confused on exactly what the prediction type breakdown in Figure 1 means. For instance, the grey area of the \"Irrelevant\" bar means what, exactly? Issues like this should be clarified.",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper does a good job at assessing prior attempts at data duplicate and puts together a nice methodology for studying different data duplication detection mechanism."
    },
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Diffusion models have demonstrated potential to separate individual sources from music mixtures in a generative fashion, enabling a new solution for this challenging problem. However, existing works require clean multi-stem data, which is scarce for several repertoires, consequently compromising generalization. We explore the potential of generative modeling to perform weakly-supervised singing voice separation for Carnatic Music, a music repertoire for which large quantities of multi-stem recordings with bleeding between sources have been collected from live performances. We pre-train a latent diffusion model to perform preliminary vocal separation conditioning on the corresponding mixture. Then, using a regressive model which is separately trained on a clean, smaller, and out-of-domain dataset, we estimate the level of bleeding in the preliminary separations and use that information to guide the diffusion model toward generating cleaner samples. The objective and perceptual evaluations show the potential of the proposed generative system for Carnatic vocal separation. Code, weights, and further materials are available online.",
      "abstract": "Diffusion models have demonstrated potential to separate individual sources from music mixtures in a generative fashion, enabling a new solution for this challenging problem. However, existing works require clean multi-stem data, which is scarce for several repertoires, consequently compromising generalization. We explore the potential of generative modeling to perform weakly-supervised singing voice separation for Carnatic Music, a music repertoire for which large quantities of multi-stem recordings with bleeding between sources have been collected from live performances. We pre-train a latent diffusion model to perform preliminary vocal separation conditioning on the corresponding mixture. Then, using a regressive model which is separately trained on a clean, smaller, and out-of-domain dataset, we estimate the level of bleeding in the preliminary separations and use that information to guide the diffusion model toward generating cleaner samples. The objective and perceptual evaluations show the potential of the proposed generative system for Carnatic vocal separation. Code, weights, and further materials are available online.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Gen\u00eds Plaja-Roglans",
        "Xavier Serra",
        "Mart\u00edn Rocamora"
      ],
      "authors_and_affil": [
        "Gen\u00eds Plaja-Roglans (Music Technology Group)*",
        "Xavier Serra (Music Technology Group)",
        "Mart\u00edn Rocamora (Music Technology Group)"
      ],
      "channel_url": "",
      "day": "4",
      "keywords": [
        "Transformations",
        "Sound source separation",
        "MIR tasks",
        "Generative Tasks"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1zETT39AYll1U6xdSgJFAMVuzWabmJYne/preview",
      "poster_pdf": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-12-leveraging-carnatic-live",
      "title": "Leveraging Carnatic live recordings for singing voice separation using regression-guided latent diffusion",
      "video": ""
    },
    "forum": "191",
    "id": "191",
    "pic_id": "",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nStrongly agree\n\n**Q5 ( Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chosen, otherwise write \"n/a\"))**\n\nSince the method proposed in the paper is not specific to Carnatic music, I would not insist on this on the title; but rather focus on the fact that the bleeding estimator has been trained with out-of-domain data.\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nStrongly agree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nStrongly agree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nStrongly agree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nStrongly Agree (Very novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nStrongly agree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe proposed regression guidance (here applied to the bleeding factor trained with out-of-domain data) can have a strong impact outside the MIR community.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nVocal source separation using v-objective latent diffusion with a novel bleeding estimation (model trained using out-of-domain data) regression guidance can reduce interference.\n\n**Q17 (This paper is of award-winning quality.)**\n\nYes\n\n**Q18 ( If yes, please explain why it should be awarded.)**\n\nThe proposed method is highly innovative: it uses a generative model (v-objective latent diffusion applied to Music2Latent) guided by a bleeding factor trained using data from another domain. A novel regression-guidance (RG) method is proposed, which will likely have an impact beyond the MIR community.\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nStrongly agree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nPros:\n- the paper is very well written and clear\n- the subject is relevant (vocal source separation) and the authors considers here a new paradigms (weakly supervised learning, i.e. ground-truth vocal with bleeding)\n- the method proposed here is novel: using generative model (v-objective latent diffusion applied to Music2Latent) guided by a bleeding estimator (trained using data from another domain) through a novel regression-guidance (RG) method. This regression-guidance method will probably have impact outside the MIR community.\n- the models are clearly described\n- the evaluation is well-performed including a statistical test\n- additional materials are provided in terms of audio examples obtained with the proposed method and compared to results obtained with cold-diff, mixer and msdm\n\nCons:\n- while the perceptual evaluation indicate an improvement in terms of reduction of the interference, the audio quality seems to be much lower than other approaches (cold-diff, mixer and MSDSM). This is of course disappointing.\n- Line 479-483 attempts to explain why (increasing guidance level reduce interference up to a point but then degrade the quality); however it is unclear which model has been used for this experiment ? From line 467 it seems to be FTRG^{10} with T=32 which is not the best model in terms of FAD/LSD and PESQ.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThis is the meta-review summing up reviews for your paper \"Singing voice separation from Carnatic Music mixtures using a regression-guided latent diffusion model\".\n\nMost reviewers found that the paper provides a strong contribution to the domain by training the bleeding estimator with out-of-domain data and proposing the regression-guidance idea. Unfortunately, the results (especially in terms of audio quality) remains lower than other approaches.\n\nAmong current issues raised, reviewers would like to have\n\n* an improved evaluation, especially\n- for a fair comparison between no-FT, FT, and FT-RG models, the no-FT model should also be trained with additional data (e.g., musdb18hq)\n- to precisely evaluate the merit of the proposed method, a model that's using the \"finetuning\" part of the pipeline, but trained from the clean MUSDB dataset would help\n- the performance of the bleeding estimator (the simulated bleeding data used to train this model will behave differently from the actual bleeding in the real world).\n- add SDR as an evaluation metric\n- subjective performance comparisons between the \"no FT\" and \"FT-RG\" models to demonstrate the practical perceptual benefits of the proposed bleeding guidance were not provided\n\n* better justifications\n- to convincingly justify the complex approach proposed, the authors should first experimentally demonstrate that models trained on sufficient Western music data (e.g., Demucs or the proposed LDM) exhibit significantly lower performance on Carnatic music.\n- why using the M2L encoder-decoder model if it's performances are so low (PESQ of 2.739) ?\n\nWe encourage the authors to strictly follow the recommendations made by the reviewers in submitting the final version of their work.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper presents an LDM-based source separation, which is guided by a regression model. Instead of the common classification guidance, it's estimating the level of bleeding, which is estimated by a separate model, to leverage both datasets, one that's with compromised stem signals (i.e., with bleeding sources) and a clean source dataset. \n\nOverall, the proposed method is making sense and the manuscript explains the process in an effective way. It is indeed interesting to see that the dataset with bleeding can pretrain a generative model and then finetuning can further improve the performance. \n\nHowever, I believe that the paper can benefit from more thorough ablation tests. \n\n- The model relies a lot on the proposed transfer learning structure, assuming that pretraining on Saraga Carnatic and finetuninig the model on MUSDB is beneficial. To precisely evaluate the merit of the proposed method, a model that's using the \"fintuning\" part of the pipeline, but trained from the clean MUSDB dataset is going to help. I understand that there is a domain mismatch in this combination, as MUSDB is largely western. \n\n- The performance of bleeding estimator, since it's a separately trained network, needs to be evaluated. It's because the simulated bleeding data used to train this model will behave differently from the actual bleeding in the real world. \n\n- It might have been more convincing if the system was also tested on different types of music as the model seems to be general enough.",
      "review2": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nThis paper proposes a method for music source separation using generative models in scenarios where clean multi-stem recordings are not available (e.g., in Carnatic music), and where recordings often contain bleeding. The paper is generally well-written, and it offers clear explanations of latent diffusion and classifier guidance, making it easy to follow. The motivation for using latent diffusion and the necessity of a bleeding level estimator specifically for Carnatic singing voice separation are well explained in the introduction. The proposed regression-guidance, inspired by forward diffusion fine-tuning and classifier guidance, is also a very smart idea.\n\nHowever, despite these strengths, it is difficult to recommend acceptance because the proposed methods do not demonstrate sufficiently strong performance. First, in Table 2, the proposed method underperforms in terms of perceptual quality when compared to other models, making it difficult to claim its superiority. In the quantitative evaluation in Table 1, the PESQ improvement between FT and no-FT for Proposed (T=32) is only 0.032, and even the best-performing regression-guidance model (FT-RG5) shows only a 0.022 PESQ improvement over the standard FT model\u2014both of which are not clearly significant differences.\n\nMoreover, for a fair comparison between no-FT, FT, and FT-RG models, the no-FT model should also be trained with additional data (e.g., musdb18hq). From what I understand, the no-FT model was trained only on the Saraga Carnatic dataset. This would mean the FT and FT-RG models were trained on a larger dataset (Saraga Carnatic + musdb18hq), and their improved performance could simply result from having more training data rather than from the proposed methodology itself (and unfortunately, even then, the improvements are not particularly convincing based on the current results).\n\n\nMinor Comments:\n\nLine 91: The notation \"cf\" might be misinterpreted as \u201cc times f.\u201d I suggest using a different notation (e.g., $c_f$).\nLine 180: Please include citations for GroupNorm and SiLU.\nLine 385: Since Sanidha was used only for testing, I suggest changing the subtitle \u201cSanidha (A)\u201d to just \u201cSanidha.\u201d The \u201c(A)\u201d may imply it was used for training, which is not true.\nLine 434: The use of \u201cpropose\u201d might be misleading unless the authors are introducing a novel method for preference-based experiments. Consider using \u201cconduct\u201d or \u201cperform\u201d instead.\nLine 466\u2013467: It\u2019s unclear why the listening test was conducted with a guidance parameter of 10, when the performance was actually better with a value of 5.",
      "review3": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\n[Strength]\n1. The paper proposes a novel regression-based bleeding level guidance approach extending the conventional classifier guidance framework, demonstrating the potential to develop a high-quality source separation model even when only noisy databases are available.\n\n[Weaknesses]\n1. Limited justification for applying the proposed method specifically to Vocal Separation:\n- The justification for applying the proposed method specifically to the vocal separation task lacks strength. Many instruments used in Carnatic music have structural and acoustic similarities with Western instruments. (Example instruments: the Carnatic violin closely resembles the Western violin; the mridangam shares acoustic similarities with other percussive instruments used in Western ensembles.) Furthermore, acoustic characteristics between Western vocals and Carnatic vocals are relatively similar compared to differences between Western vocals and instrumental music. Thus, a vocal separation model trained on Western music is likely to perform adequately on Carnatic music as well.\n- To convincingly justify the complex approach proposed, the authors should first experimentally demonstrate that models trained on sufficient Western music data (e.g., Demucs or the proposed LDM) exhibit significantly lower performance on Carnatic music.\n\n2. Lack of comparison with transfer learning:\n- The scenario assumed by the paper (completely lacking clean in-domain data) is overly restrictive. A common and practical approach, transfer learning (pre-training on noisy Carnatic data followed by fine-tuning on small amounts of clean Carnatic data), was not tested. This omission reduces the practical relevance and applicability of the research.\n\n3. Bleeding estimator trained only on out-domain data:\n- The authors assume low domain-specificity for the task of bleeding estimation, training the bleeding estimator exclusively on out-of-domain data. However, this assumption was not experimentally validated. A comparative experiment demonstrating the performance difference between bleeding estimators trained on in-domain versus out-of-domain data would significantly strengthen the paper's claims.\n- Designing a robust bleeding estimator is critical for achieving high-quality separation performance. However, the artificial bleeding dataset constructed in this paper is likely to differ significantly from the bleeding characteristics produced by the actual latent diffusion model (LDM). Moreover, considering instrument-specific bleeding characteristics, as well as adaptive mechanisms that account for time-varying interference levels frequently encountered in practical source separation scenarios, would have provided deeper and more valuable insights for future research.\n\n4. Limited upper bound due to poor performance of the M2L encoder-decoder:\n- The performance of the proposed system is fundamentally constrained by the low quality of the M2L encoder-decoder model. Encoding and decoding vocals through the M2L model yielded a PESQ score of only 2.739, indicating poor audio quality and limiting the potential for improvement \n\n5. Insufficient experimental validation: Objective experiments\n- Omission of SDR as an evaluation metric: Typically, mixture signals in music source separation tasks are linear summations of individual sources, implying identical phases between the target source and its mixture counterpart. Even generative models, which generally maintain phase coherence, should include SDR as an evaluation metric, as demonstrated in prior work such as Multi-Source Diffusion Models [5]. Furthermore, unless bleeding is severe, source phases generally remain similar to those in the original mixture, thus enabling the use of SDR or other signal-domain metrics for accurate performance evaluation. Additionally, the Fr\u00e9chet Audio Distance (FAD) metric employed in the paper may also exhibit low correlation with perceptual quality in certain cases. Therefore, it is strongly recommended to provide as many objective metrics as possible to ensure a comprehensive and reliable evaluation.\n- Absence of interference removal metric: Despite interference removal being the paper's key contribution, no relevant objective metrics such as Source-to-Interference Ratio (SIR) were provided.\n- Missing experimental results: The LSD performance was always degraded when regression guidance (RG) was applied at sampling steps = 32 and 64. However, at sampling steps = 128, LSD results for the FT and FT-RG\u2075 models were omitted, with only FT-RG\u00b2\u2070 performance presented as best without sufficient context.\n\n6. Insufficient experimental validation: Subjective experiments\n- Lack of subjective assessment regarding regression-based bleeding guidance effectiveness: Crucially, subjective performance comparisons between the \"no FT\" and \"FT-RG\" models to demonstrate the practical perceptual benefits of the proposed bleeding guidance were not provided, significantly weakening the validity of subjective claims.\n\n[Overall review]\nThe architecture proposed by the authors is interesting and presents novel elements, specifically the extension of classifier guidance into a regression-based bleeding level guidance structure. However, the assumed scenario\u2014using exclusively bleeding-contaminated in-domain data\u2014is highly restrictive and limits real-world applicability.\nAdditionally, despite employing a complex approach, the performance is inherently constrained by the poor audio quality delivered by the M2L encoder-decoder framework. Furthermore, insufficient experimental validation, as highlighted above, significantly reduces the reliability and credibility of the reported results.\nDue to these critical limitations and insufficient experimental validation, the recommendation for this paper is weak reject."
    },
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Vocoders, which reconstruct time-domain waveforms from spectral representations such as mel-spectrograms, are essential in modern music and speech synthesis. Traditional signal-processing techniques like the Griffin-Lim algorithm have largely been replaced by neural vocoders, which leverage generative models to achieve superior audio quality. However, these models can introduce artifacts and biases, potentially affecting their output in unforeseen ways.\nIn this study, we examine how different musical tunings affect neural mel-to-audio vocoders within the context of Western music, where performances do not necessarily adhere to the modern 440 Hz standard tuning. As a key contribution, we evaluate several recent neural vocoders on datasets containing piano, violin, and singing voice recordings. Our results reveal that different vocoders exhibit distinct biases, causing deviation in tuning, and affecting waveform reconstruction quality in case of non-standard tuning. \nOur work underscores the need for improved vocoder robustness in music synthesis and provides insights for refining future models.",
      "abstract": "Vocoders, which reconstruct time-domain waveforms from spectral representations such as mel-spectrograms, are essential in modern music and speech synthesis. Traditional signal-processing techniques like the Griffin-Lim algorithm have largely been replaced by neural vocoders, which leverage generative models to achieve superior audio quality. However, these models can introduce artifacts and biases, potentially affecting their output in unforeseen ways.\nIn this study, we examine how different musical tunings affect neural mel-to-audio vocoders within the context of Western music, where performances do not necessarily adhere to the modern 440 Hz standard tuning. As a key contribution, we evaluate several recent neural vocoders on datasets containing piano, violin, and singing voice recordings. Our results reveal that different vocoders exhibit distinct biases, causing deviation in tuning, and affecting waveform reconstruction quality in case of non-standard tuning. \nOur work underscores the need for improved vocoder robustness in music synthesis and provides insights for refining future models.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Hans-Ulrich Berendes",
        "Ben Maman",
        "Meinard M\u00fcller"
      ],
      "authors_and_affil": [
        "Hans-Ulrich Berendes (International Audio Laboratories Erlangen)*",
        "Ben Maman (International Audio Laboratories Erlangen)",
        "Meinard M\u00fcller (International Audio Laboratories Erlangen)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Music synthesis and transformation",
        "Generative Tasks",
        "Evaluation metrics",
        "Music and audio synthesis",
        "MIR tasks",
        "Evaluation, datasets, and reproducibility",
        "Evaluation methodology",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/18yvPjp36oCdYngVAQZ8yJw-vaKaRqCTD/preview",
      "poster_pdf": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-6-tuning-matters-analyzing",
      "title": "Tuning Matters: Analyzing Musical Tuning Bias in Neural Vocoders",
      "video": ""
    },
    "forum": "199",
    "id": "199",
    "pic_id": "",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q5 ( Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chosen, otherwise write \"n/a\"))**\n\nThe authors might or might not consider to see if this little study on tuning frequency artifacts in classification might be relevant:\nY. Qin and A. Lerch, \u201cTuning Frequency Dependency in Music Classification,\u201d in Proceedings of the International Conference on Acoustics Speech and Signal Processing (ICASSP), Brighton, UK: Institute of Electrical and Electronics Engineers (IEEE), 2019, pp. 401\u2013405. doi: 10.1109/ICASSP.2019.8683340.\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nStrongly agree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nStrongly agree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nStrongly agree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nStrongly agree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe results of this study are relevant for use of neural vocoders, which people tend to use without being aware of limitations and constraints. It is also important for future development of vocoders, where proper data augmentation might mitigate the problems when detuned.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nNeural vocoders do not provide consistent output quality for non-standard tuning frequencies.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis is a interesting study systematically evaluating how the tuning impacts the quality of a pre-trained neural vocoder generated output. This well-planned and well-executed study that is also well-presented. It provides useful insights into the limitations of current neural vocoders and highlights a cause for quality impairments that is often overlooked. I congratulate the authors for a very interesting read.\n\n One result that I would have wished to see is the tuning frequency estimate deviations on the unaugmented data. I understand that these data would be imbalanced, but it would be good to verify that the error is indeed larger for tuning freqs far away from 440 as a sanity check.\n\nJust as a note:\nA suggestion for possible future studies: My personal suggestion would have been to run the subjective test on synthesized piano data that can be tuned without artifacts; that way the confounding pitch shifting artifacts could have been eliminated (also, simply using a decent quality pitch shifter might have helped).\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nStrong accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThe authors present a methodological investigation of an overlooked potential problem - the tuning frequency dependency of neural vocoders. The study provides valuable insights for future research and applications, is well-written and well-structured and explores the research question systematically.\n\nThe reviewers agree on the strong points of the paper and that this is a contribution that should be of interest in the research community. In addition to some short-comings that are not easily addressed, such as some shortcomings of the listening test design and associated limited insights, other points of critique are easily addressed, and I urge the authors to implement the following suggestions for the camera-ready paper in case of acceptance:\n1. Expanded literature review on tuning frequency detection and justification of the used tuning freq detection methods\n2. Inclusion of the BigVGAN scatter plot: as that model is trained on speech, it should not have any tuning frequency dependency\n3. Better description of the sample rates at different processing steps and sample rate conversion method: what is being resampled to what rate when.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper proposes that state of the art neural vocoders introduce a systematic bias towards standard tuning.\n\nThe idea makes sense and proving this bias is an important contribution. However, the formulation of the problem seems a bit naive at some points, for example the author state\n\"Despite their impressive audio quality, neural vocoders are sensitive to their training data\" This should be beyond obvious, and the ability of vocoders to generalize will be related to the distribution of the training data. The authors should try to explain what data is used to train each vocoder.\n\nThe authors propose that the problem is circular but tis only seems necessary because of the algorithms used for tuning estimation. I think more discussion of the state of the art in tuning estimation is needed. With respect to the methodology, it would have been better to evaluate with real data that uses non-standard tuning than use pitch shifting for all the experiments.\n\nBeyond these issues, the most important problem in my view is that the authors do not attempt to explain their findings, In particular, one of the vocoders (BV2-128) competes with the unbiased LSGL approach (Fig 4), and generally results improve with the number of bands. Yet, this vocoder is inexplicably missing from Fig 5. Moreover, the listening experiment seems to ask an orthogonal question. It almost seems like he authors changed their mind and decided that changing the tuning is OK as long as the vocoder sounds good.\n\nIn all I think there is a missed opportunity in obtaining a better insight about the effect of neural vocoders on tuning based on these experiments.",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper investigates the tuning bias of popular neural vocoders for music that uses mel-spectrograms as control parameters. The objective evaluation is well-designed and shows apparent biases among different vocoders, favouring a few common tunings. Subjective evaluations confirm that feeding less-common tunings does affect some vocoders' synthesis quality. These findings are valuable for future research in removing potential biases in music generation/synthesis with neural networks. Nevertheless, the paper does not touch much on possible directions to mitigate the biases.\n\nI personally feel the topic is novel and important. The evaluations and results are presented well, so I don't have much criticism, but some minor comments.\n\nSuggestions for improvements:\n\nIn Section 3.2, the authors said that the artefacts caused by pitch shifting are negligible, but later in Section 5, they also mentioned that the artefacts can affect perceived quality, which contradicts this. I understand that for the objective evaluation, the minor artefacts do not affect the targeting variable, the tunings. However, it would still be good to clarify in the text why artefacts have different degrees of impact in other evaluations.\n\nIn Fig. 7, I would still recommend including the \"no preference\" for a more intuitive analysis. The authors mentioned that all the items are vocoded in the second-to-last paragraph of Section 5.1. It would have been better if this had been mentioned at the beginning of the section, since the first time I read it, I was confused and wondered, \"Are all the items being vocodered or just the pitch-shifted ones?\"\n\nI have some issues understanding what sample rates were used in the experiments. Since the vocoders are not all operated at 16 kHz, did the authors 1) first downsample all the audio to 16 kHz then depends on which vocoder is using, upsampled the audio to its operating rates, or 2) downsampling the original audio (>44.1 kHz) to the operating rates of the vocoders? Please clarify this in the text since it affects the resulting mel-spectrograms.\n\nThere are short paragraphs consisting of no more than two sentences. I recommend merging them to improve the readability. Isolated subsections like Section 5.1 should be discouraged. A footnote linking to YouTube isn't necessary. The authors can simplify equation 1 as ` (\\tau_2 - \\tau_1 + 50) \\mod 100 - 50` to save some space, maybe connect this to the classic phase unwrapping problem, where the phases are wrapped inside [-\\pi, \\pi). In this way, the explanation in Section 3.3 can be even shorter. Keyword: Itoh's condition.\n\nLastly, I really think the author can discuss what kind of sources contribute to those biases and how to mitigate this issue in the future. Even a brief paragraph on this is enough.\n\nRegarding reference entry format:\nPlease cite the conference version of GANSynth [19] instead of its preprint version.",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper provides a well-executed investigation into the issue of musical tuning preservation in neural vocoders. The study is relevant to the ISMIR community, as vocoders are integral components of modern music generation systems. The paper appears clear and well-written.\nThe authors clearly articulate the problem and deliver results across different datasets and 3 vocoder models.\n\nThe presentation of the listening test appears a bit limited: first of all, both pitch-shifting artifacts and vocoding artifacts can be present. Also, in line 425 the authors write: \u201cWhen aggregating preferences into groups of \u201coriginal\u201d and \u201cpitch-shifted\u201d, listeners show a slight preference towards the original items for the neural vocoders, indicating that pitch-shifting also has a negative influence on quality (see supplementary website).\u201d\nIt would be preferable to have an accompanying website already available for the review period, or at least try to include this plot inside the page limit.\n\nAlso, when comparing results from different models, it is not clear which sampling rate is used for each model. BigVGAN and BigVGAN2 are available with different sampling rates, and it would be helpful to disclose how the metric is calculated. You mention that the pitch-shifted dataset samples are downsampled to 16 kHz. Are all the samples (including reconstructions from the 2 BigVGAN models) downsampled to 16 kHz before calculating the metric (in order to be fair with HAWT and LSGL)? If not, will the comparison still be fair? Some clarifications on this matter would be greatly beneficial.\n\nAdditionally, why was the BigVGAN model included in the comparison, since it was trained only on speech? I am not suggesting to remove this baseline, but rather to properly justify its inclusion. Also, I notice that the scatter plots are presented for all baselines except this model (BV). It would be of great interest to visualize the tuning bias on music samples of a model trained only on speech. I would personally not expect to have a \u201cquantization\u201d effect to 440 Hz, since it did not \u201csee\u201d music samples during training, which I assume is what biases these models to this effect. I would recommend including this plot in the final version.\n\nIt would be also interesting to further discuss the potential root causes of these tuning biases within the vocoder architectures themselves. Is it primarily due to the training data distributions? \n\nDespite these points, the paper presents a solid contribution, highlighting an important aspect of vocoder performance and offering a possible evaluation framework. The work is well-written, the results are quite clear, and the implications are useful/practical for the community."
    },
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Generating expressive audio performances from music scores requires models to capture both instrument acoustics and human interpretation. Traditional music performance synthesis pipelines follow a two-stage approach, first generating expressive performance MIDI from a score, then synthesising the MIDI into audio. However, the synthesis models often struggle to generalise across diverse MIDI sources, musical styles, and recording environments. To address these challenges, we propose MIDI-VALLE, a neural codec language model adapted from the VALLE framework, which was originally designed for zero-shot personalised text-to-speech (TTS) synthesis. For performance MIDI-to-audio synthesis, we improve the architecture to condition on a reference audio performance and its corresponding MIDI. Unlike previous TTS-based systems that rely on piano rolls, MIDI-VALLE encodes both MIDI and audio as discrete tokens, facilitating a more consistent and robust modelling of piano performances. Furthermore, the model\u2019s generalisation ability is enhanced by training on an extensive and diverse piano performance dataset. Evaluation results show that MIDI-VALLE significantly outperforms a state-of-the-art baseline, achieving over 75\\% lower Fr\u00e9chet Audio Distance on the ATEPP and Maestro datasets. In the listening test, MIDI-VALLE received 202 votes compared to 58 for the baseline, demonstrating improved synthesis quality and generalisation across diverse performance MIDI inputs.",
      "abstract": "Generating expressive audio performances from music scores requires models to capture both instrument acoustics and human interpretation. Traditional music performance synthesis pipelines follow a two-stage approach, first generating expressive performance MIDI from a score, then synthesising the MIDI into audio. However, the synthesis models often struggle to generalise across diverse MIDI sources, musical styles, and recording environments. To address these challenges, we propose MIDI-VALLE, a neural codec language model adapted from the VALLE framework, which was originally designed for zero-shot personalised text-to-speech (TTS) synthesis. For performance MIDI-to-audio synthesis, we improve the architecture to condition on a reference audio performance and its corresponding MIDI. Unlike previous TTS-based systems that rely on piano rolls, MIDI-VALLE encodes both MIDI and audio as discrete tokens, facilitating a more consistent and robust modelling of piano performances. Furthermore, the model\u2019s generalisation ability is enhanced by training on an extensive and diverse piano performance dataset. Evaluation results show that MIDI-VALLE significantly outperforms a state-of-the-art baseline, achieving over 75\\% lower Fr\u00e9chet Audio Distance on the ATEPP and Maestro datasets. In the listening test, MIDI-VALLE received 202 votes compared to 58 for the baseline, demonstrating improved synthesis quality and generalisation across diverse performance MIDI inputs.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Jingjing Tang",
        "Xin Wang",
        "Zhe Zhang",
        "Junichi Yamagish",
        "Geraint Wiggins",
        "George Fazekas"
      ],
      "authors_and_affil": [
        "Jingjing Tang (Queen Mary University of London)*",
        "Xin Wang (National Institute of Informatics)",
        "Zhe Zhang (National Institute of Informatics)",
        "Junichi Yamagish (National Institute of Informatics)",
        "Geraint Wiggins (Queen Mary University of London\t)",
        "George Fazekas (Queen Mary University of London\t)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Generative Tasks",
        "Musical features and properties",
        "Music and audio synthesis",
        "MIR tasks",
        "Expression and performative aspects of music",
        "Music synthesis and transformation",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "TRUE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1RA4UQL2qt5E2R1maoZh5Dp_IQczXBbEH/preview",
      "poster_pdf": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-1-midi-valle-improving",
      "title": "MIDI-VALLE: Improving Expressive Piano Performance Synthesis Through Neural Codec Language Modelling",
      "video": ""
    },
    "forum": "208",
    "id": "208",
    "pic_id": "",
    "position": "01",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\ncode and demos\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nValle's methodology used in MIDI-Audio for expressive performance rendering\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper presents MIDI-VALLE, a neural codec language model for expressive piano performance synthesis, inspired by VALLE from the speech domain. The authors introduce Piano-Encodec, a piano-specific audio tokenizer, and propose a discrete tokenization pipeline for both MIDI and audio, enabling high-fidelity synthesis from symbolic input with either MIDI or audio prompts. The paper is solidly grounded in prior work and demonstrates clear technical contributions, especially in bridging symbolic and acoustic domains via discrete representations.\n\nThat said, some limitations are noted. The model primarily targets classical piano and struggles with jazz/generalization; its evaluation lacks comparison to stronger baselines like Pianoteq; and claims around style prompting and codebook interpretability would benefit from more evidence. Importantly, while the architecture supports audio-prompt-based generation, the potential for style transfer (as explored in VALLE) is not fully demonstrated or evaluated. Highlighting this as a future direction\u2014e.g., transferring expressive characteristics from a performer\u2019s prompt to unseen MIDI\u2014would significantly strengthen the work\u2019s broader impact.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThis paper combines a custom-trained audio codec with a discrete token-based MIDI-to-audio generation pipeline. The work is technically sound, well-motivated, and includes solid subjective/audio results demonstrating high-quality piano synthesis.\n\nreviewers all agree on an acceptance. For the final version, please pay attention to the weakness parts posted by reviewers, especially the todo list by R3.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nStrengths: \n\n- Flexible Framework: MIDI-VALLE presents an adaptable framework that can be applied to various music synthesis and transcription tasks, such as music score prediction, audio-to-MIDI transcription, and music generation beyond classical piano. \n\n- The model introduces a novel method of treating MIDI as discrete tokens, avoiding traditional piano rolls and spectrograms, which improves generalization and synthesis quality. \n\n- MIDI-VALLE\u2019s training on transcribed MIDI data enhances its ability to generalize to recorded data without the need for fine-tuning, which is beneficial for real-world applications where recorded data is often limited. \n\n- The model demonstrates improved synthesis quality (audio examples are provided), outperforming state-of-the-art baselines such as M2A on multiple datasets, with better preservation of timbral and ambient features. \n\nWeaknesses : \n\n- MIDI-VALLE struggles to generalize beyond classical music, particularly with genres that involve richer harmonic content, syncopation, and subtle expressive variations. \n\n- The remaining FAD gap between MIDI-VALLE generations and the ground truth may be due to the noisier outputs of the non-autoregressive model. \n\n- The distinction between MIDI-VALLE\u2019s MIDI tokenization and the Octuple MIDI method is unclear, particularly without a clear reference to the original model. Also, claims about note-wise encoding and reduced complexity lack sufficient explanation or comparison with the original model. \n\n- It would have been interesting to provide audio examples to support this claim: \u201cThe first codebook captures primary acoustic features, such as pitch, note duration, and timbre, while the subsequent codebooks focus on finer details of these features.\u201d",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nStrengths: \n- Clear writing and analysis\n- Superior Synthesis Quality on Classical Music\n- Robust Tokenization Approach\n- Enhanced System Compatibility\n\nWeaknesses of the model\n- Struggles with Jazz, authors acknowledge this and provide appropriate justifications\n- Prompt Alignment Sensitivity, also discussed in the article\n\nAccept (confidence 4/5): this paper adapts the VALLE framework with discrete tokenization for MIDI and audio to achieve state\u2011of\u2011the\u2011art expressive piano synthesis, showing substantial improvements over the M2A baseline\u2014particularly on classical repertoire\u2014via better FAD scores, listening\u2011test preferences, generalization to recorded MIDI, and prompt\u2011acoustic adaptability. While limitations remain in jazz performance, prompt alignment, and pedal synthesis (all acknowledged for future work), the strong technical contributions and clear results on the primary task justify acceptance.",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe paper presents MIDI-VALLE, a transformer-based model for expressive performance synthesis based on the architecture of VALL-E, a model for text-to-speech synthesis. The work introduces a fine-tuned audio codec for piano music, Piano-Encodec, and a MIDI-to-audio generative model with additional audio and MIDI acoustic prompts for conditioning. The results are convincing and well presented.\n\nThe main strengths of the paper are:\n1. Theoretically sound and solid paper. Clear motivation and application of existing approaches from the related domain of speech synthesis to expressive music performance synthesis.\n2. The choice of model design is validated and all steps are explained in detail. The choice for audio tokenization is also validated and contrasted with the piano roll representation.\n3. The trained Piano-Encodec shows good reconstruction quality and can be used for any other research related to piano audio generation. To the best of my knowledge, there are no open-source audio codecs specifically tuned for piano music.\n4. The subjective evaluation and the demo samples on the website are convincing and show the effectiveness of the designed approach for performance synthesis.\n\nThe main weaknesses of the work are the computational complexity of the model and the suboptimal subjective evaluation:\n1. The task of MIDI-to-audio inference does not involve generating an expressive performance from scratch, and thus is easier than end-to-end expressive performance rendering and audio synthesis. While the model design is sound, a 12-layer transformer may be overkill for this problem. The autoregressive part of the first inference stage makes inference slower than alternatives. An ablation on the model size and replacing tokens with mel spectrograms will be interesting.\n2. The work does not contribute much to the architectural design of audio codecs and synthesis. It is a successful adaptation of existing methods to a task of MIDI-to-audio synthesis. The choice of EnCodec might be a bit outdated when there are more advanced audio codecs in terms of compression and number of tokens. For example, DAC [1] as a better version of EnCodec or WavTokenizer [2] with a single codebook.\n3. MIDI-VALLE is only compared with the M2A model [3]. In the M2A paper, however, the model loses against MIDI files synthesized with Pianoteq. This raises the question: is MIDI-VALLE better than the Pianoteq synthesis? A direct comparison would strengthen the paper. In addition, the work on diffusion-based performance conditioning for preserving acoustics and style in audio synthesis can be used for a comparison [4].\n\nSome questions and comments that may be addressed in the final version of the paper:\n1. In Section 3.1.1, is there any scientific evidence that the first codebook models pitches, durations, and timbre? It follows intuitively, but without confirmation, e.g. by training only on the selected codebooks, it is an unconfirmed statement.\n2. In Table 1, why the vocabulary size for speech is 512 when each RVQ has a codebook of size 2048 (Section 4.2)?\n3. In Section 3.3, does it mean that for the MIDI prompt, some audio-to-MIDI transcription is required, when initially we only have audio for inference?\n4. In Section 4.1, pedals are excluded but do durations encode raw or sustained MIDI durations?\n5. In Section 4.2, why only 60 hours of the entire ATEPP dataset are used for codec tuning?\n6. Does the model skip or repeat notes in the middle of the sequence? For example, VALL-E is known to struggle with word skips/repeats for non-trivial sentences due to attention failures in the autoregressive token modeling. It is interesting to observe the attention maps for the trained transformer model.\n7. The model is trained on 15-20s snippets. Can it be used to synthesize a full-length MIDI performance? How well will the acoustic conditions be preserved?\n\nMinor:\n1. The abstract contrasts a two-step approach, and the wording implies that the paper solves its challenges, but the paper solves only the second step.\n2. In Section 3.2, the formal definition does not distinguish between MIDI prompt and target MIDI. If $x$ is the target MIDI, then the MIDI prompt should also be defined.\n3. In Section 3.2, is it correct that the AR model does not work with acoustic prompt? From Figure 1, this information is not trivial.\n4. In Section 6, split the Results section into several subsections for better readability.\n5. Line 415: \"taht\" -> \"that\"\n\nOverall, this is a solid paper that should be accepted for presentation at the conference.\n\nReferences:\n[1] Kumar, Rithesh, et al. \"High-fidelity audio compression with improved rvqgan.\" Advances in Neural Information Processing Systems. 2023\n[2] Ji, Shengpeng, et al. \"Wavtokenizer: an efficient acoustic discrete codec tokenizer for audio language modeling.\" ICLR. 2025.\n[3] Tang, Jingjing, et al. \"Towards An Integrated Approach for Expressive Piano Performance Synthesis from Music Scores.\" ICASSP. 2025.\n[4] Maman, Ben, et al. \"Performance conditioning for diffusion-based multi-instrument music synthesis.\" ICASSP. 2024."
    },
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Recent advances in music foundation models have improved audio representation learning, yet their effectiveness across diverse musical traditions remains limited. We introduce CultureMERT-95M, a multi-culturally adapted foundation model developed to enhance cross-cultural music representation learning and understanding. To achieve this, we propose a two-stage continual pre-training strategy that integrates learning rate re-warming and re-decaying, enabling stable adaptation even with limited computational resources. Training on a 650-hour multi-cultural data mix, comprising Greek, Turkish, and Indian music traditions, results in an average improvement of 4.9% in ROC-AUC and AP across diverse non-Western music auto-tagging tasks, surpassing prior state-of-the-art, with minimal forgetting on Western-centric benchmarks. We further investigate task arithmetic, an alternative approach to multi-cultural adaptation that merges single-culture adapted models in the weight space. Task arithmetic performs on par with our multi-culturally trained model on non-Western auto-tagging tasks and shows no regression on Western datasets. Cross-cultural evaluation reveals that single-culture models transfer with varying effectiveness across musical traditions, whereas the multi-culturally adapted model achieves the best overall performance. To support research on world music representation learning, we publicly release CultureMERT-95M and CultureMERT-TA-95M, fostering the development of more culturally aware music foundation models.",
      "abstract": "Recent advances in music foundation models have improved audio representation learning, yet their effectiveness across diverse musical traditions remains limited. We introduce CultureMERT-95M, a multi-culturally adapted foundation model developed to enhance cross-cultural music representation learning and understanding. To achieve this, we propose a two-stage continual pre-training strategy that integrates learning rate re-warming and re-decaying, enabling stable adaptation even with limited computational resources. Training on a 650-hour multi-cultural data mix, comprising Greek, Turkish, and Indian music traditions, results in an average improvement of 4.9% in ROC-AUC and AP across diverse non-Western music auto-tagging tasks, surpassing prior state-of-the-art, with minimal forgetting on Western-centric benchmarks. We further investigate task arithmetic, an alternative approach to multi-cultural adaptation that merges single-culture adapted models in the weight space. Task arithmetic performs on par with our multi-culturally trained model on non-Western auto-tagging tasks and shows no regression on Western datasets. Cross-cultural evaluation reveals that single-culture models transfer with varying effectiveness across musical traditions, whereas the multi-culturally adapted model achieves the best overall performance. To support research on world music representation learning, we publicly release CultureMERT-95M and CultureMERT-TA-95M, fostering the development of more culturally aware music foundation models.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Angelos-Nikolaos Kanatas",
        "Charilaos Papaioannou",
        "Alexandros Potamianos"
      ],
      "authors_and_affil": [
        "Angelos-Nikolaos Kanatas (School of ECE, National Technical University of Athens)*",
        "Charilaos Papaioannou (School of ECE, National Technical University of Athens)",
        "Alexandros Potamianos (School of ECE, National Technical University of Athens)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Computational ethnomusicology",
        "Machine learning/artificial intelligence for music",
        "Automatic classification",
        "MIR tasks",
        "Metadata, tags, linked data, and semantic web",
        "MIR fundamentals and methodology",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1jv2olgDNTX6u2OVUvIvF8PtwMELFAiWH/preview",
      "poster_pdf": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-7-culturemert-continual-pre",
      "title": "CultureMERT: Continual Pre-Training for Cross-Cultural Music Representation Learning",
      "video": ""
    },
    "forum": "210",
    "id": "210",
    "pic_id": "",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nStrongly agree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nStrongly agree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe paper provides how continual pretraining can be effectively applied to adapt foundation models like MERT for cross-cultural music tasks, even with limited computational resources.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nA two-stage training strategy allows foundation models to be adapted for cross-cultural music understanding with limited data.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper presents a method that fine-tuning the MERT model so that it can become a multicultural music understanding model. The retrained MERT model shows improved performance across four multicultural music tasks and two Western music tasks, outperforming uni-cultural systems and other state-of-the-art baselines.\n\nThe main contribution is a two-stage pretraining strategy. In the first stage, the Transformer encoder is frozen and only a small subset of the data was used to train. The second stage performs full adaptation. The model is designed to predict correct EnCodec tokens to make sure the acoustic features are learned, as well as a CQT reconstruction loss.\n\nOverall, the paper is readable and the results are promising, showing strong performance on several tasks.\n\nThis strong paper could be strengthened by addressing the following points.\n\n- The authors chose to update the CNN part only while freezing the Transformer encoder, but the rationale for this decision is not clearly explained. Other strategies, such as gradually updating a few layers of the Transformer encoder, could also be considered. It would strengthen the paper to discuss these options and explain why the chosen method was selected.\n\n- The paper already covers four non-western music genres, however, to be more inclusive and general, it would be helpful to discuss how to handle new genres beyond the four. Section 5.1 touches on this but not general enough to support the claim of a multicultural model.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nStrong accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThis paper presents a method that fine-tuning the MERT model so that it can become a multicultural music understanding model. The retrained MERT model shows improved performance across four multicultural music tasks and two Western music tasks, outperforming uni-cultural systems and other state-of-the-art baselines.\n\nThe main contribution is a two-stage pretraining strategy. In the first stage, the Transformer encoder is frozen and only a small subset of the data was used to train. The second stage performs full adaptation. The model is designed to predict correct EnCodec tokens to make sure the acoustic features are learned, as well as a CQT reconstruction loss.\n\nThe paper is well-written, well-organized, and makes a meaningful contribution toward more inclusive, cross-cultural MIR. As all reviewers have noted, its strengths are substantial and its weaknesses are minor and addressable.\n\nThus, I recommend a strong accept.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nIn this paper, the authors present a framework to adapt the pre-trained MERT foundation model to more diverse music, i.e., from non-western traditions. Specifically, two different approaches are discussed. In the first, a single multi-cultural MERT model is fine-tuned on the entire merged dataset, while in the second, a culturally specialized MERT model is created for each cultural dataset with the outputs aggregated through task-arithmetic. Additionally, in both cases, a continual pre-training strategy is adopted to improve the stability of the fine-tuning and ensure knowledge retention. The evaluation is performed on a music-tagging task by adding a simple MLP on top of each MERT feature extractor. Results demonstrate that both strategies outperform the original MERT on non-western tasks without degrading performances on the original western music datasets.\n\nThe paper addresses a highly relevant topic and offers useful insights for the MIR community. The experimental setup is detailed explicitly, and results are clearly commented on. Overall, the paper is clear and well-written. However, I do have one minor comment for the authors. I believe it would be interesting to apply a similar evaluation to other MIR tasks which are less dependent on a music's cultural origin, such as music emotion recognition.",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe paper is well written and structured. \nIt addresses the Western-centric bias prevalent in current foundation models for music by exploring two adaptation strategies aimed at improving performance on underrepresented musical traditions. The authors demonstrate state-of-the-art results on a variety of downstream tasks (e.g., genre and instrument classification) across multiple non-Western benchmarks.\n\nThe proposed approach leverages a two-stage continual pretraining scheme to produce both single-culture and multi-cultural variants of the original MERT model. Futhermore single-culture models are aggregating using task-arithmetic. The results show that the multicultural model performs more robustly across non-Western benchmarks than models adapted to individual cultures.\n\nMethodology: \n- In Section 3.2, it is unclear whether the single-culture adapted models also undergo the stabilization phase (Stage 1) using 20% of the Music4All data.\n\nQuestions:\n \n- Can a track be in the training split for pre-training and then be in the test set for the probing? \n- Given the comparable size of pre-training data (1k hours vs 650) did you try training from scratch a culturally balanced?\n- If I understand Section 3.2 correctly, is the two-stage CPT strategy primarily motivated by memory or batch size constraints? In other words, if one were able to match the original model's batch size (1.5 hours of audio), would it be possible\u2014and perhaps preferable\u2014to train all layers directly on the multi-cultural data without the need for a stabilization phase?\n- In Table 2, could the performance gap between CultureMERT and single-culture MERT be attributed to the different amounts of data used during the CPT phase?",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper generally reads well, and the introduction is particularly well formed to ease into the topic. The paper clearly wants to explain concepts, methods and their goals to the reader, and generally constructs good connections to existing works and raises notable points, e.g. explanation of Eq. 5 simplifying to weight averaging under a certain condition.\nAlong this route, some redundancy is introduced across Chapters 1 to 3, which can occasionally create some confusion as necessary details for understanding are explained later, but that is not referred to in the earlier parts (i.e. \u201csee details in Section 3.2\u201d).\nThe method of Task Arithmetic is particularly interesting and may generate discourse due to its \u201cintuitiveness\u201d and capability of reducing computational cost. The results of CultureMERT-TA are slightly below CultureMERT, with both models exhibiting different strengths on individual music traditions, but still well above the baseline.\nAll in all, the foundation seems solid, and the figures help in supporting claims and gaining some insights into the architecture and model performances.\n\nThe training details could be clearer, as across the paper different numbers are scattered and only fall together in Section 4.3: \u201c30-second segments\u201d (146), \u201c160 seconds per step\u201d (251), \u201c5-second audio segments\u201d (318), and \u201cbatch size of 32 recordings (160 seconds)\u201d (349). This information could be made clearer across the manuscript (e.g. simple references).\n\nThe improvement average of 4.43% in ROC-AUC only appears in the abstract and the introduction, but cannot be found for example in Table 2: the average here is only 3.2% between MERT-v1 and CultureMERT.\n\nSection 2, 159: The last point (ii) is unclear. You want to generalize but ensure that the model was exposed to everything before evaluation?\n\nSection 3.2: Solid approach to stabilize CPT. This section explains approaches, methods, and problems equally (e.g. stability gap). Instead of simply introducing the term \u201cplasticity gradient\u201d (234), you could make the connection clearer to the stability gap (257) from the get-go. Generally, the \u201cStaged Adaptation\u201d paragraph in this section repeats info, and introduces concepts in the beginning that are explained again at the end, but in a much clearer way (256-264). I suggest to merge the info here for better flow and reducing confusion.\n\n241: Why is Music4All incorporated here for the Western \u201cdistribution shift safety\u201d? Two other datasets were mentioned initially to represent Western music: MTAT and FMA-medium. What is the reasoning here, or the differences?\n\n276: This may be a confusion in my understanding, but model (i) is not only trained on non-Western due to the inclusion of Western music in stage 1 for the stability gap, correct? Maybe it should say that it spans the four non-Western musical traditions AND the Western one, because preventing forgetting is one of the core goals?\n\n343: \u201cWe apply a maximum duration cut as in [45].\u201d It would be helpful to give a short detail here.\n\nTable 2: How would you explain why the performance of LyraMERT on itself is worse than e.g. CarnaticMERT on Lyra? In 411-413 you give hints for makam vs Carnatic, that they share theoretical foundations etc. Do you think that extends to Lyra as well? According to Fig. 3, there are no striking similarities between Carnatic and Lyra as compared to others.\n\nFigure 1, arrows pointing from the dataset to several parts of the architecture: not completely clear what they signify.\n\nTable 1 caption: What does \u201cWestern replay\u201d mean?\n\nReferences are generally well formatted, but could be reduced with abbreviations (e.g. Proc. vs. Proceedings, removing editors where not strictly necessary, and writing the year only once, for example in [33], [34] and more...)"
    },
    "session": "5"
  },
  {
    "content": {
      "TLDR": "We explore transfer learning strategies for musical onset detection in the Afro-Brazilian Maracatu tradition, which features complex rhythmic patterns that challenge conventional models. We adapt two Temporal Convolutional Network architectures: one pre-trained for onset detection (intra-task) and another for beat tracking (inter-task). Using only 5-second annotated snippets per instrument, we fine-tune these models through layer-wise retraining strategies for five traditional percussion instruments. Our results demonstrate significant improvements over baseline performance, with F1 scores reaching up to 0.998 in the intra-task setting and improvements of over 50 percentage points in best-case scenarios. The cross-task adaptation proves particularly effective for time-keeping instruments, where onsets naturally align with beat positions. The optimal fine-tuning configuration varies by instrument, highlighting the importance of instrument-specific adaptation strategies. This approach addresses the challenges of underrepresented musical traditions, offering an efficient human-in-the-loop methodology that minimizes annotation effort while maximizing performance. Our findings contribute to more inclusive music information retrieval tools applicable beyond Western musical contexts.",
      "abstract": "We explore transfer learning strategies for musical onset detection in the Afro-Brazilian Maracatu tradition, which features complex rhythmic patterns that challenge conventional models. We adapt two Temporal Convolutional Network architectures: one pre-trained for onset detection (intra-task) and another for beat tracking (inter-task). Using only 5-second annotated snippets per instrument, we fine-tune these models through layer-wise retraining strategies for five traditional percussion instruments. Our results demonstrate significant improvements over baseline performance, with F1 scores reaching up to 0.998 in the intra-task setting and improvements of over 50 percentage points in best-case scenarios. The cross-task adaptation proves particularly effective for time-keeping instruments, where onsets naturally align with beat positions. The optimal fine-tuning configuration varies by instrument, highlighting the importance of instrument-specific adaptation strategies. This approach addresses the challenges of underrepresented musical traditions, offering an efficient human-in-the-loop methodology that minimizes annotation effort while maximizing performance. Our findings contribute to more inclusive music information retrieval tools applicable beyond Western musical contexts.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Ant\u00f3nio Pinto"
      ],
      "authors_and_affil": [
        "Ant\u00f3nio Pinto (INESC TEC",
        "University of Porto - Faculty of Engineering)*"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Human-computer interaction",
        "Musical features and properties",
        "Knowledge-driven approaches to MIR",
        "Automatic classification",
        "Music signal processing",
        "Reproducibility",
        "MIR tasks",
        "Evaluation, datasets, and reproducibility",
        "Rhythm, beat, tempo",
        "Machine learning/artificial intelligence for music",
        "MIR fundamentals and methodology",
        "Human-centered MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/156jrIQCaisgTBJ1L37u0gSVF7pZAwvSc/preview",
      "poster_pdf": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-9-towards-human-in",
      "title": "Towards Human-in-the-loop Onset Detection: A Transfer Learning Approach for Maracatu",
      "video": ""
    },
    "forum": "211",
    "id": "211",
    "pic_id": "",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "3"
  },
  {
    "content": {
      "TLDR": "This study introduces and evaluates a new methodology for cross-cultural ethnomusicological analysis of symbolic music. We investigate music similarity in popular traditions rooted in oral transmission by identifying shared patterns at scale across multiple hierarchies. The novelty of our approach lies in expanding musical similarity phylo-analysis-typically adopting alignment metrics that compare entire scores-to structurally aware phrases and macro-structure (i.e., form) alignment. Additionally, we explore patterns derived from multiple representations (chromatic interval, diatonic interval, rhythmic ratios, and a combination of them) to facilitate the exploration of stylistic affinities across musical genres and traditions. Our method is tested on a new dataset of 600 Galician and Irish popular music scores, which includes expert annotations for 21 genres (four shared between the two traditions) and detailed phrase information, all made available as open-access data. We use the genre separation ratio to examine how alignment strategies capture stylistic structure, providing insights that support musicological exploration across genres and traditions. The resulting phylogenetic trees and distance matrices reveal relationships among traditions, genres, and scores, facilitating the exploration of cross-cultural influences and enabling the identification of shared patterns at multiple hierarchies.",
      "abstract": "This study introduces and evaluates a new methodology for cross-cultural ethnomusicological analysis of symbolic music. We investigate music similarity in popular traditions rooted in oral transmission by identifying shared patterns at scale across multiple hierarchies. The novelty of our approach lies in expanding musical similarity phylo-analysis-typically adopting alignment metrics that compare entire scores-to structurally aware phrases and macro-structure (i.e., form) alignment. Additionally, we explore patterns derived from multiple representations (chromatic interval, diatonic interval, rhythmic ratios, and a combination of them) to facilitate the exploration of stylistic affinities across musical genres and traditions. Our method is tested on a new dataset of 600 Galician and Irish popular music scores, which includes expert annotations for 21 genres (four shared between the two traditions) and detailed phrase information, all made available as open-access data. We use the genre separation ratio to examine how alignment strategies capture stylistic structure, providing insights that support musicological exploration across genres and traditions. The resulting phylogenetic trees and distance matrices reveal relationships among traditions, genres, and scores, facilitating the exploration of cross-cultural influences and enabling the identification of shared patterns at multiple hierarchies.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Hilda Romero-Velo",
        "Gilberto Bernardes",
        "Susana Ladra",
        "Jos\u00e9 R. Param\u00e1",
        "Fernando Silva"
      ],
      "authors_and_affil": [
        "Hilda Romero-Velo (Universidade da Coru\u00f1a)*",
        "Gilberto Bernardes (INESC TEC, Faculty of Engineering, University of Porto)",
        "Susana Ladra (Universidade da Coru\u00f1a)",
        "Jos\u00e9 R. Param\u00e1 (Universidade da Coru\u00f1a)",
        "Fernando Silva (Universidade da Coru\u00f1a)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "",
        "Computational ethnomusicology",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1sG5bfKOGXA9LOk7vZMcQXNE4E1FSdH5i/preview",
      "poster_pdf": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-9-phylo-analysis-of",
      "title": "Phylo-Analysis of Folk Traditions: A Methodology for the Hierarchical Musical Similarity Analysis",
      "video": ""
    },
    "forum": "212",
    "id": "212",
    "pic_id": "",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nDisagree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe core idea of this paper is to investigate music similarity by identifying shared patterns at multiple representations (chromatic interval, diatonic interval, rhythmic ratios) across multiple hierarchies (phrase, form, global).\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThe similarity metrics applied to phrase and macro structures derived from chromatic pitch and duration ratios are more effective in recognizing genres.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper develops a methodology for the cross-cultural ethnomusicological analysis of symbolic music. The core idea is to investigate music similarity by identifying shared patterns at multiple representations (chromatic interval, diatonic interval, rhythmic ratios) across multiple hierarchies (phrase, form, global). The methodology was tested on a new dataset comprising 600 Galician and Irish popular music scores, which features expert annotations for 21 genres (including four shared between the two traditions) and detailed phrase information. The findings indicate that similarity metrics applied to phrase and macro structures derived from chromatic pitch and duration ratios are more effective in recognizing genres. The authors position this methodology as an analytical tool for ethnomusicologists to explore large datasets, facilitating the study of cross-cultural influences and the identification of similar scores across genres and traditions.\n\nThe main contributions of the paper include:\n1. Novel methodology: The paper presents a novel approach by applying phylogenetic analysis to incorporate the hierarchical structure of music, including phrases and musical form. This paper also tests the classification/similarity analysis method on several musical features (diatonic interval, chromatic interval, rhythmic ratio, and their combinations), which provides an overview of how different musical elements contribute to similarity and genre differentiation\n\n2. New open dataset: An open-access of a new dataset of 600 Galician and Irish folk music scores with expert phrase annotations is a valuable contribution to the research community for further research.\n\n3. Systematic tool for quantitative analysis: This paper adapts methods from bioinformatics, particularly phylogenetic techniques and alignment algorithms, to ethnomusicological analysis. The use of the Genre Separation Ratio (GSR) metric provides a quantitative way to evaluate and compare the effectiveness of different features and similarity methods in separating genres.\n\nHowever, there are several aspects of this paper that can be further improved:\n1. It is not entirely clear from the description how the results of the QT Clustering of phrases are converted into the final alignment value used for the shared phrases similarity method. Further detail on the rationale and computation of this \"alignment value\" derived from the clustering results would enhance clarity.\n2. The procedure or standard by which an ethnomusicologist selects target genres for deeper analysis and how the tree of scores is specifically computed for these preferred genres requires more explicit explanation. Detailing the interface between the expert's analytical decisions and the automated tool's functions would be beneficial.\n3. The choice of the threshold for QT Clustering seems empirically derived from analyzing distance distributions. A discussion or justification for this specific threshold, or an exploration of the sensitivity of the results to different thresholds, might strengthen the methodology.\n\nThis paper presents an innovative methodology for applying phylogenetic analysis to folk music incorporating musical structure hierarchy, and also contribute to the creation of a new, open-access dataset for Galician and Irish popular music. The proposed method is evaluated using the GSR metric, particularly for the comparison of different rhythmic and chromatic-rhythmic features. Despite some areas requiring further clarification regarding specific computational steps (phrase clustering to similarity) and the user workflow (genre selection for score trees), the fundamental approach is sound and the results are compelling. The paper has demonstrated that this methodology can serve as a valuable tool for ethnomusicologists studying cross-cultural relationships in folk music.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nPaper 212 has a mixed set of reviews: 1 strong accept, 2 weak accepts, and 1 weak reject. Below is a synthesis of the main points raised by the reviewers:\n\nStrengths:\n\u2022 Novel methodological contribution (all reviewers): This paper presents a novel integration of cultural evolutionary analysis with MIR techniques. The methodological innovation is viewed as a valuable contribution to the field.\n\u2022 New open dataset (all reviewers): The introduction of a new dataset was positively received by all reviewers, who noted its potential utility for future research.\n\u2022 Timely and underexplored topic (Reviewers #2 and #3): This paper addresses an important and relatively underexplored area in computational ethnomusicology through computational methods.\n\nWeaknesses:\n\u2022 Insufficient methodological detail (Meta-review, Reviewer #1): The paper lacks justification for key methodological choices, such as the selection of the corpus and the choice of data representations (e.g., melodic and rhythmic patterns). Clarifying these decisions would strengthen the reproducibility and interpretability of the study.\n\u2022 Limited depth in analysis and interpretation (Reviewers #1 and #3): Reviewers request a more thorough discussion of the analytical results, including:\n- How features from different musical genres manifest in both global and phrase-level similarity.\n- How the outcomes of the phylogenetic analysis relate to known characteristics of musical genres.\n- A clearer explanation of the correlation measures and the genre separation ratio.\n- Additional analyses to support statistical significance of the findings.\n\nOverall Assessment:\nThe reviewers generally view the paper as a valuable contribution, particularly in terms of its dataset and the promise of the proposed analytical framework. While there are areas for improvement, especially in terms of methodological transparency and interpretive depth, the novelty and potential impact of the work support an accept recommendation.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nThe paper presents some interesting methods to analyze relationships between folk songs. The authors plan to provide the corpus, along with the analysis code and a Docker image, with is an excellent initiative towards reproducibility. The paper is overall well-written, but is not always easy to follow.\n\nFirst of all, the argument for the selection of the corpus is somewhat weak. Even if Irish and Galician folk traditions have influenced each other, the proposed dataset does not provide much beyond some shared genres to corroborate claims of interrelated musical traditions. If the dataset was enriched with annotations of melody equivalents, there would be a testable hypothesis. Being able to identify genres based on melodic similarity is perhaps not too surprising, as dance music entails different rhythmic structures.\n\nThe authors compare a number of different music representations - diatonic and chromatic intervals, as well as rhythm intervals, and combinations thereof. While these are some good starting points, why these representations? Why report two different types of pitch intervals, and not explore representations such as pitch classes?\n\nThe authors also compare a number of different similarity metrics - one based on global alignment, one based on phrases shared according to the Quality Threshold clustering algorithm, one based on sequences of such phrases, and weighed combinations of the latter two. Figure 3 presents the genre classification success of the different pitch representations and similarity measures. This is an interesting overview. What it tells me is that in some genres, such as al\u00e1l\u00e1s, genre classification works well based on global alignment of melodies, while in others, such as Waltz, it works poorly. In my opinion, it would be much more interesting to zoom in here and ask why some genres are easier to identify, and why the rhythmic feature is successful. Especially given the success of the rhythmic feature, it seems that the easy to distinguish genres have distinctive rhythmic patterns.\n\nThe authors do not go into these kinds of analyses, however, but seem to take these results as a justification for dropping pitch representations and global alignment from their further analyses. As for the former: it seems strange to expect that pitch interval sequences would be so uniform within a genre that they can be used as an identifying feature. As for the latter: why is global alignment not analyzed further, given that it achieves better classification success than the shared phrases feature?\n\nThe method to derive clustering of phrases - Quality Threshold clustering - is interesting, even though it is doubtful whether the clustering would correspond to human judgements, as the shown phrases in Figure 8 reveal. It also seems that phrase correspondences between songs matter less than the sequences of phrases within a song. This is interesting, but perhaps also not surprising: specific genres may have typical form structures. It would be welcome to see form examples here. While QT clustering is one way of generating labels for repeating phrases within a song, it stands to question whether other metrics, which compare phrases of songs in isolation, might not be just as successful, or even more successful, at generating a form representation.\n\nFurthermore, phylogenetic analysis is performed. Generally, this is an interesting way to study folk songs. Given the corpus, of which -- at least as far as the authors inform us -- there is no information about the time period in which the folk songs were recorded, it is not logical to represent relationships between genres, which may concur in time, in a phylogenetic tree. If one took Figure 4 at face value, this would meen that pasacorredoiras and mazurcas are the \"oldest\" genres, from which others are derived. This seems counter-intuitive, to say the least.\n\nIn summary, the paper introduces many interesting ways of analyzing music, but it offers little in the way of musicological insights. While the methods are reproducible, they also rely on the presence of annotated patterns, which means they cannot be transferred onto an arbitrary dataset of symbolic music. That being said: there are some well-studied folk song datasets which do have such annotations, and in which there are also annotations of finer-grained musical relationships (e.g., tune families in Dutch folk songs). I would recommend that the authors verify their methods on such a dataset before applying them to a dataset of which we know less about melody relationships. Arguably, genre labels are too coarse to study musical relationships.\n\nRemarks\n107 and elsewhere: there are references to \"score\" here, while only the melodies of the folk songs are considered, and not the accompaniment.\nFigure 1: this figure contains many unnecessary or even unclear graphical elements: why are the circles with \"CR\" next to the arrows? Why are some arrows dotted, and others strong? What does the symbol right of the different similarity measures mean? \"D\", \"R\" etc. also are not explained in the caption.\n132 ff.: the representation as pitch intervals and duration intervals is not per se novel. Perhaps references to other studies could also be given here, as music representation for similarity metrics has been widely studied. Representing rests is an interesting choice, as these are often ignored when using inter-onset intervals. Whether they add any information is an open question.\n287 her/him -> them\n252 ff. Combined smilarity - why was only form and shared phrases investigated here, seeing as the global similarity gave better results than the shared phrases?\nFigure 4 and 5 mention \"the rhythmic feature\". This seems to indicate that after the analysis of Figure 3, the rhythmic representation was chosen and the others were discarded. This is not stated clearly in text.\n326 and elsewhere: form information is referred to as \"hierarchical relationship\" of music, while it is about sequences of melodic phrases. I find the term \"hierarchical\" misleading, here and elsewhere.\n340 ff. This section is presented as a case study, while it is actually a qualitative evaluation of the model whose development was described in the previous sections. I would place question marks after any conclusions drawn from the resulting similarity metrics: perhaps they simply fail to capture essential information (meter!) to group genres such as waltz and valses together.\n382 He/She -> They\n399 ff. the text states here valses adopt binary meter, while earlier, valses were related to waltz (triple meter)\nFigure 8 shows a cluster of phrases, but I would doubt whether humans would perceive them as similar.\n\nTypos\n90 case ~~of~~ study\n340 study case -> case study\n\n\nEDIT: most reviewers argue in favour of accepting this paper, and I certainly share their enthusiasm for the methodology and open science. To address some of my criticisms, for the camera-ready version I would like to request that the authors add more details on the corpus, perhaps with exemplary melodies from one or two genres, which might help clarify the reasoning for choosing this task and the given methodology. To make space for this, perhaps Figure 1 could be left out. As mentioned above, I do not think it adds much information.",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe paper studies the relationship between genres of folksong from two traditions, Irish and Galician, based on a combination of rhythmic and melodic patterns using an unrooted phylogenetic tree. The genre tree is built in two stages: first, a tree is built over melodies, without including genre information, and in the second stage the resulting distances between melodies of the same genre are aggregated and used to build the genre tree itself. Both trees are done with neighbor-joining. The result is convincing, generally grouping genres of the individual traditions into separate high-level clades, but with individual genres \u2014 waltzes and alalas \u2014 ending up in the \u201eother\u201c clade. The data is made publicly available, including the phrase annotations, as well as all code.\n\nThe paper is well-crafted, aware of related work, understands \u2014 as far as I can tell, as I am not familiar with the Galician tradition \u2014 the musical materials to the level required to make its point. I specifically appreciate the use of genre as an independent proxy for assessing the quality of different input representations properly, using the genre separation metric. (Though I would rather suggest phylogenetic signal as a possible alternative metric down the road.) It is an interesting, well-balanced mix between (ethno)musicological motivation, material, and appropriate computational methods. It puts a number of well-selected pieces together to work with folk music analysis, which is a field that has not been studied computationally as much \u2014 there is a lot of opportunity with these materials. Good to also see at least some overlap with cultural evolution, which is a field that could provide a principled link between MIR methods and models and conclusions about music and musical life. \n\nAnd, importantly, the paper is 1) not overclaiming, 2) not underestimating the complexity of dealing with ethnomusicological data, 3) took the effort to open its data already in the anonymous regime, and 4) reviews related work well, and justifies its choices on top of that well. It is not really a breakthrough in and of itself, but I think it clearly deserves a place at ISMIR.\n\n\nSome aspects of the method, however, could still be improved.\n\n(Most significant) Fig. 3 \u2014 There is some analysis of significance missing. Bootstrap sampling is generally an accepted method for this in the life sciences. Actually, perhaps better than bootstrapping and confidence intervals would be establishing a random baseline: what are the GSR values for a tree built randomly, rather than by neighbor joining? (Better, what is the distribution of GSR values across 10,000 or so random trees?) What are GSR values for a perfect genre tree (\u201echeat\u201c and use genre directly to compute NJ distance, basically binary), and how much does it change when the \u201ewrong\u201c tune is chosen and the perfect clustering is disturbed with some measure of probability? Then, the significance of having a higher vs. lower GSR value in the relatively small observed ranges could be made clear.\n\n\nL.240: Why Euclidean distance over counts? Wouldn\u2019t a metric based on some distribution over pattern counts be more appropriate? (Binomial? Poisson?)\n\nFigs. 4 and 6 visually imply a root and passage of time, but the tree inference method does not really output any of that. The unrooted should have been presented differently. (See e.g. https://open.lib.umn.edu/humanbiology2e/chapter/1-5-introduction-to-phylogenies/ Fig. 1b for a more typical, better visualisation that I would recommend.)\n\nIt may also have been worth it, perhaps instead of the large Figure 3, to provide a figure for all melodies, or at least a substantial sub-sample across all genres, which was in the supplementary materials repository.\n\nFig. 5 \u2014 It is somewhat suspicious that the greatest distance aggregated over the first-stage melody tree is between waltzes and waltzes. It would be worth a comment, perhaps in the conclusions. Also, color-coding the genre names into Irish vs. Galician might help readability.\n\nIt is not clear what the Pearson correlation between the two genre trees actually measures.\n\nNote also that given the presence of horizontal transmission in the 20th century, I\u2019m not sure this is actually the best dataset to work with. Building a tree and evaluating it is a good first step before getting into network models with horizontal transmission, but if this is a methods development paper it is possibly more suitable to apply it on less complex material.\n\nFinally, I want to point to what I think is an unused opportunity: discussing the kinds of relationships between genres more. One idea is building a genre network instead of a tree (or at least did a NeighborNet visualisation, though these are also problematic because of distances scaling issues). Fig. 3 also could have been discussed more, beyond just the \u201erequires more musicology\u201c statement on L.374. The low numbers in the table are actually just as interesting as the high numbers, because they show which genres are not necessarily distinguished from others via their rhythmic and melodic patterns at all, pointing to extra-musical source of their identity. This goes back to the missed opportunity to use this study for insight into what makes genres coherent categories, and to what extent is it the music. It may turn out from such an analysis that certain genres for instance should *not* be included in analyses based on musical content, for example, because these categories are not salient to it. \n\nAnyway, I will look forward to the presentation of this paper.",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper introduces a methodology for hierarchical musical similarity analysis of folk traditions, specifically applied to a new dataset of 600 Galician and Irish folk music scores. The approach extends standard score alignment techniques to consider structural elements (phrases and form) and multiple representations (chromatic intervals, diatonic intervals, and rhythmic ratios).\n\nStrengths:\n1. The paper addresses an important topic in computational ethnomusicology, offering a systematic approach to comparing folk music traditions at scale.\n\n2. The dataset is substantial (600 scores) and constructed with expert annotations for phrases and genre classification, providing a valuable resource for the MIR community.\n\n3. The hierarchical approach considering multiple levels of musical structure (notes, phrases, form) is innovative and well-motivated by the nature of oral music traditions.\n\n4. The adaptation of bioinformatics methods (sequence alignment, phylogenetic trees) to musical analysis is creative and potentially powerful.\n\n5. The visualizations effectively illustrate relationships between genres within and across traditions.\n\nWeaknesses:\n1. Limited scope of representation: The paper acknowledges but does not address the limitation of working with symbolic scores, which omit crucial aspects of folk music traditions such as timbre, instrumentation, ornamentation and performance practice, elements that often carry significant cultural information.\n\n2. Methodological concerns: The extremely high correlation (0.99) between chromatic-rhythmic and rhythmic trees suggests that the analysis pipeline may be introducing biases that override the actual discriminative power of different feature types.\n\n3. Statistical significance: The paper lacks statistical analysis to determine whether the observed differences between genres (GSR values) are statistically significant or merely artifacts of the processing pipeline.\n\n4. Narrow range of results: The averaged across genres separation ratio values (1.04-1.21; last row in Fig. 3) fall within a tight range (1.04-1.21), raising questions about the discriminative power of the proposed methodology.\n\n5. Unclear details: Several technical aspects are insufficiently explained, such as the exact implementation of the Quality Threshold Clustering algorithm and the specifics of the Neighbor Joining method used for phylogenetic tree construction.\n\n6. Expert dependency: The methodology relies heavily on expert annotations for phrases, which limits its scalability and application to other datasets without significant manual effort.\n\nRecommendations:\n1. Provide a deeper analysis of the correlation between different feature representations (chromatic, diatonic, rhythmic). The near-perfect correlation between some trees suggests potential issues with the methodology.\n\n2. Include statistical significance testing to validate that the observed genre separations are meaningful.\n\n3. For each distance matrix between genres (e.g., Figure 5), compare the diagonal elements (within-genre similarity) to the average of off-diagonal elements (which represent across-genre similarity), to directly test the assumption made in Section 3.2, that \"pieces within each genre tend to share patterns at the pitch and rhythmic levels\". \n\n4. Provide more details on the QT Clustering algorithm parameters and the Neighbor Joining method implementation.\n\n5. Investigate the potential for automating phrase detection to reduce dependency on expert annotations.\n\nMinor suggestions:\n1. Figure 1: A description of 'D', 'R', 'CR' etc. would help the reader.\n2. line 113: \"**kern\": Undefined term.\n3. lines 267-268: \"Neighbour Joining\": missing reference\n4. lines 323,350 etc: ensure the usage of latex `\\ref` command for the figures.\n\nThis paper presents a valuable contribution to computational ethnomusicology with its hierarchical approach to musical similarity analysis. The methodology shows promise for understanding relationships between folk traditions, though there are concerns about the robustness of the results given the high correlations between different feature representations. With refinements to address the methodological issues and more comprehensive statistical analysis, this work could provide significant insights into cross-cultural musical influences and similarities."
    },
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Foundation models have revolutionized music information retrieval, but questions remain about their ability to generalize across diverse musical traditions. This paper presents a comprehensive evaluation of five state-of-the-art audio foundation models across six musical corpora spanning Western popular, Greek, Turkish, and Indian classical traditions. We employ three complementary methodologies to investigate these models' cross-cultural capabilities: probing to assess inherent representations, targeted supervised fine-tuning of 1-2 layers, and multi-label few-shot learning for low-resource scenarios.  Our analysis shows varying cross-cultural generalization, with larger models typically outperforming on non-Western music, though results decline for culturally distant traditions. Notably, our approaches achieve state-of-the-art performance on five out of six evaluated datasets, demonstrating the effectiveness of foundation models for world music understanding. We also find that our targeted fine-tuning approach does not consistently outperform probing across all settings, suggesting foundation models already encode substantial musical knowledge. Our evaluation framework and benchmarking results contribute to understanding how far current models are from achieving universal music representations while establishing metrics for future progress.",
      "abstract": "Foundation models have revolutionized music information retrieval, but questions remain about their ability to generalize across diverse musical traditions. This paper presents a comprehensive evaluation of five state-of-the-art audio foundation models across six musical corpora spanning Western popular, Greek, Turkish, and Indian classical traditions. We employ three complementary methodologies to investigate these models' cross-cultural capabilities: probing to assess inherent representations, targeted supervised fine-tuning of 1-2 layers, and multi-label few-shot learning for low-resource scenarios.  Our analysis shows varying cross-cultural generalization, with larger models typically outperforming on non-Western music, though results decline for culturally distant traditions. Notably, our approaches achieve state-of-the-art performance on five out of six evaluated datasets, demonstrating the effectiveness of foundation models for world music understanding. We also find that our targeted fine-tuning approach does not consistently outperform probing across all settings, suggesting foundation models already encode substantial musical knowledge. Our evaluation framework and benchmarking results contribute to understanding how far current models are from achieving universal music representations while establishing metrics for future progress.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Charilaos Papaioannou",
        "Emmanouil Benetos",
        "Alexandros Potamianos"
      ],
      "authors_and_affil": [
        "Charilaos Papaioannou (School of ECE, National Technical University of Athens)*",
        "Emmanouil Benetos (Queen Mary University of London)",
        "Alexandros Potamianos (National Technical University of Athens)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Computational ethnomusicology",
        "Machine learning/artificial intelligence for music",
        "Musical features and properties",
        "Musical style and genre",
        "Automatic classification",
        "MIR tasks",
        "Metadata, tags, linked data, and semantic web",
        "MIR fundamentals and methodology",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/19JXQwCfcQER_CtStKeSN1XcGg_Ow9Vc4/preview",
      "poster_pdf": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-7-universal-music-representations",
      "title": "Universal Music Representations? Evaluating Foundation Models on World Music Corpora",
      "video": ""
    },
    "forum": "213",
    "id": "213",
    "pic_id": "",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe work's conclusion provides reusable insight: The foundation models trained on Western-centered catalog can be biased towards it and may exhibit relatively inferior effectiveness on set of music from different cultural context (i.e., \"world music\").\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThe foundation models trained on Western-centered catalog can be biased towards it and may exhibit relatively inferior effectiveness on set of music from different cultural context (i.e., \"world music\").\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\n# Summary\n\nThe work evaluates music representations from recent foundation models, especially comparing the performance between Western and non-Western music tagging datasets to investigate a potential cultural bias in those models whose training set typically includes a substantially high proportion of Western music. The experimental design involves three transfer strategies: probing, supervised fine-tuning, and multi-label few-shot learning (ML-FSL). A total of 6 music tagging datasets, including 2 Western (MTAT, FMA-medium), Greek (Lyra), Turkish (Turkish-makam), and two different styles of Indian (Hindustani, Carnatic) music. While achieving state-of-the-art performance in most datasets, the results indicate that the models' absolute performance measure degrades going from Western to non-Western music, implying there might be a bias towards Western music. The work also introduces a performance optimization method for ML-SFL by de-duplicating concept representations, achieving 100 times more efficiency in the best-case scenario.\n\n\n# Major Comments\n\n## Strengths\nThe work presents a cross-cultural evaluation of music audio foundation models, a novel and relevant topic. Through it, we gain information on the strengths and weaknesses of current foundation models in a multi-cultural context, which can be further used to improve models, opening avenues for future research.\n- The conducted experiment has a good coverage on datasets (6 multi-cultural music tagging datasets), transfer strategies (probing, fine-tuning, few-shot learning).\n- The work generally reads well\n\n## Weaknesses\n- There are a few parts where the experimental design could have even better coverage:\n * Other \"world\" music datasets can give an interesting data point to the study, such as ones from Western music tradition. It will help refine the hypothesis and result by checking whether the bias factor is on Western music tradition or other more nuanced factors such as contemporariness, pop-music-ness. One example could be the Slovenian folk music dataset[^1]\n * The model selection could be expanded by employing foundation models that use different architectures (e.g., convolutional architectures such as MULE).\n * There are a few potentially invalid statements, which will be presented in the following section.\n\n\n# Minor Comments\n\n- p2.l87 \"Early efforts ... \": These works are the modern ones that explicitly call them \"Foundation models\", while numerous other even earlier works qualify, enabling them to capture rich musical features applicable across diverse tasks via unsupervised learning. To exemplify a few (Hoffman et al., 2008; Vaizman et al., 2014; Nam et al., 2012; Nam et al., 2015).\n- p2.l140 \"... only an MLP classifier ...\": As linear probing was mentioned several times, I assumed it would have been a linear model on top of the features. Which one is correct?\n- p2.l170 \"... supervised learning on mel-spectrograms to predict tags.\": Is this trained on each dataset, or only on the Western dataset, and transferred?\n- p3.l199 \"Specifically, we ... binary cross-entropy loss.\": MLP is a non-linear model. To the best of my knowledge, linear-probing refers to the case where a linear model is trained taking representation output from foundation model as input. Thus I doubt the use of MLP falls into linear-probing category. Throughout the text, it might have to be rephrased as a probing experiment.\n- p5.l266 \"During inference, ... LCP representations.\"Applying a weighting scheme for each of them with some heuristics would provide additional benefits, which could be an interesting future research topic.\n- p6.l373 \".. we observe a ... foundation models.\"The question that could be posed here is, \"Would a foundation model trained on non-Western music perform better?\"\n\u2014p6. l397 \"However, their consistent ... Western musical traditions.\" Combined with the previous comment, this statement is still not completely proven, as it could just be that the non-Western music datasets are just \"harder.\" To check that, one could train a foundation model only using non-Western models and see if the appropriate result is drawn.\n\n\n\n[^1]: https://gitlab.com/algomus.fr/slovenian-folksongs / Vanessa Nina Borsan, Mathieu Giraud, Richard Groult, Thierry Lecroq: Adding Descriptors to Melodies Improves Pattern Matching: A Study on Slovenian Folk Songs. ISMIR 2023\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\n# Summary of the reviews\n\n## Strengths:\n- The main topic of the work is relevant, novel, and important\n- The work, in general, is structured and written, except for a few parts\n- The work provides code that significantly improves the reproducibility\n- The experimental design is sound, with a good breadth in terms of datasets\n\n## For improvement:\n- Justification for several methodological choices is insufficient\n- The few-shot learning methodology and evaluation could be more clearly communicated\n- The few-shot inference optimization would be less stressed, as it may distract readers from focusing on the main contribution\n\n# Overall comment on the decision\n\nThe reviewers found that the work tackles a novel and relevant topic in the current MIR problem space by evaluating pre-trained foundation models on culturally diverse datasets, revealing the potential Western music bias in these models. The work presented the study well, also providing the source code, which improves the reproducibility. On the other hand, reviewers pointed out that the work could benefit from focusing more on the core topic of evaluation, where the focus was divided into the few-shot optimization, which itself is an interesting contribution while took up a considerable portion of the paper that could be used to shed light on a few other additional evaluation or further elaboration/justification of methodologies. Overall, the scientific quality is sound, and the conclusion is insightful, which the reviewers found to be a sufficient reason to consider the work a valuable contribution to the ISMIR proceedings.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe authors of this paper investigate the ability of various state-of-the-art music representation models to encode non-Western music and evaluate them in different scenarios, namely linear probing, fine-tuning and few-shot learning. They report that the examined models have a tendency to perform better on typical MIR datasets which exhibit Western bias, while datasets comprising non-Western music represent a more challenging basis.\n\nThe paper is slightly challenging to follow at times and could be structured a bit better. Some choices of parameters are not justified and no information is given on the potential impact of dataset sizes on the results. While the \"Multi-label few-shot learning optimization\" may be an insightful improvement, it feels a little misplaced in the scope of this paper while it is given a major amount of attention. This could have been used better for the discussion and evaluation for the vast amount of numbers the authors present.\n\nSome general points:\n\n- The term \"world music\", as it is prominently used in the paper, is very generic. I think it would be a better choice to go with a term like \"non-Western\" music, as the authors also define MagnaTagATune and Fma-medium as \"Western-centric\". \n\n - The last paragraph of \"2. Related Work\" discusses the authors' choice of employing LC-Protonets. However, this is not really a discussion of related work but would rather belong to \"3. Methodological Framework\".\n\n- In 3.1., the authors discuss the examined models mostly in terms of their architectural characteristics. For the scope of this paper, I believe an outline of the used training data would have been a lot more meaningful, as this should be impacting the found results most. Also, in the discussion the authors only address the models' number of parameters and not their network designs.\n\n- In 3.2., the authors only state that they use the datasets \"Following [28]\". It would be nice to get some more detail here than just a reference. While the authors of [28] at least recognize that the size of the respective datasets may impact the results, this fact is not mentioned here at all. In fact I believe that it might be a crucial factor and it is questionable if comparing datasets of such different nature can allow for direct comparison at all. It would at least be necessary to include this into the discussion.\n\n- In section 3.3., the authors describe: \"For MERT-\n 95M, we unfreeze the last two transformer layers, while\nfor MERT-330M only the last layer. For both CLAP models, we unfreeze the last group of swin-transformer blocks\nof the audio encoder along with the normalization and two projection layers. In Qwen2-Audio, we fine-tune the last layer of the audio tower along with the normalization layer before multi-modal projection.\" - how are these choices made? While it is likely that it is by RAM limitations, this would at least have to be stated. Again, it is questionable if such fortuitous choices allow for a fair comparison - simply agreeing on a single layer to be unfrozen would have been the more acceptable approach.\n\n- If I understand correctly, for the ML-FSL approach, the authors first train the models on a specific dataset and then apply Few Shot Classification on the same data they trained the model on. Is this a common practice? I would assume that the representations should then already be overfit to this data. Also, while it is reasonable to apply LC-Protonets, it would have been more intuitive to at least also provide results from a common few shot learning case as to the best of my knowledge, LC-Protonets do not represent the most standard approach for few shot classification.\n\n- Section 3.4. seems a bit off in the context of this paper. While it makes sense to apply a technique as such, I wonder if this is not rather an implementation detail and not really helpful for the actual research topic. \n\n- Section 4: \"We conducted 5 runs with different random seeds for both Probing and ML-FSL tasks, but a single run for SFT due to computational costs\", \"[...] and we used Qwen2-Audio in half-precision (FP16) in all our methodologies to fit in this card.\". These are again examples of a non-standardized approach which weakens the reliability of the results. Please, make sure to align your parameterization wherever possible.\n\"These representation extraction strategies, number of fine\ntuned layers, and other design choices of our method were optimized through preliminary experiments.\": Again, it is not clear what these experiments were and what impacts they may have had on the results. It is preferable to stick with the most basic setup as possible. \n\n- Section 5.1.: Please attempt to align the Figures / Tables with the text as good as possible. Figure 2 is very far from its text reference. Figure 2 shows rather unsurprising results and I think it could have benefited from instead displaying the performance of the models across datasets. \nLine 372, Line 387: Please either leave out the headline (Probing. / Supervised Fine-Tuning.) here or use a sub-sub-section to align with the formatting requirements. \n\nI believe that while the topic of the paper is of high importance, the two main flaws are that while\n \n1) the methodology seems a bit shaky as outlined in my comments above,\n\n2) I struggle to see a major novel insight from this paper. \n\nGiven this, I recommend a weak reject for this paper. I would like to encourage the authors to proceed with their research and believe that with some refactoring, their work can be a valuable contribution to ISMIR.\n\nEdit: After Discussion, I changed my recommendation to weak accept.",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper conducts an extensive evaluation of state-of-the-art music foundation models to assess their generalization across different cultures. The motivation is clearly articulated, relevant prior work is well summarized, and the experimental design is sound. The authors\u2019 main contributions include the large-scale empirical study, revealing the vulnerability of foundation models on non-Western music, and proposing a more efficient assessment method through multi-label few-shot learning optimization. The open-sourced code is another notable strength. However, certain parts of the paper are somewhat distracting, and the analysis and discussion of the experimental results are insufficient to provide clear conclusions. I believe further analysis would yield more reusable insights for the community.\n\nAbstract\nThe abstract is clearly written, effectively outlining the potential concerns regarding foundation models and the proposed evaluation methods. However, the statement in line 16 about achieving state-of-the-art performance is somewhat distracting, as outperforming previous methods on non-Western music is not the central goal or contribution of the paper.\n\nIntroduction\nThe introduction presents a clear motivation for the study. However, again, achieving state-of-the-art performance is not the main contribution of this work, and emphasizing it may detract from the paper\u2019s core message.\n\nRelated work\nThe discussion of related work on foundation models is thorough, and key auto-tagging papers are appropriately cited. Additionally, the rationale for adopting few-shot learning evaluation is clearly explained and well justified.\n\nMethodological framework\n- Line 154: Strictly speaking, the model does not reconstruct Mel-spectrograms. Instead, it reconstructs EnCodec tokens or k-means-based audio features. I think the reference model used EnCodec reconstruction.\n- Line 172: The phrase \u201cOur work\u201d breaks anonymity. The paper should avoid self-referential language. I refrained from checking the reference to preserve the double-blind review process.\n- Line 215: It is unclear whether the authors used the same learning rate for both the foundation model and the MLP blocks. This detail is important, as it can help mitigate catastrophic forgetting.\n- Section 3.4 \u2013 Multi-label Few-Shot Learning Optimization: While this section presents a meaningful contribution, it occupies a large portion of the paper relative to its role. I recommend allocating more space to analyzing and discussing the experimental results, which are currently underdeveloped.\n\nExperimental setup\n- Line 304: Previous works often use intermediate layers for probing. Is there a specific reason or reference supporting the decision to average across layers in this study?\n- Line 304: It is known that different layers in self-supervised models encode different types of semantics. For CLAP-like models, it is reasonable to assume that tag-related information is concentrated in the final layer due to its alignment with text. However, in masked token modeling models such as MERT and MusicFM, intermediate layers often yield better performance than the final layer.\n- Line 310: Could the authors elaborate on the rationale for averaging the representations across layers?\n- Line 320: While not critical to the overall findings, I am curious about the motivation for using different optimizers (Adam vs. AdamW). \n- Overall, the evaluation setup appears sound and appropriate.\n\nResult\n- Line 368: The authors attribute the performance drop largely to differences in training data. However, this point would benefit from a more detailed and concrete discussion. It is also crucial to identify where the performance degrades. A tag-wise analysis could reveal which tags perform particularly well or poorly. This could be followed by an exploration of the reasons behind such performance gaps\u2014through distributional analysis at the signal level, feature level, and deep embedding level. Reducing the space allocated to the few-shot optimization section in favor of this kind of deeper analysis would significantly enhance the paper\u2019s contribution.\n- Line 395: Rather than referring to the result as \u201cstate-of-the-art,\u201d it would be more appropriate to simply state that the method outperforms the supervised baseline. Achieving SoTA is not the main focus of this study.\n- Table 2: The performance of CLAP-M appears very low. It would be helpful if the authors could investigate and discuss the potential reasons behind this result.\n- Line 416: This observation could be sensitive to the choice of aggregation method. Further clarification or ablation would be useful to confirm its robustness.\n\nConclusion\nThe paper presents a valuable and thorough evaluation, but the analysis remains insufficient. As a result, the core message to the reader is somewhat unclear. For instance, while Qwen2-Audio demonstrates strong overall performance, it still underperforms on world music. However, the root cause of this limitation is not well established\u2014whether it stems from training data bias, the choice of optimization strategy (e.g., self-supervised vs. contrastive learning), or model capacity.\n\nTo provide meaningful guidance for future foundation model development, the paper should discuss:\n- What considerations are critical when training the next generation of foundation models?\n- If using existing models on underrepresented data like world music, what adaptation strategies should practitioners consider?\nArriving at such conclusions would require more detailed and systematic analysis. Strengthening this aspect would significantly improve the paper\u2019s overall impact.\n\nReferences\nPlease double-check the reference list. Some conference names are incorrect, several entries are listed as arXiv links without proper citation details, and the formatting is inconsistent throughout.",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper evaluates audio/music foundation models on non-western corpora using linear probing, supervised fine-tuning (SFT), and multi-label few-shot learning. The key findings are:\n1. Models: Qwen2-Audio is the overall best performer across evaluation strategies and datasets (which is not very surprising as it is the largest model evaluated here). Training data bias may be important, as MERT-300M trained on additional Western music data does worse than MERT-95M. Including speech in training data may be useful, since CLAP-M&S > CLAP-M.\n2. Transfer learning: SFT is generally better than fine-tuning, however the degree of improvement is model dependent. CLAP-M may be the most improved in this regard.\n3. Few-shot learning: Foundation models don't significantly outperform VGG-ish in a multi-label few-shot learning setup.\n\nStrengths:\n1. The paper presents novel insights related to the generalizability of foundation models to non-western corpora using experiments on several music tagging datasets.\n2. The paper is well organized and structured.\n3. The experiments are well documented and reproducible.\n\nWeaknesses:\n1. The few-shot learning sections would benefit from better motivation and explanation. Few-shot learning typically involves episodic learning, as seen in prior few-shot audio tagging work [1, 2]. However in this work, the model is only evaluated on the predicted labels based on proximity in the embedding space. Maybe this section should be called few-shot evaluation, since there is no \"learning\" taking place. It may also be useful to additionally evaluate few-shot learning by fine-tuning with episodic training. \n2. The evaluation setup in the few-shot experiment is unclear---what exactly are the unseen classes? Are they unseen only in fine-tuning, or in pre-training as well? The paper would also benefit from a more detailed analysis of these results, e.g. seen vs. unseen classes.\n3. While useful contributions, the few-shot inference optimizations in Section 5.2 don't fit well into the main story of the paper, which is more about model performance on non-western music corpora.\n4. Some methodological choices are not well justified and/or discussed. For instance, the fact that only last layer is updated in MERT-330M while the last two layers are updated in MERT-95M (lines 205-207) could explain some of the performance differences in Table 2. This should be discussed in Section 5.1 to contextualize the findings.\n\nReferences\n[1] Papaioannou, C., Benetos, E., & Potamianos, A. (2025). LC-Protonets: Multi-label Few-shot learning for world music audio tagging. IEEE Open Journal of Signal Processing.\n[2] Wang, Y., Bryan, N. J., Salamon, J., Cartwright, M., & Bello, J. P. (2021, October). Who calls the shots? Rethinking few-shot learning for audio. In 2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA) (pp. 36-40). IEEE."
    },
    "session": "3"
  },
  {
    "content": {
      "TLDR": "In music information retrieval (MIR), contrastive self-supervised learning for general-purpose representation models is effective for global tasks such as automatic tagging.\nHowever, for local tasks such as chord estimation, it is widely assumed that contrastively trained general-purpose self-supervised models are inadequate and that more sophisticated SSL is necessary; e.g., masked modeling.\nOur paper challenges this assumption by revealing the potential of contrastive SSL paired with a transformer in local MIR tasks.\nWe consider a lightweight vision transformer with one-dimensional patches in the time--frequency domain (\\melt) and train it with simple contrastive SSL through normalized temperature-scaled cross-entropy loss (NT-Xent).\nAlthough NT-Xent operates only over the class token, we observe that, potentially thanks to weight sharing, informative musical properties emerge in ViT-1D's sequence tokens.\nOn global tasks, the temporal average of class and sequence tokens offers a performance increase compared to the class token alone, showing useful properties in the sequence tokens.\nOn local tasks, sequence tokens perform unexpectedly well, despite not being specifically trained for.\nFurthermore, high-level musical features such as onsets emerge from layer-wise attention maps and self-similarity matrices show different layers capture different musical dimensions.\nOur paper does not focus on improving performance but advances the musical interpretation of transformers and sheds light on some overlooked abilities of contrastive SSL paired with transformers for sequence modeling in MIR.",
      "abstract": "In music information retrieval (MIR), contrastive self-supervised learning for general-purpose representation models is effective for global tasks such as automatic tagging.\nHowever, for local tasks such as chord estimation, it is widely assumed that contrastively trained general-purpose self-supervised models are inadequate and that more sophisticated SSL is necessary; e.g., masked modeling.\nOur paper challenges this assumption by revealing the potential of contrastive SSL paired with a transformer in local MIR tasks.\nWe consider a lightweight vision transformer with one-dimensional patches in the time--frequency domain (\\melt) and train it with simple contrastive SSL through normalized temperature-scaled cross-entropy loss (NT-Xent).\nAlthough NT-Xent operates only over the class token, we observe that, potentially thanks to weight sharing, informative musical properties emerge in ViT-1D's sequence tokens.\nOn global tasks, the temporal average of class and sequence tokens offers a performance increase compared to the class token alone, showing useful properties in the sequence tokens.\nOn local tasks, sequence tokens perform unexpectedly well, despite not being specifically trained for.\nFurthermore, high-level musical features such as onsets emerge from layer-wise attention maps and self-similarity matrices show different layers capture different musical dimensions.\nOur paper does not focus on improving performance but advances the musical interpretation of transformers and sheds light on some overlooked abilities of contrastive SSL paired with transformers for sequence modeling in MIR.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Yuexuan KONG",
        "Gabriel Mesegues-Brocal",
        "Vincent Lostanlen",
        "Mathieu Lagrange",
        "Romain Hennequin"
      ],
      "authors_and_affil": [
        "Yuexuan KONG (Deezer)*",
        "Gabriel Mesegues-Brocal (Deezer)",
        "Vincent Lostanlen (LS2N)",
        "Mathieu Lagrange (LS2N)",
        "Romain Hennequin (Deezer)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Music signal processing",
        "MIR fundamentals and methodology",
        "Musical features and properties"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1eTe5c5v1pzMLUWxlkxWiy0jCuGGgc5HB/preview",
      "poster_pdf": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-14-emergent-musical-properties",
      "title": "Emergent musical properties of a transformer under contrastive self-supervised learning",
      "video": ""
    },
    "forum": "216",
    "id": "216",
    "pic_id": "",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nDisagree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nStrongly disagree\n\n**Q10 (Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chose, otherwise write \"n/a\"))**\n\nSee below\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nDisagree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nStrongly disagree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nDisagree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nSee below\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThis submission poses that transformers models trained contrastively on clip-level class tokens, can learn frame-level temporal structures that can be used to solve sequential, MIR tasks.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong reject\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis submission explores an interesting premise, that transformers models trained contrastively on clip-level class tokens, can learn frame-level temporal structures that can be used to solve sequential, MIR tasks. While this is intuitive given how transformers work, this form of \u201cweak\u201d self-supervised learning is not common in MIR. At first glance, the results in section 4 are promising. However, upon closer inspection it is unclear that they support the claims made by the authors. I will elaborate in the following.\n\nFor context: The key claim is that transformer models trained using clip-level contrastive self-supervision, are able to learn frame-level structures that non-transformer models cannot learn. To test this claim the authors train their own contrastive transformer model on 4 second clip, where positive pairs are sampled from the same 30s recording with no augmentation. The frame/token rate is ~30Hz. The model is evaluated on two sequential tasks, beat tracking and chord estimation, and two global tasks, music tagging and genre classification. It is compared against a contrastive model (CLMR), on global tasks, and a predictive model (M2D), on both global and sequential tasks. \n\nThe global tasks\u2019 comparison: for music tagging, the proposed approach performs the worst in MAP of all compared approaches, but competitively with CLMR when using token averages instead of the class token. This is an arbitrary choice that is not clearly justified and that, as is the case in sections 5 and 6, just so happens to maximize the results on the test set. Admittedly the approach performs well with nearly 20 times less parameters than M2D. By the same token, it performs worst than CLMR with double the number of parameters. In any case, it is unclear that the transformer-based approach explains these performance differences. For key estimation, the proposed approach, again in its average version, performs better than M2D and CLMR. However, it is worth noting that at least CLMR is trained using an augmentation-based framework that includes pitch shifting, therefore explicitly making the learned representation invariant to pitch variations. This invariance would explain the poor performance in key estimation for CLMR. At the same time, choosing positive pairs from segments of the same 30s music clip, where key changes are unlikely, and without pitch-based augmentation, is likely to result in a representation space that favors key similarity. I would argue that these differences in sampling and augmentation are more plausible explanations for the key estimation results than the emerging sequential properties of the transformer.\n\nFor the sequential (local) tasks: the proposed approach performs worst than M2D, albeit it remains competitive with a lot less parameters. It is worth noting that the approach is designed to operate at the frame resolution needed for these tasks, while M2D needs an additional upsampling layer that the authors learn for the comparison. It is unclear what the effect of this upsampling is on the results, nor why a similar upsampling is not used for CLMR (which is trained on 2.6s-long segments) to achieve the desired resolution. This is the most important test of the paper, since it underpins the claim of emerging temporal structures. Yet, the authors only compare with one approach, thus limiting any potential insight that we might gain on this. Perhaps the authors could have trained a vanilla, non-transformer contrastive model at frame level resolution, just to disambiguate the contribution of the transformer. Also, to simply use multiple approaches with a longer temporal scope, like CLMR, as a moving filter to produce outputs at the right temporal resolution. Absent this, it is hard to argue that these results sustain the main claims in the paper. As before, results on chord estimation are at least partly explained by the pitch-preserving sampling and augmentation strategies used. Also it is worth noting that the chord ID datasets used are not the most common, and that the major/minor vocabulary is the simplest possible version of the task. \n\nFor sections 5 and 6.1, the authors evaluate on a new set of tasks: onset detection and structural analysis. There are several issues with these sections. First, the representations used in the evaluations are chosen arbitrarily. For example, for onset detection the authors use the output of the second attention head from the 9th layer of the transformer with no clear justification. For structural analysis we look at the outputs of the 3rd and 12th layer. As with the token average before, the impression is that these outputs are chosen post-hoc to report the best possible results. Same with the baselines: spectral flux for onset detection (instead of another DNN approach) or a random projection of the input sequence for structural analysis. Further, the dataset chosen for onset detection is not in common use and simpler (single instrument, homogeneous in timbre) that others in the literature. There is no formal evaluation of the structural analysis, just a visual comparison for only one example, with many subjective and unsubstantiated claims in the relevant discussion. In summary, sections 5 and 6 are not sufficiently rigorous and therefore difficult to consider as providing valid evidence of the overall claims in the paper. \n\nThus, while based on an interesting premise, I believe the experimental design needs to be developed further to explicitly and rigorously test the claims in this paper. IMO the experimental design needs to (a) adopt common data, metrics and task definitions for each of the tasks; (b) test the specific claim of emerging frame-level structure thanks to the combination of a transformer with clip-level contrastive learning; (c) use baselines that provide a fair comparison and that control for factors that might influence the results, such as learned invariances, temporal resolution, and other differences in learning objective and scope; and (d) avoid arbitrary choices of internal representation, and subjective comparisons.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak reject\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nAll reviewers agree that the central idea of this paper, that \"a contrastive loss applied to a transformer\u2019s class token is enough to induce emergent local musical representations in sequence tokens\" (R1) is interesting and valuable. On the one hand, this idea has been extensively explored in other domains such as NLP and vision; on the other hand, the authors make it possible in MIR by using 1D patching in time-frequency instead of the common 2D patches used with ViTs. This is a valuable insight for the MIR community. Thus novelty is the strongest argument for acceptance.\n\nThe main weaknesses of the paper are connected to the experimental validation of this idea:\n\n(a) The experiments in sections 3 and 4 fail to provide a fair comparison with alternative approaches including non-contrastive, frame-level approaches like MERT, and contrastive, frame-level, approaches trained with the same sampling and augmentation pipelines -- see (b) below.\n(b) When comparing contrastive approaches, the authors choose baselines that operate at different frame rates and use different sampling and augmentation baselines. For example, unlike the proposed approach, the baselines are trained to be pitch-invariant via augmentation. Thus, the \"improvement\" in chord and key estimation can be better explained by the choice of augmentation than by the emergent properties of the proposed approach. The failure to control for implementation differences that are tangential to the properties being testes render the comparisons in sections 3 and 4 invalid. \n(c) The experiments in sections 5 and 6, while interesting, are arbitrary and lack rigor. In both sections the authors use manually selected internal representations to demonstrate their point. Section 5 only compares with a classical, not data-driven solution. Section 6 does not compare with other approaches, and makes informal visual assessments of performance that appear biased and unsupported by evidence. To be fair, the results show that some of these internal representations encode local, temporal structure. But it remains unclear whether these representations are as or more informative than local representations obtained using alternative methods.\n(d) There are many questionable methodological choices, e.g. chord and onset detection datasets, chord vocabularies and metrics, choice of baselines and hand-picked internal representations, etc that are not properly justified. \n\nThese are important concerns that would require major changes to the paper. Thus the recommendation to reject.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\n- Paper is clear and well structured overall. \n\n- The general approach and analytical tools are borrowed directly from prior work in the image domain, so the conceptual and architectural novelty is limited. The work here is essentially applying what DINO and Caron et al. did for images, to the audio/music domain.\n\n- The use of attention maps and self-similarity matrices to study emergent properties is also not new; again, this is common in CV (e.g. self supervised based image segmentation) and NLP.\n\n- With that said, the application to music/audio is somewhat novel, and the findings challenge prevailing assumptions specific to MIR about the limitations of contrastive SSL for local tasks.\n\n- The authors make a valuable enough empirical contribution by showing that emergent properties seen in vision also manifest in music in a semantically meaningful way, and can be practically exploited. \n\n- I'm scoring this as a weak accept given a) the limited novelty in terms of the overall approach, with the novelty being primarily in the application to the audio/music domain and b) I would have liked to see a few real examples where we can visually correlate a given input with the generated internal representations in the context of different applications, like onset detection.",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nStrengths:\n1. See #Q16 and #Q18\n2. This paper pushes forward self-supervised contrastive learning in MIR by demonstrating a transformer's ability on local tasks such as beat tracking and chord recognition, which differs from existing approaches that only consider global tasks like music tagging and key estimation.\n3. This paper proposes a novel 1D patching method of spectrogram, which enables the model to learn frame-level representations.\n\nWeaknesses:\nThe paper has no significant weaknesses.\n\nMinor comments:\n1. In the fourth paragraph of the Introduction section, one sentence is \"... ranges from 89M (M2D) to 5B (Jukebox).\" Note that Music2latent is 58M, smaller than M2D.\n2. The paper does not describe what kind of positional encoding is used. I recommend to state it explicitly in the paper.\n3. Regarding Figure 4, the paper suggests Layer 3 has clearer block pattern than Layer 12 in the first row. However, I cannot make this conclusion merely based on the figure - I think they are pretty similar.",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper introduces a new contrastive target for foundation models. The main idea is to introduce both global and local tokens but only perform contrastive loss on global tokens. While similar methods is often seen in other modalities in image, I think it is quite novel in the music domain.\n\nThe paper also provides an interesting inspection of the attention and SSM of features, which could be useful for future works toward the explainability of music foundation models.\n\nThough the performance of certain tasks (e.g., music tagging) is not as good compared to CLMR, it is reasonable since the hyperparameter of the model is smaller than a normal 12-layer transformer (with only 3 attention heads and embedding dim of 192), and a shorter window length.\n\nHowever, I do find that the chord estimation results are too low, even if it is fine-tuned on a relatively small dataset. It is even lower than rule-based methods like Chordino. A deeper look into the issue would be appreciated. (i.e., shortcut learning that ignored chords?)\n\nOther weakness:\n\n1. No comparison to other foundation models with frame-level representation (i.e., MERT). I assume that the model is not strong enough to compare against other baselines, since CLMR is already a weak baseline.\n2. Lack of other downstream tasks. See also [1].\n3. Section 6.2: Since manually selected layers yield better results, it would be meaningful to use a (1) hyperparameter search, or a (2) learned weighted sum module that automatically detects useful layers and aggregates them to acquire a better result.\n\nQuestions:\n1. Line 180: what is the pretraining dataset?\n2. Line 212: \"Probe on the average of the entire token sequence\" - have you tried to probe on local tokens only? I.e., is the tagging information stored more on global tokens, or the average of local tokens?\n\nI still think this paper should be accepted by ISMIR even if it is not an SOTA model. That being said, I still recommend the authors to do more comparitive experiments.\n\n[1] Yuan, R., Ma, Y., Li, Y., Zhang, G., Chen, X., Yin, H., ... & Fu, J. (2023). Marble: Music audio representation benchmark for universal evaluation. Advances in Neural Information Processing Systems, 36, 39626-39647."
    },
    "session": "2"
  },
  {
    "content": {
      "TLDR": "The recent rise in capabilities of AI-based music generation tools has created an upheaval in the music industry, necessitating the creation of accurate methods to detect such AI-generated content. This can be done using audio-based detectors; however, it has been shown that they struggle to generalize to unseen generators or when the audio is perturbed. Furthermore, recent work used accurate and cleanly formatted lyrics sourced from a lyrics provider database to detect AI-generated music. However, in practice, such perfect lyrics are not available (only the audio is); this leaves a substantial gap in applicability in real-life use cases. In this work, we instead propose solving this gap by transcribing songs using general automatic speech recognition (ASR) models. We do this using several detectors. The results on diverse, multi-genre, and multi-lingual lyrics show generally strong detection performance across languages and genres, particularly for our best-performing model using Whisper large-v2 and LLM2Vec embeddings. In addition, we show that our method is more robust than state-of-the-art audio-based ones when the audio is perturbed in different ways and when evaluated on different music generators. Our code is available at https://github.com/deezer/robust-AI-lyrics-detection.",
      "abstract": "The recent rise in capabilities of AI-based music generation tools has created an upheaval in the music industry, necessitating the creation of accurate methods to detect such AI-generated content. This can be done using audio-based detectors; however, it has been shown that they struggle to generalize to unseen generators or when the audio is perturbed. Furthermore, recent work used accurate and cleanly formatted lyrics sourced from a lyrics provider database to detect AI-generated music. However, in practice, such perfect lyrics are not available (only the audio is); this leaves a substantial gap in applicability in real-life use cases. In this work, we instead propose solving this gap by transcribing songs using general automatic speech recognition (ASR) models. We do this using several detectors. The results on diverse, multi-genre, and multi-lingual lyrics show generally strong detection performance across languages and genres, particularly for our best-performing model using Whisper large-v2 and LLM2Vec embeddings. In addition, we show that our method is more robust than state-of-the-art audio-based ones when the audio is perturbed in different ways and when evaluated on different music generators. Our code is available at https://github.com/deezer/robust-AI-lyrics-detection.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Markus Frohmann",
        "Elena Epure",
        "Gabriel Meseguer Brocal",
        "Markus Schedl",
        "Romain Hennequin"
      ],
      "authors_and_affil": [
        "Markus Frohmann (JKU)*",
        "Elena Epure (Deezer)",
        "Gabriel Meseguer Brocal (Deezer)",
        "Markus Schedl (JKU)",
        "Romain Hennequin (Deezer)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Applications",
        "Automatic classification",
        "MIR tasks",
        "Lyrics and other textual data",
        "MIR fundamentals and methodology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1qF4Yq4mVYSPTiaxBQMwGwPW0TAljeT4D/preview",
      "poster_pdf": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-13-ai-generated-song",
      "title": "AI-Generated Song Detection via Lyrics Transcripts",
      "video": ""
    },
    "forum": "219",
    "id": "219",
    "pic_id": "",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nDisagree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nDisagree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe core idea of the paper is that AI-generated songs can be distinguished from human-generated songs, and it justifies the use of lyrics transcription as a more robust way to achieve that compared to audio-based approaches. While I can agree on the latter, I personally think the former is misleading and promotes research in the wrong direction, so it goes in the opposite direction of \"accurate and deep understanding\" or \"reusable insights\". Let me explain here.\n\nThe goal of any generative model (including LLMs) is to mimic the real-world training distribution. In the limit, when a generative model is good \"enough\", there should be almost no distinction between real and generated text. Thus, I'd argue that current models (and specially LLMs), given enough data, already have reached this point (one only needs to look at common LLM benchmarks out there, or the trend in FID for image generation). The fact that they maybe have not reached this point for music lyrics is, I believe, just a matter of time, not enough or inadequate data, or non-careful implementation or lack of attention of LLM providers to the particular sub-task of lyrics generation (or the blending of lyrics and accompaniment).\n\nGiven this premise, I think the classifiers employed in the paper are just overfitting to LLM problems in generating lyrics or the lack of proper alignment between lyrics and music (which is generated by another generative model conditioned on the lyrics). In the near future, generative models will continue to become better, cut these \"lyric generation problems\" to almost zero, and ultimately render AI-generated music detectors as the one proposed in the paper quite useless. It is true that maybe lyrics-based detectors are better and more robust than audio-based ones, and I agree with the authors that they may work better in out-of-domain cases (but I would not dare to say Udio and Suno are different systems; on the contrary, I think they might be largely the same, see below). Overall, I think the assumption that generative modeling will not do a decent job in modeling the real-world distribution is not pointing in the right direction and promotes further research on such \"wrong\" direction...\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThe paper proposes to detect AI-generated songs by partially transcribing lyrics to text embeddings and training a classifier, which should be more robust than audio-based classifiers.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThe research seems properly conducted and the methodology and results, although basic, are probably better and more comprehensive than previous research works on the topic. As meta-reviewer, I temporarily set a weak accept even though I'm personally inclined towards a weak reject for the reasons I exposed above (Reusable Insights question). I'll wait and carefully read the other reviews before agreeing on a decision. Below I put both major and minor comments to further substantiate the discussion.\n\nMajor comments:\n- The research does only address the case where everything (music and lyrics) is AI-generated. However, I think it is precisely the opposite case we will mostly see in the future (and the one that is more interesting to tackle): The case where only some (potentially small) parts of the composition are made with the help of some AI assistant. Thus, AI generation tools will become more of a companion and hence one will have to accept them in the composition process as we accepted synthesizers and loop machines.\n- Introduction, \"Crucially, it does so besides the extra difficulty of transcription\" -> I think this is the only point that the paper is (indirectly) proving: That current transcriptors are quite good, and that tasks requiring transcription can be approached by using such tools. Notice that this is a different point than the one in the title and abstract of the paper.\n- As mentioned, I think the paper only leverages the still poor performance of LLMs in generating lyrics and/or the still poor performance of music generation models in combining lyrics and music. Generative models will become better and outdate the paper's research. Notice also that the fact that the embeddings with more dimensions (LLM2Vec-LLAMA and BGE-ML-Gemma) are the ones with better scores already points towards some form of overfitting (that is, capacity to focus on a random error from the generative model).\n- Details about how the (pseudo-) transcription process is done are missing.\n- I think it is wrong to consider Suno and Udio as two different models, and hence to consider the evaluation with the latter as \"out-of-distribution\". We do not know the details of the two models, but I personally think it is highly probable that both train on similar data, that both are latent diffusion models, that both have a similar architecture, that both have used a similar set of lyrics, etc.\n- Therefore, the study of generalization to unseen generative models, which is the crucial (and only?) aspect that should be properly assessed in these deepfake detection setups, is largely missing.\n\nMinor comments:\n- Introduction, first paragraph -> A paragraph should be three sentences or more, not just one.\n- Introduction -> Maybe it could be a good idea to discuss other possible options like a \"certification of human artist\", which is much easier to do (and for which processes exist), rather than attempting to perform automatic detection.\n- Introduction, \"audio-generated lyrics\" -> I think this is troubling in the grammatical sense?\n- Introduction, \"leveraging lyrics should lead to...\" -> I think the authors should better develop pros and cons here; develop the topic further.\n- Introduction, \"real music with AI-generated lyrics...\" -> I think this is not true. Many artists may be already getting inspiration from ChatGPT to do lyrics. Do the authors have any numbers or research to go beyond opinions or perceptions?\n- Related work -> Perhaps some mentions about what is done in the image domain could have added value to the section.\n- Related work \"necessity of accessing model logits\" -> I think this is not true. There are plenty of black-box or gray-box approaches (and white-box approaches not restricted to the logits).\n- Method, \"we also experiment with various types...\" -> Maybe explain more those ones and why they did not improve performance?\n- Method, \"learning rate to 1e-3\" -> Watch out the line break.\n- Experimental setup, \"lyrics dataset Here,...\" -> Dot missing.\n- Table 5 -> Should be \"UAR-MUD\" instead of just \"MUD\"?\n- Organization -> Maybe switch the order of sections 5.1 and 5.2?\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nAfter discussing with the reviewers we decided to set a \"weak accept\" score for the paper. Although the lack of generalization and future utility of these type of works was stressed during the discussion, it was found the quality and overall topic were interesting for triggering future discussion at ISMIR. Also, the fact that the paper tackles a real and current problem was positively valued.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe first phrase in the abstract is greatly exaggerated. Not *all* industry has been disrupted, and not *all* industry *necessitates* detection of AI-generated content.\n\nI find this paper very interesting. It has a straightforward methodology. Section 5.3 is especially interesting. \n\nThe main weakness of this paper is that it only used one generator for testing, which may limit the generalization of the results.\n\nAbout the paper structure, I miss a figure indicating how each dataset was used (this information is scattered all over the paper).",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis is a deepfake detection task for lyrics-conditioned music generation. The method for topic modeling and labeling is appropriate for this purpose. The experiments are solid and reveal findings from multiple perspectives. The structure is easy to follow. \n\nHere are some comments that could potentially improve the paper:\n- Regarding the problem formulation (L324), this is a classification task distinguishing between fake lyrics/fake audio and real lyrics/real audio. Here, the fake lyrics/fake audio class represents AI-generated music. However, AI-generated music with human-written lyrics (L80 mentioned this), instrumental tracks, or singing voice without linguistic meaning all fall outside the scope. Careful wording is needed when using terms such as AI-generated music, AI-generated songs, and AI-generated audio, and the scope/limitation should be clearly stated. \n- L29: It would be helpful to include links to the commercial products mentioned.\n- L52: This statement needs a citation, or the authors could note that it will be demonstrated later in the paper. \n- L125: It would be helpful to explain how the limited architectural details relate to the challenges of the deepfake detection task.\n- L129: There is a longer history of deepfake detection in CV, NLP, and speech. The cited works represent the first attempts in singing voice. \n- Text encoders are discussed extensively in the method section, but the results provide little insight into how they compare.\n- It would be helpful to dedicate a subsection in the experimental setup to describe the three sets of experiments (Tables 4, 5, and 6). Currently, the unseen data experiment is only referenced in the dataset discussion (L327\u2013341), and the out-of-distribution setup is not mentioned beforehand.\n- Whisper performs differently across languages, which might partially explain some observed trends. Would it be possible to compute WERs between Whisper\u2019s predictions and the ground truth lyrics?\n- The authors of [10] are missing",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper presents a timely and practical method to detect AI-generated music by transcribing audio into lyrics and applying text-based detection approaches. The writing is clear and the experiments, particularly the ablation studies, are thorough and informative, which strengthens the overall scientific quality. Furthermore, the analysis of cross-lingual performance is appreciated, as it shows awareness of the real-world multilingual landscape of music. However, several areas require improvement. First, while the authors provide some qualitative observations on language differences (e.g., Arabic vs. English), no quantitative evidence or detailed analysis (such as statistics or correlation studies) are offered to support these claims, limiting the strength of these insights. Second, regarding input modalities, the work only considers audio transcriptions and overlooks other important generative models like YUE or Jukebox, and does not investigate music reconstructed from latent codes (e.g., Encodec), which may behave differently. Third, although the work claims robustness, the evaluation does not explore newer LLMs (e.g., GPT-4o, Gemini) for lyric generation, which may present more challenging detection scenarios. Finally, the presentation can be further polished \u2014 for example, Figure 1 uses color coding that does not reflect any meaningful distinction, which affects visual clarity. Overall, while this paper proposes a valuable direction with clear strengths in experimental design and writing quality, further exploration and evaluation are suggested for the claims."
    },
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Music representation models are widely used for tasks such as tagging, retrieval, and music understanding. Yet, their potential to encode cultural bias remains underexplored. In this paper, we apply Concept Activation Vectors (CAVs) to investigate whether non-musical singer attributes - such as gender and language - influence genre representations in unintended ways. We analyze four state-of-the-art models (MERT, Whisper, MuQ, MuQ-MuLan) using the STraDa dataset, carefully balancing training sets to control for genre confounds. Our results reveal significant model-specific biases, aligning with disparities reported in MIR and music sociology. Furthermore, we propose a post-hoc debiasing strategy using concept vector manipulation, demonstrating its effectiveness in mitigating these biases. These findings highlight the need for bias-aware model design and show that conceptualized interpretability methods offer practical tools for diagnosing and mitigating representational bias in MIR.",
      "abstract": "Music representation models are widely used for tasks such as tagging, retrieval, and music understanding. Yet, their potential to encode cultural bias remains underexplored. In this paper, we apply Concept Activation Vectors (CAVs) to investigate whether non-musical singer attributes - such as gender and language - influence genre representations in unintended ways. We analyze four state-of-the-art models (MERT, Whisper, MuQ, MuQ-MuLan) using the STraDa dataset, carefully balancing training sets to control for genre confounds. Our results reveal significant model-specific biases, aligning with disparities reported in MIR and music sociology. Furthermore, we propose a post-hoc debiasing strategy using concept vector manipulation, demonstrating its effectiveness in mitigating these biases. These findings highlight the need for bias-aware model design and show that conceptualized interpretability methods offer practical tools for diagnosing and mitigating representational bias in MIR.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Roman Gebhardt",
        "Arne Kuhle",
        "Eyl\u00fcl Bektur"
      ],
      "authors_and_affil": [
        "Roman Gebhardt (Cyanite / Audio Communication Group, TU Berlin)*",
        "Arne Kuhle (Cyanite)",
        "Eyl\u00fcl Bektur (TU Berlin / Cyanite)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Evaluation metrics",
        "Representations of music",
        "Philosophical and ethical discussions",
        "Ethical issues related to designing and implementing MIR tools and technologies",
        "Evaluation, datasets, and reproducibility",
        "Evaluation methodology",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1j66ElKlmTmNVXIQTpQcTEFaIBh47qfQY/preview",
      "poster_pdf": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-4-beyond-genre-diagnosing",
      "title": "Beyond Genre: Diagnosing Bias in Music Embeddings Using Concept Activation Vectors",
      "video": ""
    },
    "forum": "220",
    "id": "220",
    "pic_id": "",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe manuscript does a good job outlining the linearity of representation among some widely-used music embeddings with respect to several basic musical concepts. The general approach would also be applicable to other models and concept ontologies.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nConcept activation vectors (CAVs) can be used to find linear hyperplanes separating some high-level musical concepts (gender, language, and genre) in modern neural embeddings.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis manuscript explore the effectiveness of concept activation vectors (CAVs) in a selection of widely-used neural music embeddings to show whether certain high-level musical concepts are represented linearly. Overall, it is an interesting glimpse into the state of music embeddings today, and it would surely generate some discussion and interest at ISMIR. The manuscript does suffer, however, from several methodological limitations that reduce its impact. While I personally lean toward accepting the work, there would be stronger motivations possible.\n\nThe key limitation is inherent in the technique: concept activation vectors can only identify linear relationships. In a somewhat confusing footnote (3), the authors mention that any linear classifier would be suitable while also mentioning the possibility of adding hidden layers. This should be fixed for the camera-ready: adding any hidden layers would almost surely generate a non-linear classifier...and the author's model in Equation 1 already encompasses all possible linear ones. I *think* that the authors mean that any differentiable classifier would be applicable (and indeed, given that \u00a75 goes on to avoid using derivatives, perhaps any classifier would in fact be applicable).\n\nBut enforcing linearity rather sharply limits the usefulness of the technique for the authors' purposes: there is no particular reason to believe that the concepts considered would be linearly separable. Where the authors find positive results, these can certainly be seen as evidence for the presence of a particular concept in a neural model. Where the authors do not, it may be that the concept is even strongly present, just non-linearly. The authors should clarify this limitation in the camera-ready version.\n\nI am also not fully convinced by the bias-reducing approaches, neither for generating the dataset nor for vector manipulation. In both cases, the approaches seem too ad-hoc to be fully sound or widely scalable.\n\nFundamentally, I see this manuscript as a proof of a methodological concept, and as such, the particular choice of concepts considered is reasonable enough. That said, I still would have found the manuscript more interesting with a richer set of concepts. We don't really need AI to make a gender assessment of a vocalist, and genre is notoriously difficult to formulate as a well-posed scientific classification. If the authors tested any other concepts, it would be wonderful to add some information about them to the camera-ready. \n\nFinally, as a small point, it is not clear from the manuscript whether the authors used *all* embedding layers for the CAV training or chose specific layers.\n\nIn the spirit of moving science forward, however, I still think there is benefit to sharing this work as is with ISMIR. The work is well written, and extending the authors' approach to incorporate non-linearity or other musical concepts would be straightforward. Some of the conclusions do rely on potentially biased concept interactions, but the authors have made a good-faith, if ad-hoc, effort to reduce this bias. The results as presented do show that some current models can already convincingly incorporate certain musical concepts.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThe reviewers agreed that the approach in this manuscript is interesting, although collectively, the group struggled to understand all of the methodological detail. The most important issue in the discussion was the issue of balance (or lack thereof) in the dataset. In the case of imbalance, there seems to be a substantial risk that the results as presented are primarily a reflection of the imbalances rather than the desired message. In addition to the other comments from reviewers, the authors should spend particular attention clarifying balance or imbalance in the dataset and how it affects interpretation of the results. \n\nIf, after closer inspection, the authors find that class imbalance is a serious enough problem to have thrown off the entire analysis, it would be most appropriate to withdraw the paper. For the purposes of the review, however, the group decided to give the authors the benefit of the doubt.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nSummary: This is a well-written paper looking into an interesting application of the TCAV method. The topic is relevant (and would benefit) the ISMIR community, and has the potential for reusable insights or generating discourse, as this could be used in the future to investigate biases of music representation models (which gain popularity). Unfortunately, the description of how CAVs are computed exactly (i.e., their data setup) should be clarified, as the current state makes the presented results rather hard to interpret, and I am unsure whether the proposed adaptation of the TCAV score actually reflects what is desired. Also the debiasing method would need further elaborations as to how this could actually help in debiasing a system (post-hoc). \n\nNovelty: While TCAV has been previously used within MIR, and beyond this field as a way of detecting biases in systems, to the best of my knowledge applying TCAV to detect biases in MIR systems is novel. \n\nReproducibility: The dataset modifications to account for underrepresented genre-gender combinations are provided via additional material, and the code is also said to be released upon acceptance, aiding the reproducibility of this work. The only difficulty for reproduction purposes might be the difficult-to-understand setup of training/test sets for deriving CAVs . \n\nPioneering proposals: While the proposed work exhibits a level of novelty, this application to a new field does not really provide any pioneering proposals (as TCAV has previously been used to detect biases in systems). \n\nReadability and paper organisation:\n- The title reflects the content of the paper, and the abstract is written well (the only minor issues I have are discussed in detail in the scholarly and scientific comments). \n- References should be touched up and made consistent (e.g., all caps Journal names, Proc. vs. Proceedings, no venue for [12], all lower case acronyms...)\n- The paper is very well-written and nicely structured. Some minor remarks about rephrasings are listed below. \n- Sec. 1: It was not immediately clear to me what 'audio representations' are referring to (i.e., internal representations vs. pre-processed audio), maybe this could be clarified by adding something like 'internal' or audio representations learned by a system when talking about them first (l40). \n- Sec. 2.1: The section jumps around a bit between (T)CAV and alternative approaches, which could be restructured slightly (this would probably allow for some space to briefly mention other concept-based approaches as discussed in 'References'). \n- l165-176: To make the differences between [5] and this work clearer, 1) the 'datasets' (l169) should be clarified (e.g., to separate two different (?) datasets), and 2) the sentence from l169-l176 should be split up, where the content of the second half should follow the initial description of [5] (e.g., ... to separate two datasets. This method addresses the domain.... . In contrast, our CAV-based method... ). \n- Figure 1: As this figure consists of two figures, I would make them subfigures (this also allows for easier referencing); the titles of the individual model plots could be improved by using only the model name also used in the paper, e.g., MERT instead of mert_v1_95m; the legend should be made a bit bigger and clearer, I am unsure what the TCAV dist. (distance? distribution?) is, and it might be easier to understand if the three colours are explained separately; also, the results (i.e., passing or lack thereof) of the significance tests should be indicated somehow, or is that reflected in the colours as well? Finally, it should be stated somewhere why there are fewer genres depicted in the lower part of the figure, is it because language-genre examples were missing?\n- The symmetry of the concepts 'male' and 'female' go beyond intuition (l497) - at least depending on how the according CAVs are computed. If the corresponding CAV of 'male' is just -CAV of female (which I assume could be the case if this is derived via a binary classifier as suggested in l250), then (4) and (5) should indeed be equivalent (which then could raise the question as to why the results in Figure 2 are different at all, numeric instabilities?)\n- l485: The 'sorting' should be explained once more at this point (e.g., where we compute the TCAV values for ... and sort ... according to...)\n\nPotential to generate discourse: I could see this work having the potential to generate discourse, as it might be an interesting approach to look into biases within MIR models. However, the work might need some touch ups and more concrete ideas of how the debiasing could be realised. \n\nRelevance of the topic to ISMIR: Both the bias in musical data and systems, as well as the interpretation of musical embedding systems is relevant to the ISMIR community. As the work on interpretable deep learning is limited in the MIR community, work like this is even more valuable. \n\nMinor/detailed remarks:\n-----------------------\nl23: more recently\nl31: linebreak \nl47: maybe: unexplored\nl154: as -> via (as neurons etc. are not biases themselves)\nl159: remove '.'; sources [29-31] should preferably be referenced immediately after the according concept (e.g., counterfactual attention learning [30]), same for references in l164\nl170: undesireable biases as concepts\nl241: footnote should be after '.'\nl258: To construct the training and test sets required for the computation of CAVs (as these have not been defined yet)\nl311: this acronym was already introduced (a few times), can be used as is\nformula (3): 'I' needs to be defined\nl322: Some gradient information can certainly be extracted of an embedding system, just not the needed one (e.g., w.r.t. a target class) - this needs to be corrected\nl359: rank higher in terms of ('rank' is not really used a lot in this context, so it is not entirely clear what that means)\nl301, l390: linearly encoded -> not necessarily, it can be linearly separated well enough from embeddings of random samples; maybe: indicating that the CAVs should represent the concepts they are targeting reasonably well, as the concept and random activations can be linearly separated. \nl356: briefly clarify 'balancing constraints' here -> e.g., without balancing the distribution of other factors like gender or language in the training set, and ... \nl393: indicated by the 95% confidence intervals not spanning across the 0.5 mark (or similar?)\nl451: meaning (?) it is still above chance but (?) should be interpreted...",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis is an interesting, if frustrating, paper using some interpretable-AI tricks to identify when embeddings of music implicitly are learning concepts about their data that oughtn't be relevant to classification, such as classifying genre based on the perceived gender of a performer in a song.\n\nI will say that I don't understand some of the goals of this study: the claim is essentially that, \"the only thing that should be different if you substitute out a male singer for a female singer is the one variable about gender of performer\". But that's obviously not right, despite the claim, \"we expect female-led and male-led English language jazz to be musically comparable, despite differences in vocal timbre.\" Why would this be true? The instrumental blend would be different! You might have differen backing performers. They might emphasize different octaves in their accompaniment. Etc. Like, I get what they're trying to say, but there's almost a claim that the k.d. lang version of \"Hallelujah\" should be just as easy to classify as the Leonard Cohen version, and ... why would that be true? \n\nSo I admit I was a little suspicious, and I'm still a little suspicious: the fix is literally just \"take a convex combination of the classifier for genre and the classifier for the lead singer being female and use that, and not just the uncorrected classifier for genre.\" \n\nThis overall approach does \"work\", in that if we emphasize the classifier for gender, the songs that are rated as most clearly hip-hop start to be also with female singers. But it's a little frustrating, in that it suggests that the hardest-to-classify-for-gender songs will probably get tossed out quickest (like, e.g., k.d. lang). I'd love a much more detailed study of what actually is returned than just the tiny bit that's in 6.3.\n\nAll told, it's in interesting overall idea, and I'd watch a talk on this paper.",
      "review3": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nIn this paper, the authors propose to use a CAV-based method to investigate the bias presented in pretrained models (embeddings). The method trains a linear classifier of concepts and explore whether some genre of music tends to be classified as positive or negative (or, in authors\u2019 words, whether they align with the concept). The authors investigate four SOTA music embeddings and reveal that there exist strong gender bias and language bias in these embeddings. Based on these findings, the authors propose a strategy to adjust the bias, and it is able to provide a less-biased CAV.\n\nStrengths\n- The paper is clearly motivated. It reveals and tries to address an important issue in the era of large foundation models. It is crucial to be aware of the gender, cultural, and any kind of potential biases introduced in these data-based models.\n- The choice of different embeddings spans a wide variety of pre-training strategies, and the resulting model bias reflect the bias introduced in the pre-training.\n- The careful curation of datasets reflects a rigorous experimental design to minimize the bias introduced in the dataset.\n- The results are clear and easy to explain, which show the clear bias presenting in the embeddings.\n- The proposed de-biasing strategy is simple yet effective.\n\nWeaknesses and Questions\n- In section 5.1, the authors mention that there is no downstream classifier and therefore \u201cno gradient information can be extracted\u201d, but the genre classification seems to be a straight-forward downstream task to extract logits and gradient so that [7] can be fully adapted.\n- The description of how TCAV is done is confusing. CAV has different meanings, sometimes it means \u201cconcept activation vectors\u201d and sometimes it represents a vector, as in Eq. (2). The vector CAV is the same as the weight vector in Eq. (1). There concepts are mixed together and makes it hard to read.\n- (Important) Following the previous point, the process described in Section 5.1 suggests that the TCAV score is actually the percentage of test samples per genre that are classified as positive samples by the classifier in Eq. (1), and since each genre has a balanced test set, the ideal percentage is 50%. However, this is not pointed out, and the authors opt for a more abstract and complicated way to explain the idea - the idea of \u201chow well two concepts align\u201d, which comes from [7] (but the authors\u2019 implementation has significantly deviated from [7]). Also, I suggest creating a new name since this is no longer the TCAV introduced in [7].\n- Line 348-349: the authors mention the statistical test but do not present the results in Section 6. The statistical test with Bonferroni correction would fail to reject the null hypothesis in many cases because there is at least one genre where there is no bias and Bonferroni correction requires that there is significance in all sub-tests. \n- The section 5.2 and 6.3 also read a bit confusing to me. The authors mention the ranking of different tracks. I suppose this is the ranking of the p_{CAV} scores as in Eq. 2 (the likelihood of the song\u2019s being a hip-hop song)? This should be explicitly described.\n- A minor question: while the authors have discussed the difference between the proposed de-biasing strategy and [5], I think both of them assume linearity. Therefore, I am curious how they will perform differently. The discussion in Section 2.2 only points out the difference in motivation (dataset bias in [5] and demographic bias in this paper). What I see is that [5] debias the embedding and the paper debias the CAV (the classifier), but this is not pointed out in the paper.\n\nWhile I think the paper is addressing the crucial problem of implicit bias in music embeddings, and the authors have done extensive and careful studies to show interesting and meaningful results, the description of methods is really unclear (see weaknesses and questions). Since the methods deviate significantly from the TCAV reference [7] and therefore no reference could be found, it is crucial to clearly state everything in the implementation. Therefore, I could not recommend accepting the paper as it is. While my main criticism is about presentation, which can be done in the camera-ready version, I expect not minor but a great amount of adjustment for a good, clearly-written paper, so I would have to recommend a weak reject even though I like all the results and discussions.\n\nTo improve the paper, I would suggest considering the problems of presentations mentioned above. Even though I did not mention in the weaknesses, the process of dataset construction is also a bit hard to read. Section 4 and 5 takes great effort to understand because of the unclarity. Visualization (both of dataset and methods, as in [7]) could help. Also, without explaining original TCAV in detail, mentioning \u201cgradient information\u201d and why the bias term is required can be confusing. The authors could opt for either including the details of [7] or focusing on their implementations.\n\nMinor corrections\n- Eq. (1) suggests linear regression but what is done here is (I suppose) logistic regression.\n- Many references are not properly formatted. For example, [12] and [16] don\u2019t show proceeding names; [7] is published in ICML not in NeurIPS; \u201cMuChoMusic\u201d in [6] should contain upper cases; [6] and [14] are both from ISMIR and should be in the same format (including abbr. in [14])."
    },
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Music source separation extracts individual instrument/performer stems from mixed musical recordings. Performance is typically evaluated using metrics like source-to-distortion ratio (SDR), with higher values indicating better separation. However, relying on global SDR averages across test datasets provides limited insight into model performance. While improved average SDR suggests superior performance, it reveals little about specific strengths and weaknesses. Additionally, averaged metrics fail to account for SDR variance, which depends heavily on the musical characteristics of the test set. These limitations make cross-task/stem comparisons potentially misleading. To address these issues, we conducted a listening study evaluating source separation models across three tasks: 6-stem separation, Lead vs. Backing Vocal Separation, and Duet Separation. Participants assessed diverse examples, particularly those with poor objective or subjective performance. We categorized failure cases into three error types and found that while SDR generally correlates with perceptual ratings, significant deviations occur. Some errors substantially impact human perception but aren't well captured by SDR, while in other cases, listeners perceive better quality than SDR suggests. Our findings reveal nuances missed in current evaluation paradigms and highlight the need to include error categorization and performance distribution alongside averaged metrics.",
      "abstract": "Music source separation extracts individual instrument/performer stems from mixed musical recordings. Performance is typically evaluated using metrics like source-to-distortion ratio (SDR), with higher values indicating better separation. However, relying on global SDR averages across test datasets provides limited insight into model performance. While improved average SDR suggests superior performance, it reveals little about specific strengths and weaknesses. Additionally, averaged metrics fail to account for SDR variance, which depends heavily on the musical characteristics of the test set. These limitations make cross-task/stem comparisons potentially misleading. To address these issues, we conducted a listening study evaluating source separation models across three tasks: 6-stem separation, Lead vs. Backing Vocal Separation, and Duet Separation. Participants assessed diverse examples, particularly those with poor objective or subjective performance. We categorized failure cases into three error types and found that while SDR generally correlates with perceptual ratings, significant deviations occur. Some errors substantially impact human perception but aren't well captured by SDR, while in other cases, listeners perceive better quality than SDR suggests. Our findings reveal nuances missed in current evaluation paradigms and highlight the need to include error categorization and performance distribution alongside averaged metrics.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Saurjya Sarkar",
        "Victoria Moomijan",
        "Basil Woods",
        "Emmanouil Benetos",
        "Mark Sandler"
      ],
      "authors_and_affil": [
        "Saurjya Sarkar (Queen Mary University of London)*",
        "Victoria Moomijan (Queen Mary University of London)",
        "Basil Woods (AudioStrip)",
        "Emmanouil Benetos (Queen Mary University of London)",
        "Mark Sandler (Queen Mary University of London)"
      ],
      "channel_url": "",
      "day": "4",
      "keywords": [
        "Knowledge-driven approaches to MIR",
        "Evaluation methodology",
        "MIR tasks",
        "User-centered evaluation",
        "Evaluation, datasets, and reproducibility",
        "Sound source separation",
        "Machine learning/artificial intelligence for music",
        "Human-centered MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1HiK6n7M1ix5YwqlM1RgSE9TX8P4IRs4j/preview",
      "poster_pdf": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-13-perceptual-errors-in",
      "title": "Perceptual Errors in Music Source Separation: looking beyond SDR averages",
      "video": ""
    },
    "forum": "221",
    "id": "221",
    "pic_id": "",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Polyphonic music presents a unique challenge for computational modeling due to the complex interactions of multiple simultaneous musical streams and the need to capture both local and global structural relationships. We propose Adaptive Path of Prediction, a discrete diffusion model that learns the informational hierarchy of polyphony in an unsupervised manner. By training the model to find optimal note-removal paths, and to reversibly reconstruct these selectively removed notes, we reveal how critical musical events\u2014that sustain to later stages of data corruption\u2014maximize the preserved information and guide the prediction of remaining content. Drawing on compression learning theory, we posit that such adaptively-discovered \u201canchor notes\u201d reflect the system\u2019s ability to make an explicit abstraction of polyphonic music. Our experiments demonstrate that the model converges on consistent note-importance distinctions and can achieve better reconstruction performance in selected denoising paths than random ones. Furthermore, the model\u2019s assignment of note importance during the training process increasingly aligns with a reductive music analysis dataset, suggesting that our unsupervised framework can uncover structural hierarchies consistent with established music-theoretical views.",
      "abstract": "Polyphonic music presents a unique challenge for computational modeling due to the complex interactions of multiple simultaneous musical streams and the need to capture both local and global structural relationships. We propose Adaptive Path of Prediction, a discrete diffusion model that learns the informational hierarchy of polyphony in an unsupervised manner. By training the model to find optimal note-removal paths, and to reversibly reconstruct these selectively removed notes, we reveal how critical musical events\u2014that sustain to later stages of data corruption\u2014maximize the preserved information and guide the prediction of remaining content. Drawing on compression learning theory, we posit that such adaptively-discovered \u201canchor notes\u201d reflect the system\u2019s ability to make an explicit abstraction of polyphonic music. Our experiments demonstrate that the model converges on consistent note-importance distinctions and can achieve better reconstruction performance in selected denoising paths than random ones. Furthermore, the model\u2019s assignment of note importance during the training process increasingly aligns with a reductive music analysis dataset, suggesting that our unsupervised framework can uncover structural hierarchies consistent with established music-theoretical views.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Xiaoxuan Wang",
        "Martin Rohrmeier"
      ],
      "authors_and_affil": [
        "Xiaoxuan Wang (EPFL)*",
        "Martin Rohrmeier (EPFL)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Digital musicology",
        "Symbolic music processing",
        "MIR fundamentals and methodology",
        "Musical features and properties",
        "Cognitive MIR",
        "Computational musicology",
        "Structure, segmentation, and form",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1zD_qKWGjbX83veAHWeoRl4FSFFfTrXyS/preview",
      "poster_pdf": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-8-adaptive-path-of",
      "title": "Adaptive Path of Prediction: An unsupervised method for modeling note-level informational hierarchy of polyphony",
      "video": ""
    },
    "forum": "227",
    "id": "227",
    "pic_id": "",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "5"
  },
  {
    "content": {
      "TLDR": "The rapid rise of generative AI has transformed music creation, with millions of users engaging in AI-generated music. Despite its popularity, concerns regarding copyright infringement, job displacement, and ethical implications have led to growing scrutiny and legal challenges. In parallel, AI-detection services have emerged, yet these systems remain largely opaque and privately controlled, mirroring the very issues they aim to address. This paper explores the fundamental properties of synthetic content and how it can be detected. Specifically, we analyze deconvolution modules commonly used in generative models and mathematically prove that their outputs exhibit systematic frequency artifacts -- manifesting as small yet distinctive spectral spikes. This phenomenon, related to the well-known checkerboard artifact, is shown to be inherent to a chosen model architecture rather than a consequence of training data or model weights. We validate our theoretical findings through extensive experiments on open-source models, as well as commercial AI-music generators such as Suno and Udio. We use these insights to propose a simple and interpretable detection criterion for AI-generated music. Despite its simplicity, our method achieves detection accuracy on par with deep learning-based approaches, surpassing 99\\% accuracy on several scenarios.",
      "abstract": "The rapid rise of generative AI has transformed music creation, with millions of users engaging in AI-generated music. Despite its popularity, concerns regarding copyright infringement, job displacement, and ethical implications have led to growing scrutiny and legal challenges. In parallel, AI-detection services have emerged, yet these systems remain largely opaque and privately controlled, mirroring the very issues they aim to address. This paper explores the fundamental properties of synthetic content and how it can be detected. Specifically, we analyze deconvolution modules commonly used in generative models and mathematically prove that their outputs exhibit systematic frequency artifacts -- manifesting as small yet distinctive spectral spikes. This phenomenon, related to the well-known checkerboard artifact, is shown to be inherent to a chosen model architecture rather than a consequence of training data or model weights. We validate our theoretical findings through extensive experiments on open-source models, as well as commercial AI-music generators such as Suno and Udio. We use these insights to propose a simple and interpretable detection criterion for AI-generated music. Despite its simplicity, our method achieves detection accuracy on par with deep learning-based approaches, surpassing 99\\% accuracy on several scenarios.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Darius Afchar",
        "Gabriel Meseguer Brocal",
        "Kamil Akesbi",
        "Romain Hennequin"
      ],
      "authors_and_affil": [
        "Darius Afchar (Deezer)*",
        "Gabriel Meseguer Brocal (Deezer)",
        "Kamil Akesbi (Deezer)",
        "Romain Hennequin (Deezer)"
      ],
      "channel_url": "",
      "day": "4",
      "keywords": [
        "Music generation",
        "Automatic classification",
        "Philosophical and ethical discussions",
        "MIR tasks",
        "Music signal processing",
        "Ethical issues related to designing and implementing MIR tools and technologies",
        "MIR fundamentals and methodology"
      ],
      "long_presentation": "TRUE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://drive.google.com/file/d/1qkNSmq3r5wVmyNakbZrfM2KN6owNgi_o/preview",
      "poster_pdf": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-1-a-fourier-explanation",
      "title": "A Fourier Explanation of AI-music Artifacts",
      "video": ""
    },
    "forum": "229",
    "id": "229",
    "pic_id": "",
    "position": "01",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nStrongly agree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nStrongly agree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nStrongly agree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nStrongly agree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nStrongly Agree (Very novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nStrongly agree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nRather than developing a blackbox model to detect AI-generated music, this paper instead tries to understand the theory behind the artifacts from neural codecs. This provides more understanding of where these artifacts come from.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nModern AI music generators leave predictable artifacts in the music's spectrum; these artifacts can be understood from a Fourier analysis of the deconvolution operation.\n\n**Q17 (This paper is of award-winning quality.)**\n\nYes\n\n**Q18 ( If yes, please explain why it should be awarded.)**\n\nA few reasons come to mind: (a) The problem this paper tackles is very timely and relevant to our community, (b) rather than simply training a blackbox model, this paper offers insight and understanding into the problem based on a thoughtful analysis of modern AI music generation architectures, and (c) it further validates the theory by running experimental simulations and developing a simple, elegant, yet effective discriminator. It is also very well written. A delight to read!\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nStrongly agree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nSome of the paper\u2019s strengths:\n+ The problem that this paper tackles is very timely and relevant. A large dataset for detection of AI-generated music was only recently proposed at ICLR this year. This is a new task that has only been recently studied, and this paper makes a substantial contribution to the topic.\n+ The paper proposes a very simple, elegant, and effective method for detecting AI-generated music when the architecture uses deconvolution modules (as most do today). The performance is on par with much bigger models, but is interpretable and compact. And the absolute performance is very high (>99%).\n+ More importantly, the paper offers insight and understanding into the nature of the artifacts in AI-generated music. Rather than simply training a blackbox model that performs effective classification, this model sheds light on the reasons for the artifacts. Often research papers do not provide this kind of insight, so I applaud the authors for their work. This paper was a delight to read!\n+ The paper validates the theory with empirical analysis of recent datasets and popular AI-generators, including commercial generators like Suno and Udio. I also appreciate that the authors were upfront and transparent about the limitations of their analysis, and where empirical simulation is needed. The empirical results also bring up some interesting questions (like why the discriminator works so well even when the spectral peaks do not appear).\n+ The paper is well written. Even as someone with a background in signal processing, there are parts of the paper that are quite dense. But the authors did a good job explaining the key concepts in a concise manner.\n\nA few suggestions for improvement:\n- One question I have is, does this only apply to waveform-based 1-D CNN models (as opposed to models that work with a spectrogram and then invert it back to the time-domain as a separate stage/step)? I think it is important to explicitly define the boundaries of what this analysis pertains to.\n- It would be helpful to provide more details when describing the artifact fingerprint (last paragraph of section 4.1). Even though it is not the main focus of the paper, future researchers may want to compare their models to the approach described in this paper. I would recommend including a few sentences explaining the FFT analysis window size (does it depend on the sampling rate?), how the averaging is done (I assume it is averaging the DFT magnitudes over all analysis windows, but this was not explicitly described), etc. Of particular note, I did not understand this phrase (line 368): \u201csubtract local minima of the spectrum over sliding windows\u201d. A more detailed and comprehensive explanation here would be helpful.\n- Line 148: there is a typo in the equation for convolution \u2013 the t(t-tau) should be r(t-tau).\n- In line 104-105, it is claimed that \u201cOne difference is that we do not find this artifact related to kernel overlap, but to spectral periodization.\u201d A similar statement is also made in line 220-221. Aren\u2019t these describing the same phenomenon, but just in different domains (time vs frequency)? Either this needs to be explained more fully, or the language should be softened or adjusted (e.g. \u201cThis provides a Fourier domain perspective that compliments\u2026\u201d).\n- In lines 289-292, I would recommend using either subscripts or put superscripts in parentheses. Right now the notation looks like k raised to the (i+1) power.\n\nThis paper raises many interesting questions, so I look forward to seeing the authors\u2019 future work!\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nStrong accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThe reviewers discussed the strengths and weaknesses of the paper, which we summarize below.\nStrengths\n\u2022 The paper tackles a very relevant and timely topic: detecting AI-generated music. This topic has only been explored very recently due to the rise of commercial services for generating music.\n\u2022 The paper investigates what a deconvolution layer does through the lens of Fourier analysis. Based on this reasoning, it points out that deconvolution layers leave predictable artifacts in the spectrum.\n\u2022 Based on this argument, the paper then conducts experiments to show that a very simple logistic regression model can be used to detect AI-generated music. It shows very strong results that are on par with deep learning models. This is a very surprising and unexpected result. This model is interpretable, well motivated by theory, and shows strong empirical results on both academic and commercial AI-music generators. The authors also show that the artifacts are consistent, even when training data or seeds vary.\n\u2022 Many reviewers commented that the paper was a very interesting and enjoyable paper to read.\n\nWeaknesses and suggestions for improvement:\n\u2022 The current manuscript is very dense and several reviewers commented on having to read it several times before being able to understand it. There was a consensus among reviewers that a complete description of the theory would be difficult to fit into a short conference paper, so our recommendation is to focus more on the practical side of proposing a simple approach to detecting AI-generated music. We encourage the authors to also write a more complete journal article in which the ideas can be fully fleshed out and described more rigorously and comprehensively.\n\u2022 Some aspects of the paper were confusing to reviewers: (a) Figure 1 was very confusing to multiple reviewers and needs to be explained more clearly. (b) The manuscript has uneven coverage of requisite background knowledge. For example, space is given to describing basic Fourier definitions, but more advanced concepts like deconvolution and the equivalence of zero-padding in time and spectral interpolation are not reviewed or explained. (c) Section 3 can be polished quite a bit, and section 4 can be developed more fully (especially the description of the artifact fingerprint). However, given our recommendation above to focus on the practical side of proposing a simple way to detect AI-generated music, the authors may wish to omit some content and save it for an extended journal article on this topic.\n\u2022 We urge the authors to clarify that the \u201ccheckerboard\u201d effect is not a novel insight of the paper, and to explain that the novelty in this paper is in exploiting this phenomenon to detect AI-generated music. \n\u2022 There was a request to replace non peer-reviewed sources with more peer-reviewed sources where possible, especially on the \u201ccheckerboard\u201d artifacts.\n\nIt should be mentioned that the reviewers were not able to come to a consensus: three reviewers felt very enthusiastic about the insight and impact of this paper, and one reviewer felt that the shortcomings in the presentation of content were severe enough to warrant a rejection. Since three reviewers nominated the paper for a best paper award, we have retained a joint recommendation of \u201cstrong accept\u201d and ask the authors to consider the reviewers\u2019 suggestions to focus more on the practical application and leave more extensive treatment for an extended journal article.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nGeneral comments:\n- This is a very interesting article, I personally enjoyed reading it. I love the use of basic signal processing techniques to solve such a problem (it reminded me of works done for audio codec detection). It shows that not every problem needs \"training\" to be tackled. My main suggestions would be to polish section 3 (see details below) and develop a bit section 4, perhaps spent less time on the theory and more on the practice, as I think the reader can get the gist of the approach (you can refer to particular references for more details) but they would be more interested in the practicality of it. You could expand on the theory in a journal article for example! I would also proofread everything as there are times it feels that the authors rushed to write some parts, the text feels too linear, there are distracting typos; some rephrasing here and there would really make the article look better. Thank you for your work!\n\n\nDetailed comments:\n \n1. \n- I understand the appeal to show that quote from the CEO of Suno, but not only it is a very debatable statement (I personally disagree with it), but he most likely said that to make a point for people to use his technology. My point is that it is certainly not a fact but an opinion, so I am not sure it has value being shared here.\n\n- I believe both Suno and Udio got sued.\n\n2. \n- Could you briefly explain here what spectral periodization is?\n\n3. \n- Spectrograms are not being used because they \"better align[s] with the human ear's perception,\" but because they are a convenient visual representation of a signal (audio or not); some spectrograms do align better with the human perception of sounds, such as the CQT-based spectrogram. I think this whole paragraph could be rephrased better. You don't have to motivate the use of the FT from the audio spectrogram; FT are commonly used for non-audio inputs, including images. I would also clarify the following statement: \"Our overall proof sketch is to show that deconvolution operation periodize the spectra of hidden layers, hence creating peaks by tiling the constant component of the signal.\"\n\n- \"Since not all of the ISMIR community is familiar with signal processing techniques, ...\" I think it's not necessary to mention this. This article will (hopefully!) reach to other communities as well, so perhaps do not specifically target the ISMIR community :)\n\n- I would be careful when talking about convolution in the context of CNNs. The convolution operation in a CNN is more accurately a cross-correlation; while the FT of a convolution is equal to the point-wise multiplication of the FTs (as per the convolution theorem), the FT of a correlation will involve a complex conjugate. \n\n- What happens to the parameter k when going from the deconvolution with stride k to the zero-completion+1-strided convolution in Figure 1?\n\n- Typo: \"We have seen that a the zero-upsampling of a ...\"\n\n- Footnotes 2 and 3 could be in the text. This is actually the core of your approach, I would make these paragraphs as clear as possible for the reader. I am actually unclear at this point; do you use the 2d FT for 2d kernels? And please, I am seeing few distracting typos, make sure to proofread everything.\n\n- What would the x-axis represents for the spectra of the latent representations in Figure 3?\n\n- The last paragraph of 3.3 is very interesting statement; please, say more!\n\n- Section 3 needs a bit of polishing. There is a lot to digest and I feel that everything is going a bit too linearly. This section could really benefit from some better formatting and clarifications; they would really help the reader.\n\n4.\n\n- The idea of averaging music patches to emphasize the artifact peaks might be worth more details. What would be the length of those patches, for example?\n\n- 4.2 is a very welcome and interesting subsection!\n\n- \"audios\" -> audio files?\n\n- Figure 5 is too small.\n\n6.\n- Please, make sure your references are correct:\n- Use capitalized letters when needed (e.g., for acronyms)\n- Avoid repetition (e.g., the year shown twice)\n- Be consistent (e.g., ICASSP citations should show the same info)",
      "review2": "- **Overall Evaluation**: Strong reject\n\n#### Main Reviews\n\nThis article interprets the \u201cdeconvolution\u201d operation in generative AI from a signal processing perspective to explain the \u201ccheckerboard\u201d artifact resulting from repeated convolutions in CNN. Overall, the text is poorly written, poorly organized, technically flawed and unbalanced. Many aspects of the presentation need a lot of improvement, such as a more mathematically rigorous argumentation instead of relying on confusing illustrations and rewriting most of the explanations of signal processing concepts, which is currently the weakest aspect of the manuscript. There might be a contribution to the community in this manuscript, but the presentation is so poor that it obscures any potential contribution. Other works have investigated the \u201ccheckerboard artifact\u201d in CNN (see, for example, Sugawara et al. 2019) and it is well known to be caused by \u201cforward-propagation of upsampling layers and backpropagation of convolutional layers. [Sugawara et al. 2019]\u201d So, what exactly is the novelty or contribution of this work? The idea of using it in AI detection might be a contribution provided that the work is redone from scratch focusing on that instead. In my opinion, this work hasn\u2019t been fully developed yet into a solid and long-lasting contribution because most basic aspects are still very unclear and confusing. The first thing to do is a thorough review of the SOTA to avoid rehashing ideas and build on it to describe a solid and long-lasting contribution to the community. My review will focus on what I think must be improved.\n\nThe current manuscript does not provide answers to some basic questions, such as: What is the goal of the work? What is the contribution of the work described? What is the state of the art (SOTA)?\n\nCurrently, the goal of the work is unclear. The abstract revolves around \u201cAI detection\u201d, but line 50 and then 70-73 state otherwise without any clear reason. Whatever the goal, everything in the manuscript must revolve around it, from the review of the state of the art (SOTA) and contribution to the experimental protocol and takeaway message. AI detection seems like a clear goal that can be easily motivated, justified, and evaluated in comparison with the SOTA. If the goal is interpretability, the experiments must be designed with that in mind, but I personally find it more abstract and harder to do.\n\nWhen the goal is unclear, so is everything else. Interpretability requires a review of other works that focus on interpretability aspects and techniques. Similarly for AI detection. Finally, the Introduction must clearly state what the contribution of the work is. The current text hints at a potential contribution, but it\u2019s still unclear to the reader. The potential contribution seems to be \"the detection of checkerboard artifacts in AI-generated music, and how that detection can be used to differentiate it from non-AI music\" based on the insight that \"CNN architectures leave certain artifacts in their generated audio due to the deconvolution layers, and these artifacts can be detected in a very simple and straightforward way\". By the way, the current title is very generic and does not reflect core aspects of the work. Terms like \u201cConvolutional Neural Networks\u201d and \u201cAI-generated music detection\u201d should appear in the title to better reflect the work.\n\nA quick search for \u201ccheckerboard effect\u201d returned several results from the image processing literature that explain it (similarly to the current text) and propose ways to avoid it. For example, Sugawara et al. 2019. The citations in the current manuscript (also see comment below about it) indicate a lack of knowledge of the SOTA.\n\nI find the technical aspects of the presentation very uneven (probably due to an assumption about the reader\u2019s background and knowledge stated in the first paragraph of Sec 3.1). While Sec 3.1 presents a very superficial (and superfluous, in my opinion) recap of a few concepts from Fourier analysis, the most important subsection of 3.1 (deconvolution) is not presented in enough detail. For example, the arguments in 244-245 rely on the equivalence of zero-padding to spectral interpolation, but this is never reviewed or explained (the reader is assumed to be familiar with this property of the DFT but not with the fundamentals of Fourier analysis?). My background is signal processing (SP) and, in my opinion, the current version of the manuscript has a very patchy presentation of SP concepts. In what follows, I\u2019ll focus on that.\n\nI understand that the manuscript is following the jargon and presentation style commonly found in the AI literature, but it needs to bridge the gap between SP and AI carefully. I find the presentation of most SP concepts uneven and confusing. For example, in my understanding, the SP use of \u201cdeconvolution\u201d refers to an operation that reverts the effect of a convolution. Given a signal that is a convolution between two others, the deconvolution operation would attempt to retrieve one of these original signals. For example, given the LTI system response to some known input, retrieve the system\u2019s transfer function. The deconvolution operation in Fig 1 seems to illustrate a different operation (but I find Fig 1 hard to understand, so I don\u2019t understand how the operation is done or the impact or the operation). Additionally, the \u201cconvolution\u201d operation in SP is different from the one suggested by Fig. 1, which seems to better correspond to autocorrelation in SP terms.\n\nIn SP, the upsampling operation comprises two steps, namely expansion and low-pass filtering. Upsampling by $L$ inserts $L-1$ zeros between samples, and low-pass filtering (when done appropriately), replaces the zeros by interpolated values.\n\nFig 1 is intended to illustrate the core argument of the manuscript, but I find it unclear and unhelpful. I think Fig 1 needs more details and annotations (part a, b, and c, etc) and possibly to be redesigned from scratch to clearly illustrate the mathematical operations (adding mathematical formulas would also go a long way in clarifying the operations). I\u2019ll refer to parts a), b), and c) going left to right for simplicity. For example, are the vectors X in (a) and (b) the same? The figure seems to indicate that they have different lengths because the \u201cconvolution matrix\u201d is not square. I do not see how the matrix multiplication in the middle is equivalent to the one on the right-hand side. How many zeros are there in the beginning of the vector X? More importantly, as far as I can understand, the operations in b) and c) are equivalent if there are $k$ zeros between samples in vector $X$, which corresponds to an expansion of $k-1$ (I reached this conclusion after a lot of time and effort trying out different examples until it seemed to work, but the text cannot expect the reader to have to do that to be able to understand anything, let alone its main argument). However, Figs 1 and 2 seem to illustrate upsampling by $2$. Very unclear to me. By the way, even if Fig 1 was clear, it is not a proof, just an illustration. All mentions of \u201cproof\u201d in the text must be replaced because it makes the text come across as very naive.\n\nSec 3.2 seems to indicate that it is indeed an expansion (not upsampling, just the first step) by $k-1$, but the SP jargon used is very inconsistent and confusing throughout. For example, Fig 1 clearly illustrates a discrete convolution, but part of the argument (e.g., line 202) relies on a \u201ccontinuous transform perspective\u201d? Even Sec 3.1 uses continuous transform definitions that are inconsistent with the discrete nature of the argument, which relies on resampling. Sec 3.1 on onwards that must be rewritten and clarified.\n\nDouble-check English with the help of a native speaker.\n\nThere are currently 9/37 citations to non peer-reviewed sources. Replace these with peer-reviwed when possible.\n\nREFERENCES\nSugawara Y, Shiota S, Kiya H. Checkerboard artifacts free convolutional neural networks. APSIPA Trans Sig Inform Proc. 2019.",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper provides a theoretically rigorous and empirically grounded analysis of systematic artifacts in AI-generated music, rooted in Fourier analysis. The authors demonstrate that deconvolution layers in generative models inherently produce spectral peaks due to periodization effects from zero-upsampling, a property tied solely to architecture\u2014not training data or weights. By decomposing deconvolution operations mathematically, they explain how spectral peaks emerge recursively through layers, offering a clear, step-by-step framework. Experiments across open-source (DAC, Encodec) and commercial (Suno, Udio) models confirm these artifacts\u2019 consistency, enabling a simple frequency-based detector that matches deep learning approaches in accuracy. The work bridges signal processing theory and MIR, advancing interpretability in AI detection and addressing ethical concerns around opaque systems.\nStrengths:\nThe Fourier-based explanation of artifacts is novel and mathematically sound, linking architectural choices to spectral patterns.\nEmpirical validation across diverse models shows artifacts are consistent, even when training data or seeds vary.\nThe detection method\u2019s simplicity and interpretability contrast with black-box alternatives, offering practical value.\nSuggestions for Improvement:\nDataset Overlap: I wonder if there is overlap between the MTAT and Jamendo datasets? This could weaken the claim of data independence and should be addressed.\nFigure 5 Peaks: Why are there less peaks in DAC and Encodec (24) compared to the ones in figure 4? I'd assume, if the decoders introduce more artifacts, that there would be more peaks. It's fine to leave this as future work, but it would be nice to maybe mention this.\nMinor Fixes:\nLine 36: Include references to lawsuits against Suno and Udio (e.g., [1], [2]).\nLine 235: Remove the extra \"a\".\nLine 261: Correct \"there [is] one case that\" to \"there is one case that\".\nReadability: Font sizes in Figure 5 are too small.\n\nConclusion: This is a standout contribution to MIR, offering both theoretical insights and practical tools for detecting AI-generated music. While minor edits have been suggested, the work\u2019s clarity, technical depth, and societal relevance make it a strong accept. Its Fourier-based framework sets a new standard for interpreting generative models and could earn an award for its interdisciplinary impact.\n\nReferences for Lawsuits:\n[1] https://www.courtlistener.com/docket/68878608/umg-recordings-inc-v-suno-inc/\n[2]https://www.courtlistener.com/docket/68878697/umg-recordings-inc-v-uncharted-labs-inc-dba-udiocom/"
    },
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Piano sustain pedal detection has previously been approached as a binary on/off classification task, limiting its application in real-world piano performance scenarios where pedal depth significantly influences musical expression. This paper presents a novel approach for high-resolution estimation that predicts continuous pedal depth values. We introduce a Transformer-based architecture that not only matches state-of-the-art performance on the traditional binary classification task but also achieves high accuracy in continuous pedal depth estimation. Furthermore, by estimating continuous values, our model provides musically meaningful predictions for sustain pedal usage, whereas baseline models struggle to capture such nuanced expressions with their binary detection approach. Additionally, this paper investigates the influence of room acoustics on sustain pedal estimation using a synthetic dataset that includes varied acoustic conditions. We train our model with different combinations of room settings and test it in an unseen new environment using a \u201cleave-one-out\u201d approach. Our findings show that the two baseline models and ours are not robust to unseen room conditions. Statistical analysis further confirms that reverberation influences model predictions and introduces an overestimation bias. ",
      "abstract": "Piano sustain pedal detection has previously been approached as a binary on/off classification task, limiting its application in real-world piano performance scenarios where pedal depth significantly influences musical expression. This paper presents a novel approach for high-resolution estimation that predicts continuous pedal depth values. We introduce a Transformer-based architecture that not only matches state-of-the-art performance on the traditional binary classification task but also achieves high accuracy in continuous pedal depth estimation. Furthermore, by estimating continuous values, our model provides musically meaningful predictions for sustain pedal usage, whereas baseline models struggle to capture such nuanced expressions with their binary detection approach. Additionally, this paper investigates the influence of room acoustics on sustain pedal estimation using a synthetic dataset that includes varied acoustic conditions. We train our model with different combinations of room settings and test it in an unseen new environment using a \u201cleave-one-out\u201d approach. Our findings show that the two baseline models and ours are not robust to unseen room conditions. Statistical analysis further confirms that reverberation influences model predictions and introduces an overestimation bias. <br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Hanwen Zhang",
        "Kun Fang",
        "Ziyu Wang",
        "Ichiro Fujinaga"
      ],
      "authors_and_affil": [
        "Hanwen Zhang (McGill University)*",
        "Kun Fang (McGill University)",
        "Ziyu Wang (New York University)",
        "Ichiro Fujinaga (McGill University)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Musical features and properties",
        "Music transcription and annotation",
        "MIR tasks",
        "Music signal processing",
        "Expression and performative aspects of music",
        "MIR fundamentals and methodology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1IffoT21p4HpWSPGi7TSjL2qE2Gek-dD_/preview",
      "poster_pdf": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-11-high-resolution-sustain",
      "title": "High-Resolution Sustain Pedal Depth Estimation from Piano Audio across Room Acoustics",
      "video": ""
    },
    "forum": "233",
    "id": "233",
    "pic_id": "",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Digital music learning applications have become a popular option for self-guided learning of musical instruments. Personalization of the learning curriculum in such applications hinges on two essential components: The learning unit (song arrangement) and the learner (student). While previous research has focused extensively on quantifying and characterizing the musical content, learner representation remains largely unexplored. \nIn this paper, we introduce interpretable representations for these components, in the context of digital guitar learning. \n\nWe propose a methodology to embed song arrangements and individual guitar students into a shared, interpretable skill vector space. \nTo achieve that, we employ an automated profiling technique for guitar tablatures, generating granular vocabulary and difficulty descriptors.\nWe validate the effectiveness of these representations by predicting the proportion of onsets played correctly by students, using a large-scale dataset from an online guitar learning platform. \n\nOur results demonstrate that models leveraging the combined representation of students and song arrangements outperform informed baselines and show better predictive accuracy when compared to models using either representation individually. \nThese findings underscore the value of joint learner-song arrangement representations for digital music learning.",
      "abstract": "Digital music learning applications have become a popular option for self-guided learning of musical instruments. Personalization of the learning curriculum in such applications hinges on two essential components: The learning unit (song arrangement) and the learner (student). While previous research has focused extensively on quantifying and characterizing the musical content, learner representation remains largely unexplored. \nIn this paper, we introduce interpretable representations for these components, in the context of digital guitar learning. \n\nWe propose a methodology to embed song arrangements and individual guitar students into a shared, interpretable skill vector space. \nTo achieve that, we employ an automated profiling technique for guitar tablatures, generating granular vocabulary and difficulty descriptors.\nWe validate the effectiveness of these representations by predicting the proportion of onsets played correctly by students, using a large-scale dataset from an online guitar learning platform. \n\nOur results demonstrate that models leveraging the combined representation of students and song arrangements outperform informed baselines and show better predictive accuracy when compared to models using either representation individually. \nThese findings underscore the value of joint learner-song arrangement representations for digital music learning.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Manuel M\u00fcllersch\u00f6n",
        "Anssi Klapuri",
        "Marcelo Rodriguez",
        "Christian Cardin"
      ],
      "authors_and_affil": [
        "Manuel M\u00fcllersch\u00f6n (Yousician)*",
        "Anssi Klapuri (Yousician)",
        "Marcelo Rodriguez (Yousician)",
        "Christian Cardin (Yousician)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Personalization",
        "Applications",
        "User behavior analysis and mining, user modeling",
        "Music recommendation and playlist generation",
        "Knowledge-driven approaches to MIR",
        "Music training and education",
        "Machine learning/artificial intelligence for music",
        "Human-centered MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1Hd7q2QiiCLidc3sY7SHFoTC1mKhOCu5A/preview",
      "poster_pdf": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-2-playability-prediction-in",
      "title": "PLAYABILITY PREDICTION IN DIGITAL GUITAR LEARNING USING INTERPRETABLE STUDENT AND SONG REPRESENTATIONS",
      "video": ""
    },
    "forum": "238",
    "id": "238",
    "pic_id": "",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Recently, the information content (IC) of predictions from a Generative Infinite-Vocabulary Transformer (GIVT) has been used to model musical expectancy and surprisal in audio. We investigate the effectiveness of such modelling using IC calculated with autoregressive diffusion models (ADMs). We empirically show that IC estimates of models based on two different diffusion ordinary differential equations (ODEs) describe diverse data better, in terms of negative log-likelihood, than a GIVT. We evaluate diffusion model IC's effectiveness in capturing surprisal aspects by examining two tasks: (1) capturing monophonic pitch surprisal, and (2) detecting segment boundaries in multi-track audio. In both tasks, the diffusion models match or exceed the performance of a GIVT. We hypothesize that the surprisal estimated at different diffusion process noise levels corresponds to the surprisal of music and audio features present at different audio granularities. Testing our hypothesis, we find that, for appropriate noise levels, the studied musical surprisal tasks' results improve. Code is provided on github.com/SonyCSLParis/audioic.",
      "abstract": "Recently, the information content (IC) of predictions from a Generative Infinite-Vocabulary Transformer (GIVT) has been used to model musical expectancy and surprisal in audio. We investigate the effectiveness of such modelling using IC calculated with autoregressive diffusion models (ADMs). We empirically show that IC estimates of models based on two different diffusion ordinary differential equations (ODEs) describe diverse data better, in terms of negative log-likelihood, than a GIVT. We evaluate diffusion model IC's effectiveness in capturing surprisal aspects by examining two tasks: (1) capturing monophonic pitch surprisal, and (2) detecting segment boundaries in multi-track audio. In both tasks, the diffusion models match or exceed the performance of a GIVT. We hypothesize that the surprisal estimated at different diffusion process noise levels corresponds to the surprisal of music and audio features present at different audio granularities. Testing our hypothesis, we find that, for appropriate noise levels, the studied musical surprisal tasks' results improve. Code is provided on github.com/SonyCSLParis/audioic.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Mathias Rose Bjare",
        "Stefan Lattner",
        "Gerhard Widmer"
      ],
      "authors_and_affil": [
        "Mathias Rose Bjare (Johannes Kepler University Linz)*",
        "Stefan Lattner (Sony CSL Paris)",
        "Gerhard Widmer (Johannes Kepler University Linz)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Musical features and properties",
        "Music signal processing",
        "Machine learning/artificial intelligence for music",
        "MIR fundamentals and methodology",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1IcrPek0vcQdYgt20GuagKFGkiyeQHonp/preview",
      "poster_pdf": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-8-estimating-musical-surprisal",
      "title": "Estimating Musical Surprisal from Audio in Autoregressive Diffusion Model Noise Spaces",
      "video": ""
    },
    "forum": "244",
    "id": "244",
    "pic_id": "",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nDisagree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q10 (Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chose, otherwise write \"n/a\"))**\n\nsee below\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nStrongly agree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nDisagree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe paper proposes a new way for modeling musical suprise which might open up interesting direction of future research (not really discussed in the paper)\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\ninformation content (IC) computed via neural ODE-based likelihood estimation in autoregressive diffusion models\ncan be used to predict monophonic pitch surprisal and segment boundaries in multi-track audio\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak reject\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThere is a very interesting review of related literature that is well presented (given the limited amount of space). This alone provides a value to the readers.\n\nHowever, the paper could be very much improved in terms accessibility. To begin with, the abstact already contains acronyms (IC, ODEs) that should not be taken for granted but be properly introduced. The same issue is repeated in the text (GIVT, EEG, D-REX, MEG, GNN, MLP). Further, the presentation is very heavy on math. I understand this comes with the approach chosen. Still I feel very little effort was made to explain things. It rather felt like the descriptions were just thrown at the reader with the content being very obvious.\n\nI am very familiar with diffusion and score models and also with Normalizing Flow approaches. But I never really learned about ODEs and really struggled to follow the method section of the paper. I am sure it will be much worse for the average ISMIR attendee. There has to be a better way to present this.\n\nNLL was used to measure how well the music data is decribed by the model. This was taken as a given. A short discussion would be nice to justify this choice. Are there any alternatives?\n\nThere is also no human listening / perception study. \n\nGiven the hypothesis from l231, shouldn't one be able to actually hear the pitch be better preserved than the timbre at moderate noise levels? It appears as if the noisy outputs were not actually listened to. Also, some listening examples for the readers would have been nice - for this specifically but also in general.\n\nminor points:\n- In Eq. 1, it is unclear what the t in dz(t) means.\n- l63 differ in how they noise details\n- l231 4.5we\n- Reproducibility limited because one of the datasets used in the experiments is private. The authors should at least consider sharing the Music2Latent embeddings.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThe review generally argue in favor of accepting the paper. Still, several issues are raised that need to be addressed for the final revision. \nIn particular, to increase the value of this paper to the ISMIR community, the clairty and accessibility of the descriptions should be improved. Furthermore, adding listening examples / studies would be appreciated.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper proposes a framework for estimating musical surprisal directly from audio using autoregressive diffusion models in a continuous latent space, allowing surprisal to be modeled without symbolic input or fixed output distributions. The key innovation lies in using diffusion-based negative log-likelihood (information content) as a frame-level signal of musical unexpectedness, while leveraging noise-level control to modulate abstraction \u2014 from timbre-sensitive to pitch-sensitive to structure-sensitive representations. This enables the model to isolate different layers of musical information depending on the noise level, a novel idea with implications for multi-layered music understanding. The paper is well-structured and technically sound, with a clear modeling pipeline and evaluations that span model fitness (NLL), pitch surprise correlation (IDyOM), timbre invariance, and segment boundary detection. These experiments offer meaningful insight into how noise levels affect the perceived resolution of musical content, and position the work as a valuable contribution to audio-based MIR.\n\nAlthough I believe the paper introduces a novel and promising direction, I would like to share a few points that could be clarified or expanded, along with some broader suggestions to strengthen future iterations.\n\n(1.) Definition of \u201cmusical surprise\u201d:\nWhile the paper consistently refers to its output as \u201cmusical surprisal,\u201d the measured quantity is more accurately a form of information-theoretic surprise or prediction error. Without perceptual evaluation or alignment with listener-annotated moments of musical salience (e.g., drops, climaxes, transitions), the use of \u201csurprise\u201d risks being misleading. I encourage the authors to either clarify the distinction or better ground the term in listener-centered or cognitive models.\n\n(2.) Evaluation of noise-level abstraction hypothesis:\nOne of the paper\u2019s most compelling ideas is that adjusting the noise level changes the model\u2019s sensitivity to different musical dimensions (timbre, pitch, structure). However, this claim is not evaluated systematically \u2014 the results are more observational than diagnostic. I suggest including controlled probes or attribution studies to show what kinds of musical features dominate IC estimates at different noise levels.\n\n(3.) Limited qualitative musical examples:\nThe evaluation relies largely on synthetic melodies and statistical metrics (e.g., correlation with IDyOM, boundary F1). To support broader MIR relevance, I suggest including qualitative case studies using real music (e.g., genre transitions, DJ cuts, expressive phrasing) to demonstrate how IC curves reflect human-perceived surprise or structure.\n\n(4.) No human perceptual validation:\nGiven that the paper positions its contribution around \u201cmusical surprise,\u201d the absence of even a small-scale listener study or alignment with behavioral annotations is a missed opportunity. A few listener-marked surprise points would significantly strengthen the perceptual claim and help calibrate the model\u2019s outputs to real human responses.\n\n(5.) Interpretability of IC spikes:\nThe IC signal is promising, but what do its spikes mean musically? Are they driven by pitch jumps, texture changes, or dynamics? Introducing simple attribution (e.g., correlation with chroma shifts, energy flux, or timbral novelty) could help demystify the model\u2019s behavior and make it more interpretable for downstream use.\n\nBeyond these main points, I would like to suggest a few broader ideas for future work:\n\n(1.) While the paper focuses on high-IC moments as indicators of surprise, low-IC regions may be equally valuable as indicators of regularity. These could correspond to loops, grooves, or repeated motifs, enabling tasks such as loop extraction, pattern discovery, or motif clustering. Treating surprisal as a continuous signal \u2014 where both peaks and plateaus are musically meaningful \u2014 could unlock broader applications in structure-aware generation, DJ-style segmentation, or form analysis.\n\n(2.) Rather than treating surprisal as a scalar, this work opens the door to multi-dimensional surprisal \u2014 for example, separate IC signals for pitch, timbre, and rhythm, each derived via tailored noise-level filters. I encourage the authors to explore or at least discuss this possibility.\n\n(3.) Since diffusion models are commonly used in generation, one exciting direction would be to use IC as a control signal in generative models \u2014 for example, regulating musical surprise over time, or aligning generative transitions with IC peaks. This could bridge analysis and creative synthesis.\n\nOverall, I believe this paper contributes a valuable and technically novel framework for unsupervised surprisal estimation in music audio. It opens important conversations around abstraction control, audio-based expectation modeling, and the relationship between statistical prediction and perceptual response. However, I am assigning a Weak Accept rather than a Strong Accept because two key aspects remain underdeveloped: (1) the definition and framing of \u201cmusical surprise\u201d lacks grounding in perceptual or listener-based evidence, and (2) the abstraction-control claim via noise level is intriguing but not yet supported with systematic, feature-specific evaluation. Still, the direction is promising, the methodology is solid, and I believe it should be included at ISMIR as a strong foundation for future work.",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nStrengths:\n1. Overall, the writing is very coherent and logically clear, friendly for even an audience not so close to the field (like me).\n2. Section 4 shows comprehensive experiments, and the logic for experiment design looks solid to me. The discussions show very clear relationships between each table of results and corresponding findings /claims with adequate justifications.\n3. The research perspective is interesting and insightful, especially from 4.5 to 4.6, showing the qualities and potential of diffusion noise space in the music audio domain.\n\nWeakness-ish:\n1. Small writing suggestions: in the abstract and conclusion, the authors like to refer to \"previous models\" or \"alternative methods\" without specifically pointing out what they are comparing against exactly, I find it more useful if they make it clear about the difference between diffusion and past models in both sections.\n2. Line 462, isn't the highest correlation for EDM t=20 instead of 50, according to my understanding of Table 4? I can understand that this sentence is trying to make a connection to Table 3, but I feel it will be more helpful if they point this out directly so the audience will know where to look for the evidence.\n3. In sec 4.6, line 482 says \"extra peaks not attributed to segment boundaries\", interesting findings, but I wonder if the authors can offer some insights/analysis on why and how this appears, especially when in line 77 they say \"peaks related to other musical events\". I also wonder if there's a relationship between IC estimations and segmentation performance across \"t,\" possibly helpful for explaining any findings like line 487.\n4. Out of curiosity for future work, I wonder if the contribution to IC estimation is coming from timbre invariance. How would other models that specifically extract pitch information from audio perform in such tasks, compared to these methods? \n5. In Figure 1, it's visually not intuitive for me to compare blue line with others across the two plots, at a glance all colors look similar between above and below, if the point is to say blue is more sensitive to timbre change compared to other colors, might be good to overlay the same approach in different samples vs. now different approaches overlaid in same sample.\n\nSummary:\nWithin my bias in a field that I'm not so familiar with, I believe this work is technically solid and insightful for our field with some minor improvements.",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nStrengths:\n\nMethodological novelty\u2014The work is the first to use autoregressive diffusion ODEs (EDM & Rectified-Flow) to compute frame-level log-likelihood and Information Content (IC) in a continuous audio-latent space, eliminating the Gaussian-mixture assumption of the prior GIVT baseline.\n\nWeaknesses:\n\n1. Lack of perceptual studies\u2014No behavioral or listening-test evidence is provided to confirm that lower NLL or higher \u03c1 actually correlate with human-felt surprise.\n\n2. Limited evaluation scope\u2014Pitch-surprise tests rely on just 49 synthetic Irish tunes and 18 recorded vocal tracks, while segmentation is evaluated only at Salami\u2019s lowercase level; broader genre coverage would strengthen the claims\n\n3. No comparison to flow or VQ-VAE likelihoods\u2014without comparisons to modern flow-based and VQ-VAE likelihood models\u2014current standards for explicit audio density\u2014the reported gains lack context, leaving it unclear whether diffusion truly surpasses state-of-the-art alternatives.\n\nComment:\nThis paper introduces an autoregressive diffusion-ODE framework for estimating musical surprisal directly in a continuous audio latent space and demonstrates clear quantitative gains over the current GMM baseline. Strengths include a well-motivated methodological innovation, rigorous derivation, thorough error-speed analysis and convincing improvements in NLL, pitch-surprise correlation and structure boundary detection. The noise-level\u2013as-semantic-granularity insight is particularly compelling and should inspire follow-up work. Weaknesses lie in the limited baselines\u2014no comparison with flow or VQ-VAE likelihoods\u2014and a narrow evaluation corpus lacking perceptual validation. Runtime statistics and a public replication dataset would also strengthen reproducibility."
    },
    "session": "6"
  },
  {
    "content": {
      "TLDR": "In recent years, the guitar has received increased attention from the music information retrieval (MIR) community driven by the challenges posed by its diverse playing techniques and sonic characteristics. Mainly fueled by deep learning approaches, progress has been limited by the scarcity and limited annotations of datasets. To address this, we present the Guitar On Audio and Tablatures (GOAT) dataset,  comprising 5.9 hours of unique high-quality direct input audio recordings of electric guitars from a variety of different guitars and players. We also present an effective data augmentation strategy using guitar amplifiers which delivers near-unlimited tonal variety, of which we provide a starting 29.5 hours of audio. Each recording is annotated using guitar tablatures, a guitar-specific symbolic format supporting string and fret numbers, as well as numerous playing techniques. For this we utilise both the Guitar Pro format, a software for tablature playback and editing, and a text-like token encoding. Furthermore, we present competitive results using GOAT for MIDI transcription and preliminary results for a novel approach to automatic guitar tablature transcription. We hope that GOAT opens up the possibilities to train novel models on a wide variety of guitar-related MIR tasks, from synthesis to transcription to playing technique detection.",
      "abstract": "In recent years, the guitar has received increased attention from the music information retrieval (MIR) community driven by the challenges posed by its diverse playing techniques and sonic characteristics. Mainly fueled by deep learning approaches, progress has been limited by the scarcity and limited annotations of datasets. To address this, we present the Guitar On Audio and Tablatures (GOAT) dataset,  comprising 5.9 hours of unique high-quality direct input audio recordings of electric guitars from a variety of different guitars and players. We also present an effective data augmentation strategy using guitar amplifiers which delivers near-unlimited tonal variety, of which we provide a starting 29.5 hours of audio. Each recording is annotated using guitar tablatures, a guitar-specific symbolic format supporting string and fret numbers, as well as numerous playing techniques. For this we utilise both the Guitar Pro format, a software for tablature playback and editing, and a text-like token encoding. Furthermore, we present competitive results using GOAT for MIDI transcription and preliminary results for a novel approach to automatic guitar tablature transcription. We hope that GOAT opens up the possibilities to train novel models on a wide variety of guitar-related MIR tasks, from synthesis to transcription to playing technique detection.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Jackson Loth",
        "Pedro Sarmento",
        "Saurjya Sarkar",
        "Zixun Guo",
        "Mathieu Barthet",
        "Mark Sandler"
      ],
      "authors_and_affil": [
        "Jackson Loth (Queen Mary University of London)*",
        "Pedro Sarmento (Queen Mary University of London)",
        "Saurjya Sarkar (Queen Mary University of London)",
        "Zixun Guo (Queen Mary University of London)",
        "Mathieu Barthet (Queen Mary University of London)",
        "Mark Sandler (Queen Mary University of London)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Symbolic music processing",
        "Generative Tasks",
        "Musical features and properties",
        "Music transcription and annotation",
        "Representations of music",
        "Novel datasets and use cases",
        "Music and audio synthesis",
        "MIR tasks",
        "Expression and performative aspects of music",
        "Evaluation, datasets, and reproducibility",
        "MIR fundamentals and methodology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1Q_EHvjCdRn8EME98EiiwNg6iEya62Lpm/preview",
      "poster_pdf": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-5-goat-a-large",
      "title": "GOAT: A Large Dataset of Paired Guitar Audio Recordings and Tablatures",
      "video": ""
    },
    "forum": "245",
    "id": "245",
    "pic_id": "",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly disagree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nDisagree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nDisagree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nAs this is a dataset paper, the focus right now is on dataset documentation rather than offering broader reusable insights; I would expect for these to emerge later when the dataset will be used.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nGOAT offers a new, rich dataset of guitar audio, associated tablatures, and various augmentations that are realistic for guitar players (such as different amplifier renderings)\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis dataset paper offers new, rich data on guitar playing, including guitar audio by different players, associated aligned tablature, and different amplifier renderings. I appreciate the care that seems to have gone into the creation of the dataset, and the way in which the authors really seem to consider actual playing considerations specific to the guitar.\n\nWhile I think this article would fit well at ISMIR and benefit the ISMIR community, I do have a few questions or remarks. Generally, as also acknowledged in the ethics statement and in the paper itself, use permission (and possible copyright/licensing issues) may be a possible problem for data like this. As mitigation, the authors indicate data only will be released for research purposes, and metadata removal/anonymization was performed as described in Section 3.2. I wonder to what extent that would be sufficient to avoid identifiability - and relating to this, what informed the choice of data in the first place.\n\nThe choice of data/songs is less clearly documented in the paper, which I again assume is for protection against possible copyright challenges, but some further justification would be welcome - why a cover of an existing piece, rather than e.g. new material that is not yet under protection, or test samples that represent playing in relation to tablature but would have less possible controversy, such as arpeggios, scales or other exercise fragments? What would generally be considered a comprehensive dataset in the eyes of the creator, and to what extent is GOAT meeting this in its choice of material?\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThis paper presents a new dataset that will clearly have value to the ISMIR community. While the reviews were not unanimous in their verdicts, in the discussion phase, no strong objections were raised against acceptance, but rather, the positive contributions of the paper were further emphasized. As such I consider it well above the acceptance bar, and recommend inclusion at ISMIR.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis is an interesting paper on a novel guitar transcription dataset, which provides both MIDI and Tab representations and is therefore useful for several task.\nThe variety of different file formats and the active attempt to ensure compatability with other research as well as the critical discussion in 4.3. is appreciated.\nI like the hypothesis-driven experimental design.\nThe paper is well structured and easy to read.\n\nI have the following comments / suggestions:\n\nIntroduction\n- \"(3) an evaluation of results\" -> evaluation of methods \n- preliminary results -> how strong of a contribution is this, explain why preliminary (maybe this term can be avoided to make a stronger point even if the method is improved later)\n\nSec 2.1.\n- the list \"[12][5][6][18][19]\" would benefit from a short list of general approaches for tab generation, to be a bit more specific\n\nSec 3.1.\n- \"community-created tablatures\" -> how are copyright concerns adressed?, same question for 3.2., first sentence\n\nSec 3.2.\n- give some insight which method for fine-aligning the MIDI notes to the performance was used\n- it is not clear, why the tuning was added as additional annotation\n\nSec. 3.3.\n- to what detail are re-amping model parameters stored as well?\n\nFig. 3\n- change y-axis to logarithmic scale (left three subplots)\n- in the right subplot -> add the number of no playing techniques as reference / for comparison\n\nSec 4.1.\n- you might consider adding subsubsections for better structuring\n\nSec. 5.1\n- \"Following [4] [2], we finetune ...\" > gie some more details about network architecture\n- \"its zero-shot learning capabilities\" > I think this needs some justification, why the same task (transcription) but on different instrument timbres is really zero-shot learning\n\nSec. 5.2.1.\n- \"The transcription results ... do show some promise\" > which metric values would you consider satisfactory for the task?\n- \"...it is possible for the model to simply learn ...\" > did you check, was that the case?\n- \"The model tends to overfit ...\" > can you share some quantitative evidence for this?\n\nSec. 8\n- \"...we intend to make the dataset ... upon request\" > is this allowed? Would you not need consent from the original artists / their labels?\n\nReferences\n- revise for consistent labeling of conference names ([5], [6] \"The ...\")\n- [8] has been published at ICASSP 2023 and should replace the arxiv pre-print\n- [15, 16] add page range\n- [17] -> give full name for PMLR\n- remove publisher for IEEE",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\n* GOAT fills a major gap in guitar MIR by offering real-world, expressive annotations beyond basic MIDI.\n* Careful collection, alignment, tone augmentation, and documentation practices make the dataset credible.\n* Proposing DadaGP-based audio-to-text transcription expands the MIR frontier, aligning with current trends in large language/audio models.\n* Clear structure, accessible explanations, thorough statistical analysis of the dataset.\n\nMinor Suggestions:\n* Since GOAT primarily uses covers and popular songs, it would be helpful to provide a short paragraph or table summarizing the genre/style diversity (e.g., proportion of rock, metal, pop, etc.) to give future users a clearer idea of dataset biases.\n* Even a tiny preliminary human evaluation (e.g., 5\u201310 samples rated for tablature transcription quality) could have illustrated the qualitative potential of the Whisper fine-tuning approach, alongside WER scores.\n* Showing a few sample outputs of the DadaGP transcription (both successful and failed examples) would help readers better understand the kinds of errors and structure the model is learning.",
      "review3": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nThis paper introduces GOAT, a large dataset of paired electric guitar audio and tablature annotations, created using real performances and extended through a comprehensive amplifier-based data augmentation pipeline. It provides real DI (direct input) recordings with symbolic Guitar Pro annotations and offers experiments for guitar MIDI transcription and automatic guitar tablature transcription (AGTT) using Whisper.\n\nStrengths: \nThe introductory sections and dataset description are clearly written with detail, statistics, and metadata provided.\n\nThe dataset provides annotation richness, the inclusion of expressive guitar techniques (e.g., bends, mutes, legato) in tablatures, is a valuable addition over typical MIDI-only datasets.\n\nSuggestions:\n\nThe real value lies in the dataset creation tool and pipeline, not necessarily the data itself. This distinction should be emphasized. A lot of existing datasets (e.g., GuitarSet, GAPS) could be transformed similarly. For all the midi transcription experiments it would be nice to also include ablation studies on the pipeline applied to other datasets, assessing actual performance improvements from this synthetic tonal variety. \n\nFor the MIDI transcription experiments, while the test set from GuitarSet is fair, I would suggest comparing with a separately created test set (recorded with real amps and effects, if possible), or at least processed with different effects and not the same pipeline. Even with proper splitting, having test data created using the same synthetic procedure as the training data might bias results, as the model could have learned artefacts specific to that pipeline. For example, in the case of AMP-XL, you might have seen more meaningful improvements if it had been tested on real data or data created using a different amping plugin.\n\nThe AGTT task is underspecified. The audio-to-text formulation of AGTT (automatic guitar tablature transcription) is novel, but it\u2019s not well-explained what this task is. If this is a first introduction of AGTT as audio-to-text, more explanation and justification is needed. For example, what makes DadaGP suited to Whisper? How are timing, rhythm, or note grouping handled in tokenization? What is the ultimate downstream application? This approach may confuse readers unfamiliar with Whisper, token-based encodings, or tablature formats."
    },
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Recent advances in audio-text large language models (LLMs) have opened new possibilities for music understanding and generation. However, existing benchmarks are limited in scope, often relying on simplified tasks or multi-choice evaluations that fail to reflect the complexity of real-world music analysis. We reinterpret a broad range of traditional MIR annotations as instruction-following formats and introduce CMI-Bench, a comprehensive music instruction following benchmark designed to evaluate audio-text LLMs on a diverse set of music information retrieval (MIR) tasks. These include genre classification, emotion regression, emotion tagging, instrument classification, pitch estimation, key detection, lyrics transcription, melody extraction, vocal technique recognition, instrument performance technique detection, music tagging, music captioning, and (down)beat tracking \u2014 reflecting core challenges in MIR research. Unlike previous benchmarks, CMI-Bench adopts standardized evaluation metrics consistent with previous state-of-the-art MIR models, ensuring direct comparability with supervised approaches. We provide an evaluation toolkit supporting all open-source audio-textual LLMs, including LTU, Qwen-audio, SALMONN, MusiLingo, etc. Experiment results reveal significant performance gaps between LLMs and supervised models, along with their culture, chronological and gender bias, highlighting the potential and limitations of current models in addressing MIR tasks. CMI-Bench establishes a unified foundation for evaluating music instruction following, driving progress in music-aware LLMs.",
      "abstract": "Recent advances in audio-text large language models (LLMs) have opened new possibilities for music understanding and generation. However, existing benchmarks are limited in scope, often relying on simplified tasks or multi-choice evaluations that fail to reflect the complexity of real-world music analysis. We reinterpret a broad range of traditional MIR annotations as instruction-following formats and introduce CMI-Bench, a comprehensive music instruction following benchmark designed to evaluate audio-text LLMs on a diverse set of music information retrieval (MIR) tasks. These include genre classification, emotion regression, emotion tagging, instrument classification, pitch estimation, key detection, lyrics transcription, melody extraction, vocal technique recognition, instrument performance technique detection, music tagging, music captioning, and (down)beat tracking \u2014 reflecting core challenges in MIR research. Unlike previous benchmarks, CMI-Bench adopts standardized evaluation metrics consistent with previous state-of-the-art MIR models, ensuring direct comparability with supervised approaches. We provide an evaluation toolkit supporting all open-source audio-textual LLMs, including LTU, Qwen-audio, SALMONN, MusiLingo, etc. Experiment results reveal significant performance gaps between LLMs and supervised models, along with their culture, chronological and gender bias, highlighting the potential and limitations of current models in addressing MIR tasks. CMI-Bench establishes a unified foundation for evaluating music instruction following, driving progress in music-aware LLMs.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Yinghao MA",
        "Siyou Li",
        "Juntao Yu",
        "Emmanouil Benetos",
        "Akira Maezawa"
      ],
      "authors_and_affil": [
        "Yinghao MA (Queen Mary University of London)*",
        "Siyou Li (Queen Mary University of London)",
        "Juntao Yu (Queen Mary University of London)",
        "Emmanouil Benetos (Queen Mary University of London)",
        "Akira Maezawa ( Yamaha Corporation)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Musical features and properties",
        "Representations of music",
        "Multimodality",
        "Evaluation, datasets, and reproducibility",
        "Evaluation methodology",
        "MIR fundamentals and methodology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1QgMxIA-e4ADZ_dII4_l0FXURVckY3uZA/preview",
      "poster_pdf": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-6-cmi-bench-a",
      "title": "CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction Following",
      "video": ""
    },
    "forum": "246",
    "id": "246",
    "pic_id": "",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe evaluation of multiple music-related LLMs in a broad set of tasks helps to understand the capabilities of such models, which are still far from optimal.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nA new dataset useful for finetuning and evaluating music-related LLMs, built as a reformulation of many MIR datasets in an instruction form.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper presents a new dataset created from several other MIR datasets by reformulating the tasks in an instruction form, suitable for finetuning and evaluating multimodal music-related LLMs. Then the authors evaluate a number of available models in the dataset. \nThe paper is well written and structured, and its main contribution\u2014the unification of MIR tasks into a standardized instruction-tuning benchmark\u2014is timely and highly relevant. The catalog of MIR tasks and LLMs surveyed is particularly useful to the community, and the benchmark aligns well with both NLP and MIR research interests. That said, there are areas that could be improved or clarified to strengthen the paper\u2019s long-term utility.\n- The evaluation of zero shot learning with LLMs is heavily dependent on the prompt used, which in this case, is determined by the authors. Many of the models evaluated may have been trained with different set of instructions, which makes difficult to really trust the results provided in the paper. In addition, there is no ablation study or prompt variants that help the reader to trust the decided prompts used by the authors. This dataset may be more useful for finetuning of newer models that follows the defined instructions.\n- The paper refers to CMI-Bench as a benchmark, but lacks a clear leaderboard or scoring protocol that would encourage external adoption. A discussion about future integration with platforms like HuggingFace leaderboards would help clarify its long-term role.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThe topic is highly relevant to the ISMIR community, especially considering the growing interest in applying LLMs to musical domains. The paper is well written and presents a substantial and clearly motivated contribution. Reformulating MIR tasks for instruction-based evaluation is a timely and important idea that responds to the way LLMs are increasingly used in practice.\n\nReviewers appreciated the breadth of the evaluation, the inclusion of multiple models and tasks, and the open release of the benchmark. The analysis of genre and cultural bias is also a welcome addition, but can be improved following the recommendations of reviewer #3.\n\nOne central issue is the lack of prompt ablation or prompt robustness analysis. Since the zero-shot results depend heavily on prompt wording, it\u2019s difficult to assess whether the poor performance observed in some tasks reflects model limitations or suboptimal prompt design. Including even a small prompt variation experiment would have helped to clarify this.\n\nAnother limitation is the lack of a formal leaderboard structure or evaluation protocol. While the dataset is positioned as a benchmark, it would benefit from clearer guidance to encourage adoption\u2014such as standard scoring procedures or integration with leaderboard platforms.\n\nSome reviewers also found that the results are under-analyzed, especially given the number of metrics and tasks. The discussion of failures (e.g., hallucinations, invalid outputs) is often brief or qualitative. More quantitative data\u2014for instance, the rate of invalid responses\u2014would make the analysis more useful. In addition, the comparison with previous studies that showed better LLM performance on music tasks needs to be better contextualized.\n\nFinally, a deeper reflection on the reliability of the underlying datasets, especially for subjective tasks like emotion annotation, would strengthen the benchmark's credibility. In several cases, it is unclear whether model \u201cerrors\u201d are due to actual model failures or limitations in the data itself.\n\nDespite these limitations, this paper makes a valuable contribution by providing the community with a reusable and extensible framework to evaluate LLMs on music-related tasks. While the methodology is still in early stages, the benchmark can serve as a foundation for future work, and will likely stimulate further discussion and experimentation in this space.\n\nI recommend acceptance, with the hope that the authors can expand on some of the open questions and strengthen the benchmark for broader adoption.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nThe paper is relatively well written and understandable. The amount of work presented is commendable, with 11 models and 20 different metrics applied to 14 different MIR tasks. Despite this impressive breadth, the paper remains readable and well structured. Section 2, although dense, offers a comprehensive state of the art. However, there are a number of typos (listed at the end of this review), and most of the figures and tables are difficult to read (I assume due to space constraints that led to font size reduction).\n\nThe authors propose using LLMs for a wide range of MIR tasks by reformulating task annotations into an instruction-following prompt paradigm to leverage the capacities of LLMs. Ultimately, \"all models in our study fall significantly short of the performance achieved by task-specific supervised systems when evaluated using standard MIR metrics.\"\n\nTo me, the paper exhibits two major flaws:\n\n1) The impact of the specific reformulations chosen in the paper on the results. If LLMs underperform so noticeably compared to task-specific SOTA models, is it solely due to the inherent limitations of LLMs and the explanations offered in the paper? Could different prompts have yielded better performance?\nA more systematic analysis of prompt engineering choices, or a prompt ablation study, would have been highly valuable to isolate the source of performance gaps.\n\n2) The comparison with previous attempts at music-related instruction-following tasks. Section 5.1.1 cites other papers in which LLMs achieved excellent results\u2014why do these discrepancies arise here?\n\nThe filtering of outputs produced by the selected LLMs (Section 4 Experiments) would have benefited from a more detailed analysis.\nFor downbeat tracking, the authors note: \"We filter non-numeric outputs\". How frequently do such outputs occur? The model was expected to produce a list of tuples only.\nSimilarly, for melody extraction: \"We discard invalid tuples (e.g., missing pitches, or improperly formatted entries, etc.).\"\nHow often do models fail to produce valid outputs? How frequent are hallucinations? To what extent does post-processing affect the final results?\n\nSection 5 Results is unfortunately hindered by the number of tasks being addressed simultaneously. Table 3 is not sufficiently referenced in the text. Given the number of metrics reported, it would be helpful for Table 3 to include arrows or annotations indicating which metrics are better when lower or higher.\nSubsection 5.1.3, \"All Models Perform Poorly on DSing Transcription\", fails to provide insight into why performance is so low.\n\nSection 5.2, Culture and Gender Bias, raises interesting issues but suffers from several weaknesses.\nAccordion is not an orchestral instrument.\n\"Performance drops significantly on bongo and harmonica -commonly associated with world, folk,\" Is this not simply because such instruments are underrepresented in the dataset? Is folk music really that rare in genre datasets?\n\nThe distinctions drawn are also inconsistent:\n\"Western genres (e.g., 80s, 90s)\" vs. \"music traditions (e.g., Medieval, 60s)\" : why are the 80s and 90s considered genres but the 60s a \u201ctradition\u201d?\nIs chanson considered world music?\n This section lacks both detail and quantitative results, although the topics discussed are undoubtedly of high relevance to the field.\n\n\nI would like to emphasize that my decision to recommend a weak reject is in no way due to the presence of negative results. On the contrary, negative or underwhelming results are important and valuable. The work presented is substantial and of genuine interest.\n\nHowever, the paper lacks fine-grained analysis of the results and shows little critical perspective on the design of the prompts\u2014an issue that, in my view, is insufficiently addressed in the paper, except briefly in Section 5.1.4.\n\nMinor remarks\nFigure 1, Figure 2, Table 1, and Table 2 are nearly illegible.\n\nl.141: \"sequential or sequential tasks\" -> repetition\n\nl.174/175: \"seuqen-tiall\" -> typo\n\nl.232: \"tupiles\" -> should be \"tuples\"\n\nTable 2: Checkmark and cross symbols are visually confusing\n\nl.326: \"Trainingset\" -> spacing issue\n\nl.330: \"generalization.Qwen2-Audio\" ->missing space\n\nl.365: \"While, different\" formulation is strange\n\nl.424+: \"Audio-Flamingo\u2019s performance on Bossanova and Chanson drops severely, respectively.\" \u2192 \"respectively\" is misused here",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nI think this paper will generated lots of discussion, and its topic is central to current conversations about music LLMs.",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe increase in popularity of LLMs, and their recent adaptation for various music-related tasks certainly makes this work one with a timely topic. As popular as LLMs are, a consistent challenge is figuring out how to measure their performance. Thus, contributing to the evaluation of LLMs for MIR tasks makes this topic even more relevant at ISMIR. \n\nI find the approach of instruction-following to be an interesting method for developing the benchmark, in particular when the output is a number within a range on a scale. I do miss an assessment of test-retest reliability of LLM responses, however. As they are stochastic by definition, I would be curious to also see how consitent they are. There are energy costs to this of course, but one might interpret the results of a top performing model differently if its output varies substantially when given the same input multiple times. As I also expect that this variance will vary based on LLM and task, I feel it would add substantial resolution to the results. Of course, I acknowledge the limited space and the necessity for the page-long table to show results. \n\nThe work further allows for an initial view on the state of LLMs applied to MIR related tasks. There are some clear successes and failures, which I expect will be of interest to our community. \n\nI further appreciate the lack of overinterpretation of results that is so common in LLM work."
    },
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Foundation models have become increasingly prevalent in tackling Music Information Retrieval (MIR) tasks. Although they can be a powerful tool for understanding music, the computation required for the training and inference of these models continues to grow as they become more complex. Specialized acceleration, such as Graphical Processing Units (GPUs), has become necessary for operating these models, as they are mostly based on large Deep Learning (DL) architectures. Furthermore, it is difficult for users to interpret them due to their black-box nature. In this work, we propose Quantizers and Factorizers for Music embeddings (QFM), a fast, unsupervised audio representation for music understanding backed by a wide range of rich MIR features and efficient feature learners. Experimental results show that QFM models perform within the range of results achieved by recent previous open source DL models on all evaluated tasks, with competitive results on a subset. This is surprising given the significantly smaller computational requirements of QFM models for training and inference.",
      "abstract": "Foundation models have become increasingly prevalent in tackling Music Information Retrieval (MIR) tasks. Although they can be a powerful tool for understanding music, the computation required for the training and inference of these models continues to grow as they become more complex. Specialized acceleration, such as Graphical Processing Units (GPUs), has become necessary for operating these models, as they are mostly based on large Deep Learning (DL) architectures. Furthermore, it is difficult for users to interpret them due to their black-box nature. In this work, we propose Quantizers and Factorizers for Music embeddings (QFM), a fast, unsupervised audio representation for music understanding backed by a wide range of rich MIR features and efficient feature learners. Experimental results show that QFM models perform within the range of results achieved by recent previous open source DL models on all evaluated tasks, with competitive results on a subset. This is surprising given the significantly smaller computational requirements of QFM models for training and inference.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Jaehun Kim",
        "Matthew C. McCallum",
        "Andreas F. Ehmann"
      ],
      "authors_and_affil": [
        "Jaehun Kim (Pandora / SiriusXM)*",
        "Matthew C. McCallum (Pandora / SiriusXM)",
        "Andreas F. Ehmann (Pandora / SiriusXM)"
      ],
      "channel_url": "",
      "day": "4",
      "keywords": [
        "Musical features and properties",
        "Representations of music",
        "Automatic classification",
        "Music signal processing",
        "MIR tasks",
        "Machine learning/artificial intelligence for music",
        "MIR fundamentals and methodology",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/14hmzfUcf0KvWEo1TC1QNBwFjwrRO5qap/preview",
      "poster_pdf": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-7-quantize-factorize-a",
      "title": "Quantize & Factorize: A fast yet effective unsupervised audio representation without deep learning",
      "video": ""
    },
    "forum": "247",
    "id": "247",
    "pic_id": "",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nStrongly agree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nNo\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nBy showing that multiple downstream tasks can work well on a set of selected MIR features, the paper gives insights into which features could be selected for MIR tasks. Also it can raise ideas on how to make existing DL-based representation learning methods more efficient\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nA music representation based on MIR features (instead of deep learning) can provide competitive downstream task performance, while offering faster training and inference.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper proposes an unsupervised music representation for which training and inference is faster than current DL-based methods, while offering similar performance. It also claims to provide a generic architecture in which feature engineering and ML can be used side-by-side in future MIR work.\n\nThe paper is well written, the proposed system is sensible and reasonably novel (parts of the system existed before, but it was not used in this context to my knowledge) and the experiments are conducted rigorously, proving the paper's claims described above to a large extent. The efficiency claims would be better supported if there was an analysis of training and inference cost (in real-world currency, or FLOPS). Also, GPU acceleration is not used, and the benefit of the proposed system depends on how costly GPU usage is compared to the author's CPU setup - as GPU acceleration would substantially reduce training and inference time (and perhaps cost) for the approaches the paper is using for comparison, but likely not for the proposed system.\n\nAs a more minor point, the paper also suggests non-DL representations like the proposed one could be more interpretable, but unfortunately does not include any analysis on this.\n\nMinor remarks:\n- Some links in the references are broken, sometimes entries are incomplete (e.g. missing authors)\n- Figure 1 could be interpreted as factorization being applied for each audio feature in each chunk independently, but in Section 3.2 WMF is applied to the whole dataset, this should be clarified more\n- L33 \u201chandful of works\u201d implies citing more than one paper\n- L164 - I assume the audio features\u2019 mean and standard deviation, not the audio chunk waveform itself?\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nSummary of Reviews and Discussion\n\nThis paper proposes an unsupervised music representation learning pipeline that combines classical MIR features with quantization and matrix factorization, offering a computationally efficient alternative to deep learning-based models. Reviewers generally found the approach well-motivated, the writing clear, and the empirical results compelling, particularly given the simplicity of the method.\n\nHowever, all reviewers noted a lack of methodological detail, particularly in the description of the Quantization-Factorization module. Clarifications on hyperparameters, model architecture, and mathematical formulation are needed, and several figures need to be clarified.\n\nAll reviews provided a \"weak accept\" recommendation.\n\nFinal Recommendation\n\nI recommend accepting this paper to ISMIR. Despite the noted lack of clarity, the paper offers a timely and thoughtful contribution to the ISMIR community. Its practical relevance, solid experimental results, and potential to broaden the conversation around music representation learning justify its inclusion. The authors are encouraged to enhance methodological clarity in the camera-ready version.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper aims to improve music foundation models without the usage of deep neural network, and shows improvements over some of the deep learning based method like CLMR. FM without deep learning is a highly underresearched topic. Even though the results are not STOA, it is still very impressive and provides helpful insights to the community.\n\nThe model choice (quantization+factorization) is also reasonable since it retains compact sequential information. Although I do wish the author could explain more the design choice. See comment #5.\n\nMy main concern is about the description of the method. The author skips all details on the QF model, leaving only citations, making this paper very hard to understand even with traditional machine learning background. The paper definitely can describe all methods in detail potentially with formula to reduce confusion (currently there is none), and shrink the length of the experiments (Fig. 2 & 3 definitely occupied unnecessarily much space).\n\nOther comments:\n\n1. Fig. 1: What is a ZScore in Fig. 1? Also in Fig. 1 KMeans should be \"KMeans/GMM.\" Also, the node QF_total should connect to all feature_{1...N} instead of each QF local block.\n\n2. In sec 3.2: there are lots of important hyperparameters in the NGram and WMF model, but are never described in the paper.\n\n3. Line 164: The description of G1: It calculates \"...each audio chunk\u2019s mean and standard deviation.\" I assume the author wants to say the feature of each audio chunk, instead of the raw audio content.\n\n4. How did you train the model? What are the training hyperparameters? I assume that the KMeans/GMM, NGram and WMF all require training and they are trained sequentially using different algorithms. More clarification is needed.\n\n5. It would be better if the author explained the model choices in their introduction: (1) Why quantization? (2) Why use NGram+WMF?\n\nOverall, I think the paper is a clear accept seeing the result, but more refinement is required.",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper presents a method to leverage shallow feature learners (read classical MIR features) to compete with large foundation models. The authors present a Quantization-Factorization module which operates on groups of feature sets to produce \"embeddings\" for music in an unsupervised manner. The resulting embeddings can be used across several downstream MIR tasks achieving comparable (though slightly worse) metrics when compared to significantly larger and computationally expensive large foundation models for music. \n\nStrengths:\n- Well-motivated and clearly written. \n- Shows that there is still room to leverage shallow feature learners in the context of MIR.\n- Evaluation and ablation studies are well presented\n\nWeaknesses:\nThe main drawback of the paper is that the method section needs to be described a little bit better. There are some technical aspects that are not clear to the reader. \n- The temporal resolution of the feature vectors seems inconsistent. It would seem like for every 9 second chunk, each feature set has a resolution of 43 ms (22050 / 512). However, that doesn't seem to be the case for the Patches feature set (which is randomly sampled across the entire 9 second chunk)\n- The process of converting the codes to the unigram matrix as well as the WMF step can be explained a bit better. The authors should consider adding some mathematical notations to help the reader better understand the different steps within the QF module along with how the dimensionality of the computed features / final embeddings progresses through the different sub-modules (starting from an audio chunk of say length N). \n- Since WMF is such a core part of the proposed method, the authors should provide some background and description about the method. Simply referring to prior work is not sufficient in this case. \n- If space if a concern, the experimental set-up (downstream datasets and metrics) can be compressed by the moving the details to an Appendix / Supplementary material. \n\nThere are also a couple of questions related to the experiments that the authors should clarify:\n- It is not clear why the final PCA and QF_total modules were excluded for the ablation experiments in Section 5.3.1.\n- One thing that seems to be missing in the ablation study is the influence of the G1 modules. While the authors report the metrics just using the G1 module (as a baseline), it would really interesting to report the results of the QFM models with the G1 modules removed. \n\nOther minor comments:\n- Line 127: Typo: `a quantization` -> either `a quantization module` or `the quantization module`\n- Line 164: Instead of `audio chunk` it should be `feature sets'`\n- Line 210: Should be `(micro, nano)`\n- Line 243-244: It is a little weird for the default chunk time to be set based on the default setup of the Tempogram features. Isn't that configurable? \n- Line 295-296: 60 (and 200) vectors are sampled from all the vectors in a chunk. It might be useful to add the total number from which these are sampled\n- Line 433: \"when computation requirements dictate, a primary embedding\". it's not clear what primary embedding means here?\n- There seems to be a missing reference to the footnote below Line 474",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nIn this paper, the authors compare recent deep-learning foundation models with a feature fusion method of classic music features through quantisation and factorisation. A set of features are first separately quantised using KMeans and then factorised using Weighted Matrix Factorisation. The resulting embeddings are then concatenated with the mean and standard deviation of the original features. Finally, feature fusion is achieved by concatenating the single embeddings and applying PCA. The model is trained on the FMA and Million Song Dataset and evaluated on standard downstream tasks through probing. The authors show that for some downstream tasks, this approach produces results comparable to much more complex deep-learning based foundation models.\nThe paper is well-written and easy to follow. The proposed method is described in detail and the experimental setup is technically correct. However, I would recommend that the authors improve the readability of Figures 2 and 3, or consider reporting the results in two tables. Strictly speaking, the paper's novelty is somewhat limited, but I believe it could stimulate potential discussions in the community.\nGenerally, I think the authors missed an opportunity to reflect on why such a simple approach (and sometimes even the considered baseline) can compete with much more complex deep-learning foundation models, especially for some tasks. Such an analysis might highlight potential weaknesses of the popular probing-based evaluation or of the considered datasets, and lead to potential improvements in the evaluation of music foundation models in general."
    },
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Loops--short audio segments designed for seamless repetition--are central to many music genres, particularly those rooted in dance and electronic styles. However, current generative music models struggle to produce truly loopable audio, as generating a short waveform alone does not guarantee a smooth transition from its endpoint back to its start, often resulting in audible discontinuities.\nWe address this gap by modifying a non-autoregressive model (MAGNeT) to generate tokens in a circular pattern, letting the model attend to the beginning of the audio when creating its ending. This inference-only approach results in generations that are aware of future context and loop naturally, without the need for any additional training or data. We evaluate the consistency of loop transitions by computing token perplexity around the seam of the loop, observing a 55% improvement. Blind listening tests further confirm significant perceptual gains over baseline methods, improving mean ratings by 70%. Taken together, these results highlight the effectiveness of inference-only approaches in improving generative models and underscore the advantages of non-autoregressive methods for context-aware music generation.",
      "abstract": "Loops--short audio segments designed for seamless repetition--are central to many music genres, particularly those rooted in dance and electronic styles. However, current generative music models struggle to produce truly loopable audio, as generating a short waveform alone does not guarantee a smooth transition from its endpoint back to its start, often resulting in audible discontinuities.\nWe address this gap by modifying a non-autoregressive model (MAGNeT) to generate tokens in a circular pattern, letting the model attend to the beginning of the audio when creating its ending. This inference-only approach results in generations that are aware of future context and loop naturally, without the need for any additional training or data. We evaluate the consistency of loop transitions by computing token perplexity around the seam of the loop, observing a 55% improvement. Blind listening tests further confirm significant perceptual gains over baseline methods, improving mean ratings by 70%. Taken together, these results highlight the effectiveness of inference-only approaches in improving generative models and underscore the advantages of non-autoregressive methods for context-aware music generation.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Davide Marincione",
        "Giorgio Strano",
        "Donato Crisostomi",
        "Roberto Ribuoli",
        "Emanuele Rodol\u00e0"
      ],
      "authors_and_affil": [
        "Davide Marincione (Sapienza University of Rome)",
        "Giorgio Strano (Sapienza University of Rome)",
        "Donato Crisostomi (Sapienza, University of Rome)*",
        "Roberto Ribuoli (Sapienza University of Rome)",
        "Emanuele Rodol\u00e0 (Sapienza University of Rome)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Evaluation metrics",
        "Generative Tasks",
        "Music and audio synthesis"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/14Y348WifQi_a8ZQWDv5NQe133Z-NxovU/preview",
      "poster_pdf": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-5-loopgen-training-free",
      "title": "LoopGen: Training-Free Loopable Music Generation",
      "video": ""
    },
    "forum": "248",
    "id": "248",
    "pic_id": "",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nDisagree\n\n**Q5 ( Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chosen, otherwise write \"n/a\"))**\n\nWhile a minor issue, there is prior work that does non-autoregressive music loop generation. See \u201cDITTO: Diffusion Inference-Time T-Optimization\u201d. https://arxiv.org/abs/2401.12179. The overall idea is generally similar, but this work uses inference-time techniques for standard diffusion models (not on MAGNET) which would also be considered a non-autoregressive (NAR) model.\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe reusable insight is 1) focused view on how to modify MAGNet style non-AR generation methods at inference-time to better create musical loops 2) Proposal to use time-signature-aware length control 3) a proposed evaluation metrics for measuring loop seems.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nModify MAGNet-style non-autoregressive music generation at inference-time to generate loops with more seamless loop boundaries together with a beat-aware algorithm improvement and new eval metric.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nSummary: \nIn this work, the authors present a new algorithm to extend the music generation method of MAGNet to better generate seamless musical loops. MAGNet is a non-autogressive generation technique distinct from LLM-based generation and diffusion models, but similar in spirit to the latter. In addition to the main modification to the MAGNet inference-time generation algorithm the authors propose time-signature-aware length control modification to better focus on loops with proper meter/tempo and a new evaluation metric for measuring seamlessness of musical loops.\nMajor/minor comments:\nOverall, very nice work! The focus and application of music generation for real musician workflows of using and creating loops is very nice. The work is well organized at a high-level and has well written sentence structure at a low level. In terms of the propose work, 1) the proposed algorithm is relatively clear at a high-level 2) the beat or signature-aware length control is very nice and 3) the motivation of the proposed evaluation metric is very nice as well. \nIn terms of areas for improvement, \n1) Although the high-level details of the proposed algorithm are explained well in prose and the accompanied supplementary pdf with code, the low-level details of the proposed algorithm are a little vague. Much of the low-level details are simply deferred to the MAGNet paper/code. Ideally there is an official \u201calgorithm\u201d, more details on the model, inference speed, etc.\n2) The evaluation metric of \u201cseam perplexity\u201d is very nice, but ideally is a little more clear to improve the reusable insight. In section 5.1.1, the phase \u201ca well-trained model M assigns a probability M(x_i) to each token x \u2013 this phrase implies that the metric only works for generation techniques with discrete token generation and not diffusion-based models that use continuous latents. Please confirm. Furthermore, it is unclear why the time-window around the seam is only forward-looking and not zero-centered around the seam. Please explain.\n3) Please see the past work of \u201cDITTO: Diffusion Inference-Time T-Optimization\u201d. https://arxiv.org/abs/2401.12179. The work proposed an inference-time algorithm for loop generation using pre-trained non-AR models (diffusion models). This is a minor issue to just the related work and how the method is related to other generation techniques including LLM-based loop generation, diffusion-based loop generation, and MAGNet-style generation.\n4) The evaluation is very small scale and only done with 100 text prompts. This is so small that most evaluation metrics, particularly FAD_vggish have very high variance and likely wiggle around a lot solely based on the small evaluation sample size.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nSummary: In this work, the authors present a new algorithm to extend the music generation method of MAGNet to better generate seamless musical loops. MAGNet is a non-autogressive generation technique distinct from LLM-based generation and diffusion models, but similar in spirit to the latter. In addition to the main modification to the MAGNet inference-time generation algorithm the authors propose time-signature-aware length control modification to better focus on loops with proper meter/tempo and a new evaluation metric for measuring seamlessness of musical loops.\n\nInitial Scores: 1 strong accept, 2 weak accepts, 1 strong reject\n\nMetareview: Overall, the reviews are generally positive. In summary, there are three main topic areas for improvement\nIssue #1 - R2 Intuition of \"Section aware length control (section 4.4)\" is unclear\nIssue #2 - R1, R2 Validity of proposed eval metrics\nIssue #3 - R1, R3, and MR - Commentary on related work could be improved.\n\nDiscussion: The discussion was brief and focusing on clarification of #1 and consensus that #2 would likely be the best area for improvement. From the perspective of the reviews and overall contribution, however, I recommend these issues don\u2019t hold back the paper as they can be mostly all addressed via light editing.\n\nRecommendation: Accept",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe paper describes an inference based technique for generating loops by controlling the generation of tokens of a non autoregressive music generator. The approach is sound, the different design options evaluated make sense and are evaluated with FAD and the authors performed a subjective evaluation to demonstrate that their deisgn of choice is better than a strong baseline.\n\nMy main concern is the proposal of the perplexity metric. It seems to be overcomplicated, plus the authors do not evaluate its effectiveness on a controlled dataset, plus they do not give it any credit to decide which algorithm design is the best.\n\nThe authors argue that FAD is not suitable, but I fail tu understand why a FAD computed on 1) circular shifted by half the length or 2) on x times repeated loops would not be effective. If the looping lead to artefacts that are or not seen in the reference dataset, this will induce a systematic distribution shift that will be captured by the Fr\u00e9chet metric.\n\nMinor comments:\n\n\"A low seam perplexity indicates that the seam is \u201ceasy\u201d for a strong reference model to predict, suggesting a smooth transition. Conversely, a high value suggests abrupt discontinuities or other artifacts at the loop boundary\": demonstrate validity on two examples with spectrograms and a small two classes dataset\n\n\"via a LLM\": which one ?\n\n\"and what, going forward, we call LoopGen.\" Consistency in naming is mandatory in a technical paper. \n\n\n\n\"However, this adjustment has minimal impact on the overall mean ratings for both models: indicating that ratings are stable between users.\" If it is not needed, processing of ratings shall be avoided.\n\n\"perceptibility of the seam, as can also be seen in Figure 5.\" From reading it is unclear if the authors are refering to a spectrogram of the seam or the rating's analysis.",
      "review2": "- **Overall Evaluation**: Strong reject\n\n#### Main Reviews\n\nThis paper identifies a gap in the ability of generative models to create loops that can repeat seamlessly and also proposes a strategy to make the duration of these loops amenable to production settings via a \u2018beat-aware technique\u2019. This method uses a circular padding technique that allows the end of the loop to attend to the beginning and the beginning to attend to the end during inference. The motivation is compelling and the solution being inference-only is also powerful as it becomes scalable to multiple pre-trained large models. The code for this project is also provided which is much appreciated.\n\nI think the paper presents an interesting idea, however I believe some important details need to be sorted out to ensure scientific and technical soundness. I have discussed these aspects below. \n\nSection aware length control (section 4.4): My understanding of the motivation for this method is that the generative model, MAGNeT, generates tokens (which can be thought to operate in the time domain) whereas musicians or humans think of loops in terms of bars. In order to match these two units, the authors propose a method to calculate the number of seconds of audio to generate given an initial audio prompt. They do this by calculating the BPM in the initial audio prompt and making the assumption that the generated sample will follow the same tempo and calculate the duration of generation given a user-defined \u2018preferred number of bars\u2019, n. I don\u2019t understand the intuition behind doubling or halving the duration in Algorithm 1. I understand that there are constraints on the duration of audio that can be generated, perhaps based on external factors such as compute. However from a user point of view, I expect that halving or doubling the number of bars that is recommended would be quite confusing. Perhaps there is some intuition that I am missing, which I recommend is more explicitly stated in the paper. \n\nAnother minor point: Is this technique addressing \u2018coherency\u2019 of the repetition (line 62)? To my understanding, based on section 4.4, this method simply allows for the generated loop to fit in an integer number of bars. This feels more like a method to make generated samples more amenable to real-world production use cases. The coherency of the loop generated is addressed by the tiling method proposed. \n\nRelated Work: There is another line of work that tries to make existing loops vary or react to a musician as seen with Vampnet [1] (as you have already mentioned), and Reflexive Looper [2]. Drawing the distinction that you are trying to in fact generate these loops as opposed to varying an existing loop would be nice. \n\nSeam perplexity: Cross-entropy and perplexity are both measures used to evaluate how similar two distributions are. The definition of cross entropy in equation 1 is incorrect as it isn\u2019t taking into account the true distribution p. The equation holds good if one has ground truth tokens and x_i is from the ground truth sequence. However in your case, there is no ground truth and thus this equation doesn\u2019t refer to cross entropy and by extension perplexity. An alternative measure that could be used with only one distribution is perhaps the entropy. I also wonder if one could define some hand-crafted metrics such as beat continuity, harmonic consistency based on chroma at the seam to check the continuity of the loop.\n\nAdditionally, the duration of the window used to calculate \u2018seam perplexity\u2019 and how that value was decided upon is not discussed anywhere. Since this is a crucial hyperparameter, it should be discussed in the text.\n\nExperiments:\n\n4 a. Text prompts: How are the textual prompts generated? What was the goal of creating the prompts? Were there considerations to ensure diversity among prompts, are there certain use cases (live performance, studio settings), particular instrumentation, tempos that you considered while making these prompts. I think this information should be discussed in the text since all the generations are conditioned on these prompts.\n\n4 b. FAD: What was the duration of generated samples considered when computing FAD. Were the samples looped, if so how many times (what was the duration of the samples)? And how did that duration compare to the ground truth data from FMA-Pop? Additionally, I wonder if there may be a better suited loop-based dataset to better highlight the ground truth distribution in the FAD comparison. Maybe the datasets in LoopNet [3], that you have cited in your related work or Freesound [4] might be helpful.\n\n4 c. Seam Perplexity: Seam Perplexity is used as a measure to compare between tiled and hybrid tiled models even thought lines 385-389 suggest that the hybrid tiled variant has a higher value because tokens are drawn from two different distributions. In this case, this feels like an unfair measure to use and I wonder if alternatives can be used such as beat-continuity or chroma-based measures to check the continuity of the generation at the seam (as suggested above). \n\n4 d. Table 1: I think the table would be much easier to digest as a line plot with w on the x axis and FAD / Seam Perplexity on the y axis. Additionally the caption can be more informative.\n\n4 e. Table 2: What is the difference between Vanilla and Naive versions of the model? If Naive is just a looped version of vanilla, why are the FAD CLAP scores slightly different? This should be made clear.\n\nI also wonder if there can be a stronger baselines established in the comparisons stated in Table 2 to make a more compelling argument. Perhaps a generative model fine tuned on a loop dataset could replace the naive MAGNeT and MusicGen models. Additionally a ground truth dataset of loops could be the \u201cgold standard\u201d or a possible ceiling on metric values that we could look for. \n\n4 f. Fig 4: It is quite difficult to read figure 4 especially without a y-axis and overlapping charts. I also believe that the content of this figure is already conveyed in Table 2 with the Seam perplexity column. \n\n4 g. Fig 5: Figure 5 is quite confusing as the bar considers the x-axis to be discrete values (understood by the different coloured bars corresponding to the same number being present on either side of that value) but the lines are drawn considering the axis to be continuous. Additionally I think error bars on the bars should be present to convey a clearer picture.\n\n4 h. Samples: In the samples provided, it would be nice to be able to predict baseline and loopgen generations with the same \u2018conditional audio prompt, c\u2019. I believe only \u2018rock\u2019 was common in the examples provided. \n\n\n[1] Garcia, Hugo Flores, et al. \"Vampnet: Music generation via masked acoustic token modeling.\" ISMIR (2023).\n[2] Pachet, Fran\u00e7ois, et al. \"Reflexive loopers for solo musical improvisation.\" Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 2013.\n[3] Chandna, Pritish, et al. \"Loopnet: Musical loop synthesis conditioned on intuitive musical parameters.\" ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021.\n[4] freesound.org",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nStrengths:\n1. The paper is well written and easy to follow, with clear and informative figures that effectively illustrate the proposed method.\n2. The experimental setup is sound, and the proposed evaluation metric (seam perplexity) is well motivated and aligns with the goal of loopable audio generation.\n3. The paper introduces a new task\u2014generating loopable music content from scratch using a pre-trained model\u2014with an elegant and reusable inference-time solution.\n\nWeaknesses and Comments:\n1. While the specific formulation of \"generating loopable content *from scratch*, leveraging pre-trained text-to-music model\" is a valuable refinement, the task itself is only moderately novel. Prior works like VampNet have already explored loopable audio generation, albeit from audio prompts rather than text.\n2. The notation \u03bb is reused in different contexts (classifier-free guidance and beat duration), which may cause confusion. It would help to disambiguate or re-label one of the usages.\n3. In the demo samples, different models are conditioned on different prompts, making it difficult to compare their outputs fairly. Using the same prompts across models would provide more direct and interpretable comparisons."
    },
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Raga classification in Indian Art Music is an open set problem where unseen classes may appear during testing. However, traditional approaches often treat it as a closed set problem, rejecting the possibility of encountering unseen classes. In this work, we first employ an Uncertainty-based Out-Of-Distribution (OOD) detection, given a set containing known and unknown classes. \nNext, for the audio samples identified as OOD, we employ Novel Class Discovery (NCD) approach to cluster them into distinct unseen Raga classes. We achieve this by harnessing information from labelled data and further applying contrastive learning on unlabelled data.  \nWith thorough analysis, we demonstrate how different components of the loss function influence clustering performance and how varying the openness affects the NCD problem in hand. ",
      "abstract": "Raga classification in Indian Art Music is an open set problem where unseen classes may appear during testing. However, traditional approaches often treat it as a closed set problem, rejecting the possibility of encountering unseen classes. In this work, we first employ an Uncertainty-based Out-Of-Distribution (OOD) detection, given a set containing known and unknown classes. \nNext, for the audio samples identified as OOD, we employ Novel Class Discovery (NCD) approach to cluster them into distinct unseen Raga classes. We achieve this by harnessing information from labelled data and further applying contrastive learning on unlabelled data.  \nWith thorough analysis, we demonstrate how different components of the loss function influence clustering performance and how varying the openness affects the NCD problem in hand. <br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Parampreet Singh",
        "Adwik Gupta",
        "Aakarsh Mishra",
        "Vipul Arora"
      ],
      "authors_and_affil": [
        "Parampreet Singh (IIT Kanpur)*",
        "Adwik Gupta (IIT Kanpur)",
        "Aakarsh Mishra (IIT Kanpur)",
        "Vipul Arora (IIT Kanpur)"
      ],
      "channel_url": "",
      "day": "4",
      "keywords": [
        "Representations of music",
        "Automatic classification",
        "Music signal processing",
        "MIR tasks",
        "Pattern matching and detection",
        "Machine learning/artificial intelligence for music",
        "MIR fundamentals and methodology",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1u-PAxmpxKCu1xn3SCc38LkGhOgl6sQu7/preview",
      "poster_pdf": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-8-identification-and-clustering",
      "title": "Identification and Clustering of Unseen Ragas in Indian Art Music",
      "video": ""
    },
    "forum": "256",
    "id": "256",
    "pic_id": "",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nDisagree\n\n**Q5 ( Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chosen, otherwise write \"n/a\"))**\n\nI feel it misses the papers on Carnatic music: Sankalp Gulati et al, Shrey Dutta et al. on \"Raga ID\" for Carnatic Music. Hindustani music is not very different from Carnatic music.\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nStrongly agree\n\n**Q10 (Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chose, otherwise write \"n/a\"))**\n\nDividing HM excerpts into 30 s chunks is totally incorrect. One should segment at the level of gat. Phrases purvanga and uttaranga can be distinctly different for the same raga -- by forcing a match is terribly wrong. In Misra ragas there could just a phrase that comes from a different raga. You need to segment this at the phrase level not arbitrarily at 30s intervals.\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nI think the insights are pretty wrong. Let me give examples: Ragas in CM Sriranjani- Abhogi, kirvani-simhendramadhyamam. Similarly bimpalasi-bageshri from Kafi thaat for HM. What about the thaats, and the putra ragas of the corresponding thaats? Some details MUST be given -- while they have stated that it does not work well for ragas belonging to the same thaat -- some explanation of why -- is required.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nIdentification of unseen ragas\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nI feel the authors do not have an idea of IAM at all. IAM like Indian languages are phrase structured. You can not pick up 30s sections and believe that they represent the raga. Also purvanga and uttaranga -- even if they belong to the same raga can have completely different movements.\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nWhile the idea is new, the work does not use any culture specific methodology. My recommendation: Reject. Such papers should not be given importance even if there is novelty in terms of technology. It has to relate the music and the science of the art form. This is completely missing.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\n- This paper discusses OOD and NCD methods for Indian Raga classification in the wild. The experiments are scientific, and the results are promising on the public saraga and PIM datasets. \n- The paper is a little hard to follow considering multiple experiments and results. Consider restructuring and proof-reading for better readability.\n- Please consider making the models, training and inference code opensource. This will help the community as you have leveraged opensource datasets.\n- Typo in the first line of sub-subsection'4.3.2 Open-ness'",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThanks for the great work. I agree that the type of methods presented in this paper are necessary for those wishing to study hindustani raga computatonally at scale - a task for which OOD samples are common and expected. I have selected a weak accept because I think the methodology is novel in this context, necessary, and yields decent results. However I would request the authors to make some quite significant additions/changes to help with the reproducibility and interpretability of the study...\n\n- Most importantly, you present a very multi-facceted approach that requires many design decisions. However you provide little explanation of how you made those. A non-exhaustive list would be...\n- selecting K for k means and for UMAP\n- the cosine similarity clustering threshold\n- model hyperparameters\n- why chromogram features?\n- etc...\n\n- You include no comparison to existing raga classification studies, even though you use a dataset from a raga classification paper. This combined with a poor description of design decisions makes it hard to evaluate the value of the trained model.\n- looking at the results quickly in [6] I dont think you achieve a higher f1. You need to be explicit as to why even with that being the case, this paper constributes a valuable contribution (namely the ability to handle out of distribution samples).\n\n- The structure of the paper could be improved.\n- Figure 1 could be bigger and more explicative. You present a process with many parts, think about making this diagram a lot clearer, bigger, and use labelling that corresponds with the text in the methodology section\n- Algorithm 1 is included but not introduced or referenced in text\n- does algorithm 1 cluster? it seems like it is the training process not the prediction process?\n- some terms in algorithm 1 are not defined, e.g. x_i_u \n- if you are to include it make it clear and a valuable addition\n- be clearer about exactly what data is being used for training and testing and how big it is, how many ragas it includes etc..\n- consider separating experiments out into named experiments: introduce them briefly in some summary introduction to section, explain them in detail in their respeective subsections, reference them by name in results section\n\n- Please consider including the code to reproduce this analysis or at the very least allow others to use your model. You may also want to consider including some more detailed results in that repository.\n\n\nSome smaller comments....\n\n- Line 209: finally\n- line 384: openness\n- line 380-383: sentence is hard to parse. perhaps an incorrect use of wherever.\n- line 385: colon at start of sentence\n- The hindustani dataset in saraga is quite noisy, how did you clean the data or exclude non melodic instruments?\n- presumably the entire work is based only on hindustani ragas. You could be explicit about this since Saraga contains many carnatic performances.\n- What about audio style. Would this model work on reordings with just a vocalist? What about with/without a harmonium etc... Maybe you can comment on the nature of the two datasets and its implication for using this model in the real world\n- You make no comment on the scalability using pairwise distance metrics - how does this influence training time/prediction time. It doesn't take many samples for relying on pairwise distancing to become infeasible. \n- If you have expertise on hindustani music perhaps you could use the extra space you have to talk a bit more about some of the results from a musicological perspective. Why does the model get confused in the cases where it predicts incorrectly? You mention thaats very briefly but at this point have given no introduction to what a thaat is (nor pancham, or aaroh). If you have a bit of space left think about refining this section, it may be useful for other researchers/musicologists.\n- btw, you could probably free up some space by reducing the amount of subsections you have (see: sec. clustering and sec. evaluation)",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper presents a thorough and interesting method to classify/cluster future Ragas. The experiments appear thorough, and a lot of important information is scattered throughout the manuscript. It would be tremendously beneficial to have a flowchart or table with all experiments, listing which parts of f, g and g\u2019 are used where, and which datasets were used for (pre-)training, testing and obtaining embeddings. It would also be nice if all symbols (i.e. x, y, z) appeared in the listing for Algorithm 1.\nFor someone less versed with Raga, it would be good to know what kind of Raga classes a classifier would classify them into. Is there a hierarchy (i.e., general Ragas and subclasses of them)? The introduction states that a Raga is a distinct set of notes, so is the idea to classify unseen set of notes simply as Ragas or certain sub-types of Ragas? This becomes clearer much later around line 273, but could be mentioned earlier.\nThere are quite a lot of errors in sentences and inconsistencies in labeling (e.g. f(.) vs f(\u00b7)), please do another round of proof-reading.\n\n48: Even though \u201ctarget classes <= training classes\u201d is a common assumption in NCD, this directly contradicts \u201cthe number of Ragas is not fixed\u201d (33). But I am assuming that the model will have a certain \u201ctarget space\u201d for unseen targets that theoretically enables it to recognize any unseen Raga class when it is run individually?\n143: feature extractor has an error in the \u201cdenotion\u201d\n152: unclear; in 142 it was said that the softmax layer is removed\n185: what is an audio chunk?\n214: The criticality of the scaling hyperparameteres demands an explanation of how it is done.\nSection 3.4: This can all be written in a single paragraph without the need for sub-sections 3.4.x\n256: Write out ACC once.\n269: This could mention once more that S^l is sourced from the PIM dataset. An overview of all used datasets would be useful (number of files, length, train/val/test size, number of classes...)\n381-383: This is confusing. Did the dataset description get mixed up?\n419-421: It would be nice to see a confusion matrix for this statement?\n\nTable 2: According to 254-255, higher values indicate greater similarity between two clusterings. Is it then not a goal to reduce similarity between clusterings?\n\nFigure 1 could be more self-explanatory, particularly the two OOD blocks. The caption should explain crucial parts.\n\nFigure 2: Could be a normalized confusion matrix."
    },
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Current methods for Music Structure Analysis (MSA) focus primarily on audio data. While symbolic music can be synthesized into audio and analyzed using existing MSA techniques, such an approach does not exploit symbolic music's rich explicit representation of pitch, timing, and instrumentation. A key subproblem of MSA is section boundary detection-determining whether a given point in time marks the transition between musical sections. In this paper, we study automatic section boundary detection for symbolic music. First, we introduce a human-annotated MIDI dataset for section boundary detection, consisting of metadata from 6134 MIDI files that we manually curated from the Lakh MIDI dataset. Second, we train a deep learning model to classify the presence of section boundaries within a fixed-length musical window. Our data representation involves a novel encoding scheme based on synthesized overtones to encode arbitrary MIDI instrumentations into 3-channel piano rolls. Our model achieves an F1 score of 0.77, improving over the analogous audio-based supervised learning approach and the unsupervised block-matching segmentation (CBM) audio approach by 0.22 and 0.31, respectively. We release our dataset, code, and models.",
      "abstract": "Current methods for Music Structure Analysis (MSA) focus primarily on audio data. While symbolic music can be synthesized into audio and analyzed using existing MSA techniques, such an approach does not exploit symbolic music's rich explicit representation of pitch, timing, and instrumentation. A key subproblem of MSA is section boundary detection-determining whether a given point in time marks the transition between musical sections. In this paper, we study automatic section boundary detection for symbolic music. First, we introduce a human-annotated MIDI dataset for section boundary detection, consisting of metadata from 6134 MIDI files that we manually curated from the Lakh MIDI dataset. Second, we train a deep learning model to classify the presence of section boundaries within a fixed-length musical window. Our data representation involves a novel encoding scheme based on synthesized overtones to encode arbitrary MIDI instrumentations into 3-channel piano rolls. Our model achieves an F1 score of 0.77, improving over the analogous audio-based supervised learning approach and the unsupervised block-matching segmentation (CBM) audio approach by 0.22 and 0.31, respectively. We release our dataset, code, and models.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Omar Eldeeb",
        "Martin Malandro"
      ],
      "authors_and_affil": [
        "Omar Eldeeb (Technical University of Munich)*",
        "Martin Malandro (Sam Houston State University)"
      ],
      "channel_url": "",
      "day": "4",
      "keywords": [
        "Symbolic music processing",
        "MIR fundamentals and methodology",
        "Musical features and properties",
        "Representations of music",
        "Novel datasets and use cases",
        "Automatic classification",
        "MIR tasks",
        "Structure, segmentation, and form",
        "Evaluation, datasets, and reproducibility",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1t7eIK4p5KhEA16GDKXdq25YF7xOE0NgU/preview",
      "poster_pdf": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-14-barwise-section-boundary",
      "title": "Barwise Section Boundary Detection in Symbolic Music Using Convolutional Neural Networks",
      "video": ""
    },
    "forum": "259",
    "id": "259",
    "pic_id": "",
    "position": "14",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nStrongly agree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nStrongly agree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe discovery that there are \"markers\" in the Lakh dataset that have not been studied but that might relate to music structure is a great discovery that could benefit models in this field.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThe Lakh dataset may have more information in it than we yet know how to use. Also, predicting boundaries in MIDI scores is possible using standard techniques.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThe article makes two major contributions that I think should be presented at ISMIR:\n\n* It introduces a new dataset of boundary annotations in over 6000 MIDI files.\n* It adapts the boundary prediction algorithm of Grill and Schl\u00fcter 2015 (GS15) to a MIDI context, and studies some parts of the adapted model in an ablation study.\n\nHowever, describing a new dataset and a new algorithm in the same work leaves less room to do either of them justice. For the dataset, we learn mostly about the preparation of the data and less about the contents. For the algorithm, we understand the method well but not the design choices made, and there are many missed opportunities in the evaluation.\n\nExtended comments:\n\nThe motivation for this work and the rationale for the method they present are made clear in the Introduction and Related Work sections. The dataset \u2014 which could have been reported in a publication on its own \u2014 is described clearly in Section 4, and is a delightful discovery of the authors. On the other hand, since the dataset is only part of the paper, there is no room to present and discuss an illustrated example, or to discuss basic statistics of the dataset (e.g., number of artists; variety of genres; average segment duration; etc.). How \"diverse\" datasets are is discussed twice in the paper (line 93, line 238), so the lack of detail here is surprising.\n\nThe explanation of the method (Section 3) was clear, although the rationale for the overtone encoding feature was not clear to me. It seems like many choices were made in designing it that are not discussed or defended. Why 3 overtones? Why randomise their frequency and velocity? Why not linear decay? What exponent of decay was used and why? Was it the same for each overtone? Why or why not?).\n\nI found Section 5 harder to understand. The ablation study (Section 5.2) was the most interesting part of this section. The evaluation (Section 5.3) was disorienting, since it discusses the results of the baseline algorithms before they are explained (in line 381 and line 395). The audio-based approaches (Section 5.4) described algorithm designs, unofficial iteration, new aspects of prior work (like HPSS), and evaluation strategies, all in a subsection of the \"Experiment\". I recommend moving the explanation of the analogous audio method earlier, possibly to Section 3. Also, it would be valuable to perform another ablation study on the analogous audio method and report the results.\n\nOther comments:\n\n* The authors mention that RWC is more diverse than SALAMI, but the SALAMI set is fairly diverse, with lots of jazz, classical and \"world\" music.\n* The word \"our\" in the section titles \"Our Method\" and \"Our Dataset\" is not needed.\n* How was the harmonic overtone series encoding inspired by the \"Attention is All You Need\" paper [27]?\n* The analogous audio method and the CBM system are listed as \"baselines\". Aren't they just competing systems?\n* Given that converting MIDI to audio is possible, why not compare the proposal with other audio-based systems like those discussed in [8]?\n* The phrase \"measure endpoints\" (line 255) leads to a garden-path sentence. They could also be called \"bar lines\".\n* When positive examples are oversampled by a factor of 2 (lines 318\u20139), does this mean each positive example is viewed twice, or that some negative examples are ignored? This could be clarified in lines 340\u20134.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThe reviewers agreed that the two main contributions, the dataset and the boundary detection algorithm, were valuable. Outside of that, reviews were mixed: initial reviews ranged from weak reject to strong accept, and each reviewer's constructive comments touched on different parts of the paper.\n\nOur average recommendation is to accept the paper. If it is accepted, the authors will find valuable suggestions among all of the reviews on how to improve each section of their paper, and on what aspects of the work should be defended better.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper presents a new model for section boundary detection for symbolic music. Besides the model implementation and data representation method, the paper also invented a dataset by filtering out songs from LMD. The dataset preparation process itself is highly novel, carefully manually curated, and described in detail. As far as I know there is not a symbolic structure dataset of this size and quality. I also checked the validity of some annotations by myself, and I could say the dataset itself is highly useful for future works and provides a big bonus for the paper.\n\nThe methodology of the paper also includes high novelty. One question: I think the whole section 3.1 aims to represent the symbolic music in an audio-like format. In this case, a pretrained audio model might be helpful since the downstream data format is close to the pretraining format. But in section 3.2 the author says the model is pretrained from ImageNet instead of audio, which is highly unexpected (but also gives interesting results). I wonder why the author chose this method and whether the author has tried to perform pretraining on audio and fine-tune on symbolic music.\n\nThe experiments are generally sound and the results are very promising. One limitation is that the results on other datasets (like RWC Pop, which has audio-aligned MIDI scores + structure annotation) are not reported. I would suggest to modify/replace Tab 3 with results on other datasets.",
      "review2": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nThis paper concerns section boundary detection for symbolic music, which can be considered as an early step in music structure analysis. Although most prior work for this task uses audio data, the authors see merit in improving methods that solve this problem in the symbolic domain because:\na. According to the authors, audio approaches do not best exploit representations of pitch, timing and instrumentation, so it is better to conduct structure analysis of symbolic data in symbolic form rather than to synthesize the files and conduct the analysis in audio. \nb. They are further motivated due to the potential impact of structural boundary information on quality of the symbolic music generation results, but conducting an experiment to verify this aspect is left as future work. \n\nTo acheive these goals, the authors:\na) Rely on existing metadata in LMD to create a new annotated dataset for the section boundary detection. The authors manually verify the annotations by visual inspection .\nb) Train several models to classify presence of section boundaries with fixed length windows\n i. Create 4 training setups and compare their results + the result of an ensemble of them all together\n ii. One of their training setups involved encoding MIDI instrumentations into a 3 channel piano roll, based on overtone relationships. \nc) Compare with an audio baseline by synthesizing their midi evaluation set and conducting a parameter search. \n\nThey find that their method achieves an improvement to the audio baseline. \n\nOverall, the work is interesting and has the potential to become a more solid contribution, but the writing structure and organization of ideas (not the language) has a lot to improve.\n\nFor example, a very big part of the introduction was devoted to the potential positive impact of symbolic section boundary detection on symbolic music generation systems, whereas verifying this is something left as future work. I agree that referring to this aspect is an important motivation but it has taken too much space in the introduction given that it is not a core part of the work presented. Perhaps just a hint of this should be in the intro but much of it can be moved to the discussion section, \n\nThen, the related work starts by mentioning the best performing system and the related tolerance, without mentioning beforehand how boundary detection is even evaluated. Even prior to that, it would be more readable to explain to readers (who might not be very familiar yet with structure analysis or boundary detection). the difference between hierarchical and non hierarchical approaches and the supervised + unsupervised approaches.\n\nAnother point which is important but not clearly articulated in the introduction and the abstract is the source of the data; the fact that the annotations are based on metadata in LMD that was verified by the authors, and what the motivation was to search through LMD in the first place (which seems to be mentioned in 87 - 103), should be clearer at the start. \n\nOther comments:\n104 - 110 seems to be yet another motivation for solving this task in the symbolic domain. Perhaps it is more suited for the introduction, or just simply not included in the related work section.\n119 - 120: the manuscript does not discuss related work thoroughly enough.\n245: what is meant by \u2018appeared to be a valid segmentation\u2019? I believe this by comparing the ratios and so but perhaps something more rigorous needs to be done. \n373 - 374: please explain what the output is and how the peak picking method works. \n\nAlthough I believe that there could be more insights than what is currently expressed in the paper just by an improvement of the writing structure and an extension to the analysis, I have chosen to reject the paper because in its current form I don't think it is ready to be published yet.",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe authors present an approach for section-boundary detection in symbolic music. In general, the paper is well detailed and clear in the approach. I have some minor comments/questions:\n\n- The authors, inspired by audio spectrograms, add a series of overtones to each note. This is explained with an example in lines 162-172 but, was this consistent for all the training data? Were different amounts and combinations of overtones tested? \n- Similarly, please detail the linear decay applied to the overtones.\n- Disregarding boundaries that fall within 16 bars of the first and last note seems to be a considerable amount of bars. It is certain that many MSA papers avoid giving details about edge cases, so it is appreciated that the authors of this paper are open about this. However, I see an excellent opportunity to show this effect in either the ablation study or in a separated experiment. Identifying section boundaries \"in the middle of a song\" is useful, but it also seems to omit an important aspect of structure analysis. The authors briefly mentioned that this issue \"can be addressed in future work\", which is understandable. Nevertheless, including these edge cases in the accuracy metrics could help to assess the results of the proposed method.\n- Lines 369-371: if no peak picking was chosen as a first approach and several consecutive frames exceed the threshold, which one determines the start/end of the section?\n- Lines 428-431: what do the authors mean with oversampling positive examples and undersampling negative examples? Is this target smearing and weighting?"
    },
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Multi-Pitch Estimation (MPE) continues to be a sought after capability of Music Information Retrieval (MIR) systems, and is critical for many applications and downstream tasks involving pitch, including music transcription. However, existing methods are largely based on supervised learning, and there are significant challenges in collecting annotated data for the task. Recently, self-supervised techniques exploiting intrinsic properties of pitch and harmonic signals have shown promise for both monophonic and polyphonic pitch estimation, but these still remain inferior to supervised methods. In this work, we extend the classic supervised MPE paradigm by incorporating several self-supervised objectives based on pitch-invariant and pitch-equivariant properties. This joint training results in a substantial improvement under closed training conditions, which naturally suggests that applying the same objectives to a broader collection of data will yield further improvements. However, in doing so we uncover a phenomenon whereby our model simultaneously overfits to the supervised data while degenerating on data used for self-supervision only. We demonstrate and investigate this and offer our insights on the underlying problem.",
      "abstract": "Multi-Pitch Estimation (MPE) continues to be a sought after capability of Music Information Retrieval (MIR) systems, and is critical for many applications and downstream tasks involving pitch, including music transcription. However, existing methods are largely based on supervised learning, and there are significant challenges in collecting annotated data for the task. Recently, self-supervised techniques exploiting intrinsic properties of pitch and harmonic signals have shown promise for both monophonic and polyphonic pitch estimation, but these still remain inferior to supervised methods. In this work, we extend the classic supervised MPE paradigm by incorporating several self-supervised objectives based on pitch-invariant and pitch-equivariant properties. This joint training results in a substantial improvement under closed training conditions, which naturally suggests that applying the same objectives to a broader collection of data will yield further improvements. However, in doing so we uncover a phenomenon whereby our model simultaneously overfits to the supervised data while degenerating on data used for self-supervision only. We demonstrate and investigate this and offer our insights on the underlying problem.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Frank Cwitkowitz",
        "Zhiyao Duan"
      ],
      "authors_and_affil": [
        "Frank Cwitkowitz (University of Rochester)*",
        "Zhiyao Duan (University of Rochester)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "MIR fundamentals and methodology",
        "Musical features and properties",
        "Music transcription and annotation",
        "Representations of music",
        "MIR tasks",
        "Music signal processing",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1369Vqvy2eZXXzf41IIHwet8X-tBB4kSc/preview",
      "poster_pdf": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-12-investigating-an-overfitting",
      "title": "Investigating an Overfitting and Degeneration Phenomenon in Self-Supervised Multi-Pitch Estimation",
      "video": ""
    },
    "forum": "261",
    "id": "261",
    "pic_id": "",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nDisagree\n\n**Q10 (Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chose, otherwise write \"n/a\"))**\n\nPls see my comments about $L_{eg}$ in the detailed discussion.\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nDisagree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nStrongly Disagree (Well-explored topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe paper implements some ways to incorporate invariance and equivariance principles for data augmentation. These may be useful for other applications in music and audio processing.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nData augmentation methods that help deep models; the paper proposes a self-supervision method that needs more work.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak reject\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nContributions:\n- The paper presents learning methods for multi-pitch estimation.\n- The paper uses transformations such as pitch shift and time stretching for augmenting the supervised data.\n\nLimitations:\nThe loss function $\\mathcal L_eg$ in eq (4) leads to degradation of performance. I would expect a deeper analysis of this loss function. E.g., plot the target $\\tilde X$ alongside the true $\\tilde Y$ to see if it really captures what we want the model to learn. To me, it seems that $\\tilde X$ must be smeared out leading to a trivial solution, like a uniform distribution. \n\nThis is an important contribution of the paper, and hence, must be studied properly. Without this, the contributions of the paper seem to little to accept the paper. I would recommend to submit the work after this analysis and the subsequent improvements.\n\nOther comments:\n- Line 150: what is $u[k]$? How is applied to the input spectrograms? Please write it in the paper.\n- In eqs. (3) and (3), $\\hat Y$, the estimated output, should be replaced with $\\tilde Y$, the target output. $\\mathcal F(t(X))$ already means the estimated output.\n- There are minor typos, such as \"other other\" in line 342.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nAll reviewers appreciate the paper and the efforts that went into it. There are some critical comments and some suggestions for further investigations that the authors may look into.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper presents a method that uses self-supervised learning objectives within a supervised and semi-supervised approach to music transcription. It shows increase in performance when used in a supervised setting, and a surprising loss of generalization when used in a semi-supervised setting. Evaluation is thorough and although no \"solution\" to the problem is given, the obtained insights are useful.\n\nI suggest to accept the paper, it is well written, of course more experiments and possible solutions would be welcome.",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nIn this paper, the authors present a series of experiments that combine supervised and self-supervised learning paradigms for training a multi-pitch estimation model. A key finding is that the model's performance deteriorates when additional unlabeled data from other datasets\u2014used to enable self-supervised learning\u2014is introduced. The authors report the effects of this behavior in detail.\n\nOverall, the paper is very well written, with each section building logically on the previous one. It also provides a useful summary of best practices in data preparation and loss function design, making it a self-contained and informative read for those seeking insight into current state-of-the-art approaches.\n\nA particularly strong aspect of the paper is its systematic and well-documented experimental setup. Table 1 presents a range of strong baselines, followed by results from the proposed model using various combinations of supervised and self-supervised loss functions on datasets that contain appropriate labels.\n\nHowever, when additional unlabeled data is introduced for self-supervised training, model performance declines. The discussion in Section 4 is especially insightful, as it attempts to diagnose the reasons behind this drop. The authors suggest that when datasets closely related to the supervised data are added for self-supervision, the model performance is negatively impacted\u2014an effect supported by the experimental results.\n\nOne point that remains unclear is whether the authors believe self-supervision still holds promise for this task. Could alternative datasets better complement the supervised data and address its limitations? I also wondered about the behavior of the loss functions when combining labeled and unlabeled samples. Are supervised losses deactivated for unlabeled samples? Furthermore, do the supervised and self-supervised losses operate on the same numerical scale, or does one dominate the other? It would be helpful if the authors could report the typical ranges of these losses\u2014e.g., mean supervised loss vs. mean self-supervised loss for labeled and unlabeled data\u2014though this might be more appropriate for future work.\n\nI strongly recommend accepting this paper. It is a valuable contribution and is likely to spark productive discussions at ISMIR 2025.",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper builds upon the self-supervised objectives proposed in the preprint [1] and performs a series of experiments\n\nIt compares how the self-supervised objectives interact with the usual supervision. It provides interesting insights regarding the behavior. One could imagine that the self-supervised objectives could help improve the models in data for which no label is present, addressing the difficulty of annotating large amount of data in MPE. However, it is observed that worse results are achieved.\n\nThe selection of dataset seems sufficient, although Maestro and Slakh dataset are not included. Surprising given how much data is available in those. But again, the data selection seems to contain enough variety of domains.\n\nThe insights provided through the different experiments look very interesting as well, demonstrating that there is still a requirement for work in this domain.\n\nI think that something is missing however during evaluation.\nAs acknowledge by the authors, pitch is something perceptual. While labels employed for training consists usually of 1s and 0s, the truth is that the self-supervised objectives do not enforce this extreme choices. \nMoreover, if one thinks of the signal evolution over time, one could expect the pitches of certain instruments to decay over time, contrasting with the rigid 1s and 0s found in labels.\nGiven that a threshold is applied to the model's output, it is expected that if the model is actually starting to predict such smooth changes over time.\nI would suggest to provide another extra evaluation:\n- For those frames with active pitch: are the active pitches from the labels within the top K active pitches of the model's prediction? In this way another type of evaluation that does not rely on choosing an appropriate threshold could be employed.\n\nOther comments:\n- Line 69: What kind of refinement? They seem to me exactly like in [1]\n- There is no formal definition of degeneration\n- \"teaches the model to degenerate on a specific distribution\" sounds like a very weird phrase to me.\n\n[1] F. Cwitkowitz and Z. Duan, \u201cToward fully self supervised multi-pitch estimation,\u201d arXiv preprint arXiv:2402.15569, 2024.\""
    },
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Musical form is one of the most central aspects of musical\nstructure, as it concerns the overarching organization\nprinciples of music across genres and styles. Therefore,\nunderstanding the formal characterization of musical form\nis a central topic in music theory, computational music\nanalysis, MIR, and music generation. This paper makes\na theoretical contribution proposing a formal model that\ncharacterizes the main aspects of musical form. We characterize\nmusical form by the following aspects: Grouping\nstructure, rhythmic partitioning, formal functions, repetition\nstructure, schemata, and harmonic anchor points. As\nthe structures of hierarchical segmentation as well as formfunctionality\nhave previously been conceptualized in terms\nof a recursive tree-shaped hierarchy, we ground our model\nin abstract generative grammars. Our model extends this\nhierarchical analysis by an account of the rhythmical properties\nof form as well as repetition structure. The harmonic\nlayout defines constraints for motivic content (pitch and\nrhythm). Our approach also addresses repetition structure\nby modeling the location and degree of variation of repeated\nideas. This is achieved via variable binding. We\nexemplify our theoretical contribution by a detailed analysis.",
      "abstract": "Musical form is one of the most central aspects of musical\nstructure, as it concerns the overarching organization\nprinciples of music across genres and styles. Therefore,\nunderstanding the formal characterization of musical form\nis a central topic in music theory, computational music\nanalysis, MIR, and music generation. This paper makes\na theoretical contribution proposing a formal model that\ncharacterizes the main aspects of musical form. We characterize\nmusical form by the following aspects: Grouping\nstructure, rhythmic partitioning, formal functions, repetition\nstructure, schemata, and harmonic anchor points. As\nthe structures of hierarchical segmentation as well as formfunctionality\nhave previously been conceptualized in terms\nof a recursive tree-shaped hierarchy, we ground our model\nin abstract generative grammars. Our model extends this\nhierarchical analysis by an account of the rhythmical properties\nof form as well as repetition structure. The harmonic\nlayout defines constraints for motivic content (pitch and\nrhythm). Our approach also addresses repetition structure\nby modeling the location and degree of variation of repeated\nideas. This is achieved via variable binding. We\nexemplify our theoretical contribution by a detailed analysis.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Markus Neuwirth"
      ],
      "authors_and_affil": [
        "Markus Neuwirth (Anton Bruckner Privatuniversit\u00e4t Linz)*"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Musical features and properties",
        "Computational musicology",
        "Mathematical music theory",
        "Structure, segmentation, and form",
        "Computational music theory and musicology",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1TwJC-rlydQ-5EuRGcFHJ79fbe6QG2NWL/preview",
      "poster_pdf": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-8-a-theoretical-model",
      "title": "A Theoretical Model of Musical Form",
      "video": ""
    },
    "forum": "264",
    "id": "264",
    "pic_id": "",
    "position": "08",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Smoothly transitioning between chords on the guitar can be a major challenge for beginners, especially when they rely on just a few common chord diagrams. Yet many chords can be played in multiple ways (i.e., voicings), which can facilitate more comfortable hand movements on the fretboard. To address this, we present the FretboardFlow dataset, featuring 97 songs recorded with a hexaphonic pickup to capture multiple chord voicings as performed by expert guitarists. Our dataset builds upon the GuitarSet pipeline, incorporating a Python translation of Pr\u00e4tzlich et al's KAMIR algorithm for interference reduction, for automated hexaphonic transcriptions, thereby capturing harmonic structure and performance-driven voicing choices that implicitly reflect muscle memory and ergonomic habits, providing a rich resource for analyzing real-world chord transitions.\nTo predict the most convenient voicing within progressions, we propose a dual-model approach integrating both chord and voicing history, and loss functions well-suited to the flexible nature of voicings. Our research expands on prior chord prediction work by incorporating expert-recorded voicing variations of the same progressions and introducing a novel machine learning approach to fretboard navigation. We publicly release this dataset as a living resource to support data-driven exploration of context-aware guitar instructions.",
      "abstract": "Smoothly transitioning between chords on the guitar can be a major challenge for beginners, especially when they rely on just a few common chord diagrams. Yet many chords can be played in multiple ways (i.e., voicings), which can facilitate more comfortable hand movements on the fretboard. To address this, we present the FretboardFlow dataset, featuring 97 songs recorded with a hexaphonic pickup to capture multiple chord voicings as performed by expert guitarists. Our dataset builds upon the GuitarSet pipeline, incorporating a Python translation of Pr\u00e4tzlich et al's KAMIR algorithm for interference reduction, for automated hexaphonic transcriptions, thereby capturing harmonic structure and performance-driven voicing choices that implicitly reflect muscle memory and ergonomic habits, providing a rich resource for analyzing real-world chord transitions.\nTo predict the most convenient voicing within progressions, we propose a dual-model approach integrating both chord and voicing history, and loss functions well-suited to the flexible nature of voicings. Our research expands on prior chord prediction work by incorporating expert-recorded voicing variations of the same progressions and introducing a novel machine learning approach to fretboard navigation. We publicly release this dataset as a living resource to support data-driven exploration of context-aware guitar instructions.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Marcel V\u00e9lez V\u00e1squez",
        "Mari\u00eblle Baelemans",
        "Jonathan Driedger",
        "John Ashley Burgoyne"
      ],
      "authors_and_affil": [
        "Marcel V\u00e9lez V\u00e1squez (University of Amsterdam)*",
        "Mari\u00eblle Baelemans (University of Amsterdam)",
        "Jonathan Driedger (Chordify)",
        "John Ashley Burgoyne (University of Amsterdam)"
      ],
      "channel_url": "",
      "day": "4",
      "keywords": [
        "Evaluation metrics",
        "Applications",
        "Musical features and properties",
        "Harmony, chords and tonality",
        "Novel datasets and use cases",
        "Music training and education",
        "Evaluation, datasets, and reproducibility",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1m0sWG8o2RtCL0kYzTzeOZxOllkTh7E3c/preview",
      "poster_pdf": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-4-fretboardflow-a-dual",
      "title": "FRETBOARDFLOW: A DUAL-MODEL APPROACH TO OPTIMIZE CHORD VOICINGS ON THE GUITAR FRETBOARD",
      "video": ""
    },
    "forum": "266",
    "id": "266",
    "pic_id": "",
    "position": "04",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q5 ( Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chosen, otherwise write \"n/a\"))**\n\nAvoid references in languages other than English, unless they absolutely contribute something no other source can. In this case, a relatively random news article about Chordify seems cited instead of a simple link to their homepage.\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nNo\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nDisagree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nStrongly agree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nDisagree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nFew insights are derived from the raw numeric data.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nNew data available for fret board voicing prediction, accompanied by experiments with an alternative architecture\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak reject\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nSummary\n-----------\nThe submission presents a new collection of fret board voicing data, obtained from hexaphonic recordings. Experiments on the prediction of chord voicing are conducted with dual branch models that process chord symbols and temporally preceding fretboard data separately before being merged by a linear layer. Those branch networks use either Bi-LSTMs or DeepGRUs, where the latter proves to be generally more performant according to a number of metrics.\n\nPositives\n----------\n- New data to work with is always welcome.\n- The proposed DeepGRU architecture is interesting, and the results are promising.\n\nNegatives\n-----------\n- A main issue is that the evaluation is strongly numeric, without providing any insight what this means for the actual real-world problem. What does a typical prediction look like? What type of errors are being made? Is the output perceived to be good enough in practice or completely unworkable? An application or human centric addition to the evaluation would be very insightful for this kind of problem without clear ground truth.\n- The dataset could be better curated. Now it is dominated by a single person who recorded more than the rest combined. None of the challenges/opportunities coming from multiple annotators are currently explored. Only hard songs are recorded by multiple people, and the effect of difficulty, number of variations per song per player, etc. is not examined.\n- The whole discussion of \"proper scoring rule\" does not seem to lead to an approach that is different from earlier work. It seems a justification for a non-existent problem.\n- The term \"history length\" suddenly appears on line 421 and plays a prominent role in the experiments, but is not properly explained. I interpret it as a strict cut-off of the input to the recurrent layers, but see no obvious reason why that is necessary and a contradiction with the justification of using recurrent layers. At minimum a comparison with using the complete history would be needed.\n- Both datasets are only used in isolation with cross-validation, whereas there is an opportunity to do cross-dataset evaluation.\n- Representing a fretboard as a binary matrix, where the fact that only the highest fretted note on a string produces sound is not explicitly encoded, seems subobtimal compared to an integer vector representation. At least something worth exploring.\n\nOverall\n--------\nGiven that the dataset is described as a living dataset, and extended analysis and experiments would be welcome, the current submission feels very much like work in progress. Addressing the raised points would lead to a very valuable contribution to the field, but this year's ISMIR might be too soon for that.\n\nAdditional comments\n------------------------\n- The supplementary material should have been submitted as a separate file, not as part of the main paper.\n- Confusing usage of both \"Amsterdam Playability Dataset\" and \"Billboard Playability Dataset\" to refer to what seems to be the same thing.\n- The references could use cleaning up, e.g. [11] is missing its publication venue and there's a typo in [23] (\"toplay\").\n- Mentioning first assigning on l. 224 and then selecting on l. 238 of the songs is unnecessarily confusing. I interpret it as a free selection out of a predefined subset, but this should be explained once instead of splitting this information over multiple paragraphs.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nAll reviewers appreciate the work put into collecting the data for this submission and making it publicly available. The experiments show promise, though would benefit from more human interpretation and insights. We recommend looking at the individual reviews to address the points made there. Do keep expanding this resource!",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nOverall, this is a very nicely written paper, and the very first time I see the task of chord voicing estimation being formalized. \nWhile this is obviously a very niche topic, with little possibility to expand the learnings from this paper to other MIR tasks, I believe it does qualify as a valid and novel research area.\n\nThe dataset collected is, in itself, an impressive contribution. As a guitar player myself, I do understand the value in widening the possibilities of chord voicings to facilitate interpretation and I look forward to analyzing the various rendering of the same piece that were collected.\n\nThe experimental part of chord voicing estimation through machine learning technique is a bit more confusing. It's not clear from Table 1 (yet alone from table S1) what elements of the proposed method are most effective to the task. Metrics seem to be hard to jointly optimize, and while authors choose to highlight the test loss, I would intuitively have thought that ease of transition and playability should be most important here.\n\nThe discussion on the MSE being a \"proper scoring rule\" is not very convincing (tbh it reads a bit like a posteriori justification of why it should be the metric we trust the most). I am no expert in Brier score, but I believe it is mostly applicable in a binary setup. In a multi-label case such as the task at hand, and considering the rather limited amount of samples and the large imbalances, I wouldn't put too much trust on it being an unbiased estimator of actual expert annotation probabilities\n\nWe're also lacking in some respect an understanding of the type of predictions that these various systems make. There are several occurrences in the paper where author state that changing from open chords to barre can be perceived as suboptimal. Are there some configurations that effectively limits such transitions more than others? Maybe a quick qualitative analysis of some of the results could have been insightful.\n\n\ndetails\n\nAbstract\n* the last sentence (l14) is odd and lacks a verb\n* \"data-driven exploration of personalized guitar instruction.\" I'm a bit puzzled by \"personalized\" as I see no evidence in the paper about this\n\nIntroduction\n* Quick note on Ultimate Guitar, users do still have comments and ratings to help decide which versions they might find more suitable\n* the transition on l50 is a bit weird, what's the relation with the previous paragraph?\n*\"up to five voicing variations for each of 35 songs,\" -> for 35 songs\n\nSection 5\ncould a simpler approach, optimizing for hand movement (e.g a wasserstein distance on the fretboard) be also tested here? \n\nl366 this bidirectionality mirrorring player's choices seems reasonable but is it also due to lower perfs of the causal models?\n\nl400 would achived -> would be achived\n\nBibliography\n* The link chosen for chordify (2) is a dutch blog post celebrating their 10 year anniversary.. maybe a wikipedia link be more adapted\n* references 11, 12, 18 are incomplete",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\n# Overall Comments\nA lot of work has gone into this paper, from building a new dataset to comparing it to an existing one, as well as using state-of-the-art methods and introducing new ones. \nAs detailed below, my two main remarks are on the lack of information regarding the recorded guitarists (an Ethical Statement would be particularly relevant too) and the tendency to compare datasets only on ratios even though they are so different in size.\nApart from that, the paper is well written and could surely help future research in the field.\n\n## Refinements required\n- l14-15: how can one capture muscle memory through microphone recordings?\n- l151-156: I agree that it's important to have varied voicings, but is it really lacking in DadaGP?\n- l208: It's great to release the dataset! What will the licence be?\n- l236: making a web interface is a lot of work, you could show it!\n- l249-259: I think the participants' presentation should arrive earlier. We also lack a lot of information about them, like how they were recruited (was it approved by an IRB?), were they paid, what's their musical background, etc. It would also be interesting discussing why the number of recordings is so unbalanced between participants.\n- l290: rhythm* data quantised? And what is a quarter-measure interval? Is it a quarter note? How does it work in time signatures other than 4-4?\n- l301-312: the criticisms made towards DadaGP are a bit fallacious. I don't think it really makes sense comparing ratios between the two datasets and not discussing the fact that DadaGP is at least ten times larger\n- Figure 2: Same comment\n- l366-367 & 369-370: the claim that a guitarist thinks bidirectionally should be backed up/explained (or removed)\n- Table1 has a lot of values, maybe only a subset is required in the main text, especially if not everything is discussed\n- Results analysis: I feel the discussion section lacks a qualitative analysis of the predictions to better understand why the proposed models generally perform worse on string-fret F1 even though the loss is better. \n\n\n## Typesetting and Language issues \n\n- l4: the use of single is not very clear, should be rephrased\n- l158: natural rather than naturalistic?\n- Some references are incomplete: at least [11] [12] [18]",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper can serve a valuable insight and dataset in the ISMIR community. The contribution is above acceptance threshold and will help future work on performance\u2011aware voicing, auto\u2011arrangement, and guitar pedagogy.\n\nStrengths:\nNovel Dataset. While similar datasets exist, the introduction of the FretboardFlow dataset addresses a clear gap. It offers a well-structured and resourceful dataset for guitar chord voicings and progressions. With potential future extensions, it can serve as a strong resource for modeling realistic chord transitions.\n\nPractical Application. The dual-model architecture, which integrates chord-symbol sequences and voicing history, is well-motivated and offers a data-driven solution to challenges in chord position selection.\n\nCitations: The paper cites the most relevant prior work in the area and actively addresses potential gaps the reader might have, usually backed by proper citation. \n\nSuggestions:\nDual path concats and then passes through a linear layer. Fusion or attention variants can be tried in future work.\n\nThe comparison between the DadaGP dataset and FretboardFlow isn't fully direct. You would need a separately collected test set with well-defined structure to properly evaluate and present a table of losses on that set. Claims that DadaGP scores better due to its simpler format are reasonable, but they require experimental backing. The augmentation shifting you're applying may already address many of the issues DadaGP has with uniformity and chord variation. You could try test the models trained on FretboardFlow on a smaller subset of DadaGP and the other way around to confirm your claims. \n\nFor completeness you need to explicitly list all types of augmentation you are using, rather than only referencing related work that applies similar methods. (One augmentation you could possibly consider in future work: for chords that span 5 or more strings, you could randomly sample 3 notes, and then for the next chord, sample the 3 closest notes. This will maybe create the variations that are missing from DadaGP and also add more variations to your dataset).\n\nHuman validity. The loss metric is automated, a small perceptual/user study with guitarists would strengthen claims."
    },
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Audio Chord Estimation (ACE) holds a pivotal role in music information research, having garnered attention for over two decades due to its relevance for music transcription and analysis. Despite notable advancements, challenges persist in the task, particularly concerning unique characteristics of harmonic content, which have resulted in existing systems' performances reaching a glass ceiling. These challenges include annotator subjectivity, where varying interpretations among annotators lead to inconsistencies, and class imbalance within chord datasets, where certain chord classes are over-represented compared to others, posing difficulties in model training and evaluation. As a first contribution, this paper presents an evaluation of inter-annotator agreement in chord annotations, using metrics that extend beyond traditional binary measures. In addition, we propose a consonance-informed distance metric that reflects the perceptual similarity between harmonic annotations. Our analysis suggests that consonance-based distance metrics more effectively capture musically meaningful agreement between annotations. Expanding on these findings, we introduce a novel ACE conformer-based model that integrates consonance concepts into the model through consonance-based label smoothing. The proposed model also addresses class imbalance by separately estimating root, bass, and all note activations, enabling the reconstruction of chord labels from decomposed outputs.",
      "abstract": "Audio Chord Estimation (ACE) holds a pivotal role in music information research, having garnered attention for over two decades due to its relevance for music transcription and analysis. Despite notable advancements, challenges persist in the task, particularly concerning unique characteristics of harmonic content, which have resulted in existing systems' performances reaching a glass ceiling. These challenges include annotator subjectivity, where varying interpretations among annotators lead to inconsistencies, and class imbalance within chord datasets, where certain chord classes are over-represented compared to others, posing difficulties in model training and evaluation. As a first contribution, this paper presents an evaluation of inter-annotator agreement in chord annotations, using metrics that extend beyond traditional binary measures. In addition, we propose a consonance-informed distance metric that reflects the perceptual similarity between harmonic annotations. Our analysis suggests that consonance-based distance metrics more effectively capture musically meaningful agreement between annotations. Expanding on these findings, we introduce a novel ACE conformer-based model that integrates consonance concepts into the model through consonance-based label smoothing. The proposed model also addresses class imbalance by separately estimating root, bass, and all note activations, enabling the reconstruction of chord labels from decomposed outputs.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Andrea Poltronieri",
        "Xavier Serra",
        "Mart\u00edn Rocamora"
      ],
      "authors_and_affil": [
        "Andrea Poltronieri (Music Technology Group - Universitat Pompeu Fabra)*",
        "Xavier Serra (Music Technology Group - Universitat Pompeu Fabra)",
        "Mart\u00edn Rocamora (Music Technology Group - Universitat Pompeu Fabra)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Musical features and properties",
        "Music transcription and annotation",
        "Harmony, chords and tonality",
        "MIR tasks",
        "Machine learning/artificial intelligence for music",
        "Computational music theory and musicology",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1rGtA51MVnEBY0Ea0U61OfGG_Jhwiax6F/preview",
      "poster_pdf": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-15-from-discord-to",
      "title": "From Discord to Harmony: Decomposed Consonance-based Training for Improved Audio Chord Estimation",
      "video": ""
    },
    "forum": "268",
    "id": "268",
    "pic_id": "",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nStrongly agree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe perceptually motivated approach this work takes could instigate similar research in other problems where hard labels seemingly present a performance ceiling.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nA conformer-based model using label smoothing and a consonance-based evaluation metric shows strong promise for ACE.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nStrongly agree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThe presented work uses non-binary distance measures to model inter-annotator agreements together with consonance-based label smoothing as a means to train an audio chord estimation (ACE) model. This is interesting and multidimensional work with compelling results that will receive interest from the research community. \n\nAuthors introduce a perceptually-informed distance metric, in the context of Western music, that models the agreement between annotators in a musically meaningful way. They first define Mechanical Distance which quantifies the magnitude of deviation for each incorrect note from the target chord and by combining it with consonance they arrive at the Mechanical-Consonance metric which weighs each semitone deviation according to its perceptual consonance value. This metric achieves a higher inter-annotator agreement score and has better discriminative ability with respect to random data.\n\nThe proposed model contains an acoustic front-end followed by a conformer model which combines CNN and transformer models to capture local and global dependencies. Three fully-connected layers are then used to predict bass, root, and chord pitch class content all from which a symbolic prediction is made after label smoothing where more consonant intervals receive higher similarity scores.\n\nEvaluation is performed on standard datasets which reveal better performance compared to the BTC model. They also provide a simple analysis of the penultimate layer activations with and without consonance smoothing. All in all, the competitive results show the viability of the approach and this work will set a significant benchmark for ACE going forward.\n\nThe paper presents a novel approach and provides valuable insights into how to leverage the level of agreement of annotated data and label smoothing in a musically meaningful way toward improving ACE. It is well-written for the most part with strong motivation and literature review however since there are many different issues the paper tackles space seems to be tight. It would be beneficial for the authors to make another pass in order to make space for implementation details and include some additional results as suggested by the reviewers for improving the presentation. Please go through the references and format them properly (e.g. [35], [38]).\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nAll reviewers have provided positive feedback about this work and are in agreement that the ISMIR community will benefit from publication of the paper. It contains useful insights into chord estimation and will be of interest to researchers working in this field. Please carefully address the issues raised by the reviewers (especially those from reviewers #2 and #3) in the final form of the paper prior to submission for publication. Since there is limited space you will need to be creative in compressing the existing content to insert the new informational and clarifying text that we are asking. Please remember to go through your references to ensure consistency and adherence to the required format.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper improves audio chord estimation by proposing a new Mech-Cons metric to better capture the chord relationships beyond the traditional binary one, which is then incorporated as consonance-based smoothing for the better estimation performances. \n\nAlthough the improvements of the model architecture seem incremental, I really appreciate the discussion of harmonic subjectivity/ambiguity by investigating inter-rater agreement in depth, which results in a new Mech-Cons metric to better reflect the chord relationships beyond the binary metric. This metric can be used in other future research regarding chords and harmony. \n\nThe formatting of references can be better. For example, the page number for [4] is missing, and some entries provide online links while others don't. I suggest using a unified reference style. Furthermore, the important information of the dataset split in Section 4 is missing out, which should be added and specified in the final version. The performances of the BTC model do not coincide with the ones proposed in the original paper, so I assume the authors let BTC infer on their test set. Please make sure this is the case (and also make sure the author's test set do not overlap with any training data of the original BTC), and provide corresponding explanations in the text.\n\nOverall, this is a valid piece of work on audio chord estimation. I will accept this paper.",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper considers two important challenges in automatic chord estimation (ACE): class imbalance and annotation subjectivity. The paper proposes\n1) (to tackle the subjectivity) Mechanical-Consonance metric (Mech-Cons) for evaluation, which takes the consonance relationship between pitches into consideration,\n2) A Conformer-based ACE model that predicts root, bass and pitch and then decodes these information into chord symbols, and\n3) (to tackle the imbalance) consonance-based label smoothing (LS) that penalizes the musically meaningful errors less\nResults show that the proposed Mech-Cons metric achieves higher inter-annotator agreement score, which means it is able to capture the ambiguity between different chords. The proposed model outperforms BTC, a strong ACE baseline, and the consonance-based LS further improves the performance, both in terms of standard mir_eval metrics and the proposed Mech-Cons.\n\nStrengths\n- The Mech-Cons metric that authors propose show higher inter-ratter agreement than other metrics, especially the standard MIREX metric, which means that Mech-Cons better captures humans\u2019 subjectivity in chord annotations.\n- The proposed model is novel. While the authors do not point out or do any further study, the process of predicting bass, root and pitch classes and then decode them to chord labels is new. Note that it is different from [12] where there is a trainable linear layer to predict the chord label from these information.\n- The proposed consonance-based smoothing improves the performance of the system, both in terms of standard MIREX scores and Mech-Cons.\n\nWeaknesses and Questions\n- I think it is good work but the presentation can be improved to make it clearer. I understand that due to page limit, there are many things that the authors don\u2019t have space to explain in detail (e.g. TbT, mechanical distance and section 4.2, specified below), but the lack of explanation adds to the difficulty of reading.\n- Table 1 should include binary agreement since it is something the authors explicitly mention to compare to. Here, TbT and Mech are proposed in [10] and is (I believe) less familiar to the community compared to MIREX score. However, the authors use minimal effort to explain what TbT and (in particular) Mech are. Mech serves as the basis of the proposed Mech-Cons, and this significantly adds to the difficulty of reading. E.g. why we can use the consonance vector as a parameter.\n- Section 4.1 and Table 3 are the main results for the model-wise contribution. (2 and 3 in the summary above) However, this is too fast, especially for the label-smoothing technique. To show that LS improves the model\u2019s performance, we also need to see BTC with linear LS and consonance LS, otherwise it is reasonable to doubt that LS work only with the conformer model. Also, it is intuitive that consonance works the best with Mech-Cons because they share similar targets. Would authors agree with that?\n- The authors let the model predict bass, root and pitches and then use a separate decoder to convert them into chord labels. This is done by neither [12] nor BTC. It is still a fair comparison if the baseline BTC is also implemented in this way but the effect of such uncommon implementation is not investigated.\n- Section 4.2 seems a bit incomprehensive due to the limited space. The authors say that \u201cConsonance-based smoothing promotes equidistance\u2026\u201d (Line 483-485) However, this is not correct. [11] claims that LS encourages equidistance because [11] uses the standard LS (linear, in this paper\u2019s words), but since the authors uses a different, delicately designed, consonance-based LS, we should expect something different. To make the whole section 4.2 stronger, we need to compare the representation of No LS, linear LS and consonance LS, and also not only semitone-based \u201cC-C#-D\u201d but probably also fifth-based \u201cC-G-D\u201d.\n\nOverall, I think this paper covers many, and even too many contents, including evaluation metric, improving from BTC to Conformer, training with label smoothing, each of which is probably worth writing a separate paper, especially the evaluation metrics. I greatly appreciate that the authors, based on their comprehensive literature review, have put together everything and proposed the strong model. However, squeezing everything into a paper means some discussions have to be on a superficial level and bring limited insights.\nI would recommend a weak accept. If the paper is accepted, I would suggest considering the weaknesses and questions mentioned above and focus on only the essential parts of their work in the paper (e.g. Get rid of line 223-240 and give more explanation about basic mechanical distance).\n\nMinor corrections: \n- Put Figure 1 in the same page as 3.2\n- Some of the references are not properly formatted. For example, [15] is published in ICASSP. Many papers are from ISMIR but are formatted differently, e.g. [4], [5], [6] and [14]. If the paper is accepted the authors should clean the references.",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper proposes a novel conformer-based model for Audio Chord Estimation (ACE), enhanced with a consonance-based label smoothing technique. It aims to address persistent challenges in ACE by leveraging perceptually informed metrics and smoothing strategies grounded in music theory.\n\nStrengths\n\u2022 Clarity and Structure\nThe paper is well-written, with a clear structure that makes it easy to follow. The motivation is well-established, and the progression from problem identification to solution and evaluation is logically organized.\n\n\u2022 Innovative Use of Consonance-based Smoothing\nThe introduction of consonance-based label smoothing is both novel and musically meaningful. The authors make a compelling case for how this method aligns better with human perception than traditional binary or uniform smoothing techniques.\n\nWeaknesses and Suggestions\n1. Lack of Detail on Data Splits: The paper does not explain how the training, validation, and test sets were divided. This information is critical for reproducibility and should be clearly stated, including whether any cross-validation was performed or how the validation set was selected.\n2. Since the reported performance improvements are relatively marginal in some metrics, it would strengthen the empirical claims if standard deviations were included.\n3. Unclear BTC Evaluation Procedure: The paper compares the proposed model to the BTC model [24] in Table 3, but it is not clear whether the BTC results were reproduced using the original model weights, retrained under the same setup, or reimplemented. Clarifying this is essential for interpreting the fairness and reliability of the comparison.\n4. While BTC is a known method, it differs from the proposed model in both architecture (bidirectional Transformer vs. Conformer) and decoding strategy (BTC lacks the root/bass/pitch-based decoding described in Section 3.4). A more suitable baseline would be the model in [23], which includes a decoding strategy and is publicly available. Comparing against [23] could better isolate the contributions of the conformer architecture versus the decoding mechanism.\n5. Although the Mechanical-Consonance metric is well-motivated, it would be beneficial to also test label smoothing using the Tone-by-Tone metric in Table 3. This would provide further empirical grounding for the choice of Mechanical-Consonance smoothing.\n6. Justification for Decoding Choice: It would be helpful if the authors provided rationale for adopting the decoding approach from [12] instead of the one from [23], especially since [23] argues that their decoding method offers advantages and is more expressive.\n\nMinor Issues and Typos\n\u2022 Line 335 / 382: There is inconsistency in English style: \u201cnormalisation\u201d (UK) in Line 335 and \u201cnormalize\u201d (US) in Line 382. Please standardize the English style throughout the paper.\n\u2022 Table 3: The best result on the Tetrads metric is achieved by the model with Linear smoothing, but the bold text is mistakenly applied to the Consonance model. Please correct the formatting.\n\nI appreciate the potential of this work and commend the authors for addressing an important challenge in audio chord estimation using a musically meaningful approach. However, due to several unresolved issues in the experimental setup and evaluation, I believe the work would benefit from further clarification and refinement before publication."
    },
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Recent advances in generative models have made it possible to create high-quality, coherent music, with some systems delivering production-level output.\nYet, most existing models focus solely on generating music from scratch, limiting their usefulness for musicians who want to integrate such models into a human, iterative composition workflow. \nIn this paper we introduce STAGE, our \"STemmed Accompaniment GEneration\" model, fine-tuned from the text-to-music MusicGen model to generate single-stem instrumental accompaniments conditioned on a given mixture. Inspired by instruction-tuning methods for language models, we extend the transformer's embedding matrix with a context token, enabling the model to attend to a musical context through prefix-based conditioning.\nCompared to the baselines, STAGE yields accompaniments that exhibit stronger coherence with the input mixture, higher audio quality, and closer alignment with textual prompts. \nMoreover, by conditioning on a metronome-like track, our framework naturally supports tempo-constrained generation, achieving state-of-the-art alignment with the target rhythmic structure - all without requiring any additional tempo-specific module.\nAs a result, STAGE offers a practical, versatile tool for interactive music creation that can be readily adopted by musicians in real-world workflows.",
      "abstract": "Recent advances in generative models have made it possible to create high-quality, coherent music, with some systems delivering production-level output.\nYet, most existing models focus solely on generating music from scratch, limiting their usefulness for musicians who want to integrate such models into a human, iterative composition workflow. \nIn this paper we introduce STAGE, our \"STemmed Accompaniment GEneration\" model, fine-tuned from the text-to-music MusicGen model to generate single-stem instrumental accompaniments conditioned on a given mixture. Inspired by instruction-tuning methods for language models, we extend the transformer's embedding matrix with a context token, enabling the model to attend to a musical context through prefix-based conditioning.\nCompared to the baselines, STAGE yields accompaniments that exhibit stronger coherence with the input mixture, higher audio quality, and closer alignment with textual prompts. \nMoreover, by conditioning on a metronome-like track, our framework naturally supports tempo-constrained generation, achieving state-of-the-art alignment with the target rhythmic structure - all without requiring any additional tempo-specific module.\nAs a result, STAGE offers a practical, versatile tool for interactive music creation that can be readily adopted by musicians in real-world workflows.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Giorgio Strano",
        "Chiara Ballanti",
        "Donato Crisostomi",
        "Michele Mancusi",
        "Luca Cosmo",
        "Emanuele Rodol\u00e0"
      ],
      "authors_and_affil": [
        "Giorgio Strano (Sapienza University of Rome)",
        "Chiara Ballanti (Sapienza University of Rome)",
        "Donato Crisostomi (Sapienza, University of Rome)*",
        "Michele Mancusi (Sapienza University of Rome)",
        "Luca Cosmo (Ca' Foscari University of venice)",
        "Emanuele Rodol\u00e0 (Sapienza University of Rome)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "",
        "Generative Tasks",
        "Music and audio synthesis"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1PbpT8Kwha6P9VVBWhjP7bvoO_UKYqWjz/preview",
      "poster_pdf": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-6-stage-stemmed-accompaniment",
      "title": "STAGE: Stemmed Accompaniment Generation through Prefix-Based Conditioning",
      "video": ""
    },
    "forum": "269",
    "id": "269",
    "pic_id": "",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nStrongly agree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nStrongly agree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nStrongly agree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nStrongly agree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThis proposes a new, straightforward, light-weight method to perform accompaniment generation through prefix-based conditioning. The reusable insights include 1) using a context token + context is pre-pended to the models input is an efficient strategy for accompaniment generation 2) tempo conditioning via a click-track can be used for tempo/beat control 3) evaluation shows the proposed method works quite well compared to past works.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nLight-weight fine-tuning of LLM-based music generation models can enable single-stem instrumental accompaniment generation, given a mixture.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nSummary\nIn this paper, the authors propose a new, straightforward, light-weight method to perform accompaniment generation through prefix-based conditioning for LLM-based music generation models (MusicGen). The reusable insights include 1) using a context token + context is pre-pended to the models input is an efficient strategy for accompaniment generation 2) tempo conditioning via a click-track can be used for tempo/beat control 3) evaluation shows the proposed method works quite well compared to past works. The results sound very convincing.\nMajor comments\nOverall, very well done. The paper is an exciting update to stem accompaniment generation for MusicGen and related LLM-based musci generation models. The paper is well written both at a high-level and low-level gramaticaly structure, the review section is nice, compact and useful, and the proposed method is clean, straightforward, and works well. The evaluation is clear and compares against relevant past work this applicable and results show the method works well.\n\nMinor comments\n\u2022 To the best of my understanding, MusicGen is no longer state of the art. Stable Audio (Open) and even fast, distilled diffusion models like Presto outperform MusicGen. I would recommend reducing the use of such language w.r.t. MusicGen as it\u2019s not too necessary to maintain the strong impact of the proposed work.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nSummary: In this paper, the authors propose a new, straightforward, light-weight method to perform accompaniment generation through prefix-based conditioning for LLM-based music generation models (MusicGen). The reusable insights include 1) using a context token + context is pre-pended to the models input is an efficient strategy for accompaniment generation 2) tempo conditioning via a click-track can be used for tempo/beat control 3) evaluation shows the proposed method works quite well compared to past works. The results sound very convincing.\n\nInitial Scores: 1 strong accept, 1 weak accept, 1 weak reject, 1 strong reject\n\nMetareview: Overall, the reviews are generally positive. In summary, there are three main topic areas for improvement include \n#1 - R1 \"it is not clear if this method could successfully be extended to other instruments. Therefore, I believe the paper\u2019s claim of proposing a cost-effective and parameter-efficient method for single-stem generation conditioned on accompaniment is overly broad and not sufficiently supported by evidence.\"\n# 2 - R3 - \"The results strongly overlap with concurrent work by MusicGen-Stem. This is acknowledged, but I would like to see a more direct comparison. MusicGen-Stem provides audio samples, showing comparisons on the same examples would strengthen this paper.\"\n# 3 - R3 - \"No audio samples were provided, which I would expect for this type of research. No code is provided as well.\" -- Note this is incorrect and audio files were provided as well as promise of code release\"\n\nDiscussion: Issue #3 is not a real issue as examples were provided. I believe issue #2 should not be a deciding factor given the costly natural of such data collection. Issue #2 is likely the largest issue so that we have a better idea on how different methods compare, but is noted as concurrent work (within between MusicGen-Stem Jan 7th and ISMIR deadline). This gap is relatively large to note concurrent work, but it\u2019s possible this work was done beforehand and also arXiv\u2019d. For reviewers and meta-reviewers, we must avoid any potential issue of discovering author information and not search for it and should assume the benefit of the doubt here. I recommend we accept the concurrent work comment and accept.\n\nRecommendation: Accept",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong reject\n\n#### Main Reviews\n\nThe paper proposes a method for stem generation using a pretrained music generation model. The paper is well-organized and well-written, and the logic is easy to follow. The motivation for single-stem generation is clearly explained. \n\nThe drum generation model proposed in the paper clearly outperforms the state-of-the-art. The bass generation model, on the other hand, gives similar results to the methods included in the paper. Furthermore, given the performance gap between drum and bass models, it is not clear if this method could successfully be extended to other instruments. Therefore, I believe the paper\u2019s claim of proposing a cost-effective and parameter-efficient method for single-stem generation conditioned on accompaniment is overly broad and not sufficiently supported by evidence. For this reason, I recommend a rejection for the paper.\n\nBelow are a few additional comments:\n- The introduction mentions \u201cnatural music composition workflows\u201d, and how the paper focuses on a \u201chuman-centered, intuitive generation task\u201d. Without a reference, the paper sounds like it is the first one focusing on this iterative workflow. Some of the stem-based generation citations should be included here.\n- Line 208: An optional text prompt is mentioned; however, there are no experiments about this.\n- The evaluation methods are very solid and a strong point of the paper.\n- Lines 356\u2013366: The paper explains why the results for bass generation are worse compared to drum generation. These factors could be even more amplified for other instruments, and therefore make the proposed method not successful for single-stem generation. \n- Lines 414\u2013417: Although having multiple STAGE instances (one per instrument) is a good idea, claiming this implies that STAGE can be successfully extended to other instruments, which is not supported by the paper.\n\nIn conclusion, I believe the paper tackles an important task of iterative music generation. However, more experiments showing the proposed method can extend to different instrument types are crucial. I highly encourage the authors to incorporate experiments with more instruments and resubmit the work.",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nStrengths:\n- Addresses a key workflow gap\n- Insightful discussion section\n- Parameter efficient conditioning\n\nAccept: this paper presents STAGE, a parameter\u2011efficient prefix\u2011conditioning method on pretrained MusicGen that delivers state\u2011of\u2011the\u2011art rhythmic alignment, strong coherence and audio quality, and flexible tempo control without extra modules, all with minimal fine\u2011tuning; while its focus on drums and bass limits instrument scope and bass performance, the approach\u2019s efficiency, insightful comparisons of conditioning mechanisms, and planned code release make it a valuable contribution.",
      "review3": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nThe paper presents a simple method for fine tuning MusicGen to make stem accompaniments. The paper shows this is possible with very little data and compute.\n\nThe ablation of task mixing is good. I would like to see more ablations of similar nature, such as if a single model can be trained for both bass and drums.\n\nThe results strongly overlap with concurrent work by MusicGen-Stem. This is acknowledged, but I would like to see a more direct comparison. MusicGen-Stem provides audio samples, showing comparisons on the same examples would strengthen this paper.\n\nNo audio samples were provided, which I would expect for this type of research. No code is provided as well.\n\nMore exploration of different ways of adding conditioning would be interesting. The note on cross attention performing poorly is great, I would like to see more analysis of that and other methods of conditioning.\n\nOverall I believe the core idea of the paper is strong, but it isn't surprising that this works given the literature of multimodal LLMs and concurrent work in the music domain. More analysis of alternate approaches, audio samples, and reproducible code would strengthen this paper to an accept."
    },
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Recent advances in deep generative modeling have enabled high-quality models for musical audio synthesis. However, these approaches remain difficult to control, confined to simple, static attributes and, most importantly, entail retraining a different computationally-heavy architecure for each new control. This is inefficient and impractical as it requires substantial computational resources.\nIn this paper, we propose a novel approach allowing to add time-varying musical controls on top of any pretrained generative models with an exposed latent space (e.g. neural audio codecs), without retraining or finetuning. Our method supports both discrete and continuous attributes by adapting a rectified flow approach with a latent diffusion transformer. We learn an invertible mapping between pretrained latent variables and a new space disentangling explicit control attributes and style variables that capture the remaining factors of variation.\nThis enables both feature extraction from an input, but also editing those features to generate transformed audio samples. Finally, this also introduces the ability to perform synthesis directly from the audio descriptors. \nWe validate our method with 4 datasets going from different musical instruments up to full music recordings, on which we outperform state-of-the-art task-specific baselines in terms of both generation quality and accuracy of the control by inferring transferred attributes. Our code is available on the supporting webpage.",
      "abstract": "Recent advances in deep generative modeling have enabled high-quality models for musical audio synthesis. However, these approaches remain difficult to control, confined to simple, static attributes and, most importantly, entail retraining a different computationally-heavy architecure for each new control. This is inefficient and impractical as it requires substantial computational resources.\nIn this paper, we propose a novel approach allowing to add time-varying musical controls on top of any pretrained generative models with an exposed latent space (e.g. neural audio codecs), without retraining or finetuning. Our method supports both discrete and continuous attributes by adapting a rectified flow approach with a latent diffusion transformer. We learn an invertible mapping between pretrained latent variables and a new space disentangling explicit control attributes and style variables that capture the remaining factors of variation.\nThis enables both feature extraction from an input, but also editing those features to generate transformed audio samples. Finally, this also introduces the ability to perform synthesis directly from the audio descriptors. \nWe validate our method with 4 datasets going from different musical instruments up to full music recordings, on which we outperform state-of-the-art task-specific baselines in terms of both generation quality and accuracy of the control by inferring transferred attributes. Our code is available on the supporting webpage.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Sarah Nabi",
        "Nils Demerl\u00e9",
        "Geoffroy Peeters",
        "Frederic Bevilacqua",
        "Philippe Esling"
      ],
      "authors_and_affil": [
        "Sarah Nabi (IRCAM)*",
        "Nils Demerl\u00e9 (IRCAM)",
        "Geoffroy Peeters (Telecom Paris)",
        "Frederic Bevilacqua (IRCAM)",
        "Philippe Esling (IRCAM)"
      ],
      "channel_url": "",
      "day": "4",
      "keywords": [
        "Generative Tasks",
        "Representations of music",
        "Music generation",
        "Music and audio synthesis",
        "MIR tasks",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1s899zMWWZdNyl2KL_LUgGd0BwqmXDrGV/preview",
      "poster_pdf": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-6-adding-temporal-musical",
      "title": "Adding temporal musical controls on top of pretrained generative models",
      "video": ""
    },
    "forum": "274",
    "id": "274",
    "pic_id": "",
    "position": "06",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nDisagree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nStrongly agree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nSimilar generative models could be built using the proposed approach, by using different datasets and defining custom control variables. Also the results provided give some insights on which musical aspects could serve as a good set of user control variables (sufficiently disentangled, interpretable etc.) when designing generation systems\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nTime-varying musical controls can be added to a pre-trained generative model with latent variables, without needing to train the generative model, by training an invertible mapping between latent and control space\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper pursues a valuable research direction: As pre-trained models become more and more difficult to train or fine-tune, current methods for adding musical controls to them becomes less feasible. \n\nThe proposed method, while not entirely novel as it is mostly adapted from PluGeN, does entail a small extension for time-varying controls and makes a convincing case (theoretically and experimentally) for why that extension is needed.\n\nIt delivers a good set of experiments, featuring numerous baselines, different types of tasks (retrieval, editing, generation) as well as application domains (monophonic, polyphonic single instrument, full music). Using MSE as evaluation metric for the melody extraction task, while fulfilling its job in the context of the paper, is unusual, so adding some clarification on why the usual F1-based scores (e.g. Overall Accuracy) are not used would be helpful. \n\nAnother point that needs a bit more clarification is the choice of control variables. It seems they were carefully chosen, but that might limit applicability to other tasks as it is not clear how (e.g. just four tags for tagging, but all basic pitch features for melody control). Do control variables need to be very independent of each other, and what happens if they are not? Are there any guidelines for selection? \n\nThe paper is well written and flows well overall. One potential issue is that I could not fully understand how the SDEdit baseline works, as in, how the edited version is created once an input example is mapped to its corresponding noise vector.\n\nMinor issues:\nL37 - Reference needed for this claim\nL51 - Reference would be helpful here\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nSummary of Reviews\n\nThis paper proposes a method to introduce time-varying control over pretrained generative models by learning an invertible mapping between the latent space and a disentangled control space. The approach avoids retraining the generative model and is demonstrated across multiple tasks including editing, retrieval, and conditional synthesis.\n\nThree reviewers gave strong accept recommendations, and one a weak accept. The reviewers agreed that the paper is clearly written, scientifically sound, and relevant to the ISMIR community. While the model and core ideas build on prior work (notably PluGeN), the extension to temporal control is seen as a meaningful and well-executed contribution. The breadth of evaluation is also a strong point. \n\nSome concerns were raised regarding missing ablation studies, unclear implementation details for continuous controls, and assumptions about the independence of control variables. However, these issues can be considered relatively minor, as they can be addressed by improving the writing for a camera-ready version.\n\nFinal recommendation\n\nThis is a well-executed paper that makes a timely and practical contribution to controllable music generation. While its methodological novelty is relatively incremental, the practical impact is significant. I recommend acceptance, with the expectation that minor issues be addressed in the final version.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe paper is well written with good organization and story-telling ability which allows a reader who is relatively unaware of SOTA progress to follow what has been done. As mentioned earlier in this review, I believe the proposed framework is addressing an important general question and has potentials for a wide range of applications. Below, I list a few comments for the authors to consider:\n\n1. I suppose the notion of \"time\" in Eq. (2) is different from the usual definition of time is music or digital signal processing. Though this is not a new idea in machine learning and should be clear in the context, the authors may want to point out the difference explicitly (perhaps with a footnote) to avoid causing any potential confusion. Same suggestion for \"frame-rate\" near the end of page 3.\n\n2. Somehow, line 163 contains mutiple lines -- but it appears that [0,M_k]^K should be changed to [0, M_k-1]^K if M_k is the number of classes for each attribute.\n\n3. Figure 2: thank you for a very nice illustration.\n\n4. Line 307 and 357: conditionnal -> conditional\n\n5. Regarding the anonymous demo page, here are some comments:\n(a) In the \"pitch\" plots, what does the control variable represent? At first I thought the plots are pitch contours, but then some results do not look right.\n(b) for audio editing: I feel that the proposed method is indeed better than AFTER, but there is definitely room for further improvement in the future in terms of the synthesized sound clarity.\n(c) Conditional synthesis: the present results are quite thought-provoking. The female singing example especially piques my interest since the results indeed sound like somebody is humming with free pronunciation.",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\n1. The problem statement is clear. The authors attempt to add time-varying controls on top of existing generative models without requiring retraining.\n\n2. With SDE-Edit and PluGeN, the baselines are sufficient.\n\n3. The theoretical part largely inherits the ideas from PluGeN, with the main novelty being the addition of time-varying control and the use of rectified flow. A few points to consider:\n\n3.1 The core of adding time-varying control primarily lies in the alignment step. It would be helpful to describe the alignment process in more detail. For example, do you tweak the parameters of the libraries to ensure consistent hop sizes? Do you apply high-level descriptors using a single value for the entire song across the temporal axis?\n\n3.2. You may want to explain how you compute \\sigma_i.Furthermore, how are a_max and a_min defined? Are they per-batch, per-song, per-dataset, or heuristically defined global values?\n\n3.3 Please write Equation (10) more rigorously. For example: L_\\theta = min_\\theta E_{t \\sim [0, 1]} (||...||_2^2) \n\n4. The experiments can potentially be improved, in particular:\n\n4.1 There are no ablation studies. Given the introduction of both time-varying control and rectified flow as extensions of the PluGeN baseline, these two components should be ablated separately before presenting the full comparison in Tables 1 and 2.\n\n4.2 It may also be beneficial to organize the comparison tables in a way that is consistent with Section 4.2.",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper presents a solution to control existing pre-trained generative models by manipulating their latent codes. By learning a mapping between the latent space and a new space where the dimensions correspond to various user defined conditions, the authors show that generations exhibit increased controllability while maintaining fidelity of generation. Through 3 tasks, reconstruction, editing and conditional synthesis, they show that their model performs reasonably better than competitive baselines on multiple datasets. \n\nI think the paper does a very convincing job of presenting the quality of their model with well thought out experiments and discussion. The method is light-weight and impactful due to its ability to work on any exposed latent space. There will also be a release of code which is much appreciated. I only have some small suggestions which I list below:\n\nHandling of continuous variables a_c (section 3.1): Eq 7, multiplies the normal distribution function for values of i ranging from 0 to M_k. These values are discrete class values that the attribute a_k can take (interpreted from the paragraph between lines 162 - 163). However there is no mention of how this equation can be adapted for continuous attributes even though this an important part of experiments and contributions presented in this paper. \nAlgorithm 1: a_min and a_max haven\u2019t been defined anywhere. I think it should be clarified that these values are calculated across the dataset and not on a per-sample basis.\nIndependence of control variables: The PluGeN framework assumes that the control signals are independent. However the control signals defined in this paper aren\u2019t necessarily independent: for instance instrument label could be correlated with the pitch distribution, octave, sharpness and so on. Perhaps these correlations aren\u2019t strong enough to significantly affect performance. Either way, I believe this should be addressed in the text.\nDifference between table 1 and table 2: Is the difference just that the models in table 2 have additional continuous descriptors? I\u2019m surprised by how the onset F1 score and instrument accuracy values degrade just by the adding of another control variable. Is there an intuition for why this is the case? It would also be interesting to hear the difference between these samples. \nListening tests: It would be helpful and more convincing to have preference scores from actual humans, especially for higher level features like emotions. It is difficult to simply trust a quantitative method for such conditions.\nSamples page (similar to previous point): I think the samples page is very well organized and is impressive. I would love to see examples of the emotion based generations there as well. Since these features are pretty high-level, I think it is important to ground them in both human preference and highlight examples of them. \n\nMinor corrections\n7 a. Line 273: synthetis -> synthesis\n7 b. Line 307, 5.1.3 heading: conditionnal -> conditional"
    },
    "session": "7"
  },
  {
    "content": {
      "TLDR": "In this paper we address the task of keyboard temperament estimation from symbolic data. The aim is to find a keyboard temperament that minimizes the deviations from pure intervals, given a corpus of music. The problem of finding a suitable temperament has been studied for centuries. Many solutions have been proposed. By taking a data-driven approach, we contribute a method to this field. We define a loss function that measures the deviation from pure intervals, with a reward for exactly pure intervals. Three optimization methods are explored: Basin Hopping, Differential Evolution, and Dual Annealing. We validate our method with synthetic data, and by comparing with c. 1,500 existing temperaments, including equal temperament. Our method improves on any existing temperament. As a case study, we apply the method to Bach's Well-Tempered Clavier. Our findings show interesting correspondence to existing proposals in musicological literature.",
      "abstract": "In this paper we address the task of keyboard temperament estimation from symbolic data. The aim is to find a keyboard temperament that minimizes the deviations from pure intervals, given a corpus of music. The problem of finding a suitable temperament has been studied for centuries. Many solutions have been proposed. By taking a data-driven approach, we contribute a method to this field. We define a loss function that measures the deviation from pure intervals, with a reward for exactly pure intervals. Three optimization methods are explored: Basin Hopping, Differential Evolution, and Dual Annealing. We validate our method with synthetic data, and by comparing with c. 1,500 existing temperaments, including equal temperament. Our method improves on any existing temperament. As a case study, we apply the method to Bach's Well-Tempered Clavier. Our findings show interesting correspondence to existing proposals in musicological literature.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Peter Van Kranenburg",
        "Gerben Bisschop"
      ],
      "authors_and_affil": [
        "Peter Van Kranenburg (Utrecht University",
        "Meertens Institute)*",
        "Gerben Bisschop (Utrecht University)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Symbolic music processing",
        "Applications",
        "Musical features and properties",
        "Harmony, chords and tonality",
        "Music composition, performance, and production",
        "Computational musicology",
        "MIR fundamentals and methodology",
        "Computational music theory and musicology",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "TRUE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/14MFBaAro6abPbASQcG8FR5h7NyPJ2m4B/preview",
      "poster_pdf": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-1-keyboard-temperament-estimation",
      "title": "Keyboard Temperament Estimation from Symbolic Data: A Case Study on Bach's Well-Tempered Clavier",
      "video": ""
    },
    "forum": "278",
    "id": "278",
    "pic_id": "",
    "position": "01",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nStrongly agree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nStrongly agree\n\n**Q10 (Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chose, otherwise write \"n/a\"))**\n\nThe study is robust, reproducible, and technically rigorous. The authors validate their method with synthetic data and systematically evaluate across multiple target sets, optimization strategies, and historical corpora (including the full WTC).\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nStrongly agree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nStrongly Agree (Very novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nStrongly agree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe framework is generalizable beyond the WTC and could be applied to other corpora, styles, or even tuning systems outside Western music. The loss function design, optimization approach, and corpus weighting mechanisms are highly reusable.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThis paper introduces a method to estimate keyboard temperaments from symbolic corpora using weighted interval loss functions and optimization, revealing compelling links to historical temperaments.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nStrongly agree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper presents a novel and rigorously developed framework for estimating keyboard temperaments from symbolic musical corpora, using optimization over weighted interval inventories and comparing results with hundreds of historical temperaments. The use of Bach\u2019s WTC as a case study is both musically and historically relevant. The authors strike a good balance between technical modelling, algorithmic rigor, and interpretive reflection.\n\nStrengths:\n- Innovative problem formulation that bridges MIR and musicology.\n- Well-designed and theoretically grounded loss and reward functions.\n- Thorough validation and comparative analysis with historical temperaments.\n- Rich engagement with secondary literature and music theory.\n\nSuggestions for improvement:\n- Clarify parameter settings (e.g., weightings of intervals, \u03b2 in exponential decay) and their sensitivity.\n- Provide a higher-level summary of technical sections for non-specialist MIR readers.\n- Consider sharing code or a simplified demo to aid reproducibility and adoption.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nStrong accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThis paper presents a novel and technically robust framework for estimating keyboard temperaments from symbolic musical corpora, using a corpus-weighted loss function defined over mistuned intervals. The approach is applied to Bach\u2019s Well-Tempered Clavier and evaluated through a comparison with over 200 historical temperaments. The paper stands out for its synthesis of computational musicology, symbolic MIR, optimization techniques, and historical interpretation.\n\nThe reviewers unanimously rated the paper \u201cStrong Accept,\u201d praising its methodological rigor, clarity, and musical significance. Several aspects of the work deserve special mention:\n\u2022 Originality: The paper proposes a compelling new task\u2014temperament estimation from symbolic corpora\u2014grounded in centuries of musicological inquiry, yet not previously explored computationally in this way.\n\u2022 Scientific quality: The loss functions, evaluation metrics, and optimization strategies are carefully chosen and well explained. The results are both reproducible and interpretable.\n\u2022 Impact: While this is not a mainstream MIR task, it bridges computational methods with questions of historical performance practice and musical structure, making it relevant to a wide range of ISMIR attendees.\n\nSuggestions for camera-ready improvements\u2014echoed across several reviews\u2014include:\n\u2022 Clarifying parameter choices (e.g., \u03b1, \u03b2, and snapping functions) and assessing sensitivity.\n\u2022 Reframing the novelty claims to avoid overstating the task as entirely new, and better emphasizing the contribution as a well-motivated formalization of an existing musicological problem.\n\u2022 Improving reproducibility by sharing code and, if possible, sound examples (e.g., synthesized outputs using inferred tunings).\n\u2022 Minor presentation issues, such as labeling, figure reordering, and typographic adjustments.\nThese are all minor refinements that, once addressed, will further enhance the clarity and accessibility of this valuable work.\n\nThis paper introduces an innovative and rigorous method for historical temperament estimation, grounded in music theory and symbolic MIR. It makes a unique and reusable contribution to the ISMIR community and merits inclusion in the conference.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe authors propose a family of criteria to determine how well a 12-tone temperament fits to a given corpus based on (a) a set of possible target sizes for each interval (corresponding to simple frequency ratios) and (b) the harmonic and melodic intervals encountered in the corpus, weighted by metric position. They apply a range of optimization methods to find good solutions and compare them to a large set of historic temperaments. They find that under the given criteria, usually better temperaments than the historically discussed ones can be found, but that the observed patterns nevertheless reflect the historic discourse.\n\nThe paper is very straightforward and well-written. The proposed method is simple, sensible, produces interesting results, and can be applied to other historic collections, both for scholarly investigation and for performance. I would consider the novelty rather low since the problem of optimal temperaments has been widely discussed (as the paper shows), however there is clearly a contribution here. I therefore happily recommend to accept the paper.\n\nMy main complaint about the paper would be that the framing of the contribution is a bit off. For example, I would advise against phrases such as \"introducing a new task\" here since the task here since the underlying problem (finding a good temperament for a certain style of music or a given set of pieces) has been discussed in some form for quite some time, as the authors demonstrate themselves. Similarly, the authors speak of their method \"outperforming\" existing temperaments. I would argue that the main contribution is the loss function, not the optimization methods which are applied out-of-the-box and could be substituted with other non-linear optimization methods. In that respect, arguing that the proposed method works best because the results score best wrt. the proposed loss turns the argument on its head (it would be easy to define an entirely unsuitable loss for which the best solutions would naturally score better than existing temperaments). The actual usefulness of the proposed criterion is (a) derived from sensible underlying assumptions and (b) demonstrated by obtaining interesting solutions that are similar to the historic ones.\n\nI suspect that this focus on \"performance\" results from perceived norms for evaluation at ISMIR, which I see as a bit of a structural problem. In this case, I think that the theoretical discussion of the model assumptions and the results make for the better argument, and I would advise the authors to adapt their framing in this direction.\n\nMinor remarks:\n- l. 167: using the chordify method to determine harmonic intervals is a bit problematic since the weight that a particular harmonic interval receives depends on what happens in a third voice. E.g., an interval between voices 1 and 2 of a quarter is counted once in case no other voice moves, but twice or more in case another voice moves during this interval. Counting each occurring harmonic interval exactly once or weighting it by its duration would both have been sensible choices, but this method is a bit odd.\n- l. 219: for clarity, mention that interval sizes here are given in cents, not as frequency ratios (for additivity)\n- l. 290: capitalize \"validation\"",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nNovelty: Introducing a new problem of optimizing keyboard temperament based on input music (whether synthetic or real) is an interesting problem and is worth studying.\n\nScientific rigor/merit: high. Methodologically strong paper, mathematically precise. Presents finding a temperament as an optimization problem, which seems computationally and mathematically practical. Presentation of the problem and proposed solution is clear. Loss function seems grounded in reality. Evaluation through three different optimization algorithms seems fine, especially since they all get similar results. All facets are adequately described. I thought the discussion and engagement with historical musicological debates was good, although might go over some people's heads. Minor problems: loss function tuning could be better (setting parameters for alpha, beta, etc). Right now these are set to magic numbers, which isn't great, but seems to work ok.\n\nClarity/readability: High. Logical flow and organization, math is clear and precise. Some musical jargon that we don't see a lot of in ISMIR (wolf fifths, circular temperament, etc) but that can be chalked up to that ISMIR doesn't get a lot of work on temperament. Could use a picture or more explanation of the \"snapping\" effect of R_pure.\n\nRelevance: While this work is novel, scientifically rigorous, and clear, it may not be relevant to the majority of ISMIR attendees, but I do not think it should be penalized for that. It's data-driven work, which is important, as is combining computational musicology with performance practice.",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nInteresting work, aiming to infer the best temperament for a set of pieces, favoring so-called \"pure\" intervals. The experiments are solid and engaging. It's unfortunate that the code is not available for replication, as well as synthesized versions of some WTC pieces using the inferred optimal intervals\u2014this would allow for a subjective evaluation of whether the resulting tunings are indeed more compelling. A brief note discussing the practical applicability of this approach in real-world settings would be valuable: how feasible would it be to tune instruments according to the obtained temperaments? Of course, this would be much easier on a synthesizer, but that contrasts with historically informed performance practices. It would be nearly impossible, for instance, to implement such tuning on a pipe organ.\n\nI was intrigued that the optimization appears to be performed\u2014if I understood correctly\u2014on the intervals listed in Table 1. I\u2019m curious about what the results would be if the optimization were unconstrained, requiring only that the sequence of p's be increasing. Would we converge toward equal temperament when using a large corpus of pieces? Section 4 presents an interesting musicological discussion! I confess I considered recommending 'weak accept' due to the lack of code, but I believe this does not diminish the contribution of the work\u2014so I decided on a 'strong accept'.\n\nAdditional comments:\n\nLines 164\u2013165: \"but we can decide to omit intervals\" \u2014 Why would that be desirable?\n\nTable 1: Please provide a source for the \u201cacceptable\u201d intervals listed.\n\nSection 2.6 title should start with a capital letter.\n\nFigure 2: Consider reordering the pitch classes starting from C, as done in the keyboard diagram in Figure 1."
    },
    "session": "5"
  },
  {
    "content": {
      "TLDR": "We propose Expotion (Facial Expression and Motion Control for Multimodal Music Generation), a generative model leveraging multimodal visual controls\u2014specifically, human facial expressions and upper-body motion\u2014as well as text prompts to produce expressive and temporally accurate music. We adopt parameter-efficient fine-tuning (PEFT) on the pretrained text-to-music generation model, enabling fine-grained adaptation to the multimodal controls using a small dataset. To ensure precise synchronization between video and music, we introduce a temporal smoothing strategy to align multiple modalities. Experiments demonstrate that integrating visual features alongside textual descriptions enhances the overall quality of generated music in terms of musicality, creativity, beat-tempo consistency, temporal alignment with the video, and text adherence, surpassing both proposed baselines and existing state-of-the-art video-to-music generation models. Additionally, we introduce a novel dataset consisting of 7 hours of synchronized video recordings capturing expressive facial and upper-body gestures aligned with corresponding music, providing significant potential for future research in multimodal and interactive music generation. Code, demo and dataset are available at https://github.com/xinyueli2896/Expotion.git",
      "abstract": "We propose Expotion (Facial Expression and Motion Control for Multimodal Music Generation), a generative model leveraging multimodal visual controls\u2014specifically, human facial expressions and upper-body motion\u2014as well as text prompts to produce expressive and temporally accurate music. We adopt parameter-efficient fine-tuning (PEFT) on the pretrained text-to-music generation model, enabling fine-grained adaptation to the multimodal controls using a small dataset. To ensure precise synchronization between video and music, we introduce a temporal smoothing strategy to align multiple modalities. Experiments demonstrate that integrating visual features alongside textual descriptions enhances the overall quality of generated music in terms of musicality, creativity, beat-tempo consistency, temporal alignment with the video, and text adherence, surpassing both proposed baselines and existing state-of-the-art video-to-music generation models. Additionally, we introduce a novel dataset consisting of 7 hours of synchronized video recordings capturing expressive facial and upper-body gestures aligned with corresponding music, providing significant potential for future research in multimodal and interactive music generation. Code, demo and dataset are available at https://github.com/xinyueli2896/Expotion.git<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Fathinah Izzati",
        "Xinyue Li",
        "Gus Xia"
      ],
      "authors_and_affil": [
        "Fathinah Izzati (MBZUAI)*",
        "Xinyue Li (MBZUAI)",
        "Gus Xia (MBZUAI)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Generative Tasks",
        "Music generation",
        "Multimodality",
        "Creativity",
        "Novel datasets and use cases",
        "Music and audio synthesis",
        "Alignment, synchronization, and score following",
        "MIR tasks",
        "Evaluation, datasets, and reproducibility",
        "MIR fundamentals and methodology",
        "Creative practice involving MIR or generative technology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/16gGPWuE4msol558OOWLrmBjU4aLv-nbZ/preview",
      "poster_pdf": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-13-expotion-facial-expression",
      "title": "Expotion: Facial Expression and Motion Control for Multimodal Music Generation",
      "video": ""
    },
    "forum": "280",
    "id": "280",
    "pic_id": "",
    "position": "13",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nDisagree\n\n**Q5 ( Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chosen, otherwise write \"n/a\"))**\n\nThe authors claimed that \"this work is the first to leverage synchronized expressive gestures and facial expressions for music generation.\" However, a quick google scholar search brought up many papers:\n\n- Roberto Valenti, Alejandro Jaimes and Nicu Sebe, \"Sonify Your Face: Facial Expressions for Sound Generation,\" MM, 2010.\n- Jiang Huang, Xianglin Huang, Lifang Yang, and Zhulin Tao, \"D2MNet for music generation joint driven by facial expressions and dance movements Author links open overlay panel,\" Array, 2024.\n- Alexis Clay, Nadine Couture, Elodie Decarsin, Myriam Desainte-Catherine,\nPierre-Henri Vulliard, and Joseph Larralde, \"Movement to emotions to music: using whole body emotional expression as an interaction for electronic music generation,\" NIME, 2012.\n- Vishesh P, Pavan A, Samarth G Vasist, Sindhu Rao, and K. S. Srinivas, \"Movement to emotions to music: using whole body emotional expression as an interaction for electronic music generation,\" I2CT, 2022.\n- Jiang Huang, Xianglin Huang, Lifang Yang, and Zhulin Tao, \"A Continuous Emotional Music Generation System Based on Facial Expressions,\" ICID, 2022.\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nStrongly agree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nStrongly agree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nStrongly agree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe proposed adaption and finetuning methodology for equipping pretrained music generation models with additional multimodal controls can likely be reused in future work to enable other control signals.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThrough adapting and finetuning a pretrained music generation model, we can allows an user to control a music generation system through visual gestures and facial expressions.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nStrongly agree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\n### Summary\n\nThis paper proposes a novel interactive music generation system that can be controlled by visual gestures and facial expressions. The authors propose to adapt and finetune an existing music generation system to take into new multimodal control signals. The authors compile a new dataset consisting of 7 hours of video recordings with synchronized responsive facial expressions and upper-body movements. With the proposed dataset, this paper shows the effectiveness of the proposed method through objective and subjective evaluations.\n\n### Strengths\n\n- The paper is clearly-written and easy to follow.\n- The paper addresses a potentially-impactful research direction towards interactive music generation.\n- The provided qualitative examples clearly show the effectiveness of the proposed method.\n- The authors conducted extensive evaluations through both objective metrics and a subjective survey. The ablation studies are also well-designed, and the results are clearly-presented.\n- The proposed dataset will be a great contribution to the community if made publicly available. However, the authors did not discuss data release plan in the paper.\n\n### Weaknesses\n\n- The authors fail to discuss connections to existing non deep learning based interactive generative music systems. Much prior work can be found in our neighboring communities such as NIME, ICMC, and AIMC.\n- The subjective evaluation results are presented without error bars. While the best performing models achieve much higher scores than those of the baseline model, it is hard to make any strong conclusions and significance claims without error bars.\n- The performance of the model is not strong. The best tempo error is still 28 bpm.\n- While the authors claimed that \"our multimodal controls complement each other in creating better music\" (Line 118-120), the roles of the visuals and texts remain unclear to me.\n\n### Justification of the Overall Evaluation\n\nThis paper represents a significant step towards interactive music generation. The paper is clearly-written and most claims are supported with experimental results. However, the results are not strong, and it remains unclear the effectiveness of the proposed model in supporting meaningful interaction through body movements, facial expressions and text prompts altogether. I am thus recommending a weak accept.\n\n### Detailed Comments and Suggestions\n\n- (Line 118-120) \"In contrast, our multimodal controls complement each other in creating better music.\" -> This is an unsupported claim. While something similar has been discussed near the end of Section 5.2, I don't think we can arrive in this conclusion with the presented results.\n- (Section 4.1: Dataset) Will the dataset be released? If not, how do you ensure reproducibility? If releasing the raw videos is challenging, perhaps the authors can only release the extracted features, e.g., extracted facial landmark and joint positions.\n- (Line 270-272) \"We recruited volunteers to record their facial expressions and upper body movements while listening to 30-second audio clips.\" -> Reactive facial expressions and upper body movements can differ from those intended to be used as inputs to control a music generation system. Some brief discussion would help clarify this.\n- (Section 4.3: Baselines) How did you synthesize the generated MIDI files? Please clarify this.\n- (Line 378-380) \"This configuration performs best overall, with the lowest FAD and KL scores and the highest IS Score,\" -> The KL score is not the lowest.\n- (Section 5.1 and Table 1) Is the higher the IS score, the better? Isn't the optimal IS score be that of the ground truth? If so, please remove the arrow in Table 1 and restate some arguments in Section 5.1.\n- (Figure 3) The axis labels are barely visible. Also, some numbers are close. Error bars would help us see if these differences are significant enough.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nWe have mixed recommendations from the reviewers with 3 weak rejects and 2 weak accepts. The negatives come mostly from concerns about the not-so-strong performance of the proposed method and the lack of some experiment and implementation details. However, given how novel the task and methodology is, I think the weaker performance does not disqualify this paper from acceptance. I believe this paper will generate many fruitful discussions among our community and inspire much follow-up work along this direction. I'm thus recommending a weak accept for this paper.\n\nIf the paper is accepted, the authors must carefully read all the reviews and try their best to address the concerns raised by the reviewers. Specifically, here are the required revisions in the camera-ready version:\n\n- Discuss related work on non deep learning based approaches.\n- Discuss the limitations of the proposed method, especially the 28 bpm tempo error and the not-so-effective facial expression controls.\n- Provide more details about the listening test setup and add error bars to the results.\n\nHere is a summary of the reviews:\n\n### Strengths\n\n- (R3, R6, MR) Clearly-written and easy to follow.\n- (R3, R6) The novelty is significant.\n- (MR) The proposed dataset will be a great contribution to the community.\n\n### Weaknesses\n\n- (R2, R3, R6) The tempo control is not quite effective with an average error of 28 bpm.\n- (R5, MR) Missing error bars for the subjective evaluation.\n- (R1) Missing details about the subjective test.\n- (R1) Dependence of the mode on text prompts is not addressed.\n- (R2) Facial expressions have little effect on generated samples.\n- (R3) Limited objective evaluation results.\n- (MR) Lacking literature review and connections to non deep learning based interactive generative music systems.",
      "publish_reviews": "TRUE",
      "review1": "#REF!",
      "review2": "",
      "review3": ""
    },
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Florence B. Price was a composer in the early 20th century\nwhose music reflects her upbringing in the American\nSouth, her African heritage, and her Western classical\ntraining. She is noted as the first African-American woman\nto have a symphony performed by a major orchestra.\nHer music has recently received renewed attention from\nboth the public and the research community, decades\nafter her death. In addition to other genres, Price was\na prolific composer for solo voice and piano. Music\nhistorians have documented the existence of 134 art songs\nand piano/voice arrangements for spirituals and folk songs\nwritten by Price. We release a digital catalog of 112 of\nthese works in MuseScore, MusicXML, MIDI, and PDF\nformat. We also use this dataset to fine-tune a symbolic\nmusic generation model to generate accompaniments to\nmelodies, and we conduct a blind listening experiment that\nshows that accompaniments generated by our model are\nperceived as being reflective of Florence Price\u2019s style more\nfrequently than accompaniments generated by a baseline\nmodel. We release our model as the Florence Price Piano\nAccompaniment Generator alongside our dataset.",
      "abstract": "Florence B. Price was a composer in the early 20th century\nwhose music reflects her upbringing in the American\nSouth, her African heritage, and her Western classical\ntraining. She is noted as the first African-American woman\nto have a symphony performed by a major orchestra.\nHer music has recently received renewed attention from\nboth the public and the research community, decades\nafter her death. In addition to other genres, Price was\na prolific composer for solo voice and piano. Music\nhistorians have documented the existence of 134 art songs\nand piano/voice arrangements for spirituals and folk songs\nwritten by Price. We release a digital catalog of 112 of\nthese works in MuseScore, MusicXML, MIDI, and PDF\nformat. We also use this dataset to fine-tune a symbolic\nmusic generation model to generate accompaniments to\nmelodies, and we conduct a blind listening experiment that\nshows that accompaniments generated by our model are\nperceived as being reflective of Florence Price\u2019s style more\nfrequently than accompaniments generated by a baseline\nmodel. We release our model as the Florence Price Piano\nAccompaniment Generator alongside our dataset.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Tao-Tao He",
        "Martin Malandro",
        "Douglas Shadle"
      ],
      "authors_and_affil": [
        "Tao-Tao He (Vanderbilt University)*",
        "Martin Malandro (Sam Houston State University)",
        "Douglas Shadle (Vanderbilt University)"
      ],
      "channel_url": "",
      "day": "4",
      "keywords": [
        "Symbolic music processing",
        "Personalization",
        "Applications",
        "Musical features and properties",
        "Music generation",
        "Musical style and genre",
        "Novel datasets and use cases",
        "MIR tasks",
        "Digital libraries and archives",
        "Evaluation, datasets, and reproducibility",
        "MIR fundamentals and methodology",
        "Human-centered MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1pV5ZRXxWaS73Jpl9uyfQiwuS9l-TI14w/preview",
      "poster_pdf": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-5-the-florence-price",
      "title": "The Florence Price Art Song Dataset and Piano Accompaniment Generator",
      "video": ""
    },
    "forum": "281",
    "id": "281",
    "pic_id": "",
    "position": "05",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Psychoacoustical so-called \"timbre spaces\" map perceptual similarity ratings of instrument sounds onto low-dimensional embeddings via multidimensional scaling, but suffer from scalability issues and are incapable of generalization. Recent results from audio (music and speech) quality assessment as well as image similarity have shown that deep learning is able to produce embeddings that align well with human perception while being largely free from these constraints. Although the existing human-rated timbre similarity data is not large enough to train deep neural networks (2,614 pairwise ratings on 334 audio samples), it can serve as test-only data for audio models. In this paper, we introduce metrics to assess the alignment of diverse audio representations with human judgments of timbre similarity by comparing both the absolute values and the rankings of embedding distances to human similarity ratings. Our evaluation involves three signal-processing-based representations, twelve representations extracted from pre-trained models, and three representations extracted from a novel sound matching model. Among them, the style embeddings inspired by image style transfer, extracted from the CLAP model and the sound matching model, remarkably outperform the others, showing their potential in modeling timbre similarity.",
      "abstract": "Psychoacoustical so-called \"timbre spaces\" map perceptual similarity ratings of instrument sounds onto low-dimensional embeddings via multidimensional scaling, but suffer from scalability issues and are incapable of generalization. Recent results from audio (music and speech) quality assessment as well as image similarity have shown that deep learning is able to produce embeddings that align well with human perception while being largely free from these constraints. Although the existing human-rated timbre similarity data is not large enough to train deep neural networks (2,614 pairwise ratings on 334 audio samples), it can serve as test-only data for audio models. In this paper, we introduce metrics to assess the alignment of diverse audio representations with human judgments of timbre similarity by comparing both the absolute values and the rankings of embedding distances to human similarity ratings. Our evaluation involves three signal-processing-based representations, twelve representations extracted from pre-trained models, and three representations extracted from a novel sound matching model. Among them, the style embeddings inspired by image style transfer, extracted from the CLAP model and the sound matching model, remarkably outperform the others, showing their potential in modeling timbre similarity.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Haokun Tian",
        "Stefan Lattner",
        "Charalampos Saitis"
      ],
      "authors_and_affil": [
        "Haokun Tian (Queen Mary University of London)*",
        "Stefan Lattner (Sony CSL Paris)",
        "Charalampos Saitis (Queen Mary University of London)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Timbre, instrumentation, and singing voice",
        "Musical features and properties",
        "Similarity metrics",
        "Representations of music",
        "MIR tasks",
        "Evaluation, datasets, and reproducibility",
        "Evaluation methodology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1_d9SJUraegRKEzjhNtnlPnLXey8qSI61/preview",
      "poster_pdf": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-12-assessing-the-alignment",
      "title": "Assessing the Alignment of Audio Representations With Timbre Similarity Ratings",
      "video": ""
    },
    "forum": "283",
    "id": "283",
    "pic_id": "",
    "position": "12",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nSee below\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThis paper addresses the problem of evaluating how well audio representations, both handcrafted and learned, align with human perceptions of timbre similarity.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper addresses the problem of evaluating how well audio representations, both handcrafted and learned, align with human perceptions of timbre similarity. Building on the concept of timbre spaces known from psychoacoustic studies, the authors present a well-structured evaluation framework and metrics to benchmark a wide range of audio models and datasets.\n\nThe study shows that handcrafted features like MFCCs remain competitive, and although recent models offer only modest improvements, the insights gained are relevant and timely. The analysis helps clarify the current landscape of timbre-aware audio embeddings.\n\nThat said, the paper could benefit from clearer explanations of certain design choices, particularly in Section 4.2, where implementation details often take the focus away from the underlying motivation. The discussion section would also be stronger if it elaborated on why some models, such as CLAP or MFCC, perform well and what this implies for future model development. Finally, while the figures are informative, more guidance through their main takeaways would improve interpretability.\n\nOverall, this is a well-executed and thoughtful contribution. While the gains from recent models are modest, the work lays an important foundation for evaluating timbre-relevant audio representations and is likely to stimulate further research in perceptual model evaluation. \n\nFurther comments:\n\n* Line 10: \"the existing \u2018timbre space\u2019 data\" is unclear. Which data do you refer to?\n* Line 23: specify to which group \"CLAP-based models\" belong to? (One of the pretrained models)\n* Line 77: \"past 21 timbre space datasets\" remains unclear at this point. Are these all the datasets you could identify in previous work? -> Ah, it is explained later (Line 130 ff.)\n* Line 87 ff. (Related Work): You may also mention the article by Abesser et al. (How Robust are Audio Embeddings for Polyphonic Event Tagging? IEEE/ACM TASLP, 2023), which takes a similar approach by analyzing the embedding spaces of two non-trainable audio representations alongside several deep audio embeddings in the context of sound classification.\nLine 207: \"an pair\" should be \"a pair\"\n* Section 4.1.1: Could be shortened as the strategy is straightforward. However, clipping may remove relevant (e.g. decay) information, and zero padding could introduce confounding factors (e.g., correlations between sample length and specific instruments). Maybe comment on this. \n* Line 224-229: This appears straightforward and the text could be made more compact.\n* Line 252-272: Even though standard, a brief explanation of the individual metrics would be helpful; especially in clarifying what each captures. In particular, it would be valuable to explain how the metrics complement each other and why examining them together provides a more complete assessment.\n* Line 279-282: Introducing the three distance functions here may cause confusion, as the metrics mentioned do not align with those used in Section 4.1.2 (e.g., l^1 vs. l^2 norms).\n* Line 287: Give reference to \"Vital\" synthesizer -> Ah, comes later ... Not clear if last letter is an letter or number.\n* Line 300: Write out number \"7\" into \"seven\" \n* Line 312: Write out number ... \n* Line 325-326: The purpose of the eight regression outputs and the two classification heads is unclear. Please clarify what each is intended to represent and how they are used during training and evaluation.\n* Section 4.2: I find Section 4.2 a bit hard to follow as it emphasizes what is done rather than why. The motivation behind key choices, in particular the prediction targets, remains unclear. \n* Line 365: There are various ways to define multi-scale spectral losses, which critically shape the behavior of the loss function. For further discussion, see Schw\u00e4r et al., \"Multi-Scale Spectral Loss Revisited\", IEEE SPL, 2023.\n* Figure 2: It would be helpful to include comments on which models achieved the best scores and why - specifically, what architectural or methodological factors contributed to their strong performance. \n* Line 420: See comment to Line 365\n* Section 5: The discussion of results feels quite compact and ends rather abruptly. It would be helpful to guide the reader more clearly through the key insights from Figures 2 and 3, explaining what each figure illustrates and how it supports the conclusions.\n* Section 5: It would be particularly interesting to analyze how the results depend on the individual dataset and scenario. This could be effectively visualized using a heatmap that reports performance for selected models and a fixed metric (e.g. MAE).\n* Line 479: \"Ddsp\" should be \"DDSP\" (encapsulate {DDSP} in bibtex) \n* Line 564: \"Rwc\" should be \"RWC\" (encapsulate {RWC} in bibtex)\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThe reviewers agree that this paper offers interesting contributions to the topic of timbre similarity ratings, which is both timely and relevant for the ISMIR community. However, there are also concerns regarding the novelty of the approach and the lack of discussion around certain design choices and experimental results.\n\nStrengths:\n* The paper is well written, clear, and easy to follow.\n* The overall message is communicated effectively.\n* The sound matching method demonstrates good performance.\n* The work lays a valuable foundation for evaluating timbre-relevant audio representations.\n* The work is a solid effort to build a unified evaluation framework.\n* Provided Python package is a plus for reproducibility\n\nWeaknesses:\n* The novelty of the proposed approach is limited.\n* A more detailed per-dataset analysis would help build stronger intuition around the framework. \n* While the figures are informative, additional guidance on their key takeaways would improve interpretability.\n* The paper focuses on the \"how\" but provides limited insight into the \"why\"; clearer explanations of specific design choices would strengthen the work.\n* The phrase \"alignment of audio\" may mislead readers to think the paper focuses on the task of audio alignment.\n\nDespite some weaknesses, we recommend a \"weak accept\". The paper establishes an important foundation for evaluating timbre-relevant audio representations and is likely to encourage further research in perceptual model evaluation.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper is well written and clear, however at first, the terms \"alignment of audio\" leads me to think as if the paper was about the task of audio alignment. That convintion was pretty hard to remove from my head! Beside that, I think that the paper is well organized and generally well written. The proposed sound matching method shows good performances however, in my opinion the novelty of the approach is limited. The evaluation is well conducted and the metrics used seems to be well suited for the evaluation of such models (despite the fact that, as stated in the paper, three of them shows an high degrees of correlation suggesting that 2 out of 3 might be redundant). To me, the laking part is in the novelty but I would suggest a weak accept for mainly two reasons: 1) I'm not an expert on this task, so my judjment of the novelty might not be accurate, and 2) the evaluation compares several methods, included some classical signal processing alorithms, it uses different metrics and shows a good insight of the state of the art in this task.",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper presents an evaluation framework for assessing audio representations and their alignments with subjective timbre similarity ratings. The proposed framework leverages 21 existing datasets and implements several metrics in a unified manner. Additionally, a new model (i.e., sound matching), trained via self-supervised learning with synthetic data, is proposed and evaluated in the same framework. The evaluation results suggest that the proposed model and CLAP are both strong contenders compared to other models, and the widely used MFCCs still remain competitive. For the most part, the paper is easy to follow. Occasionally, I found the brief explanations within the brackets to be a little excessive and counterproductive. Nevertheless, I think the overall message of the paper is clear. \n\nThe paper consists of two main components, namely the evaluation framework and the sound matching model. Each of these components, in my opinion, could potentially be a strong contribution on its own, assuming sufficient explanations and insights are provided. When combined together, however, the authors inevitably have to leave out some details in order to make room for all the topics. Unfortunately, the current arrangement makes both components feel somewhat incomplete. For instance, in addition to the aggregated results across 21 datasets, I was hoping to see a more detailed per-dataset analysis, which could provide insights into the variance of these datasets and build a stronger intuition about this framework. Instead, I feel the discussion is shortened or limited in order to cover the sound matching model. Similarly, I feel the section on the sound matching model is too brief and many of the details are still missing (e.g., more information about the parameters in the data generation pipeline and the construction of style embeddings). As a result, neither of these two components is well-explained, in my humble opinion.\n\nAnother potential concern is around the depth of the provided information. In the evaluation framework, the authors tried to cover a variety of audio length handling methods, distance functions, and alignment scores. As such, the paper focuses on explaining \u201chow\u201d to approach these steps and is relatively light on \u201cwhy\u201d these choices were made. For example, it is unclear why there are four rank-based scores and only one error-based score. How can they complement each other? Are there considerations specific to the assessment of timbre similarity? Without further insights and explanations, the choices made in this framework may seem a bit arbitrary and are no more than a collection of standard metrics. As commented by the authors, some of the scores are highly correlated, which implies redundancy in these choices. \n\nDespite the above mentioned concerns, I appreciate the effort of building a unified evaluation framework, and I find the idea of training the sound matching model in the context of timbre similarity intriguing. I believe this work could benefit from another round of revision and polishing, and my initial recommendation was borderline leaning towards a \u201cweak reject\u201d. However, after taking other reviewers' opinions into consideration during the discussion phase, I decided to adjust my recommendation to weak accept.\n\n=============\nMinor comments: \nLine 175, \u201c... there are cases when this is only one\u201d \u2192 I suppose you meant to say \u201cthere are cases when there is only one viable option\u201d? \n\nLine 227-229, \u201cPairs outside the diagonal blocks \u2026 are not considered\u201d \u2192 if that is the case, why not compute metrics per dataset and aggregate the metrics? (as opposed to building a dissimilarity matrix and only consider the diagonal blocks)\n\nLine 251, \u201cfinal rank-based alignment scores \u2026 averaging across all rows\u201d \u2192 since some of these scores are correlation coefficients, I wonder if taking fisher\u2019s z-transform before averaging would be more appropriate? (see [1] for example) \n\nLine 260, \u201c... given margin condition \u2026\u201d \u2192 how do you decide the value of margin?\n\nLine 349, \u201c... (B, C, H, W) \u2026\u201d \u2192 no introduction of these variables? \n\nLine 406, \u201c... style embeddings converge quickly, retaining high alignment \u2026\u201d \u2192 it is hard to interpret the results here\u2026 IMHO, a correlation coefficient of 0.4 may not be considered high in other research fields. It would be helpful if the authors could help the readers set the right expectations. Also, it is a bit odd to present the test results per training epoch. In a way, it could be interpreted as maximizing the test results during training, which makes the comparison to other models somewhat unfair.\n\nLine 409, \u201c... Kendall, Spearman, and triplet metrics are highly correlated \u2026\u201d \u2192 based on visual examination? \n\nLine 421, \u201c... showing spectral distances can be problematic for pitch.\u201d \u2192 I do not understand this statement, for I thought each dataset has been pitch-normalized according to Table 1? \nInconsistency in the reference section: I noticed some minor inconsistent citation formats that could be easily improved. For instance, some of the conference papers were cited as \u201cin Proceedings of \u2026\u201d and some of them were not. Also, for the same journal (i.e., JASA), some of the entries have all upper cases and some of them are lower cases. I would recommend another pass through in the next iteration. \n\n[1] Alexander, Ralph A. \"A note on averaging correlations.\" Bulletin of the Psychonomic Society 28.4 (1990): 335-336. (https://link.springer.com/article/10.3758/BF03334037)",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nIn this paper, the authors evaluate how well some audio representations align with human perception on how similar timbre of tracks are. The audio representations include hand crafted features such as MFCC to latest deep audio understanding models such as Clap. \nThe motivation is well put as the current timbre space analysis is very limited and audio representations might be useful in mimicking human perception. Due to lack of sufficient data, authors use pre-trained models without fine-tuning or training from scratch, simply to evaluate the alignment. \n\nThe evaluation is thorough using diverse set of representations and datasets. Well-defined and interpretable metrics are used. \n\nProvided Python package is definitely a plus for reproducibility.\n\nI have only minor comments,\n- Style embeddings show promising results but they are only provided for proposed similarity model. It would have been beneficial to provide results for other models such as Clap as well since Clap already provide good results itself.\nIn Table 1, there are several \u201cSame\u201d in the Type of sounds. It is hard to track what it is same with.\nAlthough Clap and style embeddings have better results in general the improvement against MFCC\u2019s is marginal. The manuscript can benefit a deeper analysis of this fact.\nTypo: line 163 - datsets -> datasets"
    },
    "session": "6"
  },
  {
    "content": {
      "TLDR": "The idea that Gregorian melodies are constructed from some vocabulary of segments has long been a part of chant scholarship. This so-called ``centonisation'' theory has received much musicological criticism, but frequent re-use of certain melodic segments has been observed in chant melodies, and the intractable number of possible segmentations allowed the option that some undiscovered segmentation exists that will yet prove the value of centonisation, and recent empirical results have shown that segmentations can outperform music-theoretical features in mode classification. Inspired by the fact that Gregorian chant was memorised, we search for an optimal unsupervised segmentation of chant melody using nested hierarchical Pitman-Yor language models. The segmentation we find achieves state-of-the-art performance in mode classification. Modeling a monk memorising the melodies from one liturgical manuscript, we then find empirical evidence for the link between mode classification and memory efficiency, and observe more formulaic areas at the beginnings and ends of melodies corresponding to the practical role of modality in performance. However, the resulting segmentations themselves indicate that even such a memory-optimal segmentation is not what is understood as centonisation.",
      "abstract": "The idea that Gregorian melodies are constructed from some vocabulary of segments has long been a part of chant scholarship. This so-called ``centonisation'' theory has received much musicological criticism, but frequent re-use of certain melodic segments has been observed in chant melodies, and the intractable number of possible segmentations allowed the option that some undiscovered segmentation exists that will yet prove the value of centonisation, and recent empirical results have shown that segmentations can outperform music-theoretical features in mode classification. Inspired by the fact that Gregorian chant was memorised, we search for an optimal unsupervised segmentation of chant melody using nested hierarchical Pitman-Yor language models. The segmentation we find achieves state-of-the-art performance in mode classification. Modeling a monk memorising the melodies from one liturgical manuscript, we then find empirical evidence for the link between mode classification and memory efficiency, and observe more formulaic areas at the beginnings and ends of melodies corresponding to the practical role of modality in performance. However, the resulting segmentations themselves indicate that even such a memory-optimal segmentation is not what is understood as centonisation.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Vojt\u011bch Lanz",
        "Jan Haji\u010d",
        "jr."
      ],
      "authors_and_affil": [
        "Vojt\u011bch Lanz (Charles Unviersity)",
        "Jan Haji\u010d, jr. (Charles University)*"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Digital musicology",
        "Melody and motives",
        "Musical features and properties",
        "Computational musicology",
        "Structure, segmentation, and form",
        "Machine learning/artificial intelligence for music",
        "Computational music theory and musicology",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1OaCkO_tOZ62IHqeFu1Bg8QEeHnTMb1YN/preview",
      "poster_pdf": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-3-gregorian-melody-modality",
      "title": "Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics",
      "video": ""
    },
    "forum": "284",
    "id": "284",
    "pic_id": "",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Human annotations of mood in music are essential for music generation and recommender systems. However, existing datasets predominantly focus on Western songs with terms derived from English, which may limit generalizability across diverse linguistic and cultural backgrounds. We introduce 'GlobalMood', a novel cross-cultural benchmark dataset comprising 1,180 songs sampled from 59 countries, with large-scale annotations collected from 2,519 individuals across five culturally and linguistically distinct locations: U.S., France, Mexico, S. Korea, and Egypt. Rather than imposing predefined emotion and mood categories, we implement a bottom-up, participant-driven approach to organically elicit culturally specific music-related emotion terms. We then recruit another pool of human participants to collect 988,925 ratings for these culture-specific descriptors. Our analysis confirms the presence of a valence-arousal structure shared across cultures, yet also reveals significant divergences in how certain emotion terms (despite being dictionary equivalents) are perceived cross-culturally. State-of-the-art multimodal models benefit substantially from fine-tuning on our cross-culturally balanced dataset, particularly in non-English contexts. Broadly, our findings inform the ongoing debate on the universality versus cultural specificity of emotional descriptors, and our methodology can contribute to other multimodal and cross-lingual research.",
      "abstract": "Human annotations of mood in music are essential for music generation and recommender systems. However, existing datasets predominantly focus on Western songs with terms derived from English, which may limit generalizability across diverse linguistic and cultural backgrounds. We introduce 'GlobalMood', a novel cross-cultural benchmark dataset comprising 1,180 songs sampled from 59 countries, with large-scale annotations collected from 2,519 individuals across five culturally and linguistically distinct locations: U.S., France, Mexico, S. Korea, and Egypt. Rather than imposing predefined emotion and mood categories, we implement a bottom-up, participant-driven approach to organically elicit culturally specific music-related emotion terms. We then recruit another pool of human participants to collect 988,925 ratings for these culture-specific descriptors. Our analysis confirms the presence of a valence-arousal structure shared across cultures, yet also reveals significant divergences in how certain emotion terms (despite being dictionary equivalents) are perceived cross-culturally. State-of-the-art multimodal models benefit substantially from fine-tuning on our cross-culturally balanced dataset, particularly in non-English contexts. Broadly, our findings inform the ongoing debate on the universality versus cultural specificity of emotional descriptors, and our methodology can contribute to other multimodal and cross-lingual research.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Harin Lee",
        "Elif Celen",
        "Peter Harrison",
        "Manuel Anglada-Tort",
        "Pol van Rijn",
        "Minsu Park",
        "Marc Sch\u00f6nwiesner",
        "Nori Jacoby"
      ],
      "authors_and_affil": [
        "Harin Lee (Max Planck Institute for Human Cognitive and Brain Sciences)*",
        "Elif Celen (Max Planck Institute for Empirical Aesthetics)",
        "Peter Harrison (University of Cambridge)",
        "Manuel Anglada-Tort  (Goldsmiths, University of London)",
        "Pol van Rijn (Max Planck Institute for Empirical Aesthetics)",
        "Minsu Park (NYU Abu Dhabi)",
        "Marc Sch\u00f6nwiesner (Leipzig University)",
        "Nori Jacoby (Cornell University)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "MIR fundamentals and methodology",
        "Musical features and properties",
        "Music transcription and annotation",
        "Novel datasets and use cases",
        "MIR tasks",
        "Annotation protocols",
        "Evaluation, datasets, and reproducibility",
        "Metadata, tags, linked data, and semantic web",
        "Musical affect, emotion and mood"
      ],
      "long_presentation": "TRUE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1E-f8lC-SgLbTPSJTHzHexmXb3oBTcAfS/preview",
      "poster_pdf": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-1-globalmood-a-cross",
      "title": "GlobalMood: A cross-cultural benchmark for music emotion recognition",
      "video": ""
    },
    "forum": "286",
    "id": "286",
    "pic_id": "",
    "position": "01",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q5 ( Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chosen, otherwise write \"n/a\"))**\n\nYes, although some overview of other datasets that focus on getting raters from different cultures, e.g. MERP or survey could be provided.\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nDisagree\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThe paper presents a new dataset for emotion labelling with a global focus. It is always great to see more data available.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q18 ( If yes, please explain why it should be awarded.)**\n\nThe paper presents a new dataset for emotion labelling with a global focus. It is always great to see more data available.\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThe paper presents a new dataset for emotion labelling with a global focus. It is always great to see more data available. The LLM evaluation approach adopted is also interesting. \n\nThe paper is well written and provides a great new resource. \n\nThe authors may want to have a look at this recent MER dataset survey paper as well: https://arxiv.org/abs/2406.08809\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nStrong accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nAll reviewers agree this is a valuable contribution to the community.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe paper \"GlobalMood: A cross-cultural benchmark for music emotion recognition\" presents an open dataset containing music mood tags and their ratings from five locations, together with the methods for sourcing and refinement. \n\nAs the authors elaborate, mood tags may vary across different cultures, and even translation-equivalent terms may have different meanings depending on the cultural context. \n\nThe authors present several use-cases of their data by providing an such analysis of term equivalence versus translation equivalence across locations, using a subspace-mapping of tag representations collected from different locations across the music corpus.\nThey furthermore provide an evaluation of agreement of the human data with multi-modal audio LLMs.\n\nThe paper is well-structured and written and systematically describes the contributions. Figures successfully visualise the data, results and collection method. \n\nThe description of the annotation method, in my opinion a key contribution, could use some clarification, as details of the collection chain are somewhat spread across several sections (end of Section 3.2, 4.1, 4.2). \nRegarding Figure 1 - it would be great to add to either the figure or related text a description that the tagging chain (A) is (if I understand correctly) performed by annotators from the same country and location for a specific location's tagging and emotional tag repertoire (thus the tagging chain resting culture-homogenous). This could be also clarified in the text around \"two parallel chains per country\". Line 161 \"elicits mood terms across languages\" also could be rephrased to e.g. \"elicits culture-specific mood terms in local languages\" if the above assumption is correct. On the contrary, if participants in Figure 1A are from multiple locations, there would be doubts about mutual understanding along the chain.\n\nThe analysis in Section 4.2.2 shows particular promise in showing limits of \"dictionary translation\" of tags. So was the analysis of audio LLMs for individual locations. Regarding line 412, it would be interesting to know how the capacity and training data in the Gemini audio models changed. \n\nThe specific experiments of improvements for CLAP for specifically Arabic tags showcase very concrete avenues to improve tagging performance in existing methods.\n\nDepending on whether the vision is to extend the data and benchmark to further locations, I would ask the authors to consider the naming (\"global\") of the dataset/paper - notwithstanding this is a great novelty and improvement in global spread of locations - as the five used locations still are somewhat limited. Also consider the mention of \"globally balanced\" in Line 162.\n\nI recommend the paper to be presented at ISMIR 2025.\n\nNotes:\nI appreciated the sharing of the data - which helped confirmation of understanding. It seems that the csv lines with Arabic tags are ordered differently, and inconsistent regarding the csv column headers in both files for global mean and raw ratings: The Arabic tags are always at the outer (last) column, whereas the other languages position tags in the 3rd column e.g.:\n\ncountry,videoID,tag,mean_rating,sd_rating,n_ratings\nEG,L7A9gIIYE8U,\u0627\u0644\u0627\u0633\u062a\u0645\u062a\u0627\u0639,3.5,1.0801234497346435,10\nvs\nKR,Y2cyFXBo9o4,\ud65c\uae30\ucc2c,3.1818181818181817,0.8738628975053029,11\nvs\nMX,IWLcPqj3poM,amor,2.3636363636363638,1.5015143870590968,11\n\n\nSmaller notes:\n\nLine 93 - our results demonstrate - this could go into the conclusion\n\nI noted the anthropomorphising terminology of \"human-like capabilities for understanding\" for Gemini in (ln 412), and wonder if this is warranted and within the scope of the presented evidence. Just a paragraph below (ln 419) the correlation with human ratings is considered comparable in performance to pre-existing specific mood-estimation algorithms.",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper explores the culture-specific usage of music-related mood terms through a large-scale annotation study. The authors present a clearly defined research question and a carefully designed methodology to investigate it. I strongly recommend this paper for acceptance based on the following strengths:\n\n- The cross-cultural design\u2014using data from five culturally diverse countries\u2014helps overcome the common WEIRD (Western, Educated, Industrialized, Rich, and Democratic) bias in human-subject research and strengthens the generalizability of findings in emotion studies, where cultural variability in mood perception presents a significant methodological and interpretive challenge. The scale and scope of this annotation effort are particularly impressive, given the logistical and cultural complexity involved in such experiments.\n\n- A wide range of annotations in different languages broadens the applicability of Music Emotion Recognition (MER) systems and contributes to NLP research across linguistic and cultural boundaries. By capturing how mood terms are interpreted within specific cultural contexts, this work lays important groundwork for developing personalized or culturally adaptive emotion recognition systems.\n\n- A robust experimental design\u2014comprising two stages followed by model evaluation\u2014supports the reliability of the results. Careful stage-specific song selection and a bottom-up procedure for mood-term extraction ensure ecological validity. Additionally, the comparison of human annotations with two computational models strengthens the interpretive framework of the study.\n\nSuggestions for improvement:\n\n- In Section 4.2.2, where the authors compare within-country and cross-country agreement, no baseline or statistical comparison is provided for interpreting the reported coefficients. Given this, it may be premature to conclude that the emotion term \u2018happy\u2019 shows \u201ca considerable gap between cross- and within-country agreement\u201d (Lines 377\u2013378).\n\n- I suggest clarifying the phrasing in Lines 361\u2013365 on two fronts. First, if I understand correctly, the term \u201cmean correlations across language pairs\u201d refers to inter-country agreement computed as the average of pairwise correlation coefficients. If so, a brief explanation would improve clarity. Second, I would like to ask whether it is appropriate to interpret within-country agreement\u2014calculated using the Spearman\u2013Brown formula\u2014as a proxy for measurement error. If that is the intended interpretation, it may be helpful to explicitly frame it as such to guide the reader's understanding.",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe paper contributes a novel dataset that takes into account culture and allows the annotation of free-text. The top-down approach to annotation of free-text descriptors in native language is highly-relevant and a significant contribution to the task of MER. Moreover, using LLMs to analyze native language features is interesting and relevant. \n\nMajor comments:\nAlthough completely agreeing with the authors regarding the issue of using English descriptors to music, some work has been done to validate some emotion models in other languages - particularly for GEMS. See Strauss2024.\nGeneral intra-rater statistics could be added to section 4.2.1, Krippendorff\u2019s alpha or ICC could give a general notion of how agreement works within the same language as compared across languages. \nOne of the major issues of cross-cultural work is that evaluating textual information from an unknown language can be challenging. Figure 2 shows mood terms such as \u201clatino\u201d with high arousal and positive valence or \u201cforeign\u201d with maybe low arousal and negative valence? Although this is briefly mentioned in the discussion section, perhaps more could be written about the difficulty of making such translations (see next comment).\nThe finding in section 4.2.2 is very interesting. I\u2019m not sure if I understand this correctly, but having a rating for terms like \u201clatino\u201d or \u201cforeign\u201d would already bias the calculation from the MDS. If the mean rating per term is introduced for the 1180 songs, would there be a circular logic to this? Perhaps I\u2019m not understanding this section correctly and there is only a need to better clarify it. \n\n\nMinor comments:\nFigure 2 has some sliders that I would assume are in Spanish. Are they \u201calegr\u00eda\u201d, \u201cpaz\u201d and \u201cnostalgia\u201d?\nSection 3.2 is a bit unclear in L229. Can you clarify how you selected a subset of 180?\nL299 refers to mitigating a priming effect which might not be clear to the general MIR reader. Please clarify a bit further. \n\nReferences: \n@article{Strauss2024,\ntitle = {{The Emotion-to-Music Mapping Atlas (EMMA): A systematically organized online database of emotionally evocative music excerpts}},\nauthor = {Strauss, Hannah and Vigl, Julia and Jacobsen, Peer-Ole and Bayer, Martin and Talamini, Francesca and Vigl, Wolfgang and Zangerle, Eva and Zentner, Marcel},\nyear = 2024,\nmonth = jan,\njournal = {Behavior Research Methods},\nvolume = 56,\nnumber = 4,\npages = {3560\u20133577},\ndoi = {10.3758/s13428-024-02336-0},\nissn = {1554-3528},\nurl = {http://dx.doi.org/10.3758/s13428-024-02336-0}\n}"
    },
    "session": "1"
  },
  {
    "content": {
      "TLDR": "In learning music, difficulty is an important factor both in choice of repertoire, choice of tempo, and structure of practice. These choices are typically done with the guidance of a teacher; however, not all learners have access to one. While piano and strings have had some attention devoted to automated difficulty estimation, wind instruments have so far been under-served.\nIn this paper, we propose a method for estimating the difficulty of pieces for winds  and implement it for the tenor saxophone. We take the cost-of-traversal approach, modelling the part as a sequence of transitions -- note pairs.  We estimate transition costs from newly collected recordings of trill speeds, comparing representations of saxophone fingerings at various levels of expert input. We then compute and visualise the cost of the optimal path through the part, at a given tempo. While we present this model for the tenor saxophone, the same pipeline can be applied to any wind instrument, and our experiments show that with appropriate feature design, only a small proportion of possible trills is needed to estimate the costs well. Thus, we present a practical way of diversifying the capabilities of MIR in music education to the wind family of instruments.",
      "abstract": "In learning music, difficulty is an important factor both in choice of repertoire, choice of tempo, and structure of practice. These choices are typically done with the guidance of a teacher; however, not all learners have access to one. While piano and strings have had some attention devoted to automated difficulty estimation, wind instruments have so far been under-served.\nIn this paper, we propose a method for estimating the difficulty of pieces for winds  and implement it for the tenor saxophone. We take the cost-of-traversal approach, modelling the part as a sequence of transitions -- note pairs.  We estimate transition costs from newly collected recordings of trill speeds, comparing representations of saxophone fingerings at various levels of expert input. We then compute and visualise the cost of the optimal path through the part, at a given tempo. While we present this model for the tenor saxophone, the same pipeline can be applied to any wind instrument, and our experiments show that with appropriate feature design, only a small proportion of possible trills is needed to estimate the costs well. Thus, we present a practical way of diversifying the capabilities of MIR in music education to the wind family of instruments.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "\u0160imon Lib\u0159ick\u00fd",
        "Jan Haji\u010d",
        "jr."
      ],
      "authors_and_affil": [
        "\u0160imon Lib\u0159ick\u00fd (Charles University)",
        "Jan Haji\u010d, jr. (Charles University)*"
      ],
      "channel_url": "",
      "day": "4",
      "keywords": [
        "Creativity and learning",
        "Applications",
        "Creativity",
        "Novel datasets and use cases",
        "Music training and education",
        "Tools for artists",
        "Evaluation, datasets, and reproducibility",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1czK4XpQ8EiCo8AdH5MUsViYZ_Zz-qOVw/preview",
      "poster_pdf": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-2-modeling-the-difficulty",
      "title": "Modeling the Difficulty of Saxophone Music",
      "video": ""
    },
    "forum": "293",
    "id": "293",
    "pic_id": "",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nStrongly agree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q5 ( Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chosen, otherwise write \"n/a\"))**\n\nThe authors cite relevant work\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nDisagree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nDisagree\n\n**Q10 (Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chose, otherwise write \"n/a\"))**\n\nPlease see the main review for this detail\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nDisagree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nDisagree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nI think that the authors did a first pass at coming up with a fingering difficulty model for saxophone but I am not clear what reusable insights could be applied to other woodwinds, for example, since I think that the paper does not give enough detail about the data collection or the experimental validation to make it clear which of their approaches to measuring difficulty shows the most promise.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThey propose a method for estimating the difficulty of pieces for\nfor the tenor saxophone, using transition difficulty between pairs of notes to estimate difficulty. This is based on trill data.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nAs a saxophone player, I am very sympathetic to the goals of the paper. I like that the authors explored several methods of determining difficulty of passages. I think that the description of their data collection was, however, underspecified and the experimental results did not convincingly establish which of their methods (based on trill data, based on expert input, based on finger features) was the most informative. \n\nWeakness: The data collection section did not specify trills were used in this study. Only a single 1/2 step trill from B to B flat is shown. They specify use of the bis key. This key would likely not be used to play a Bb (A#) in a B major scale or a B major 7 chord, since it would leave the 2nd finger in the wrong place to cover the appropriate key when playing the next note. A different fingering (using a side key) is much more likely. Basing the transition difficulty on limited trill data may be problematic because the saxophone has multiple trill fingerings that are specifically for trills but are not appropriate for use in scalar passages or arpeggios. If trill speed is used as a proxy for transition difficulty, then trill fingerings may make transitions seem easier than they typically are in most passages.\n\nThe authors state \"Each combination of covered keys has a sufficiently unique map to which fingers are active, so the \u201cFinger\u201d features can be derived automatically from the \u201cRaw\u201d features.\" Is this true? Using their example trill fingering of the bis B flat, one can compare that to the fingering for A and the set of fingers used is identical. \n\nThe expert features section is a good attempt to capture issues that arise in saxophone fingerings.\n\nI wonder why the authors weight their model error calculation by frequency of intervals in the Weimar Jazz Database (WJD). Wouldn't it be more likely that rare intervals are, in fact, harder? One might instead use the frequency of interval as a kind of alternative ground truth that may be useful to predict difficulty: more rare intervals may be harder to play.\n\nIn section 6 the results are somewhat difficult to interpret. What are the units? When they say the linear model has MSE of 0.97 is this measured in transitions per second? When they say something has MAPE of .37, does that mean 37% or does it mean 0.37%? Also, how is precision measured? Is there an en epsilon where, if the model's estimate is less than epsilon different from truth it counts as being correct? \n\nIn section 8 the authors state \" First, because some notes have multiple fingerings, the \u201cfastest\u201d (maximal) path through the part in terms of fingerings is found with Viterbi decoding over the estimated trill speeds.\" This sounds interesting but I'm unclear on what they mean. There are, for example 5 ways to finger the B flat in the example from Figure 1. How do they estimate trill speeds for all 5 when they only measured 1 fingering? Without that information, how can they build a Markov transition graph for all 5 fingerings so the fastest one can be selected with Viterbi?\n\nThe authors mention that their method requires access to an expert to give them information and to realize some of the trills for their data collection. Is it possible that they should have collected data from people that are not experts, as it may be that novices may have difficulty in some transitions that experts have completely mastered? \n\nIt would seem natural to take their estimation of difficulty, apply it to musical passages (as shown in figure 8) and then ask saxophone players to independently rate the difficulty of the passages. One could ask them to circle the hard parts and then the model could be evaluated on precision/recall on its ability to identify the passages the humans called difficult. The authors have done all the work needed to develop the model that could be evaluated this way. Now they just need to actually do it. That would be very interesting validation.\n\nThere is a typo on line 103 \"...representation design fort the inherently limited data..\" Fort is not the right word.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nWith three strong accepts and 1 weak accept from the meta reviewer, we should clearly accept this submission.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper provides a robust pedagogy-founded approach for difficulty estimation in saxophone music, a very under-investigated repertoire in computer-supported music education research in general, but also in difficulty estimation in particular. This paper can serve as a foundation for future research on wind-family instruments.\n\nMAJOR COMMENT\nIt is not clear to me whether transition difficulties are just measured as a function of trill speeds (this seems to be implied in the lines 280-281). I think this might be a too narrow approach and because of this a clarification would be worthy here in order to highlight the interplay between other variables involved as well as their relationship to speed. \nIn addition, and related to the previous, I do not fully understand how player proficiency is controlled for. Although proficiency is directly related to the ability to play repertoire of a given difficulty level, individual skills concerning specific challenges play also a role in mastering an instrument completely. In this regard, it is not clear (even if fatigue is briefly mentioned in line 308) how individual strengths and weakness from the different players are considered in determining transitions' difficulty levels. Without controlling for proficiency, results might reflect player skill rather than the intrinsic transition challenge. \nFinally, a hypothesis about why player 5 is amongst the faster and slower players would also be important in order to understand individual differences amongst the included musicians. Was player 5 a beginner or an advanced player? Was under determined conditions when performing slow? Player 5 is referred to twice in the paper, but an interpretation of the reasons behind their performance is missing.\n\nMINOR COMMENTS\nFigure 8 seems to imply that a tool to visually inspect difficulty transitions is available and could be used to explore other content beyond the paper. No website however can be found referred to in the article, so if such a Tool exist, this should be clearly pointed out. I interpret, however, Fig 8 is merely a plot designed to illustrate the described concept. I do not see anything wrong with this, but it should be clearly indicated as such, to avoid misleading interpretations.\n\nAlthough several limitations concerning other aspects such as voicing, articulation, etc. are pointed out, the accumulative difficulty derived by cognitive overload of playing a long passage, specially containing difficult transitions, seems not to be considered. Adding a note about this could be important.\n\nTypos, missing spaces, etc.:\n- Line 59: meaningless. so -> meaningless. So\n- Line 143: and (3) and as -> and (3) as\n- Line 413: ABRSM -> ABRSM (Associated Board of the Royal Schools of Music)\n- Line 421: features.Also -> features. Also\n- Footnotes 1 and 4 should end with a dot\n- Reference [16] is published in ISMIR 2023 (please refer to the final paper, not to the arXiv version)\n- References are generally well formatted, but adding pages, or last article page when missing would be improve coherence/completeness (see [4], [7], [23]). Double check capitalisation rules and included information, such as addresses, when referring to conference proceedings in order to ensure consistency.",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nI really enjoyed reading your work, thanks. Here below I are some comments for improving the quality of the paper:\n-) Line 2 Use the word \"both\" only with two items. You have listed three.\n-) Line 13 \"trill speeds\" - give more information about the trills, assuming that this is the first time the reader is seeing the information. For example, mention that the trills are between each possible pair of notes.\n-) Figure 5 - For the same finger transition, why 10 and not just 1?\n-) Figure 8: it is difficult to understand what you mean by \"redder\", because some of the notes appear more orange than red. Maybe use a scale of blue (easier) to red (more difficult) to make it clear. \n-) Line 421: space needed between sentences\n-) Lines 525-529: Incomplete reference, should have volume and page numbers.\n-) Lines 584-605: Please check the title.\nGenerally, please check the references to make sure they are in uniform format. For example, two papers from ISMIR are formatted differently.\nThe abstract could be clearer. Please do not assume the reader is already familiar with the work described in the paper.",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nIn this paper the authors estimate the difficulty of saxophone parts based on a model created using player trill speed between pairs of notes as training data, then estimating total cost of transitions of the part. The models tested are simple regression and a multi-layer perceptron. Expert features with an MLP seemed to be the most accurate solution for this problem.\n\nThe strengths include that it is based on genuine player abilities, and even has data from players of different skill levels.\nInteresting that there is much variation in relative ease of trilling between players. This would not have been known without this experiment.\n\nMinor matters:\nLine 257 has \"embrochure\" instead of \"embouchure\".\nLine 437: \"a a \""
    },
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Neural networks have become the dominant technique for accurate pitch and periodicity estimation. Although a lot of research has gone into improving network architectures and training paradigms, most approaches operate directly on the raw audio waveform or on general-purpose time-frequency representations. We investigate the use of Sawtooth-Inspired Pitch Estimation (SWIPE) kernels as an audio frontend and find that these hand-crafted, task-specific features can make neural pitch estimators more accurate, robust to noise, and more parameter-efficient. We evaluate supervised and self-supervised state-of-the-art architectures on common datasets and show that the SWIPE audio frontend allows for reducing the network size by an order of magnitude without performance degradation. Additionally, we show that the SWIPE algorithm on its own is much more accurate than commonly reported, outperforming state-of-the-art self-supervised neural pitch estimators.",
      "abstract": "Neural networks have become the dominant technique for accurate pitch and periodicity estimation. Although a lot of research has gone into improving network architectures and training paradigms, most approaches operate directly on the raw audio waveform or on general-purpose time-frequency representations. We investigate the use of Sawtooth-Inspired Pitch Estimation (SWIPE) kernels as an audio frontend and find that these hand-crafted, task-specific features can make neural pitch estimators more accurate, robust to noise, and more parameter-efficient. We evaluate supervised and self-supervised state-of-the-art architectures on common datasets and show that the SWIPE audio frontend allows for reducing the network size by an order of magnitude without performance degradation. Additionally, we show that the SWIPE algorithm on its own is much more accurate than commonly reported, outperforming state-of-the-art self-supervised neural pitch estimators.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "David Marttila",
        "Joshua D. Reiss"
      ],
      "authors_and_affil": [
        "David Marttila (Queen Mary University of London)*",
        "Joshua D. Reiss (Queen Mary University of London)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "MIR fundamentals and methodology",
        "Music transcription and annotation",
        "MIR tasks",
        "Music signal processing",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "https://drive.google.com/file/d/10J4g8B3w-xlT1PQx6-b_ngVf-uhYQFoP/preview",
      "poster_pdf": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-9-improving-neural-pitch",
      "title": "Improving Neural Pitch Estimation with SWIPE Kernels",
      "video": ""
    },
    "forum": "298",
    "id": "298",
    "pic_id": "",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nCombination of DSP and DNN works for pitch estimation.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nSWIPE could still be a front runner by careful implementation.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis paper proposes to use the Sawtooth-Inspired Pitch Estimation (SWIPE) kernel, a classical DSP method, as a front-end for neural pitch estimation (e.g., Pitch Estimation with Self-Supervised Transposition-Equivariant Objective (PEST)), improving accuracy, robustness, and efficiency. It also demonstrates that a careful implementation of the SWIPE algorithm significantly outperforms state-of-the-art self-supervised neural pitch estimators, showing its potential was underestimated.\n\nPros:\n- Found a way of drawing the full potential of SWIPE, leading to the SOTA performance.\n- Has a potential of improving a wide variety of transcription tasks.\n- Allows significantly smaller neural networks without losing performance.\n- Offers a flexible latency-accuracy tradeoff adjustable at inference time.\n\nCons:\n- The paper is not so strong in terms of technical novelty.\n- Applicability for music recordings not limited to solo performance has not been investigated.\n\nThis is an interesting \u201cbridging old and new\u201d paper. The proposed combination of classical DSP and modern deep learning would be worth sharing among the community.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThis paper revisited SWIPE, a classical DSP method, and showed its usefulness in combination with deep learning models. The reviewers highly evaluated the idea of drawing the full potential of the traditional pitch estimator. The evaluation section, which is not fully convincing, would be significantly improved by including a comparison with the other conventional implementations.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper is about single-pitch estimation with a hybrid approach taking some inspiration from the DSP-based SWIPE method and combining it with neural architectures. Although single-pitch estimation has been a long-standing challenge in audio signal processing, it is still worthwhile to investigate further especially w.r.t. efficiency, robustness and cross-domain performance (e.g., speech vs. musical instruments).\n\nThe authors start by giving a brief overview on the SWIPE algorithm, followed by an introduction of supervised (e.g., FCNF0++) and self-supervised (e.g. PESTO) neural approaches for single-pitch estimation.\n\nThe authors continue with details about their slightly modified SWIPE baseline implementation as well as their combination with neural backends, where mostly the SWIPE-kernels and scores are used as essential front-end parts.\n\nIn their experimental section, they compare several configurations of the different approaches. For training, the use a combination of the MDB-stem-synth (musical instruments) and PTDB-TUG (speech) datasets. They also evaluate on held-out test splits as well as the completely unseen MIR-1K (singing voice) dataset. They include two DSP baselines PYIN and SWIPE (modified according to their paper) and \n\nIn the results section, the authors report the usual pitch-related scores like raw pitch accuracy (RPA), voiced/unvoiced F-measure, and overall accuracy (OA). In most of the settings, their hybrid approach outperforms FCNF0++ and PESTO as state-of-the-art examples of supervised and self-supervised neural pitch estimators/trackers. Notably, their SWIPE re-implementation surpasses PESTO when trained and tested on the MDB-stem-synth dataset (considering only RPA).\n\nThe experiments are rounded off by further explorations of noise-robustness and the trade-off between window-sizes and performance. The latter is especially important when considering low-latency scenarios.\n\nAll in all, I applaude the authors for revisiting SWIPE and combining it with recent state-of-the-art neural backends. That is a line of research that I personally would like to see explored more often. The reported results also seem to support this. However, the write-up of the paper has several flaws which almost made me reject it. The authors need to improve on the following aspects:\n\n1) The authors need to more explicitely state what SWIPE kernels and SWIPE kernel correlation scores (sometimes only called SWIPE scores here) are. What is what can be deduced if one is either familiar with SWIPE or reads through the whole paper, but the reader would have a much easier time if there was a running example introduced in the beginning with a visualization.\n\n2) Abbreviations for the various configurations are all over the place (compare tables 1 and 2 vs. 3 and 4) . It would be less confusing to the reader if the authors come up with a more systematic naming scheme (e.g., DSP-based, supervised, self-supervised followed by the signal representation and backend) and explain it once in before jumping into the experiments.\n\n3) Some choices of the algorithmic configurations seem not very well justified. For example, why does the fully supervised model have a Toplitz-style fully connected layer if it makes no use of self-supervised training paradigm? Why were the frequency resolutions choosen so differently between Mel, CQT ?\n\n4) The train and test configuration differ quite a lot between the different experiments. The authors justify this by their aim to reproduce results from earlier papers, but it seems somewhat arbitrary chosen. This is especially true when considering the different modality of the datasets musical instruments, speech and singing voice.\n\n5) My personal experience with the PTDB-TUG dataset tells me that there are some annotation errors in the reference pitch tracks. In particular, there are sometimes wrong voiced/unvoiced information that assigns no pitch, even if there's a clear fundamental and harmonics visible in the spectrogram. I fear that any trainings and evaluations including this dataset have to be treated carefully. In the worst case, they could even call the main findings into question. Why did the authos not use other datasets that are available with pitch annotations? \n\n6) The literature references must be more carefully curated and formatted. For example:\na) reference [7] should refer to the ICASSP paper instead of the ArXiV pre-print: https://ieeexplore.ieee.org/document/8461329\nb) reference [15] contains faulty characters in the paper title",
      "review2": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nThis paper presents a series of experiments aimed at coupling the self-supervised PESTO model with classical pitch estimators such as PYIN and SWIPE, with particular emphasis on SWIPE. The overarching goal is to enable pitch estimation on devices with limited computational resources, such as mobile phones.\n\nA positive aspect of the paper is the authors\u2019 effort to revisit and adapt traditional pitch estimators to better suit their specific needs. Notably, they report that replacing ERB bands with Mel bands increases SWIPE\u2019s performance on music-related data. However, this potentially interesting observation remains unsubstantiated, as the paper does not include experimental evidence to support it.\n\nOverall, I feel the paper is not yet ready for publication. The experimental results, while touching on relevant aspects, do not convincingly support the claims made. First, the authors argue that their implementation of SWIPE outperforms other versions using ERB bands. However, the comparison lacks rigor\u2014there is no clear evidence that the baseline implementations are comparable in terms of evaluation setup, metrics, or configuration. Reproducing published results before introducing modifications would lend much more credibility to the analysis.\n\nSecond, the reported evaluation metrics are presented only at a high level, with average values across test datasets. Especially when the observed improvements are small, deeper analysis is crucial to assess the reliability and significance of the results. For instance, a plot of RPA over frequency or visualizations of the Toeplitz matrix in the case of the PESTO-SWIPE-tiny model would provide valuable insight. In this regard, a more detailed error analysis would have been more informative than the brief timing and robustness evaluations included in Sections 5.4 and 5.5.\n\nOn a minor note, the notation used in the mathematical sections is somewhat confusing. For example, it is unclear why formula (1) is presented in continuous notation, or why \"i\" is used to represent pitch in Sections 2.2 and 2.3. Additionally, the relationship between y = f_{theta}(x) the same as y-tilde is not clearly explained (see caption before Section 2.3.2).\n\nI hope the authors find this feedback constructive. I encourage them to consider narrowing the scope of the paper and developing a single aspect in greater depth, with stronger empirical validation and clearer exposition. My recommendation, at this stage, is to reject the paper.",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis work proposes to use SWIPE kernels in a self-supervised training framework for pitch estimation. Overall, the text is well organized, well-structured, and well written. In general, the presentation is clear and the decisions are well justified. The introduction is clear, with a clear motivation, clear state of the art (SOTA), and clear contribution. The experimental setup is appropriate to present clear evidence of the claims. Finally, the references are appropriate, citing mostly peer-reviewed works and only including non peer-reviewed references when necessary.\n\nAll that said, I have a few suggestions to improve the manuscript.\n\nMajor suggestions:\n\nFirst, the \u201cself-supervised\u201d aspect of the work seems to lie at the core of the contribution but it does not appear in the title. I think the title should include \u201cself supervised\u201d and use \u201cfundamental frequency estimation\u201d instead of \u201cpitch estimation\u201d (see below).\n\n5.2 says (308-310) \u201cIn both the supervised and self-supervised setting, we train all models for 50 epochs using a batch size of 256 and the Adam optimizer with an initial learning rate of 10\u22124.\u201d (387-389) and also (358-360) seem to contradict it. Please clarify.\n\nTable 1 made me wonder, can OA > RPA? The definitions in 4.3 seem to indicate that OA <= RPA because RPA includes all voiced frames and OA only considers correctly classified voiced frames (a smaller number than all voiced frames). I\u2019m assuming that these calculations only change the numerator and the denominator is all voiced frames for both. Formulas in 4.3 will help clarify the confusion.\n\nMinor details:\n\n(lines 25-26) I\u2019d either change the phrasing of this particular sentence or avoid the term \u201cpitch estimation\u201d altogether. Rephrase as \u201cfundamental frequency is considered as the signal counterpart of pitch\u201d or \u201cfundamental frequency correlates with pitch\u201d. \u201cFundamental frequency estimation\u201d avoids most of the problems because it\u2019s what\u2019s being estimated.\n\nPersonally, I\u2019d lose the bullet-point lists in favor of regular paragraphs in 1 and 4.3."
    },
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Binaural audio remains underexplored within the music information retrieval community. Motivated by the rising popularity of virtual and augmented reality experiences as well as potential applications to accessibility, we investigate how well existing music source separation (MSS) models perform on binaural audio. Although these models process two-channel inputs, it is unclear how effectively they retain spatial information. In this work, we evaluate how several popular MSS models preserve spatial information on both standard stereo and novel binaural datasets. Our binaural data is synthesized using stems from MUSDB18-HQ and open-source head-related transfer functions by positioning instrument sources randomly along the horizontal plane. We then assess the spatial quality of the separated stems using signal processing and interaural cue-based metrics. Our results show that stereo MSS models fail to preserve  the spatial information critical for maintaining the immersive quality of binaural audio, and that the degradation depends on model architecture as well as the target instrument. Finally, we highlight valuable opportunities for future work at the intersection of MSS and immersive audio.",
      "abstract": "Binaural audio remains underexplored within the music information retrieval community. Motivated by the rising popularity of virtual and augmented reality experiences as well as potential applications to accessibility, we investigate how well existing music source separation (MSS) models perform on binaural audio. Although these models process two-channel inputs, it is unclear how effectively they retain spatial information. In this work, we evaluate how several popular MSS models preserve spatial information on both standard stereo and novel binaural datasets. Our binaural data is synthesized using stems from MUSDB18-HQ and open-source head-related transfer functions by positioning instrument sources randomly along the horizontal plane. We then assess the spatial quality of the separated stems using signal processing and interaural cue-based metrics. Our results show that stereo MSS models fail to preserve  the spatial information critical for maintaining the immersive quality of binaural audio, and that the degradation depends on model architecture as well as the target instrument. Finally, we highlight valuable opportunities for future work at the intersection of MSS and immersive audio.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Richa Namballa",
        "Agnieszka Roginska",
        "Magdalena Fuentes"
      ],
      "authors_and_affil": [
        "Richa Namballa (New York University)*",
        "Agnieszka Roginska (New York University)",
        "Magdalena Fuentes (New York University)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Applications",
        "Knowledge-driven approaches to MIR",
        "Novel datasets and use cases",
        "Evaluation methodology",
        "User-centered evaluation",
        "Evaluation, datasets, and reproducibility",
        "Gaming, augmented/virtual reality",
        "Machine learning/artificial intelligence for music",
        "Human-centered MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1SBY6XEWQZ7On21uZi4rjL6wPxafcWX8J/preview",
      "poster_pdf": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-7-do-music-source",
      "title": "Do Music Source Separation Models Preserve Spatial Information in Binaural Audio?",
      "video": ""
    },
    "forum": "300",
    "id": "300",
    "pic_id": "",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThis manuscript opens a new path in the well-explored territory of source separation, incorporating binaural localisation. It combines this novelty with some well-known algorithms in the area. Researchers in the area should be able to incorporate the insights easily into their own work. The work also includes a new dataset to support further research.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nBinaural source separation is a interesting problem domain for which traditional source-separation algorithms are not yet fully adequate.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nAgree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThis manuscript is a solid investigation of problem of growing importance: traditional source separation techniques don\u2019t work that well for binaural audio. The localisation binaural audio provides is essential to VR and AR, and the authors also nicely motivate other use cases, for example, benefitting people with certain types of hearing impairment. The has not yet been much work in the field, and this manuscript sets a baseline for how well current approaches work (or don\u2019t) in this context.\n\nThe work includes a synthesised dataset based on standard corpora for the field, incorporating localisation effects. This dataset is a contribution in and of itself, and will surely benefit researchers in source separation trying to innovate in binaural audio.\n\nThe discussion of evaluation metrics in the paper is readable and convincingly identifies the key challenges and limitations of using signal-based metrics for this purpose. A human evaluation would be the logical next step, but such an evaluation is clearly out of the scope of what an ISMIR-length paper could achieve on top of the other contributions in the manuscript.\n\nThe authors test several well-known source-separation models for their analysis. The manuscript gives a good high-level explanation of these models, although it lacks a sufficient *motivation* for why these algorithms were chosen (and not others). For completeness and value to the community, it would have been better if the authors could have added one or two other complementary techniques. I suspect this would be too much for the camera-ready version, but if the authors have time, it would be worthwhile.\n\nThe results are clear and highlight specific limitations of traditional source-separation approaches in binaural contexts. The manuscript tries to cover a broad ground in little space, and given that constraint, it strikes a good balance between showing detail and explaining possible causes for the results.\n\nOverall, the manuscript presents a kind of \u2018negative\u2019 result, in the sense that current algorithms are not as successful as one would hope for this task. But this negative result is also the authors\u2019 motivation. The manuscript establishes a clear baseline for the community, includes helpful reasoning for why current algorithms are not up to the task, and suggests practical paths for future work. It definitely has a place at this year\u2019s ISMIR.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak reject\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThis paper prompted a lengthy discussion among the reviewers, and in particular, there were questions about the formulae used for some of the computations, particularly SDR. It was not possible to resolve all of these questions from tracing the references. If these metrics needed to be recomputed, it could substantially alter the results.\n\nThe full reviews have more details, and several reviewers also highlighted the positive contributions in the paper \u2013 hence the final recommendation of only a weak reject. The authors should be encouraged to double-check their formulae and references for them, make them more explicit in the manuscript, and consider resubmitting here or elsewhere.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThe paper presents a good investigation into how well current music source separation (MSS) models preserve spatial information when running on binaural audio. The authors provide a novel binaural dataset derived from MUSDB18-HQ and evaluate multiple popular MSS models using both standard and spatial metrics. The results are clear and relevant, especially as interest in immersive audio continues to grow.\n\nThat said, I have two main points for improvement, both of which I believe are straightforward to address and would strengthen the manuscript:\n\n1. Clarify Practical Relevance Beyond the Atmos production Workflow\n\nIn its current form, the paper connects the problem to the growing importance of spatial audio. However, in music production, immersive formats such as Dolby Atmos, stems are typically already separated before spatialization is applied. As such, the relevance of performing source separation on already-binaural material may seem limited in this context. To be fair, the authors don\u2019t claim that this is a relevant application. Nonetheless, I suggest that the authors briefly clarify this distinction in the introduction. A more compelling case for the utility of binaural MSS could be made by highlighting real-world examples where binaural mixes exist outside the production pipeline, such as:\n\n- ambient or field recordings\n- live concert captures recorded with binaural microphones\n- consumer content or binaural podcasts where multitrack spatial mixes are unavailable\n\nThis adjustment would help ground the work in practical scenarios where such technology is indeed needed.\n\n1. Provide Guidance on Improving Binaural MSS Performance\n\nThe paper effectively demonstrates that spatial cues degrade under current MSS models, but it stops short of offering guidance on how researchers could address this issue in future work. I encourage the authors to provide concrete and constructive suggestions, such as:\n\n- Data augmentations: Leverage tools such as github:facebookresearch/BinauralSpeechSynthesis to simulate a variety of binaural conditions during training.\n- HRTF diversity: Introduce variations in HRTF profiles to help models generalize across different listener anatomies.\n- Model modifications: Explore modifications to encoder-decoder architectures or loss functions that account for interaural level/time differences (ILD/ITD), or design models that explicitly process spatial features. Is there a specific time-delay that a model should be invariant to?\n- Include a simple baseline: A version of one of the tested models (e.g., Demucs or OpenUnmix) retrained on the Binaural-MUSDB dataset would be a valuable comparison and help validate the feasibility of training on binauralized data directly. I understand that this might be out-of-scope for the paper but it would significantly improve the usefulness of the work.\n\nAdding even a short section outlining these strategies would provide practical value to readers and encourage further progress in this promising research direction.\n\nOverall, I appreciate the novelty of the work and the thorough analysis. Addressing the points above would significantly improve both the paper\u2019s clarity and its applicability.",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe authors investigate the performance of state-of-the-art music source separation (MSS) models on binaural audio. Using synthetic binaural mixtures created from MUSDB18-HQ stems and head-related transfer functions, they assess how well these models preserve spatial information. Their evaluation reveals that standard stereo MSS models often fail to maintain critical spatial cues, with degradation varying across model architectures and instrument types, highlighting the need for future research at the intersection of MSS and immersive audio.\n\nI find the work interesting, clear, and well-written. I have some comments that I would like the authors to address in order to improve the quality of the manuscript.\n\n- Introduction: I believe it is also important to mention (e.g., In the paragraph starting at line 116) and cite the SDX Workshop, from which pretty much all the state-of-the-art models come. \n- Introduction, line 154: I find cumbersome seeing a paragraph beginning with a citation. I suggest to modify it in, e.g., \u201cThe work in [30]\u201d or \u201cReference [30]\u201d.\n- Sec. 3, line 196: You reference Fig. 2 (and Fig. 3 too, later on) before Fig. 1. I think it is better to swap places.\n- Figure 2: You should swap -90\u00b0 with 90\u00b0 in order to match the positive direction of \\theta. \n- Figure 3: Do you have the thetas still in the range [-90\u00b0, 90\u00b0]? If you randomly extracted values in this range, why does not the distribution follow a normal distribution? Is it due to the fact that it is not completely random as you discarded angles to prevent the overlapping of sources in space? Please, comment on it.\n- Sec. 4.1: I think it is better to have the equations part of the text, thus avoiding to reference them as if they were figures. You can still have them in display mode, I am suggesting to consider them as part of your logical flow. Then, each variable should be formally defined, e.g., there is no definition of s, \\hat{s}, x, N, k, etc.\n- Sec. 4.2: Why did you consider those models, and not more recent models such as BSRoformer?\n- Sec. 5: the models that you considered are not the state-of-the-art anymore. It is more precise to reference them as \u201cold\u201d state-of-the-art models.\n- Table 1 and 2: the caption of tables is typically reported above tables. Again, Table 2 is referenced before Table 1. You should swap places. \n- Sec. 5.5: The work would benefit from further information about the perceptual test, and, at least, a plot/table, otherwise I do not see how this section could be useful.\n- If I did not get it wrong, you selected a single HRTF to perform the analysis. In order to have more reliable results, I think you should have selected more than one. Can you, at least, comment on the generalizability of the results? How do you think the HRTF conditions the results?\n- Finally, why did you choose to have a binaural realization? Probably, the most complete way of evaluating the spatial characteristics of MSS models would be to compute inter-channel level distances of the nth-order ambisonics encoding, and maybe computing the DOA to verify whether the angle is maintained after demixing. Please, comment on it.",
      "review3": "- **Overall Evaluation**: Strong reject\n\n#### Main Reviews\n\nIn this work, the authors present an analysis of the spatial errors introduced by music source separation models. In particular, the authors consider 3 models: Demucs v4, Spleeter, and Open-Unmix. The authors also used ITD calculated via GCC-PHAT, ILD, as well as SSR and SRR from Watcharasupat and Lerch to support their analysis. While the subject is indeed worthy of investigation, the paper in its current form is, in my opinion, not quite thorough enough to warrant its publication as a full ISMIR paper. I would encourage the authors to continue pursuing this line of analysis and resubmit in the future (or perhaps as LBD).\n\n- L31-35: It is perhaps very important to establish that the main distinction between binaural and stereo audio is the method of reproduction. Interaural cues can also be recreated (somewhat) in stereo, but would have been done without an explicit assumption on the use of headphones. \n\n- L89-91: Practically all source separation work are based on signal processing anyway. Perhaps the authors mean model-driven (as opposed to data-driven)?\n\n- L98: If anything, I would argue that deep learning undid quite a fair bit of progress on real-time source separation. What DL did offer is the ability to perform SS on single-channel or non-array signals, in a more complex environments, and generally with much higher fidelity than ICA/NMF-era systems.\n\n- L121-124, L277-279, L323-324: It appears that the authors are somewhat confused by SDR (which is admittedly a very confusing metric; see Le Roux et al., \"SDR - half-baked or well done?\", in ICASSP 2019 for more details). In fact, there are two versions of SDR, and it is unclear which version the authors are referring to. The \"correct\" version for MSS is the one that is basically the same as SNR. The one that is typically misused however does not penalize *everything* --- rather, it *forgives* significant timbral impairments that can be captured within 512-taps. It is also very important to note that even with a consistent SDR/SNR definition, there are also many other factors that can (potentially wildly) affect the calculations: \n\n(1) Was the computation done on the full-track or chunk-wise?\n(a) If full-track, was the reported summary median or mean? \n(b) If chunk-wise, was the reported summary median of median, or median or mean, of mean of median, or mean of mean? Typically it is nanmedian of nanmedian. \n\n(2) Was there significant regions of silence in the reference track? There isn't a particular standardized way of dealing with this yet, but it can affect any SNR-like metric.\n\n(3) How were the arguments of the log stabilized? Was the \"epsilon\" only in the denominator, only in the numerator, or on both? Same question for Eq. 5 between L256/257. Specifically, how was hard-panning handled in Eq 5?\n\n- L197-199: The choice of location sampling has to be justified. Also, Fig. 3 looks very non-uniform. I understand random sampling can do that, but this is very non-uniform. \n- Also, was each song only assigned one set of locations?\n\n- L259-266, L323-324: It is perhaps better if the decomposition for both SDR and SSR/SRR are written out explicitly, given that nature of this work.\n\n- Fig.4 has to be separated by stem. It is unclear whether this is a pattern across all or just one stem.\n\n- Table 1: The GCC-PHAT parameters have to be stated. The bass ITD looks very far off. This could be an issue with either SSR/SRR or GCC-PHAT and has to be more thoroughly checked and discussed.\n\n- Table 1: How was Overall $\\Delta\\text{ITD}$ calculated?\n\n- The supplementary cannot be easily judged since there is no reference track from the ground truths. \n\n- It is somewhat a missed opportunity to not compare more models. While I understand the 3 chosen have easily accessible open-source implementation, it would be more interesting to perhaps also consider other model archetypes of similar \"leagues\". For example, one could compare hybrid (Demucs v3 or 4) vs time-domain (Demucs v2) vs learnt basis (ConvTasNet) vs full-band TF-domain (ByteSep) vs subband TF-domain (Bandsplit RNN). Most of these have official open-source implementations, and those without do have a few unofficial implementations or related systems with official implementations."
    },
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Large Audio Language Models (LALMs), where pretrained text LLMs are finetuned with audio input, have made remarkable progress in music understanding. However, current evaluation methodologies exhibit critical limitations: on the leading Music Question Answering benchmark, MuchoMusic, text-only LLMs without audio perception capabilities achieve surprisingly high accuracy of up to 56.4%, much higher than chance. Furthermore, when presented with random Gaussian noise instead of actual audio, LALMs still perform significantly above chance. These findings suggest existing benchmarks predominantly assess reasoning abilities rather than audio perception. To overcome this challenge, we present RUListening, a framework that enhances perceptual evaluation in Music-QA benchmarks. We introduce the Perceptual Index (PI), a quantitative metric that measures a question's reliance on audio perception by analyzing log probability distributions from text-only language models. Using this metric, we generate synthetic, challenging distractors to create QA pairs that necessitate genuine audio perception. When applied to MuchoMusic, our filtered dataset successfully forces models to rely on perceptual information\u2014text-only LLMs perform at chance levels, while LALMs similarly deteriorate when audio inputs are replaced with noise. These results validate our framework's effectiveness in creating benchmarks that more accurately evaluate audio perception capabilities.",
      "abstract": "Large Audio Language Models (LALMs), where pretrained text LLMs are finetuned with audio input, have made remarkable progress in music understanding. However, current evaluation methodologies exhibit critical limitations: on the leading Music Question Answering benchmark, MuchoMusic, text-only LLMs without audio perception capabilities achieve surprisingly high accuracy of up to 56.4%, much higher than chance. Furthermore, when presented with random Gaussian noise instead of actual audio, LALMs still perform significantly above chance. These findings suggest existing benchmarks predominantly assess reasoning abilities rather than audio perception. To overcome this challenge, we present RUListening, a framework that enhances perceptual evaluation in Music-QA benchmarks. We introduce the Perceptual Index (PI), a quantitative metric that measures a question's reliance on audio perception by analyzing log probability distributions from text-only language models. Using this metric, we generate synthetic, challenging distractors to create QA pairs that necessitate genuine audio perception. When applied to MuchoMusic, our filtered dataset successfully forces models to rely on perceptual information\u2014text-only LLMs perform at chance levels, while LALMs similarly deteriorate when audio inputs are replaced with noise. These results validate our framework's effectiveness in creating benchmarks that more accurately evaluate audio perception capabilities.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Yongyi Zang",
        "Sean O'Brien",
        "Taylor Berg-Kirkpatrick",
        "Julian McAuley",
        "Zachary Novack"
      ],
      "authors_and_affil": [
        "Yongyi Zang (Independent Researcher)*",
        "Sean O'Brien (University of California, San Diego)",
        "Taylor  Berg-Kirkpatrick (University of California, San Diego)",
        "Julian  McAuley (University of California, San Diego)",
        "Zachary Novack (University of California, San Diego)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Evaluation, datasets, and reproducibility",
        "Evaluation methodology",
        "Novel datasets and use cases",
        "Evaluation metrics"
      ],
      "long_presentation": "TRUE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1lFUDrQF9AioV1W5ktCpAM_RMerXMY7LY/preview",
      "poster_pdf": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-1-are-you-really",
      "title": "Are you really listening? Boosting Perceptual Awareness in Music-QA Benchmarks",
      "video": ""
    },
    "forum": "308",
    "id": "308",
    "pic_id": "",
    "position": "01",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nAgree\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nStrongly agree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nStrongly agree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nStrongly agree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe paper highlights the limitations of current Q/A benchmark datasets used for evaluating large audio-language models and provides a methodology to improve them. Current datasets, such as MuChoMusic, are prone to allowing correct answers to be guessed based solely on the text input modality. The paper proposes a method for automatically creating distractor answers that are more challenging for text-only models.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nA method to increase the difficulty of Q/A benchmarks for large audio-language models to avoid their overreliance on the text modality.\n\n**Q17 (This paper is of award-winning quality.)**\n\nYes\n\n**Q18 ( If yes, please explain why it should be awarded.)**\n\nThe paper presents a methodology and a new dataset that is capable of advancing the evaluation of music understanding by large audio-text models, which is a hot topic in MIR and audio AI research.\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nStrongly agree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nStrong accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThe paper presents a methodology and a new dataset that is capable of advancing the evaluation of music understanding by large audio-text models, which is a hot topic in MIR and audio AI research.\n\nLarge audio-text models are a hot topic, and evaluating their music understanding capabilities is of high importance. The paper continues recent efforts to create QA benchmark datasets, building on the MuChoMusic dataset proposed at ISMIR 2024. The authors highlight its key drawback, demonstrating how state-of-the-art (SOTA) text-only LLMs can achieve relatively high performance on this benchmark by exploiting their prior knowledge to effectively disregard part of the distractor answers.\n\nThe paper proposes a method to measure the difficulty of distractor answers in the benchmark in terms of the actual necessity of audio inputs (perceptual index). Using LLMs, the authors propose a method to generate synthetic distractor answers that are more challenging, eliminating the text bias present in the original MuChoMusic dataset. The authors contribute the resulting dataset as a new benchmark.\n\nOverall, the approach is sound, the paper is well-written, and it represents an important contribution to the field.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nStrong accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nAll reviewers agree on the significant relevance and value of the contributions in the paper for advancing LALM benchmarks. The paper is well-written and presents a sound methodology, and the reviewers concur on a strong recommendation for acceptance. There are specific questions from reviewers that should be addressed to further improve the paper. We expect relevant changes to be implemented in the camera-ready submission.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper presents a compelling and much-needed investigation into the perceptual awareness of audio language models within music question-answering benchmarks. The authors convincingly demonstrate that MuchoMusic can often be solved by text-only models relying on reasoning and prior knowledge, rather than genuine audio perception. The proposed RUListening framework, which introduces a perceptual index metric to quantify a question\u2019s reliance on audio and a method to generate more challenging, perceptually-demanding distractors, is a valuable and well-reasoned approach, and can be very useful to the community.. \n\nThe experiments are well designed and an extensive selection of models, including very large ones, are tested. The results support the central hypothesis, showing a significant reduction in text-only model performance and increased sensitivity of audio language models to audio input. The authors also qualitatively investigate possible issues with MuchoMusic, both in how it might encourage text reasoning, but also in how some questions seem to be invalid. While the authors mention the reliance on MuchoMusic as a limitation given these issues, their method is still applicable to other datasets, so it still remains valuable.\n\nThe paper is generally very well written, particularly in the explanation of the motivation, methods, and results. Personally, I found the writing style in the introduction a bit distracting, compared to the otherwise scientific and precise style for the rest of the paper. The language can be simplified a bit, both in complexity and \u201cgrandiosity\u201d. I understand part of the intensity/confidence stems from the unconventional decision of starting off with some results to prove your motivation (which, while unexpected, I felt by the end that it was a good choice). Part of this is also the starting quote, which I really don\u2019t think adds anything meaningful to the paper, and the starting sentence, which is long and convoluted. I think simpler language and a clearer explanation of the situation would make the paper much easier to read - it could be more clearly stated that the text models are simply responding to the text question without the audio. Persisting on them not being able to perceive and being \u201cdeaf\u201d prolonged my confusion about whether you are, for example, investigating providing some text encoding of a waveform to text models. A clearer explanation of the task (and dataset) would also benefit new readers to the area.\n\nStill, as mentioned, I very much enjoyed reading the rest of the paper and think it\u2019s a very strong and valuable contribution.\n\nA paper that I think is relevant and could be discussed in related work is \u201cI can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition\u201d from last year\u2019s ISMIR.\n\nI am not sure the ISMIR template is strictly followed. There are larger than usual page margins, but I can\u2019t tell if there\u2019s just added padding or if the column sizes are affected. References could be cleaner (links, capitalization, consistency).",
      "review2": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nI recommend acceptance of this paper. \n\nStrenghts: \n* Paper is easy to read and follow. Figures are appropriate and well-discussed in my opinion.\n* Authors conduct extensive experiments to validate their Perceptual Index.\n* Results indicate that the new questions makes it harder for LLMs to have comparable performance with LALMs, which is what we should see in MusicQA benchmarks.\nWeaknesses: I don't see any major weaknesses in this paper. \n\nQuestions:\n* L186-L190: Is the prompt also changed for LALMs?\n* L304-L316: How many distractors were generated per question? What was the prompt used to generate them? Was there any comparison with other LLMs to generate such distractors?\n* L329-L335: It would be interesting to have some examples of high and low semantic similarity score for CLAP. \n* L346-L348: Are you constraining the model output to only one token? If no, how are you controlling for the case in which the model does not follow the output format. For example, instead of answering the correct answer option, the model just write a very extensive answer. \n* L443-L450: Do authors see any relationship between the length of the response and the accuracy? In my experience, MuChoMusic evaluation criteria search for common terms between the answer words and the model output. I am not sure what is the impact of this on the evaluation.\n\n* Appendix B: Maybe I am missing something, but I don't understand the role of the audio description in the model inspection. Was the description provided to the models together with the question? If yes, why? \n\n\nMinor comments:\n* MuchoMusic -> MuChoMusic\n* L203, L227, L405, L444: remove space before the \\footnote command.\n* Is there a specific reason why authors decide to refer to LLMs as text-only LMs, but Audio LLMs as LALMs?",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nThis paper focuses on evaluating the validity of multimodal system evaluation protocols and more specifically, MuchoMusic. In that sense, they ask the question, are truly Multimodal Question Answering datasets testing the multimodal capabilities of such systems? In what way is the reasoning capabilities of language models enough to come up with the right answer without regarding the audio signal?\n\nApart from that, in lines <216-221> the authors provide a statement as true, but no valid evaluation has been tested as far as I'm concerned. Therefore, they should change the phrasing to explicitly say that this is a hypothesis that models to posses the \"world-prior\" information for music with an existing corpus.\n\nIn line <262> the definition of the total probability in the parentheses is incorrect, as the total probability is defined as the multiplication between the posterior (which is in the formula) and the prior of each answer (for which we don't have access to). Also, it is implied that softmax has been applied and should be clearly stated because in multiclass classification you don't need to have that. The final answer is the most plausible one, i.e. the one with the most likelihood. Therefore, the conditional probabilities of answers conditioned to the questions are not bound to be added to one. As such, more details must be included or explicitly state the softmax approximation of the total probability.\n\nIn the paragraph between lines <284-302> it is hardly to follow the argument over the choice of perceptual index over entropy. First, evidence for correlation between PI and entropy is needed and therefore the argument between lines <292-295> should be rephrased to explicitly state that this is a hypothesis rather than a given. Also, the noun is missing from sentence at lines <296-302>, probably they imply 'confidently wrong' and correct answers?. Also the claim that high PI answers require perceptual information while high entropy ones are not, is not justified. For that to be completely true, experiments with filtering based on entropy are needed and then comparison between PI and entropy based filtered performance and similarity distributions need to be added. Otherwise, I would rephrase the paragraph in order to explicitly state that this is a hypothesis left to be explored in future work.\n\nIn line <308> context packages are not a term that is known and therefore need explicit definition. Changing the phrasing from 'with question...' to 'as a list of question...' would introduce more clarity that a package is the set of the triplets. Line <319> has a typo. I would change the x-axis label of figure-3 to explicitly state the similarity that has been chosen, which is the cosine similarity.\n\nThere is evidence that CLAP is not properly understanding musical terms and a paper was out for Instrument recognition in last years ISMIR, where the text encoder of CLAP failed to semantically understand music relationships between instruments which is proposing the directly opposite phenomenon of the example proposed. Also, the distributions themselves do not provide context. A better alternative would be to plot the distributions between c and distractors in MuchoMusic, the generated distractors and even random ones. I believe that there won't be a significant change between random distractors and chosen ones. With that, an aggregation function (such as the overlapping index) could be calculated to hint that there isn't any big difference between the distributions. Also, in these distributions, the similarity between distractors and the correct answer is approaching 1 and those cases should be considered or it should be explicitly be stated so. \n\nIn figure 4b, there are a lot of outliers with very small perceptual index and Pearson's coefficient is known to be sensitive to them. A lasso analysis should be performed given the visualization, and the R factor would be almost 0 in that case (with a threshold of 0.3). This doesn't happen with 4a, so the correlation value is mostly true. Also, it should be good to include the mean accuracy for both of the datasets to signify the difference in both cases.\n\nIn line <393> hardness value are not properly defined and in line <394-397> the argument doesn't have grounding, as this is the under investigation hypothesis. Up until this point, audio is only partially correlated with PI. Also, Figure 4 should include the same plots for MuchoMusic without the generation to properly compare between them two. I suspect that the change in the music PR correlation will be pretty much the same. \n\nApart from that, the paper was an interesting read, very precise and definitely addressed a large gap in the music-language multmodal comprehension. I think its a work properly addressing a main gap of our domain and community, the lack of rigorously defined evaluation sets. Also, a way of estimating the informativeness of specific question answer pairs and the problematic nature of evaluating multimodal systems was addressed succesfuly! As a remark, more experiments should be performed with less models on random distractors, different subsets from several steps of the RUListening framework (with variable distance from the optimal D*) and even test the entropy based filtering. That would further solidify that their approach is successful and that choosing PI is the right surrogate task for finding the right set of distarctors. With minimal phrases changed, I'm thinking that this paper is a nice addition to the ISMIR conference!"
    },
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Automatic sample identification (ASID) - the detection and identification of portions of audio recordings that have been reused in new musical works - is an essential but challenging task in the field of audio query-based retrieval. While a related task, audio fingerprinting, has made significant progress in accurately retrieving musical content under \"real world\" (noisy, reverberant) conditions, ASID systems struggle to identify samples that have undergone musical modifications. Thus, a system robust to common music production transformations such as time-stretching, pitch-shifting, effects processing, and underlying or overlaying music is an important open challenge. \nIn this work, we propose a lightweight and scalable encoding architecture employing a Graph Neural Network within a contrastive learning framework. Our model uses only 9% of the trainable parameters compared to the current state-of-the-art system while achieving comparable performance, reaching a mean average precision (mAP) of 44.2%.\nTo enhance retrieval quality, we introduce a two-stage approach consisting of an initial coarse similarity search for candidate selection, followed by a cross-attention classifier that rejects irrelevant matches and refines the ranking of retrieved candidates - an essential capability absent in prior models. In addition, as queries in real-world applications are often short in duration, we benchmark our system for short queries using new fine-grained annotations for the Sample100 dataset, which we publish as part of this work.",
      "abstract": "Automatic sample identification (ASID) - the detection and identification of portions of audio recordings that have been reused in new musical works - is an essential but challenging task in the field of audio query-based retrieval. While a related task, audio fingerprinting, has made significant progress in accurately retrieving musical content under \"real world\" (noisy, reverberant) conditions, ASID systems struggle to identify samples that have undergone musical modifications. Thus, a system robust to common music production transformations such as time-stretching, pitch-shifting, effects processing, and underlying or overlaying music is an important open challenge. \nIn this work, we propose a lightweight and scalable encoding architecture employing a Graph Neural Network within a contrastive learning framework. Our model uses only 9% of the trainable parameters compared to the current state-of-the-art system while achieving comparable performance, reaching a mean average precision (mAP) of 44.2%.\nTo enhance retrieval quality, we introduce a two-stage approach consisting of an initial coarse similarity search for candidate selection, followed by a cross-attention classifier that rejects irrelevant matches and refines the ranking of retrieved candidates - an essential capability absent in prior models. In addition, as queries in real-world applications are often short in duration, we benchmark our system for short queries using new fine-grained annotations for the Sample100 dataset, which we publish as part of this work.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Aditya Bhattacharjee",
        "Ivan Meresman Higgs",
        "Mark Sandler",
        "Emmanouil Benetos"
      ],
      "authors_and_affil": [
        "Aditya Bhattacharjee (Queen Mary University of London)*",
        "Ivan Meresman Higgs (Queen Mary University of London)",
        "Mark Sandler (Queen Mary University of London)",
        "Emmanouil Benetos (Queen Mary University of London)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Indexing and querying",
        "Fingerprinting",
        "Music retrieval systems",
        "Applications",
        "Music signal processing",
        "MIR tasks",
        "Pattern matching and detection",
        "Machine learning/artificial intelligence for music",
        "MIR fundamentals and methodology",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1c1-9zALWWtf1vFDhyV6Kt158D2eBMgnd/preview",
      "poster_pdf": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-2-refining-music-sample",
      "title": "Refining music sample identification with a self-supervised graph neural network",
      "video": ""
    },
    "forum": "312",
    "id": "312",
    "pic_id": "",
    "position": "02",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nDisagree\n\n**Q5 ( Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chosen, otherwise write \"n/a\"))**\n\nIt only compares with one previous system that doesn't seem to be peer-reviewed. Several references are listed incompletely.\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nAgree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nDisagree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nDisagree (Standard topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nDisagree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nIt is unclear to me whether the system presented in the paper is actually improving results over a previous (not peer-reviewed) method claiming to be state-of-the-art. The lack of comparison with different baselines and the closeness of the mAP question whether the presented system is a meaningful improvement.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nBetter sample detection inspired by fingerprinting.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak reject\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThe authors present a system of sample detection based on a GNN with a classifier. The topic of sample detection is somewhat underexplored, so I appreciate the work on it. The main contributions are improved annotations for an existing dataset, a meaningful way of augmenting the training data, and the application of a GNN plus a classifier for retrieving the candidates.\n\nI was a bit surprised by the general premise that a fingerprinting approach works for sample detection - it would be interesting to see a more detailed analysis on mixing levels and detection accuracy.\n\nMy main concern is the lacking comparison with previous systems (the only one is from a non-peer-reviewed study) and that the results seem to be no improvement (mAP .442 vs .441) over this previous system. What makes me more skeptical is that the larger comparison system results are omitted in Table 3, only the results for the smaller, inferior system are presented as comparison..\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nWeak accept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nThe authors present an approach for sample identification with GNNs that seems to perform on par with or outperform the state of the art at a considerably lower complexity. Sample identification is a fascinating yet generally underexplored task, and the presented detailed annotations to an existing dataset are an important contribution to the field. In addition, the augmentation strategy during training is a neat strategy for this task.\n\nThe main weak point of the paper is that the actual comparison against the state of the art is missing for what are likely resource issues, preventing a decisive conclusion. The paper also could improve the description of methodology in some parts.\n\nThe discussion mainly focused on whether the existing contributions of the paper outweigh the shortcomings of the evaluation, making this a borderline decision.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nDear authors, thank you for your clear and well-structured contribution.\n\nThe paper presents a lightweight GNN-based model for automatic sample identification (ASID), combined with a retrieval refinement step using cross-attention and a detailed evaluation pipeline.\n\nThe task itself (sample identification under realistic transformations) is highly relevant and practically valuable, particularly for industry use cases like rights management. The approach of combining approximate nearest-neighbor search with a learned ranking function is well-motivated.\n\nThe extension of the Sample100 dataset with fine-grained annotations is one of the strongest contributions of the paper, and I greatly appreciate that both the dataset and code are shared for reproducibility. This will support follow-up research in this area. Your analysis of different sample types (beat vs. riff) and time-stretching levels (Table 4) provides helpful insights and reflects a thoughtful evaluation design.\n\nThere are a few points where the paper could be clarified:\n* The equation for mean-pooling (eq 4) seems a bit redundant, as the operation is trivial and well known.\n* It might be beneficial to discuss and analyze false positives, to see if there is a pattern, e.g. similar beats or riffs.\n\nOverall, this is a useful and timely contribution with a clear structure, solid methodology, and strong practical grounding. I recommend acceptance.",
      "review2": "- **Overall Evaluation**: Weak reject\n\n#### Main Reviews\n\nPlease find below my review \n \n====== REVIEW BEGINS HERE ======\n\nThe paper is about proposing a new architecture for ASID task. The architecture is composed of lightweight GNN as the encoder and multi-head attention-based classifier. Authors also open-source the code and release an advanced annotation of the Sample 100 dataset to benefit community of the relevant research. Overall, authors did some interesting designs and do some good analysis about query length and sample characteristics. However, I have several comments about the paper: \n\nFirst, the clarity of the methodology part. In my opinion, the methodology part is hard to follow due to some not necessary math equation or notation. For example, the notation in section 3.1. I think the math notation of waveform y or fixed duration t_seg didn\u2019t introduce clarity of the paper, plain text description will be more clear. Similar issues also existed in other part, such as the equation (2) is a standard GNN operation but authors didn\u2019t state further new information. Also the similar issues from equation (3) to (5). Some informations are standard operation or it doesn\u2019t necessary need an equation to illustrate the idea. It largely affect the clarity of the paper. \n\nSecond, also about the clarity of the figure 1. It is hard to understand the concept by just checking figure 1. Although after checking the methodology part then the figure can be understood, i think authors can further enhance the clarity of the figure 1. For example, the dotted arrow from \u201cdatabase of reference songs\u201d to \u201clist of candidates\u201d is unclear for me in the first glance. Also why three query embeddings will merge to correct match, how did they merge. Those informations only can be understood after checking the paper. Similar issues for multi-head cross-attention part. Embedding matrices NM_q and NM_r appears abrupt, such as does the correct match in stage (A) and stage (B) denotes same things? Does the A and B show the sequential processing or what\u2019s the relationship between A and B? \n\nThird, the potential misleading contribution in evaluation part. From table 2, authors compare the proposed method with SOTA model, and further evaluate the performance with different batch sizes. However, it seems like author leverage the SimCLR-based learning framework already, it is not surprise to see that increasing batch size improve the performance. The paragraph from line 440 to 448 may easily misleads readers who unfamiliar with SimCLR to interpret this as the contribution of this paper. \n\nIn the end, my decision of this paper will be weak reject. Though authors demonstrate some vary good start points of the project and definitely contribute the MIR community, I expect authors to further improve the clarity of the paper. \n\n Some minor comments about the clarity or possible missing references: \n\n- line 171: k-nearest neighbour graph \u2192 k-nearest neighbors graph\n- line 149 & line 160: \u201cmel-spectrogram\u201d and \u201cspectrogram\u201d share the same notation (? I think the spectrogram in line 160 should also be \u201cmel-spectrogram\u201d not \u201cspectrogram\u201d.\n- line 393: miss a reference about Adam optimizer\n- line 235: it will be better to mention K=4 in this paper.\n\n====== REVIEW ENDS HERE ======",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nI think this paper is exceptional. I really like that it was able to build upon the successes of GNNs in audio fingerprinting where CNNs have had such popularity recently, and established a new SOTA baseline. The explanation of the methodology was clear and easy to follow, and the results speak for themselves. The augmentation to the existing dataset was really a cherry on top.\n\nI do have a few criticisms/questions/comments though -\n\nThe existing SOTA does feel a bit slighted in that it wasn't re-implemented for this paper to compare against the latest results. Not having implemented this existing baseline myself, I found myself wondering, exactly how far outside of the realm of practical are the computational needs to implement a ResNet50-IBN architecture as compared to the ResNet18-IBN model implemented in its place? Was the A100 GPU not suitable? I do understand that the replacement model used a number of parameters on the order of magnitude of the model proposed by this paper, which is part of the allure, but it doesn't feel like an *entirely* fair comparison to say you beat the benchmark. Maybe the existing SOTA architecture would have benefited from the Sample100 dataset augmentations added to this paper and performed even better? It is hard to know without trying.\n\nIt was neat to see the ablation study showing the effectiveness of the GNN approach of this paper with and without the MHCA piece, as well as with varying batch sizes. I do wonder though why you stopped at batch size 1024 once you narrowly stepped ahead of the reported SOTA mAP score - why not try one or two even larger batch sizes to see if you could knock it out of the park? Were the returns already diminishing? Were the computational needs bordering on impractical? Perhaps some sort of memory pressure or something? I just found myself wondering if there was some additional runway here.\n\nRegarding the augmentation of the reference segments - were those augmented reference segments persisted and re-used, or were new augmentations created on-the-fly for every single batch?\n\nAlso, the augmentation step described in equation (8) seems a bit wild to me in general - does it make sense to apply ALL FOUR transformations (time-offset, gain variation, pitch-shifting, time-stretching) to every single reference segment? Might it make sense to only apply one or a few, here or there? Also, to make the augmentations seem more plausible, might it make more sense to look a bit more in depth at the nature of augmentations used in Sample100 to try and closer-replicate more realistic transforms? For example, time-stretching a segment in the uniformly sampled range of 70-150% and then mixing with the remaining stems seems a bit unrealistic of real music sampling. Wouldn't a beat-matched time-stretch, even at a higher or lower tempo octave of the reference drums stem be more plausible?\n\nI thought Table 3 was quite helpful in showing model performance at varying query lengths, particularly that the proposed model excels with longer query lengths, but what about shorter query lengths? Using a minimum length of 5s feels even a bit long, but I could be wrong. This also got me wondering about the distribution of actual sample lengths present in the dataset.\n\nOn the note of dataset transparency, I also liked that Table 4 sort of presented the proportion of samples in the dataset that exhibited certain characteristics (and the success of the model in these specific areas), but it also left me kind of seeking additional information. Specifically, it broke down time stretches into >5% and <5% buckets, but this got me curious - how much more than 5%? The time-stretching augmentation in this paper varies from 70-150% - does that cover the ranges found here? Also, the \"1-note\" type of sample class present in the table with no mAP result doesn't seem to be explained anywhere. Are there any other sample types in the dataset?\n\nThanks again for your paper, I really enjoyed reading it."
    },
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Despite significant recent advances in generative acoustic text-to-music (TTM) modeling, robust evaluation of these models lags behind, relying in particular on the popular Fr\u00e9chet Audio Distance (FAD). In this work, we rigorously study the design space of reference-based divergence metrics for evaluating TTM models through (1) designing four synthetic meta-evaluations to measure sensitivity to particular musical desiderata, and (2) collecting and evaluating on MusicPrefs, an open-source dataset of pairwise human preferences for TTM systems. We find that not only is the standard FAD setup inconsistent on both synthetic and human preference data, but that nearly all existing metrics fail to effectively capture desiderata, and are only weakly correlated with human perception. We propose a new metric, the MAUVE Audio Divergence (MAD), computed on representations from a self-supervised audio embedding model. We find that this metric effectively captures diverse musical desiderata (average rank correlation 0.84 for MAD vs. 0.49 for FAD) and also correlates more strongly with MusicPrefs (0.62 vs. 0.14).",
      "abstract": "Despite significant recent advances in generative acoustic text-to-music (TTM) modeling, robust evaluation of these models lags behind, relying in particular on the popular Fr\u00e9chet Audio Distance (FAD). In this work, we rigorously study the design space of reference-based divergence metrics for evaluating TTM models through (1) designing four synthetic meta-evaluations to measure sensitivity to particular musical desiderata, and (2) collecting and evaluating on MusicPrefs, an open-source dataset of pairwise human preferences for TTM systems. We find that not only is the standard FAD setup inconsistent on both synthetic and human preference data, but that nearly all existing metrics fail to effectively capture desiderata, and are only weakly correlated with human perception. We propose a new metric, the MAUVE Audio Divergence (MAD), computed on representations from a self-supervised audio embedding model. We find that this metric effectively captures diverse musical desiderata (average rank correlation 0.84 for MAD vs. 0.49 for FAD) and also correlates more strongly with MusicPrefs (0.62 vs. 0.14).<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Yichen Huang",
        "Zachary Novack",
        "Koichi Saito",
        "Jiatong Shi",
        "Shinji Watanabe",
        "Yuki Mitsufuji",
        "John Thickstun",
        "Chris Donahue"
      ],
      "authors_and_affil": [
        "Yichen Huang (Carnegie Mellon University)",
        "Zachary  Novack (University of California, San Diego)",
        "Koichi Saito (Sony AI)",
        "Jiatong  Shi (Carnegie Mellon University)",
        "Shinji  Watanabe ( \tCarnegie Mellon University)",
        "Yuki  Mitsufuji (Sony AI)",
        "John  Thickstun (Cornell University)",
        "Chris  Donahue (Carnegie Mellon University)*"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Generative Tasks",
        "Evaluation metrics",
        "Novel datasets and use cases",
        "Music and audio synthesis",
        "Qualitative evaluations",
        "Evaluation, datasets, and reproducibility"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1uZWKKhNPqcxEW88-fEDVjoHQTActKTC0/preview",
      "poster_pdf": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-7-aligning-text-to",
      "title": "Aligning Text-to-Music Evaluation with Human Preferences",
      "video": ""
    },
    "forum": "314",
    "id": "314",
    "pic_id": "",
    "position": "07",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Musicians and nonmusicians alike use rhythmic sound gestures, such as tapping and beatboxing, to express drum patterns. While these gestures effectively communicate musical ideas, realizing these ideas as fully-produced drum recordings can be time-consuming, potentially disrupting many creative workflows. To bridge this gap, we present TRIA (The Rhythm In Anything), a masked transformer model for mapping rhythmic sound gestures to high-fidelity drum recordings. Given an audio prompt of the desired rhythmic pattern and a second prompt to represent drumkit timbre, TRIA produces audio of a drumkit playing the desired rhythm (with appropriate elaborations) in the desired timbre. Subjective and objective evaluations show that a TRIA model trained on less than 10 hours of publicly-available drum data can generate high-quality, faithful realizations of sound gestures across a wide range of timbres in a zero-shot manner.",
      "abstract": "Musicians and nonmusicians alike use rhythmic sound gestures, such as tapping and beatboxing, to express drum patterns. While these gestures effectively communicate musical ideas, realizing these ideas as fully-produced drum recordings can be time-consuming, potentially disrupting many creative workflows. To bridge this gap, we present TRIA (The Rhythm In Anything), a masked transformer model for mapping rhythmic sound gestures to high-fidelity drum recordings. Given an audio prompt of the desired rhythmic pattern and a second prompt to represent drumkit timbre, TRIA produces audio of a drumkit playing the desired rhythm (with appropriate elaborations) in the desired timbre. Subjective and objective evaluations show that a TRIA model trained on less than 10 hours of publicly-available drum data can generate high-quality, faithful realizations of sound gestures across a wide range of timbres in a zero-shot manner.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Patrick O'Reilly",
        "Julia Barnett",
        "Hugo Flores Garcia",
        "Annie Chu",
        "Nathan Pruyne",
        "Prem Seetharaman",
        "Bryan Pardo"
      ],
      "authors_and_affil": [
        "Patrick O'Reilly (Northwestern University)*",
        "Julia Barnett (NorthwesternUniversity)",
        "Hugo Flores Garcia (Northwestern University)",
        "Annie Chu (Northwestern University)",
        "Nathan Pruyne (\tNorthwestern University)",
        "Prem Seetharaman (Adobe Research)",
        "Bryan Pardo (\tNorthwestern University)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Generative Tasks",
        "Applications",
        "Musical features and properties",
        "Music generation",
        "Representations of music",
        "Music and audio synthesis",
        "Music composition, performance, and production",
        "MIR tasks",
        "Rhythm, beat, tempo",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1GvRU_IVGaSd7R_LMYstwCpI2zj6dMwBQ/preview",
      "poster_pdf": "",
      "session": [
        "4"
      ],
      "slack_channel": "p4-11-the-rhythm-in",
      "title": "The Rhythm In Anything: Audio-Prompted Drums Generation with Masked Language Modeling",
      "video": ""
    },
    "forum": "316",
    "id": "316",
    "pic_id": "",
    "position": "11",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nStrongly agree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nAgree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nDisagree\n\n**Q5 ( Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chosen, otherwise write \"n/a\"))**\n\nThe citations for KAD aren't complete - KAD was already used as \"KID\" in\nNistal et al., Comparing representations for audio synthesis using generative adversarial networks, 27th European Signal Processing Conference (EUSIPCO), 2019\n\nThe authors missed several related works regarding drum sample and drum pattern generation. As that is the main topic of the paper, please include such works as well (see below for suggestions)\n\nDrum sample generation:\n\nJ. Nistal, S. Lattner, and G. Richard,\n\u201cDrumGAN: Synthesis of drum sounds with timbral feature conditioning using generative adversarial networks,\u201d\nin Proc. Int. Soc. Music Inf. Retrieval Conf. (ISMIR), Montr\u00e9al, Canada, 2020.\n\nA. Lavault, A. Roebel, and M. Voiry, \u201cStyleWaveGAN: Style-based synthesis of drum sounds with extensive controls using generative adversarial networks,\u201d in Proceedings of the Sound and Music Computing Conference (SMC), Saint-\u00c9tienne, France, 2022.\n\nJ. Drysdale, M. Tomczak, and J. Hockman, \u201cStyle-based drum synthesis with GAN inversion,\u201d in Extended Abstracts for the Late-Breaking Demo Session of the 22nd International Society for Music Information Retrieval Conference (ISMIR), Online, 2021\n\nDrum pattern/rhythm generation (audio and symbolic):\n\nG. Alain, M. Chevalier-Boisvert, F. Osterrath, and R. Pich\u00e9-Taillefer,\n\u201cDeepDrummer: Generating drum loops using deep learning and a human in the loop,\u201d\narXiv preprint arXiv:2008.04391, 2020.\n\nI.-C. Wei, C.-W. Wu, and L. Su,\n\u201cGenerating structured drum patterns using variational autoencoder and self-similarity matrix,\u201d\nin Proc. Int. Soc. Music Inf. Retrieval Conf. (ISMIR), 2019.\n\nD. P. W. Ellis and J. Arroyo,\n\u201cEigenrhythms: Drum pattern basis sets for classification and generation,\u201d\nin Proc. Int. Soc. Music Inf. Retrieval Conf. (ISMIR), 2004.\n\nD. G\u00f3mez-Mar\u00edn, S. Jord\u00e0, and P. Herrera,\n\u201cNetwork representations of drum sequences for classification and generation,\u201d\nFrontiers in Computer Science, vol. 6, 2024.\ndoi: 10.3389/fcomp.2024.1476996\n\nS. Lattner and M. Grachten,\n\u201cDrumNet: High-level control of drum track generation using learned patterns of rhythmic interaction,\u201d\nin Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2019.\ndoi: 10.1109/WASPAA.2019.8937229\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nStrongly agree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nDisagree\n\n**Q10 (Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chose, otherwise write \"n/a\"))**\n\n- The interpretation of FAD is incorrect\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nAgree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nAgree (Novel topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nI consider the way timbre information is injected as context a reusable insight.\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nMasked language modeling can be used for timbre transfer.\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak accept\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nThe paper introduces a masked Transformer model that produces audio clips of drums based on a timbre and a rhythm prompt. During training, a chosen \"buffer\" is partially masked, and the masked tokens are predicted based on the unmasked parts. At inference, the timbre prompt is appended to a fully masked segment that contains rhythm features.\n\nThe paper is well structured and easy to follow. The sound examples are convincing, and most evaluations are reasonable. There is a major problem with the interpretation of KAD and some minor problems.\n\nPositive:\n- Well-structured and easy to follow paper with convincing results.\n- I appreciate the subjective study that shows that both models produce convincing output. It is understood that it wouldn't be fair to ask about timbre, as the MelodyFlow model isn't designed to adhere to a given timbre audio reference.\n\nQuestions/Remarks:\n- The interpretation of KAD is incorrect: \"Finally, as shown in Table 3, TRIA produces more realistic drum audio than MelodyFlow on average.\" The reference set chosen for KAD calculation is from the same distribution as the dataset used for timbre conditioning (i.e., MoisesDB dataset). Therefore it is understandable that the KAD from TRIA is lower than that from MelodyFlow, as MelodyFlow was prompted with random prompts that don't follow the reference data distribution. This test doesn't show \"realistic drum audio\"; it is rather redundant to the results in Table 2 (timbre column), where understandably, there are no results for MelodyFlow. Similar to omitting results for MelodyFlow in Table 2, the KAD comparison should also be considered invalid. Please remove or clarify that in the camera-ready version.\n- Why is the buffer chosen anywhere in the sequence during training, if at inference time, the generated part is only at the end? With enough training this doesn't really matter, but it seems to complicate things unnecessarily. Please clarify.\n- No reason is given why the range [0, 0.1, 0.2] was chosen for the re-noising parameter in MelodyFlow.\n- No mentioning about the used model architecture in either Abstract or Introduction. Please mention \"Masked Transfomer\" both in the Abstract and Introduction.\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nOverall, the reviews are positive, with two reviewers assigning Strong Accept and one Weak Accept. The reviewers consistently noted technical novelty, clarity of exposition, and practical relevance of the work. Highlights include:\n\nR1: \u201cThis is a very accomplished, polished piece of work which makes a valuable contribution,\u201d and, \u201cclear utility as a musical tool.\u201d\n\nR2: \u201cVery interesting disentangling method\u2026 clear contribution toward the emerging task of audio drum pattern synthesis.\u201d\n\nR3: \u201cStrong empirical evidence\u2026 modular conditioning and adaptive rhythm representation extendable to other tasks.\u201d\n\nAreas for improvement included:\n\nR1: Clarification of the timbre/rhythm terminology and deeper ablations (e.g., on CFG).\n\nR2: Justification of choices such as the use of DAC codec and ChatGPT-based descriptors; concerns about potential western bias and prompt modality mismatches.\n\nR3: Correction of KAD interpretation and improved clarity on architectural description and training-inference masking mismatch.\n\nMeta-reviewer: Missing references to related work (especially KID/KAD, and prior work in drum synthesis and pattern generation); incorrect interpretation of KAD; lack of related literature on drum pattern/sound generation.\n\nDespite these concerns, the methodological contribution is considered solid and the experimental evaluation largely convincing.\n\nRecommendation\nThe paper is technically sound, clearly written, and introduces a method that is novel and relevant to the ISMIR community. Remaining concerns are largely correctable in the camera-ready version. The authors are encouraged to revise the KAD interpretation, clarify architectural details in the introduction/abstract, and address the missing citations listed in the meta-review. Given the overall strong reviews and technical contribution, I recommend Accept.\n\nCongratulations to the authors for a strong and impactful paper.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nI note that anonymity has been somewhat breached here: the authors shared this work widely online at the end of 2024 and I suspect it would be hard to find a reviewer with the appropriate expertise who is not already aware of this contribution. Nonetheless, I have endeavoured to evaluate the paper as impartially as I can.\n\nOverview:\nThe work presents an audio generative model for percussion audio which is designed to disentangle rhythm and timbre conditioning signals. The model builds on VampNet to predict DAC tokens via masked generative modelling in a coarse-to-fine manner. It uses a \u201cdualized\u201d spectral representation with an adaptive crossover frequency as rhythm conditioning, and a DAC token prefix to represent timbre. The result is a model that effectively disentangles drum rhythms from drum timbre, allowing for a mapping from arbitrary recordings of \u201csound gestures\u201d to realistic drum audio.\n\nImpression:\nMy broad impression is extremely positive. This is a very accomplished, polished piece of work which makes a valuable contribution to an interesting research direction. Further, it continues in a trend of \u201cmusician-positive\u201d work on generative AI which I feel sets a constructive example to the ISMIR community. Audio examples demonstrate that the resulting model clearly works well, and has clear utility as a musical tool.\n\nI do have some concerns about the KAD evaluation, with respect to the MelodyFlow baseline. These are detailed below. However, even in light of this, I believe the work is of sufficient value to the community to warrant acceptance.\n\nSome general observations:\nI wonder about the choice of \u201crhythm\u201d and \u201ctimbre\u201d as terminology for the conditioning signals. These should perhaps be referred to as \u201cgesture\u201d and \u201csound palette\u201d signals, as the former captures extra-rhythmic aspects such as intensity and coarsely banded spectral centroid, while the latter is really capable of describing multiple timbres of which some or all may be present in the resulting signal, as well as other related timbres which are not captured in the conditioning.\n\nGiven the predictability of many drum rhythms and the extreme information bottleneck in the rhythm representation, it\u2019s something of a surprise that the disentanglement was so effective and the model so effectively ignores rhythmic content in the timbre prefix. Of course, the effects-based augmentation probably helped here, but I wonder whether the use of classifier-free guidance played a more important role. If the authors are planning any follow-up work, my opinion is that an ablation of CFG and exploration of sampling weights would be very helpful to readers in understanding how the proposed method actually works. Similarly, it would be helpful to know whether choices like the quantisation of the dualised representation are truly necessary.\n\nExcellent consideration of ethical implications. Perhaps there is a slight gap in the consideration of western bias: it is suggested that the model would function equally well if trained on percussion recordings from different musical cultures, but it does seem that the \u201cdualized\u201d adaptive rhythm representation exhibits a subtle bias to western rhythmic phraseology. All the prior work on this representation appears to focus on drum kit in western popular styles. Regardless, this does not detract from the work as presented, and is simply offered as a consideration for future research.\n\nCriticisms:\nMy main criticism is that the KAD comparison to the MelodyFlow baseline is not under apples-to-apples conditions. TRIA receives timbre conditioning via a prefix of DAC tokens computed from MoisesDB recordings, while MelodyFlow receives a ChatGPT generated text description. I feel this distinction needs to be better emphasised in the text to make clear that MelodyFlow results can not be directly compared to TRIA results.\n\nI question the value of motivating this work on the basis, given in the introduction, that: \u201cto realize a sound gesture as a fully-produced drum arrangement often requires significant time and skill\u201d. I feel this line of reasoning risks alienating musicians by suggesting that it wishes to replace their labour rather than augment their abilities. It also seems a bit contradictory in light of the statement in section 6.1 that the authors view this work \u201cas a means to provide music creators with additional agency\u201d. \n\nTo be clear, I'm not challenging the value of the work, just the framing of this particular motivation. Honestly, I feel the affordances of the proposed work stand on their own: it enables multiple new avenues for creative expression, which do not require justification as a mere time-saving contrivance.\n\nI found myself wondering about the performance of TRIA in the face of extreme OOD samples. To me, this would appear to be a very valuable creative use case: i.e. can I \u201cplay\u201d any arbitrary sound as a percussion instrument by simply tapping or beatboxing? This is briefly addressed in the online supplement and in Fig. 4, but it would be nice to offer a more concrete evaluation of the model\u2019s behaviour in the face of such extreme timbre conditioning. Honestly, I think such experimentation may have offered more insight into the musical value of the model than the FrameRNN transcription experiment.\n\nI also think this would be particularly revealing w.r.t. the MelodyFlow baseline. The paper points out that TRIA outperforms MelodyFlow despite being much smaller and trained with less data, but this is relatively unsurprising: task-specific models often outperform generalists on in-domain evaluations. I suspect that probing generalisation may reveal some (understandable) limitations of TRIA in comparison to MelodyFlow.\n\n\nMinutiae:\nTypo: Tables 1-3 all list MelodyFlow as MelodyFlowFlow",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper starts with the sentence \"Musicians and non-musicians alike use rhythmic sound gestures, such as tapping and beat-boxing, to express drum patterns.\" but doesn't quote any reference to justify this statement. Later on, TRIA is presented as a solution that \"produces audio of a drumkit playing the desired rhythm\". It would be interesting to have further justification toward why producing an audio recording directly is useful to musicians (and non-musicians), considering that the producing workflow of musicians for example in EDM typically involve some sort of symbolic notation within a DAW or Drum Machines. More interestingly, it would be interesting to justify the other way around: what musicians and non-musicians are doing that requires to produce audio drum pattern.\nRegarding the technical aspect, it would also be interesting to justify further the use of the Descript Audio Codec to encode highly percussive audio content. Was it compared to other encoders or does the choice rely on another study?\nThe Dualized Rhythm Representation is a very interesting part of this paper and it would be very useful to have some sort of graphic representation of the rhythm feature representation to visualize and insist further on one of the main contribution of the paper.\nRegarding the experiment, it was mentioned that the descriptions of the drum kits timbers was made with ChatGPT. It would be useful to explain why using a combination of MIR algorithms from the community couldn't be used at this point, or why already existing labeled drum content couldn't be used.\nIn the subjective evaluation, TRIA2Band and MelodyFlow0.2 were selected. Would it be possible to justify this decision?\nAlso since all the listeners are US speakers, it would be interesting to comment on the potential bias resulting of this decision/constraint.\nFinally, the Ethics Statement is interesting, but I do not know if all those details belongs to this paper. Maybe a separate paper about the ethical aspect of such research could be useful to address some details mentioned in this paragraph. Side comment: mentioning the energy cost of the research without the associated carbon footprint isn't very useful as different countries can have very different kWh/CO2 yearly average ratio.",
      "review3": "- **Overall Evaluation**: Strong accept\n\n#### Main Reviews\n\nStrengths\n\n1. Clear technical innovation: Dual-prompt conditioning and the adaptive 2-band dualized rhythm representation cleanly disentangle rhythm from timbre, yielding large gains over 1-band and na\u00efve splits .\n\n2. Convincing empirical evidence with small resources: no significant listener preference between TRIA and MelodyFlow; both strongly preferred over random anchors\n\n3. Thorough ablations and fair crowd-sourcing protocol: Band-count, masking-ratio, and guidance ablations plus IRB-approved listening test with 116 paid evaluators enhance credibility\n\n\nWeaknesses\n\n1. Narrow evaluation scope \u2013 All tests employ \u2264 4 s Western drum prompts, leaving long-form grooves, polyrhythms, and non-Western percussion unvalidated and raising questions about external validity.\n\n2. Prompt-modality mismatch \u2013 TRIA receives an audio timbre prompt while MelodyFlow is conditioned on text, introducing a confound that may inflate TRIA\u2019s apparent timbre adherence advantage.\n\n\nThe work delivers a significant, well-substantiated advance: a small, open, energy-efficient model that equals or beats a much larger proprietary system on multiple metrics. Experimental design is solid, writing is clear, and ethical transparency is exemplary. Remaining concerns\u2014limited baselines, short prompts, and unreleased code\u2014are important but fixable."
    },
    "session": "4"
  },
  {
    "content": {
      "TLDR": "Tasked with the challenge of designing a Generative AI system that could improvise on stage with GRAMMY-winning keyboard virtuoso Anon Visiting Artist, we developed the \"jam_bot\", a real-time performance system, that could match his eclectic improvisational aesthetics. We debuted the jam_bot at a high-stakes sold-out concert to critical acclaim, realizing a series of virtuosic tightly-coupled Human-AI free improvisations in varying musical styles. Reflecting on our year-long collaboration with the artist, we summarize learnings for AI researchers and musicians on the adaptations needed to turn state-of-the-art symbolic music Language Models (LMs) into jam_bots and the engineering required to make them performance-ready. \nWe focus on three aspects: First, to enable jam_bots to take on different musical roles, such as lead, accompany, or engage in call and response, we adapt music LMs to take on different interaction strategies by modifying the context and conditioning signals they take in. Second, for jam_bots to match the style needed for each piece, we describe how AnonArtist intentionally structures his improvisation in order to finetune music LMs to enable these strategies. Third, we show the optimizations needed to run music LMs in real-time and how to embed them in a low-latency multi-threaded system that listens, and prompts and schedules model generations seamlessly. We hope these insights enable more musician-AI symbiotic virtuosity.",
      "abstract": "Tasked with the challenge of designing a Generative AI system that could improvise on stage with GRAMMY-winning keyboard virtuoso Anon Visiting Artist, we developed the \"jam_bot\", a real-time performance system, that could match his eclectic improvisational aesthetics. We debuted the jam_bot at a high-stakes sold-out concert to critical acclaim, realizing a series of virtuosic tightly-coupled Human-AI free improvisations in varying musical styles. Reflecting on our year-long collaboration with the artist, we summarize learnings for AI researchers and musicians on the adaptations needed to turn state-of-the-art symbolic music Language Models (LMs) into jam_bots and the engineering required to make them performance-ready. \nWe focus on three aspects: First, to enable jam_bots to take on different musical roles, such as lead, accompany, or engage in call and response, we adapt music LMs to take on different interaction strategies by modifying the context and conditioning signals they take in. Second, for jam_bots to match the style needed for each piece, we describe how AnonArtist intentionally structures his improvisation in order to finetune music LMs to enable these strategies. Third, we show the optimizations needed to run music LMs in real-time and how to embed them in a low-latency multi-threaded system that listens, and prompts and schedules model generations seamlessly. We hope these insights enable more musician-AI symbiotic virtuosity.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Lancelot Blanchard",
        "Perry Naseck",
        "Stephen Brade",
        "Kimaya Lecamwasam",
        "Jordan Rudess",
        "Cheng-Zhi Anna Huang",
        "Joseph Paradiso"
      ],
      "authors_and_affil": [
        "Lancelot Blanchard (MIT Media Lab)*",
        "Perry Naseck (MIT Media Lab)",
        "Stephen Brade (Massachusetts Institute of Technology)",
        "Kimaya Lecamwasam (MIT Media Lab)",
        "Jordan Rudess (MIT Media Lab)",
        "Cheng-Zhi Anna Huang (Massachusetts Institute of Technology)",
        "Joseph Paradiso (MIT Media Lab)"
      ],
      "channel_url": "",
      "day": "4",
      "keywords": [
        "Generative Tasks",
        "Applications",
        "Creativity",
        "Human-ai co-creativity",
        "Knowledge-driven approaches to MIR",
        "Music composition, performance, and production",
        "Tools for artists",
        "Music interfaces and services",
        "Real-time considerations",
        "Machine learning/artificial intelligence for music",
        "Human-centered MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1VBezu0RDpcZMLjPDc5sqzgPv0eLxys2P/preview",
      "poster_pdf": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-3-the-jam-bot",
      "title": "The jam_bot, a Real-Time System for Collaborative Free Improvisation with Music Language Models",
      "video": ""
    },
    "forum": "321",
    "id": "321",
    "pic_id": "",
    "position": "03",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Advances in neural network design and the availability of large-scale labeled datasets have driven major improvements in piano transcription. Existing approaches target either offline applications, with no restrictions on computational demands, or online transcription, with delays of 128\u2013320 ms. However, most real-time musical applications require latencies below 30 ms. In this work, we investigate whether and how the current state-of-the-art online transcription model can be adapted for real-time piano transcription. Specifically, we eliminate all non-causal processing, and reduce computational load through shared computations across core model components and variations in model size. Additionally, we explore different pre- and postprocessing strategies, and related label encoding schemes, and discuss their suitability for real-time transcription. Evaluating the adaptions on the MAESTRO dataset, we find a drop in transcription accuracy due to strictly causal processing as well as a tradeoff between the preprocessing latency and prediction accuracy. We release our system as a baseline to support researchers in designing models towards minimum latency real-time transcription.",
      "abstract": "Advances in neural network design and the availability of large-scale labeled datasets have driven major improvements in piano transcription. Existing approaches target either offline applications, with no restrictions on computational demands, or online transcription, with delays of 128\u2013320 ms. However, most real-time musical applications require latencies below 30 ms. In this work, we investigate whether and how the current state-of-the-art online transcription model can be adapted for real-time piano transcription. Specifically, we eliminate all non-causal processing, and reduce computational load through shared computations across core model components and variations in model size. Additionally, we explore different pre- and postprocessing strategies, and related label encoding schemes, and discuss their suitability for real-time transcription. Evaluating the adaptions on the MAESTRO dataset, we find a drop in transcription accuracy due to strictly causal processing as well as a tradeoff between the preprocessing latency and prediction accuracy. We release our system as a baseline to support researchers in designing models towards minimum latency real-time transcription.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Patricia Hu",
        "Silvan Peter",
        "Jan Schl\u00fcter",
        "Gerhard Widmer"
      ],
      "authors_and_affil": [
        "Patricia Hu (Johannes Kepler University)*",
        "Silvan Peter (Johannes Kepler University)",
        "Jan Schl\u00fcter (Johannes Kepler University)",
        "Gerhard Widmer (Johannes Kepler University)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Music transcription and annotation",
        "MIR tasks"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1JpwJN_pSeFVLTeVXeW53V2Ohz7DC8kr0/preview",
      "poster_pdf": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-10-exploring-system-adaptations",
      "title": "Exploring System Adaptations for Minimum Latency Real-Time Piano Transcription",
      "video": ""
    },
    "forum": "339",
    "id": "339",
    "pic_id": "",
    "position": "10",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "**Q2 ( I am an expert on the topic of the paper.)**\n\nAgree\n\n**Q3 ( The title and abstract reflect the content of the paper.)**\n\nDisagree\n\n**Q4 (The paper discusses, cites and compares with all relevant related work.)**\n\nDisagree\n\n**Q5 ( Please justify the previous choice (Required if \u201cStrongly Disagree\u201d or \u201cDisagree\u201d is chosen, otherwise write \"n/a\"))**\n\nSome old works exist on real-time melody estimation:\n- M. Goto, A real-time music scene description system: Predominant-f0 estimation for detecting melody and bass lines in real-world audio signals, 2004\n- V. Arora and L. Behera, On-Line Melody Extraction From Polyphonic Audio Using Harmonic Cluster Tracking, 2013\n\n**Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)**\n\nAgree\n\n**Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by \u201cn\u201d pages of references or ethical considerations, references are well formatted). If you selected \u201cNo\u201d, please explain the issue in your comments.)**\n\nYes\n\n**Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)**\n\nStrongly agree\n\n**Q9 (Scholarly/scientific quality: The content is scientifically correct.)**\n\nAgree\n\n**Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view \"novelty\" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)**\n\nDisagree\n\n**Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)**\n\nAgree\n\n**Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated \u201cStrongly Agree\u201d and \u201cAgree\u201d can be highlighted, but please do not penalize papers rated \u201cDisagree\u201d or \u201cStrongly Disagree\u201d. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)**\n\nStrongly Disagree (Well-explored topic, task, or application)\n\n**Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)**\n\nAgree\n\n**Q15 (Please explain your assessment of reusable insights in the paper.)**\n\nThe methods to make a real time system could be used for other audio or music processing tasks too\n\n**Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)**\n\nThe paper explores modifying STFT window and model architecture to achieve low latency in real time piano transcription\n\n**Q17 (This paper is of award-winning quality.)**\n\nNo\n\n**Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)**\n\nDisagree\n\n**Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)**\n\nWeak reject\n\n**Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)**\n\nStrengths:\n- The paper aims at converting an offline piano transcription system to a real-time one with a low latency\n- Low latency can be achieved by modifying to STFT window to Tukey, and by network modifications such as removing velocity conditioning and sharing some computations. \n- Extensive experimentation\n\nWeaknesses:\n- The takeaways from the paper seem limited.\n- The performance improvement appears marginal at larger tolerances. E.g., in Table 4, causal-AMT is much worse than mobile-AMT at 30 ms and 50 ms tolerances. \n- I wonder, practically, how useful are the F1 scores reported for 10ms tolerance in Table 4, even for causal-AMT? The paper should include a discussion around the practical uses of the best settings suggested in the paper.\n\nTitle:\n- the title says \"Exploring network adaptations ...\" but there are modifications outside the network, such as in STFT window. May be the title could be \"Exploring adaptations ...\" or \"Exploring system adaptations ...\".\n\n**Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid \u201cweak accepts\u201d or \u201cweak rejects\u201d if possible.)**\n\nAccept\n\n**Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))**\n\nAll reviewers appreciate the work. The practical utility of the topic and the detailed analysis is admirable. There are some critical comments that the authors may take note of.",
      "publish_reviews": "TRUE",
      "review1": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThe paper is well written and clear. The non simmetric windowing function used for limit the amount of \"future\" information and steering the analysis towards causality is a nice feature. However, a certain number of \"future\" information is still needed if you want a good accuracy. I honestly think that, especially for lower frequency, this limitation it will be hard to overcome, regardles the method used! Reducing the model size for reducing complexity is another good point, and doing it without sacrifice the performances is always a challenge. The evaluation covers different aspects, however It would be beneficial to see a comparision with other methods, like for example the cited [6,7,14,15] with the same test set.",
      "review2": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper is an important step in a direction that is often overlooked in academia, yet is important for practical applications of AMT where low latency is required.\n\nStrengths:\n* Numerous experiments and detailed ablations\n* Insightful remarks on the baseline model that show deep understanding of the architecture\n* Interesting exploration of under-explored topic that is important for practical applications\n\nWeaknesses:\n* Actual processing time is not measured or discussed. Is the resulting model actually fast enough to run in 10 ms on consumer hardware? (I doubt it.) Have authors tested this? Otherwise, the choice of 10 ms frames should be revisited, since it's not practical.\n* Tables are hard to read, results could be presented in a clearer way, or some of the evaluations could be omitted to keep the more important ones\n* Asymmetric window is chosen due to improved results without discussing its effect on the STFT (I can assume it leads to increased spectral leakage due to larger main lobe and side lobes)\n\nAdditional comments:\n\nLine 36: \u201clatter two\u201d doesn\u2019t seem to match order of references. Please specify which ones.\nLine 107: I believe Kwon et al. also show an experiment with latency of 128 ms. \nLine 153: interesting insight about 10 s latency due to Squeeze and Excitation\nLine 230: Using a weighted loss to deal with class imbalance is a well known technique, used for example in AMT in the \u2018Basic Pitch\u2019 paper by Rachel Bittner et al. It\u2019d be nice to cite that or a more general reference.\nLine 304: Is \u2018assymetric Tukey window\u2019 really a correct term here? The Tukey window is a cosine-tapered rectangular window, but here there is no rectangular element. I suppose \u2018assymetric Hann window\u2019 could be a better name.\nLines 361-365: A3-5 and A6-8 are not detailed. Either omit some results from the table, or explain what the experiments are.\nLine 379: I\u2019m not sure this logic makes sense to me. Why does robustness to lower tolerance threshold imply the model is more promising? Why do authors have a strict tolerance requirement? If the main requirement is for lower latency, the requirement for accurate time-localization can/should be relaxed.\nSection 4.4: Which post-processing method is used for Mobile-AMT? Is it evaluated with the same heuristics as Causal-AMT, or with the original heuristics?\nSection 4.4: There's no sufficient explanation for why Mobile-AMT performs poorly with strict timing tolerance. This is a surprising result which should be explained.\nSection 5: First two paragraph repeat the latency discussion done in the introduction. Most of the given latency numbers are not relevant when discussing what is the desired/required latency for an AMT system.\nLine 468: \u201c...we reduce model size\u201d - what is the difference in terms of FLOPs and/or number of parameters, compared to the baseline?",
      "review3": "- **Overall Evaluation**: Weak accept\n\n#### Main Reviews\n\nThis paper has completed a detailed case study of an existing online piano transcription framework, Mobile-AMT, and proposed thorough updates to transform it into a fully causal version capable of real-time processing with a latency below 30ms.\n\nThe analysis and experiment design has shown a solid amount of work the author has put into this research. The author has analyzed the full pipeline of the Mobile-AMT, from the pre-processing window and model architecture to onset identification post-processing algorithms. A corresponding solution is proposed for each issue in the previous framework, followed up with experiment results for proper justification. Also, all the modifications deliver the overall principle of \"causality only\". Well structured.\n\nAccording to Table 4, the proposed model's performance doesn't change much under different tolerances. A more detailed analysis of this phenomenon would help us understand the pros and cons of the proposed model architecture.\n\nIt is also worth trying to loosen the n_s to adapt for longer tolerance in Table 4 for a more well-rounded comparison. Although Table 2 has already shown the preliminary results of different window settings, a comparison of the fully trained models can serve as the final performance baseline to show how your model balances latency and accuracy, which will benefit follow-up work.\n\nMinor comments:\nLine 419: duplicated \"note\""
    },
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Music adversarial attacks have garnered significant interest in the field of Music Information Retrieval (MIR). In this paper, we present Music Adversarial Inpainting Attack (MAIA), a novel adversarial attack framework that supports both white-box and black-box attack scenarios. MAIA begins with an importance analysis to identify critical audio segments, which are then targeted for modification. Utilizing generative inpainting models, these segments are reconstructed with guidance from the output of the attacked model, ensuring subtle and effective adversarial perturbations. We evaluate MAIA on multiple MIR tasks, demonstrating high attack success rates in both white-box and black-box settings while maintaining minimal perceptual distortion. Additionally, subjective listening tests confirm the high audio fidelity of the adversarial samples. Our findings highlight vulnerabilities in current MIR systems and emphasize the need for more robust and secure models.",
      "abstract": "Music adversarial attacks have garnered significant interest in the field of Music Information Retrieval (MIR). In this paper, we present Music Adversarial Inpainting Attack (MAIA), a novel adversarial attack framework that supports both white-box and black-box attack scenarios. MAIA begins with an importance analysis to identify critical audio segments, which are then targeted for modification. Utilizing generative inpainting models, these segments are reconstructed with guidance from the output of the attacked model, ensuring subtle and effective adversarial perturbations. We evaluate MAIA on multiple MIR tasks, demonstrating high attack success rates in both white-box and black-box settings while maintaining minimal perceptual distortion. Additionally, subjective listening tests confirm the high audio fidelity of the adversarial samples. Our findings highlight vulnerabilities in current MIR systems and emphasize the need for more robust and secure models.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Yuxuan Liu",
        "Peihong Zhang",
        "Rui Sang",
        "Zhixin Li",
        "Shengchen Li"
      ],
      "authors_and_affil": [
        "Yuxuan Liu (Xi'an Jiaotong-Liverpool University)*",
        "Peihong Zhang (Xi'an Jiaotong-Liverpool University)",
        "Rui Sang (Xi'an Jiaotong-Liverpool University)",
        "Zhixin Li (Xi'an Jiaotong-Liverpool University)",
        "Shengchen Li (Xi'an Jiaotong-Liverpool University)"
      ],
      "channel_url": "",
      "day": "4",
      "keywords": [
        "Ethical issues related to designing and implementing MIR tools and technologies",
        "Music signal processing",
        "MIR fundamentals and methodology",
        "Philosophical and ethical discussions"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "https://drive.google.com/file/d/1tgErXwPz9CDkqfHeh5Dlbj5aAk006Cb0/preview",
      "poster_pdf": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-9-maia-an-inpainting",
      "title": "MAIA: An Inpainting-Based Approach for Music Adversarial Attacks",
      "video": ""
    },
    "forum": "360",
    "id": "360",
    "pic_id": "",
    "position": "09",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Within music information retrieval (MIR) research, numerous beat\u2011tracking systems have been developed, targeting either audio recordings or symbolic representations such as MIDI files. However, the differences between these approaches, their respective strengths and weaknesses, and the potential for combining them have received limited attention. In this article, we compare two conceptually different beat trackers: an audio\u2011based model that operates frame by frame and a symbolic\u2011based model using an event\u2011driven approach. Specifically, we analyze the performance of two pretrained systems: the audio beat tracker madmom and the symbolic beat tracker Performance MIDI\u2011to\u2011Score (PM2S). Our evaluation is based on a cross\u2011modal dataset of Chopin\u2019s Mazurkas (Maz\u20115), which includes multiple audio recordings and MIDI representations automatically transcribed from audio. As a key contribution, we standardize the post\u2011processing pipelines for the frame\u2011based and event\u2011based beat trackers to ensure comparability and explore various late\u2011fusion methods within a unifying framework. Our results highlight the effectiveness of these fusion strategies in leveraging the strengths of both modalities while providing valuable insights into the performance of existing beat\u2011tracking models.",
      "abstract": "Within music information retrieval (MIR) research, numerous beat\u2011tracking systems have been developed, targeting either audio recordings or symbolic representations such as MIDI files. However, the differences between these approaches, their respective strengths and weaknesses, and the potential for combining them have received limited attention. In this article, we compare two conceptually different beat trackers: an audio\u2011based model that operates frame by frame and a symbolic\u2011based model using an event\u2011driven approach. Specifically, we analyze the performance of two pretrained systems: the audio beat tracker madmom and the symbolic beat tracker Performance MIDI\u2011to\u2011Score (PM2S). Our evaluation is based on a cross\u2011modal dataset of Chopin\u2019s Mazurkas (Maz\u20115), which includes multiple audio recordings and MIDI representations automatically transcribed from audio. As a key contribution, we standardize the post\u2011processing pipelines for the frame\u2011based and event\u2011based beat trackers to ensure comparability and explore various late\u2011fusion methods within a unifying framework. Our results highlight the effectiveness of these fusion strategies in leveraging the strengths of both modalities while providing valuable insights into the performance of existing beat\u2011tracking models.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Ching-Yu Chiu",
        "Lele Liu",
        "Christof Wei\u00df",
        "Meinard M\u00fcller"
      ],
      "authors_and_affil": [
        "Ching-Yu Chiu ( International Audio Laboratories Erlangen)*",
        "Lele Liu (University of W\u00fcrzburg)",
        "Christof Wei\u00df (University of W\u00fcrzburg)",
        "Meinard M\u00fcller (International Audio Laboratories Erlangen)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Symbolic music processing",
        "MIR fundamentals and methodology",
        "Music retrieval systems",
        "Applications",
        "Musical features and properties",
        "Music signal processing",
        "Rhythm, beat, tempo",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "",
      "poster_pdf": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-15-cross-modal-approaches",
      "title": "Cross-Modal Approaches to Beat Tracking: A Case Study on Chopin Mazurkas",
      "video": ""
    },
    "forum": "383",
    "id": "383",
    "pic_id": "",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "1"
  },
  {
    "content": {
      "TLDR": "Concert band and wind music are deeply embedded in society and play a significant role in the cultural landscape of many countries, including Germany and Austria, particularly within the amateur music scene. However, this type of music, as well as research on wind and brass instruments in general, remains largely overlooked in the field of music information retrieval (MIR). In this paper, we address this underexplored area by introducing ChoraleBricks, a framework featuring multitrack recordings of ten different chorales, each comprising four musical parts: soprano, alto, tenor, and bass. At its core, ChoraleBricks provides isolated recordings of individual parts performed by a diverse selection of wind instruments, including flute, oboe, clarinet, trumpet, saxophone, baritone horn, trombone, and tuba. These isolated recordings act as building blocks or \u201cbricks\u201d that can be modularly superimposed to create full mixes with varying instrumentation. In addition, ChoraleBricks provides sheet music, time\u2011aligned symbolic music representations, conducting videos, and reference annotations such as fundamental frequencies and note events. The framework is further enhanced by Python software tools that support parsing, mixing, annotation, and modular combination of the recorded audio material. With all multimedia and software components available as open\u2011source, ChoraleBricks provides a versatile framework for generating and augmenting datasets for polyphonic wind music. It supports systematic experimentation and facilitates evaluation across various research topics, including multi\u2011pitch estimation, note transcription, audio alignment, and music education applications.",
      "abstract": "Concert band and wind music are deeply embedded in society and play a significant role in the cultural landscape of many countries, including Germany and Austria, particularly within the amateur music scene. However, this type of music, as well as research on wind and brass instruments in general, remains largely overlooked in the field of music information retrieval (MIR). In this paper, we address this underexplored area by introducing ChoraleBricks, a framework featuring multitrack recordings of ten different chorales, each comprising four musical parts: soprano, alto, tenor, and bass. At its core, ChoraleBricks provides isolated recordings of individual parts performed by a diverse selection of wind instruments, including flute, oboe, clarinet, trumpet, saxophone, baritone horn, trombone, and tuba. These isolated recordings act as building blocks or \u201cbricks\u201d that can be modularly superimposed to create full mixes with varying instrumentation. In addition, ChoraleBricks provides sheet music, time\u2011aligned symbolic music representations, conducting videos, and reference annotations such as fundamental frequencies and note events. The framework is further enhanced by Python software tools that support parsing, mixing, annotation, and modular combination of the recorded audio material. With all multimedia and software components available as open\u2011source, ChoraleBricks provides a versatile framework for generating and augmenting datasets for polyphonic wind music. It supports systematic experimentation and facilitates evaluation across various research topics, including multi\u2011pitch estimation, note transcription, audio alignment, and music education applications.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Stefan Balke",
        "Axel Berndt",
        "Meinard M\u00fcller"
      ],
      "authors_and_affil": [
        "Stefan Balke (International Audio Laboratories Erlangen)*",
        "Axel Berndt (Paderborn University)",
        "Meinard M\u00fcller (International Audio Laboratories Erlangen)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Evaluation, datasets, and reproducibility",
        "Multimodality",
        "MIR fundamentals and methodology",
        "Novel datasets and use cases"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "",
      "poster_pdf": "",
      "session": [
        "1"
      ],
      "slack_channel": "p1-16-choralebricks-a-modular",
      "title": "ChoraleBricks: A Modular Multitrack Dataset for Wind Music Research",
      "video": ""
    },
    "forum": "379",
    "id": "379",
    "pic_id": "",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "1"
  },
  {
    "content": {
      "TLDR": "The Music Genome Project\u00ae is an extensive music annotation effort spanning two decades, during which a team of musicologists has been annotating a dataset of millions of songs with hundreds of musicological attributes. A derivative of this effort is presented in this paper. We are releasing MGPHot, a dataset of more than 21,000 songs that have appeared at least once in the Billboard Hot 100 charts from 1958 until 2022, annotated with 58 musical attributes that are grouped into seven different categories: rhythm, compositional focus, harmony, instrumentation, sonority, vocals, and lyrics. Given the unprecedented quality and breadth of annotation, as well as the size of the released corpus, the MGPHot dataset opens up a myriad of possibilities for musicology and music information retrieval, such as auto\u2011tagging, chart prediction, and music recommendation. To illustrate the breadth and depth of the dataset, we conduct a study on the evolution of popular music over the past 65 years, focusing on when and how changes occurred. While previous research has approached this topic through audio processing or historical methods, comprehensive musicological analyses at scale have been lacking, which this new dataset facilitates. In our study, we identify distinct eras and pivotal moments, reaffirming and broadening previous research on stylistic revolutions, and investigating the musical attributes that drive these changes. By analyzing the 58 musical attributes, we identify three major revolutions (1964, 1983, and 2016) and two minor ones (1991 and 2007), each propelled by specific musical attributes.",
      "abstract": "The Music Genome Project\u00ae is an extensive music annotation effort spanning two decades, during which a team of musicologists has been annotating a dataset of millions of songs with hundreds of musicological attributes. A derivative of this effort is presented in this paper. We are releasing MGPHot, a dataset of more than 21,000 songs that have appeared at least once in the Billboard Hot 100 charts from 1958 until 2022, annotated with 58 musical attributes that are grouped into seven different categories: rhythm, compositional focus, harmony, instrumentation, sonority, vocals, and lyrics. Given the unprecedented quality and breadth of annotation, as well as the size of the released corpus, the MGPHot dataset opens up a myriad of possibilities for musicology and music information retrieval, such as auto\u2011tagging, chart prediction, and music recommendation. To illustrate the breadth and depth of the dataset, we conduct a study on the evolution of popular music over the past 65 years, focusing on when and how changes occurred. While previous research has approached this topic through audio processing or historical methods, comprehensive musicological analyses at scale have been lacking, which this new dataset facilitates. In our study, we identify distinct eras and pivotal moments, reaffirming and broadening previous research on stylistic revolutions, and investigating the musical attributes that drive these changes. By analyzing the 58 musical attributes, we identify three major revolutions (1964, 1983, and 2016) and two minor ones (1991 and 2007), each propelled by specific musical attributes.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Sergio Oramas",
        "Fabien Gouyon",
        "Steve Hogan",
        "Camilo Landau",
        "Andreas Ehmann"
      ],
      "authors_and_affil": [
        "Sergio Oramas (Sirius XM/Pandora)*, Fabien Gouyon (Sirius XM/Pandora), Steve Hogan (Sirius XM/Pandora), Camilo Landau (Sirius XM/Pandora), Andreas Ehmann (Sirius XM/Pandora)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "Timbre, instrumentation, and singing voice",
        "Musical features and properties",
        "Novel datasets and use cases",
        "Computational musicology",
        "Annotation protocols",
        "Evaluation, datasets, and reproducibility",
        "Rhythm, beat, tempo",
        "Metadata, tags, linked data, and semantic web",
        "MIR fundamentals and methodology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "",
      "poster_pdf": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-15-mgphot-a-dataset",
      "title": "MGPHot: A Dataset of Musicological Annotations for Popular Music (1958\u20132022)",
      "video": ""
    },
    "forum": "386",
    "id": "386",
    "pic_id": "",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "2"
  },
  {
    "content": {
      "TLDR": "Hierarchical structures describing a syntax of harmony have long been studied and proposed by music theorists, but algorithms that model these structures either require costly expert annotations for training, or are based on music theorists\u2019 predispositions about harmonic syntax. We build upon a line of work that models harmonic sequences with probabilistic context-free grammars (PCFGs), inspired by the well-known formalism for syntax in human language. By using neural networks for parameter sharing when estimating PCFG rule probabilities, we learn the grammar in an entirely unsupervised manner. Our model induces a harmonic syntax purely from data, with minimal bias, and with parse trees as latent variables, while simply maximizing the likelihood of training sequences. This frees us from the need, for the first time, both for expert-annotated harmonic syntax trees and for human-defined grammar rules. We propose improvements inspired by music theory, including chord symbol representations and a training objective that incentives the inclusion of short and frequent chord progressions that are based on musical relations. Experiments show that our methods can model harmony in datasets of jazz pieces, often resulting in realistic parse trees that overlap with expert annotations, without access to these annotations during training at all. Code, models and predictions are publicly available.",
      "abstract": "Hierarchical structures describing a syntax of harmony have long been studied and proposed by music theorists, but algorithms that model these structures either require costly expert annotations for training, or are based on music theorists\u2019 predispositions about harmonic syntax. We build upon a line of work that models harmonic sequences with probabilistic context-free grammars (PCFGs), inspired by the well-known formalism for syntax in human language. By using neural networks for parameter sharing when estimating PCFG rule probabilities, we learn the grammar in an entirely unsupervised manner. Our model induces a harmonic syntax purely from data, with minimal bias, and with parse trees as latent variables, while simply maximizing the likelihood of training sequences. This frees us from the need, for the first time, both for expert-annotated harmonic syntax trees and for human-defined grammar rules. We propose improvements inspired by music theory, including chord symbol representations and a training objective that incentives the inclusion of short and frequent chord progressions that are based on musical relations. Experiments show that our methods can model harmony in datasets of jazz pieces, often resulting in realistic parse trees that overlap with expert annotations, without access to these annotations during training at all. Code, models and predictions are publicly available.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Ruben Cartuyvels",
        "John Koslovsky",
        "Marie-Francine Moens"
      ],
      "authors_and_affil": [
        "Ruben Cartuyvels (ESA)*",
        "John Koslovsky (KU Leuven)",
        "Marie-Francine Moens (KU Leuven)"
      ],
      "channel_url": "",
      "day": "1",
      "keywords": [
        "MIR fundamentals and methodology",
        "Computational musicology",
        "Systematic musicology",
        "Music signal processing",
        "Machine learning/artificial intelligence for music",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "",
      "poster_pdf": "",
      "session": [
        "2"
      ],
      "slack_channel": "p2-16-the-potential-of",
      "title": "The Potential of Unsupervised Induction of Harmonic Syntax for Jazz",
      "video": ""
    },
    "forum": "380",
    "id": "380",
    "pic_id": "",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "2"
  },
  {
    "content": {
      "TLDR": "This paper introduces the Beethoven Piano Sonata Dataset (BPSD), a multi-version dataset focusing on the first movements of Beethoven\u2019s 32 piano sonatas. Recognized as pivotal works in classical music, Beethoven\u2019s piano sonatas have profoundly shaped Western classical music, holding a significant place in cultural history. The BPSD includes sheet music in different machine-readable formats and audio recordings from 11 performances, with 4 of them being in the public domain and freely accessible for research purposes. A key feature of BPSD is its coherence, ensuring alignment of all versions on a unified musical timeline and enforcing consistent structures through careful editing of both score and audio representations. The focus and main motivation for the design choices made in BPSD are on the technical and computational level. In particular, BPSD facilitates the assessment of algorithmic approaches in tasks like harmony analysis, structure analysis, music transcription, beat and downbeat estimation, and score following. The dataset\u2019s coherence makes it an ideal platform for systematically training and evaluating deep learning methods, shedding light on their robustness and uncovering data biases across different data splits using cross-version strategies for evaluation. To ease applicability for computational approaches, the BPSD is based on various simplifications that may be disputable from a musicological perspective. Rather than providing novel musicological annotations, the main conceptual contribution of BPSD with its measure annotations is to provide a framework for transferring existing annotations from the symbolic to the audio domain. We hope that, as such, BPSD is also useful for the systematic analysis and exploration of Beethoven\u2019s piano sonatas, providing insights into their influence on the development of harmony and structure in Western classical music. Beyond research applications, the dataset also holds educational potential, aiding in the preparation and presentation of Beethoven\u2019s work to a broader audience through interactive multimedia experiences. This paper delivers a comprehensive overview of the BPSD, highlighting its potential for computational musicology and outlining future research directions.",
      "abstract": "This paper introduces the Beethoven Piano Sonata Dataset (BPSD), a multi-version dataset focusing on the first movements of Beethoven\u2019s 32 piano sonatas. Recognized as pivotal works in classical music, Beethoven\u2019s piano sonatas have profoundly shaped Western classical music, holding a significant place in cultural history. The BPSD includes sheet music in different machine-readable formats and audio recordings from 11 performances, with 4 of them being in the public domain and freely accessible for research purposes. A key feature of BPSD is its coherence, ensuring alignment of all versions on a unified musical timeline and enforcing consistent structures through careful editing of both score and audio representations. The focus and main motivation for the design choices made in BPSD are on the technical and computational level. In particular, BPSD facilitates the assessment of algorithmic approaches in tasks like harmony analysis, structure analysis, music transcription, beat and downbeat estimation, and score following. The dataset\u2019s coherence makes it an ideal platform for systematically training and evaluating deep learning methods, shedding light on their robustness and uncovering data biases across different data splits using cross-version strategies for evaluation. To ease applicability for computational approaches, the BPSD is based on various simplifications that may be disputable from a musicological perspective. Rather than providing novel musicological annotations, the main conceptual contribution of BPSD with its measure annotations is to provide a framework for transferring existing annotations from the symbolic to the audio domain. We hope that, as such, BPSD is also useful for the systematic analysis and exploration of Beethoven\u2019s piano sonatas, providing insights into their influence on the development of harmony and structure in Western classical music. Beyond research applications, the dataset also holds educational potential, aiding in the preparation and presentation of Beethoven\u2019s work to a broader audience through interactive multimedia experiences. This paper delivers a comprehensive overview of the BPSD, highlighting its potential for computational musicology and outlining future research directions.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Johannes Zeitler",
        "Christof Wei\u00df",
        "Vlora Arifi-M\u00fcller",
        "Meinard M\u00fcller"
      ],
      "authors_and_affil": [
        "Johannes Zeitler (International Audio Laboratories Erlangen)*",
        "Christof Wei\u00df (Julius-Maximilians-Universit\u00e4t W\u00fcrzburg)",
        "Vlora Arifi-M\u00fcller (International Audio Laboratories Erlangen)",
        "Meinard M\u00fcller (International Audio Laboratories Erlangen)"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Evaluation, datasets, and reproducibility",
        "Multimodality",
        "MIR fundamentals and methodology",
        "Novel datasets and use cases"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "",
      "poster_pdf": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-15-bpsd-a-coherent",
      "title": "BPSD: A Coherent Multi-Version Dataset for Analyzing the First Movements of Beethoven\u2019s Piano Sonatas",
      "video": ""
    },
    "forum": "382",
    "id": "382",
    "pic_id": "",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Open datasets with annotated corpora are crucial to foster research in music information retrieval (MIR) studies and to disseminate knowledge towards musicians and the general public. Many groups have published corpora, at times accompanied by specific visualization interfaces. However, achieving a cohesive access to such a diverse range of data poses a significant challenge. Dezrann is an open-source web platform to interact with corpora while sharing music and music analysis in the form of scores, images, audio files (waveforms), video files, and annotations. We present how we render through this platform ten curated corpora published in the last years in the MIR community, gathering 1500+ pieces and 35000+ annotations. These corpora include works by Bach, Mozart, Beethoven, and Schubert, lieder from the 19th century by female composers (OpenScore Lieder), jazz solo transcriptions (Weimar Jazz Database), piano rolls (SUPRA), Georgian sacred songs (Erkomaishvili dataset), and Slovenian folk song ballads. Showing these corpora with the cross-modal synchronization enabled by the platform improves the way to hear, study, and annotate music. This opens up new possibilities for corpus annotation and analysis in musicology and computer music research, enhances music education, fosters the promotion of music diversity, and facilitates collaborative score editing and correction.",
      "abstract": "Open datasets with annotated corpora are crucial to foster research in music information retrieval (MIR) studies and to disseminate knowledge towards musicians and the general public. Many groups have published corpora, at times accompanied by specific visualization interfaces. However, achieving a cohesive access to such a diverse range of data poses a significant challenge. Dezrann is an open-source web platform to interact with corpora while sharing music and music analysis in the form of scores, images, audio files (waveforms), video files, and annotations. We present how we render through this platform ten curated corpora published in the last years in the MIR community, gathering 1500+ pieces and 35000+ annotations. These corpora include works by Bach, Mozart, Beethoven, and Schubert, lieder from the 19th century by female composers (OpenScore Lieder), jazz solo transcriptions (Weimar Jazz Database), piano rolls (SUPRA), Georgian sacred songs (Erkomaishvili dataset), and Slovenian folk song ballads. Showing these corpora with the cross-modal synchronization enabled by the platform improves the way to hear, study, and annotate music. This opens up new possibilities for corpus annotation and analysis in musicology and computer music research, enhances music education, fosters the promotion of music diversity, and facilitates collaborative score editing and correction.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Charles Ballester",
        "Baptiste Bacot",
        "Vanessa Nina Borsan",
        "Louis Couturier",
        "Ken D\u00e9guernel",
        "Quentin Dinel",
        "Laurent Feisthauer",
        "Klaus Frieler",
        "Mark Gotham",
        "Richard Groult",
        "Johannes Hentschel",
        "Alexandre d'Hooge",
        "Dinh-Viet-Toan Le",
        "Florence Lev\u00e9",
        "Francesco Maccarini",
        "Ivana Mari\u010di\u0107",
        "Gianluca Micchi",
        "Meinard M\u00fcller",
        "Alexandros Stamatiadis",
        "Tom Taffin",
        "Patrice Thibaud",
        "Christoph Wei\u00df",
        "Rui Yang",
        "Emmanuel Leguy",
        "Mathieu Giraud"
      ],
      "authors_and_affil": [
        "Charles Ballester (Univ. Lille)",
        "Baptiste Bacot (Univ. Lille)",
        "Vanessa Nina Borsan (Univ. Lille)",
        "Louis Couturier (Universit\u00e9 de Picardie Jules-Verne)",
        "Ken D\u00e9guernel (Univ. Lille)",
        "Quentin Dinel (Univ. Lille)",
        "Laurent Feisthauer (Univ. Lille)",
        "Klaus Frieler (Max Planck Institute for Empirical Aesthetics)",
        "Mark Gotham (King\u2019s College London)",
        "Richard Groult (Univ. Rouen Normandie)",
        "Johannes Hentschel (\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne)",
        "Alexandre d'Hooge (Univ. Lille)",
        "Dinh-Viet-Toan Le (Univ. Lille)",
        "Florence Lev\u00e9 (Universit\u00e9 de Picardie Jules-Verne)",
        "Francesco Maccarini (Univ. Lille)",
        "Ivana Mari\u010di\u0107 (ZRC SAZU)",
        "Gianluca Micchi (Universal Music Group)",
        "Meinard M\u00fcller (International Audio Labs Erlangen)",
        "Alexandros Stamatiadis (Universit\u00e9 de Picardie Jules Verne)",
        "Tom Taffin (Univ. Lille)",
        "Patrice Thibaud (Univ. Lille)",
        "Christoph Wei\u00df (Universit\u00e4t W\u00fczrburg)",
        "Rui Yang (Univ. Lille)",
        "Emmanuel Leguy (Univ. Lille)",
        "Mathieu Giraud (CNRS, Universit\u00e9 de Lille)*"
      ],
      "channel_url": "",
      "day": "2",
      "keywords": [
        "Human-computer interaction",
        "Applications",
        "Multimodality",
        "Computational musicology",
        "Music videos, multimodal music systems",
        "Reproducibility",
        "Digital libraries and archives",
        "Evaluation, datasets, and reproducibility",
        "MIR fundamentals and methodology",
        "Human-centered MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "",
      "poster_pdf": "",
      "session": [
        "3"
      ],
      "slack_channel": "p3-16-interacting-with-annotated",
      "title": "Interacting with Annotated and Synchronized Music Corpora on the Dezrann Web Platform",
      "video": ""
    },
    "forum": "381",
    "id": "381",
    "pic_id": "",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "3"
  },
  {
    "content": {
      "TLDR": "Prior studies on hit song prediction have predominantly focused on forecasting a song\u2019s success in music charts, neglecting the examination of song contests such as Eurovision. This paper presents a framework for predicting Eurovision result rankings, with a particular focus on the semi-finals, which determine qualification for the grand final, and the rankings of the grand final. By integrating intrinsic song characteristics, public appeal, and contest-specific data, the study evaluates seven feature sets across multiple years of Eurovision, spanning from 2008 to 2024. The inclusion of features such as audio and lyrics attributes, YouTube daily views, the previous year\u2019s vote ratio and vote reciprocation, and performance order provides a multi-modal approach to understanding song success in the contest. Key findings indicate that the intrinsic song features employed in this study alone do not accurately predict rankings, as they account for only a minimal portion of the variance in contest results. While public appeal, represented by YouTube daily views, emerged as a significant factor, it may be influenced by post-contest exposure bias. The most effective prediction model combined intrinsic song characteristics, public appeal, and contest-specific data, yielding the most consistent results across semi-finals and grand finals over multiple years.",
      "abstract": "Prior studies on hit song prediction have predominantly focused on forecasting a song\u2019s success in music charts, neglecting the examination of song contests such as Eurovision. This paper presents a framework for predicting Eurovision result rankings, with a particular focus on the semi-finals, which determine qualification for the grand final, and the rankings of the grand final. By integrating intrinsic song characteristics, public appeal, and contest-specific data, the study evaluates seven feature sets across multiple years of Eurovision, spanning from 2008 to 2024. The inclusion of features such as audio and lyrics attributes, YouTube daily views, the previous year\u2019s vote ratio and vote reciprocation, and performance order provides a multi-modal approach to understanding song success in the contest. Key findings indicate that the intrinsic song features employed in this study alone do not accurately predict rankings, as they account for only a minimal portion of the variance in contest results. While public appeal, represented by YouTube daily views, emerged as a significant factor, it may be influenced by post-contest exposure bias. The most effective prediction model combined intrinsic song characteristics, public appeal, and contest-specific data, yielding the most consistent results across semi-finals and grand finals over multiple years.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Katarzyna Adamska",
        "Joshua Reiss"
      ],
      "authors_and_affil": [
        "Katarzyna Adamska (Queen Mary University of London)*",
        "Joshua Reiss (Queen Mary University of London)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Musical features and properties",
        "Web mining, and natural language processing",
        "Music signal processing",
        "Rhythm, beat, tempo",
        "Metadata, tags, linked data, and semantic web",
        "Lyrics and other textual data",
        "MIR fundamentals and methodology"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "",
      "poster_pdf": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-15-predicting-eurovision-song",
      "title": "Predicting Eurovision Song Contest Results: A Hit Song Science Approach",
      "video": ""
    },
    "forum": "384",
    "id": "384",
    "pic_id": "",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "5"
  },
  {
    "content": {
      "TLDR": "The Musical Instrument Digital Interface (MIDI), introduced in 1983, revolutionized music production by allowing computers and instruments to communicate efficiently. MIDI files encode musical instructions compactly, facilitating convenient music sharing. They benefit music information retrieval (MIR), aiding in research on music understanding, computational musicology, and generative music. The GigaMIDI dataset contains over 1.4 million unique MIDI files, encompassing 1.8 billion MIDI note events and over 5.3 million MIDI tracks. GigaMIDI is currently the largest collection of symbolic music in MIDI format available for research purposes under fair dealing. Distinguishing between non\u2011expressive and expressive MIDI tracks is challenging, as MIDI files do not inherently make this distinction. To address this issue, we introduce a set of innovative heuristics for detecting expressive music performance. These include the distinctive note velocity ratio (DNVR) heuristic, which analyzes MIDI note velocity; the distinctive note onset deviation ratio (DNODR) heuristic, which examines deviations in note onset times; and the note onset median metric level (NOMML) heuristic, which evaluates onset positions relative to metric levels. Our evaluation demonstrates these heuristics effectively differentiate between non\u2011expressive and expressive MIDI tracks. Furthermore, after evaluation, we create the most substantial expressive MIDI dataset, employing our heuristic NOMML. This curated iteration of GigaMIDI encompasses expressively performed instrument tracks detected by NOMML, containing all General MIDI instruments, constituting 31% of the GigaMIDI dataset, totaling 1,655,649 tracks.",
      "abstract": "The Musical Instrument Digital Interface (MIDI), introduced in 1983, revolutionized music production by allowing computers and instruments to communicate efficiently. MIDI files encode musical instructions compactly, facilitating convenient music sharing. They benefit music information retrieval (MIR), aiding in research on music understanding, computational musicology, and generative music. The GigaMIDI dataset contains over 1.4 million unique MIDI files, encompassing 1.8 billion MIDI note events and over 5.3 million MIDI tracks. GigaMIDI is currently the largest collection of symbolic music in MIDI format available for research purposes under fair dealing. Distinguishing between non\u2011expressive and expressive MIDI tracks is challenging, as MIDI files do not inherently make this distinction. To address this issue, we introduce a set of innovative heuristics for detecting expressive music performance. These include the distinctive note velocity ratio (DNVR) heuristic, which analyzes MIDI note velocity; the distinctive note onset deviation ratio (DNODR) heuristic, which examines deviations in note onset times; and the note onset median metric level (NOMML) heuristic, which evaluates onset positions relative to metric levels. Our evaluation demonstrates these heuristics effectively differentiate between non\u2011expressive and expressive MIDI tracks. Furthermore, after evaluation, we create the most substantial expressive MIDI dataset, employing our heuristic NOMML. This curated iteration of GigaMIDI encompasses expressively performed instrument tracks detected by NOMML, containing all General MIDI instruments, constituting 31% of the GigaMIDI dataset, totaling 1,655,649 tracks.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Keon Ju Lee",
        "Jeff Ens",
        "Sara Adkins",
        "Pedro Sarmento",
        "Mathieu Barthet",
        "Philippe Pasquier"
      ],
      "authors_and_affil": [
        "Keon Ju Lee (Simon Fraser University)*",
        "Jeff Ens (Simon Fraser University)",
        "Sara Adkins (Independent Researcher)",
        "Pedro Sarmento (Queen Mary University of London)",
        "Mathieu Barthet (Queen Mary University of London)",
        "Philippe Pasquier (Simon Fraser University)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Evaluation, datasets, and reproducibility",
        "Novel datasets and use cases",
        "Expression and performative aspects of music",
        "Musical features and properties"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "Virtually",
      "pdf_path": "",
      "poster_pdf": "",
      "session": [
        "5"
      ],
      "slack_channel": "p5-16-the-gigamidi-dataset",
      "title": "The GigaMIDI Dataset with Features for Expressive Music Performance Detection",
      "video": ""
    },
    "forum": "424",
    "id": "424",
    "pic_id": "",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "5"
  },
  {
    "content": {
      "TLDR": "Music Question--Answering (MQA) is a machine learning task where a computational system analyzes and answers questions about music-related data. Traditional methods prioritize audio, overlooking visual and embodied aspects crucial to music performance understanding. We introduce MusiQAl, a multimodal dataset of 310 music performance videos and 11,793 human-annotated question--answer pairs, spanning diverse musical traditions and styles. Grounded in musicology and music psychology, MusiQAl emphasizes multimodal reasoning, causal inference, and cross-cultural understanding of performer-music interactions. We benchmark AVST and LAVISH on MusiQAl, revealing strengths and limitations, underscoring the importance of integrating multimodal learning and domain expertise to advance MQA and Music Information Retrieval (MIR).",
      "abstract": "Music Question--Answering (MQA) is a machine learning task where a computational system analyzes and answers questions about music-related data. Traditional methods prioritize audio, overlooking visual and embodied aspects crucial to music performance understanding. We introduce MusiQAl, a multimodal dataset of 310 music performance videos and 11,793 human-annotated question--answer pairs, spanning diverse musical traditions and styles. Grounded in musicology and music psychology, MusiQAl emphasizes multimodal reasoning, causal inference, and cross-cultural understanding of performer-music interactions. We benchmark AVST and LAVISH on MusiQAl, revealing strengths and limitations, underscoring the importance of integrating multimodal learning and domain expertise to advance MQA and Music Information Retrieval (MIR).<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Anna-Maria Christodoulou",
        "Kyrre Glette",
        "Olivier Sergei Lartillot",
        "Alexander Refsum Jensenius"
      ],
      "authors_and_affil": [
        "Anna-Maria Christodoulou (University of Oslo)*",
        "Kyrre Glette (RITMO, UiO)",
        "Olivier Sergei Lartillot (RITMO, UiO)",
        "Alexander Refsum Jensenius (RITMO, UiO)"
      ],
      "channel_url": "",
      "day": "3",
      "keywords": [
        "Applications",
        "Multimodality",
        "Novel datasets and use cases",
        "Music videos, multimodal music systems",
        "Digital libraries and archives",
        "Evaluation, datasets, and reproducibility",
        "Machine learning/artificial intelligence for music",
        "MIR fundamentals and methodology",
        "Knowledge-driven approaches to MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "",
      "poster_pdf": "",
      "session": [
        "6"
      ],
      "slack_channel": "p6-15-musiqal-music-question",
      "title": "MusiQAl: Music Question Answering through Audio-Video fusion",
      "video": ""
    },
    "forum": "385",
    "id": "385",
    "pic_id": "",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "6"
  },
  {
    "content": {
      "TLDR": "Collaborative playlists (CPs) enable users of streaming platforms to share and discover music through co-curation. Recent studies involving predominantly North American samples have found that CPs are created for a variety of contexts, help users organize and access music, facilitate music discovery, and support social connections. Yet, despite these important benefits, little is known about how CP usage aligns or varies across different cultures. We conducted an exploratory study to better understand the landscape of collaborative music engagement with a focus on Hong Kong, South Korea, Quebec, and the United States. We found that across these cultures, previously established purposes for engaging in CPs apply, yet with different degrees of emphasis. Perceived and expected CP outcomes and broader perspectives on social connection through music also varied by location and CP user type. With these findings we discuss primary similarities and differences across the studied cultures and highlight directions for future investigations to further elucidate how music platforms with CP functionalities\u2014and social capabilities more generally\u2014can better help users achieve their desired goals around music.",
      "abstract": "Collaborative playlists (CPs) enable users of streaming platforms to share and discover music through co-curation. Recent studies involving predominantly North American samples have found that CPs are created for a variety of contexts, help users organize and access music, facilitate music discovery, and support social connections. Yet, despite these important benefits, little is known about how CP usage aligns or varies across different cultures. We conducted an exploratory study to better understand the landscape of collaborative music engagement with a focus on Hong Kong, South Korea, Quebec, and the United States. We found that across these cultures, previously established purposes for engaging in CPs apply, yet with different degrees of emphasis. Perceived and expected CP outcomes and broader perspectives on social connection through music also varied by location and CP user type. With these findings we discuss primary similarities and differences across the studied cultures and highlight directions for future investigations to further elucidate how music platforms with CP functionalities\u2014and social capabilities more generally\u2014can better help users achieve their desired goals around music.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "So Yeon Park",
        "Jin Ha Lee",
        "Audrey Laplante",
        "Xiao Hu",
        "Blair Kaneshiro"
      ],
      "authors_and_affil": [
        "So Yeon Park (Stanford University)",
        "Jin Ha Lee (University of Washington)*",
        "Audrey Laplante (Universit\u00e9 de Montr\u00e9al)",
        "Xiao Hu (University of Arizona)",
        "Blair Kaneshiro (Stanford University)"
      ],
      "channel_url": "",
      "day": "4",
      "keywords": [
        "User behavior analysis and mining, user modeling",
        "Human-computer interaction",
        "Human-centered MIR"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "",
      "poster_pdf": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-15-collaborative-playlists-around",
      "title": "Collaborative Playlists around the World: A Cross-Cultural User Study",
      "video": ""
    },
    "forum": "391",
    "id": "391",
    "pic_id": "",
    "position": "15",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "7"
  },
  {
    "content": {
      "TLDR": "Several companies now offer platforms for users to create music at unprecedented scales by textual prompting. As the quality of this music rises, concern grows about how to differentiate AI\u2011generated music from human\u2011made music, with implications for content identification, copyright enforcement, and music recommendation systems. This article explores the detection of AI\u2011generated music by assembling and studying a large dataset of music audio recordings (30,000 full tracks totaling 1,770 h, 33 m, and 31 s in duration), of which 10,000 are from the Million Song Dataset (Bertin\u2011Mahieux et al., 2011) and 20,000 are generated and released by users of two popular AI music platforms: Suno and Udio. We build and evaluate several AI music detectors operating on Contrastive Language\u2013Audio Pretraining embeddings of the music audio, then compare them to a commercial baseline system as well as an open\u2011source one. We applied various audio transformations to see their impacts on detector performance and found that the commercial baseline system is easily fooled by simply resampling audio to 22.05 kHz. We argue that careful consideration needs to be given to the experimental design underlying work in this area, as well as the very definition of \u2018AI music.\u2019 We release all our code at https://github.com/lcrosvila/ai-music-detection.",
      "abstract": "Several companies now offer platforms for users to create music at unprecedented scales by textual prompting. As the quality of this music rises, concern grows about how to differentiate AI\u2011generated music from human\u2011made music, with implications for content identification, copyright enforcement, and music recommendation systems. This article explores the detection of AI\u2011generated music by assembling and studying a large dataset of music audio recordings (30,000 full tracks totaling 1,770 h, 33 m, and 31 s in duration), of which 10,000 are from the Million Song Dataset (Bertin\u2011Mahieux et al., 2011) and 20,000 are generated and released by users of two popular AI music platforms: Suno and Udio. We build and evaluate several AI music detectors operating on Contrastive Language\u2013Audio Pretraining embeddings of the music audio, then compare them to a commercial baseline system as well as an open\u2011source one. We applied various audio transformations to see their impacts on detector performance and found that the commercial baseline system is easily fooled by simply resampling audio to 22.05 kHz. We argue that careful consideration needs to be given to the experimental design underlying work in this area, as well as the very definition of \u2018AI music.\u2019 We release all our code at https://github.com/lcrosvila/ai-music-detection.<br><br> <b><p align=\"center\">[Direct link to video]()</b>",
      "authors": [
        "Laura Cros Vila",
        "Bob Sturm",
        "Luca Casini",
        "David Dalmazzo"
      ],
      "authors_and_affil": [
        "Laura Cros Vila (KTH)*",
        "Bob Sturm (KTH Royal Institute of Technology)",
        "Luca Casini (KTH Royal Institute of Technology)",
        "David Dalmazzo (KTH Royal Institute of Technology)"
      ],
      "channel_url": "",
      "day": "4",
      "keywords": [
        "Philosophical and ethical discussions",
        "Computational musicology",
        "Evaluation, datasets, and reproducibility",
        "Musical features and properties"
      ],
      "long_presentation": "FALSE",
      "paper_presentation": "In-person, in Daejeon",
      "pdf_path": "",
      "poster_pdf": "",
      "session": [
        "7"
      ],
      "slack_channel": "p7-16-the-ai-music",
      "title": "The AI Music Arms Race: On the Detection of AI-Generated Music",
      "video": ""
    },
    "forum": "410",
    "id": "410",
    "pic_id": "",
    "position": "16",
    "poster_pdf": "GLTR_poster.pdf",
    "reviews": {
      "meta_review": "",
      "publish_reviews": "FALSE",
      "review1": "",
      "review2": "",
      "review3": ""
    },
    "session": "7"
  }
]
