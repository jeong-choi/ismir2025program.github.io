


<!DOCTYPE html>
<html lang="en">
  <head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <link rel="icon" type="image/png" sizes="32x32" href="static/images/ismir_tab_icon.png">

    <link rel="stylesheet" href="static/css/main.css" />
    <link rel="stylesheet" href="static/css/custom.css" />
    <link rel="stylesheet" href="static/css/lazy_load.css" />
    <link rel="stylesheet" href="static/css/typeahead.css" />
    <link rel="stylesheet" href="static/css/poster.css" />
    <link rel="stylesheet" href="static/css/music.css" />
    <link rel="stylesheet" href="static/css/piece.css" />


    <!-- <link rel="stylesheet" href="https://gitcdn.xyz/repo/wingkwong/jquery-chameleon/master/src/chameleon.min.css"> -->

    <!-- External Javascript libs  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

    <script
      src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
      integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
      crossorigin="anonymous"
    ></script>


    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>
    <!-- <script src="static/js/chameleon.js"></script> -->


    <!-- Library libs -->
    <script src="static/js/typeahead.bundle.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
      href="static/css/Lato.css"
      rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet" />
    <link
      href="static/css/Cuprum.css"
      rel="stylesheet"
    />
    <link
      href="static/css/Ambiant.css"
      rel="stylesheet"
    />
    <!-- <link href="static/css/chameleon.css" rel="stylesheet"/> -->
    <title>ISMIR 2025: Quantize &amp; Factorize: A fast yet effective unsupervised audio representation without deep learning</title>
    
<meta name="citation_title" content="Quantize &amp; Factorize: A fast yet effective unsupervised audio representation without deep learning" />

<meta name="citation_author" content="Jaehun Kim" />

<meta name="citation_author" content="Matthew C. McCallum" />

<meta name="citation_author" content="Andreas F. Ehmann" />

<meta name="citation_publication_date" content="21-25 September 2025" />
<meta name="citation_conference_title" content="Ismir 2025 Hybrid Conference" />
<meta name="citation_inbook_title" content="Proceedings of the First MiniCon Conference" />
<meta name="citation_abstract" content="Foundation models have become increasingly prevalent in tackling Music Information Retrieval (MIR) tasks. Although they can be a powerful tool for understanding music, the computation required for the training and inference of these models continues to grow as they become more complex. Specialized acceleration, such as Graphical Processing Units (GPUs), has become necessary for operating these models, as they are mostly based on large Deep Learning (DL) architectures. Furthermore, it is difficult for users to interpret them due to their black-box nature. In this work, we propose Quantizers and Factorizers for Music embeddings (QFM), a fast, unsupervised audio representation for music understanding backed by a wide range of rich MIR features and efficient feature learners. Experimental results show that QFM models perform within the range of results achieved by recent previous open source DL models on all evaluated tasks, with competitive results on a subset. This is surprising given the significantly smaller computational requirements of QFM models for training and inference.&lt;br&gt;&lt;br&gt; &lt;b&gt;&lt;p align=&#34;center&#34;&gt;[Direct link to video]()&lt;/b&gt;" />

<meta name="citation_keywords" content="Musical features and properties" />

<meta name="citation_keywords" content="Representations of music" />

<meta name="citation_keywords" content="Automatic classification" />

<meta name="citation_keywords" content="Music signal processing" />

<meta name="citation_keywords" content="MIR tasks" />

<meta name="citation_keywords" content="Machine learning/artificial intelligence for music" />

<meta name="citation_keywords" content="MIR fundamentals and methodology" />

<meta name="citation_keywords" content="Knowledge-driven approaches to MIR" />

<meta name="citation_pdf_url" content="" />
<meta id="yt-id" data-name= />
<meta id="bb-id" data-name= />


  </head>

  <body>
    <!-- NAV -->
    
    <!--

    ('tutorials.html', 'Tutorials'),
    ('index.html, 'Home'),
    ('special_meetings.html', 'Special Meetings'),
    -->

    <nav
      class="navbar sticky-top navbar-expand-lg navbar-light mr-auto"
      id="main-nav"
    >
      <div class="container">
        <a class="navbar-brand" href="https://ismir2025.ismir.net">
          <img
             class="logo" src="static/images/ismir_tabicon.png"
             height="auto"
             width="300px"
          />
        </a>
        
        <button
          class="navbar-toggler"
          type="button"
          data-toggle="collapse"
          data-target="#navbarNav"
          aria-controls="navbarNav"
          aria-expanded="false"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div
          class="collapse navbar-collapse text-right flex-grow-1"
          id="navbarNav"
        >
          <ul class="navbar-nav ml-auto">
            
            <li class="nav-item ">
              <a class="nav-link" href="calendar.html">Schedule</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="papers.html">Papers</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="music.html">Music</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="lbds.html">LBDs</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="industry.html?session=Platinum">Industry</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="jobs.html">Jobs</a>
            </li>
            
          </ul>
        </div>
      </div>
    </nav>
    

    
    <!-- User Overrides -->
     

    <div class="container">
      <!-- Tabs -->
      <div class="tabs">
         
      </div>
      <!-- Content -->
      <div class="content">
        

<!-- Title -->
<div class="pp-card m-3" style="">
  <div class="card-header">
    <h2 class="card-title main-title text-center" style="">
      P7-07: Quantize &amp; Factorize: A fast yet effective unsupervised audio representation without deep learning
    </h2>
    <h3 class="card-subtitle mb-2 text-muted text-center">
      
      <a href="papers.html?filter=authors&search=Jaehun Kim" class="text-muted"
        >Jaehun Kim</a
      >,
      
      <a href="papers.html?filter=authors&search=Matthew C. McCallum" class="text-muted"
        >Matthew C. McCallum</a
      >,
      
      <a href="papers.html?filter=authors&search=Andreas F. Ehmann" class="text-muted"
        >Andreas F. Ehmann</a
      >
      
    </h3>
    <p class="card-text text-center">
      <span class="">Subjects:</span>
      
      <a
        href="papers.html?filter=keywords&search=Musical features and properties"
        class="text-secondary text-decoration-none"
        >Musical features and properties</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Representations of music"
        class="text-secondary text-decoration-none"
        >Representations of music</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Automatic classification"
        class="text-secondary text-decoration-none"
        >Automatic classification</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Music signal processing"
        class="text-secondary text-decoration-none"
        >Music signal processing</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=MIR tasks"
        class="text-secondary text-decoration-none"
        >MIR tasks</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Machine learning/artificial intelligence for music"
        class="text-secondary text-decoration-none"
        >Machine learning/artificial intelligence for music</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=MIR fundamentals and methodology"
        class="text-secondary text-decoration-none"
        >MIR fundamentals and methodology</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Knowledge-driven approaches to MIR"
        class="text-secondary text-decoration-none"
        >Knowledge-driven approaches to MIR</a
      > 
      
    </p>
    <h4 class="text-muted text-center">
      Presented In-person, in Daejeon
    </h4>
    <h4 class="text-muted text-center">
      
        4-minute short-format presentation
      
    </h4>

  </div>
</div>
<div style="display: flex; justify-content: center;" class="btn-toolbar mt-4" role="toolbar">
  <div class="poster-buttons btn-group btn-group-toggle mb-3" data-toggle="buttons">
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#details">
      Abstract
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#paper">
      Paper
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#poster">
      Poster
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#video">
      Video
    </button>
    
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#meta-review">
        Meta
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review1">
        R1
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review2">
        R2
      </button>
      
        <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review3">
          R3
        </button>
      
    
    <!--  -->
  </div>
  
</div>
<div class="poster-content">
  <div id="details" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="abstractExample">
          <span class="font-weight-bold">Abstract:</span>
          <p>Foundation models have become increasingly prevalent in tackling Music Information Retrieval (MIR) tasks. Although they can be a powerful tool for understanding music, the computation required for the training and inference of these models continues to grow as they become more complex. Specialized acceleration, such as Graphical Processing Units (GPUs), has become necessary for operating these models, as they are mostly based on large Deep Learning (DL) architectures. Furthermore, it is difficult for users to interpret them due to their black-box nature. In this work, we propose Quantizers and Factorizers for Music embeddings (QFM), a fast, unsupervised audio representation for music understanding backed by a wide range of rich MIR features and efficient feature learners. Experimental results show that QFM models perform within the range of results achieved by recent previous open source DL models on all evaluated tasks, with competitive results on a subset. This is surprising given the significantly smaller computational requirements of QFM models for training and inference.<br><br> <b><p align="center"><a href="">Direct link to video</a></b></p>
        </div>
      </div>
      <p></p>
    </div>
  </div>
  <div id="paper" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "https://drive.google.com/file/d/14hmzfUcf0KvWEo1TC1QNBwFjwrRO5qap/preview#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="poster" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="video" class="collapse">
    
      <div  style="display: flex; justify-content: center;">
      <iframe src="" width="960" height="540" allow="encrypted-media" allowfullscreen></iframe>
      </div>
    
  </div>
  
  <div id="meta-review" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="metaReviewExample">
          <span class="font-weight-bold">Meta Review:</span>
          <br>
          <p><strong>Q2 ( I am an expert on the topic of the paper.)</strong></p>
<p>Agree</p>
<p><strong>Q3 ( The title and abstract reflect the content of the paper.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q4 (The paper discusses, cites and compares with all relevant related work.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by “n” pages of references or ethical considerations, references are well formatted). If you selected “No”, please explain the issue in your comments.)</strong></p>
<p>No</p>
<p><strong>Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q9 (Scholarly/scientific quality: The content is scientifically correct.)</strong></p>
<p>Agree</p>
<p><strong>Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view "novelty" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)</strong></p>
<p>Agree</p>
<p><strong>Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)</strong></p>
<p>Agree</p>
<p><strong>Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated “Strongly Agree” and “Agree” can be highlighted, but please do not penalize papers rated “Disagree” or “Strongly Disagree”. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)</strong></p>
<p>Disagree (Standard topic, task, or application)</p>
<p><strong>Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)</strong></p>
<p>Agree</p>
<p><strong>Q15 (Please explain your assessment of reusable insights in the paper.)</strong></p>
<p>By showing that multiple downstream tasks can work well on a set of selected MIR features, the paper gives insights into which features could be selected for MIR tasks. Also it can raise ideas on how to make existing DL-based representation learning methods more efficient</p>
<p><strong>Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)</strong></p>
<p>A music representation based on MIR features (instead of deep learning) can provide competitive downstream task performance, while offering faster training and inference.</p>
<p><strong>Q17 (This paper is of award-winning quality.)</strong></p>
<p>No</p>
<p><strong>Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)</strong></p>
<p>Agree</p>
<p><strong>Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)</strong></p>
<p>Weak accept</p>
<p><strong>Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)</strong></p>
<p>This paper proposes an unsupervised music representation for which training and inference is faster than current DL-based methods, while offering similar performance. It also claims to provide a generic architecture in which feature engineering and ML can be used side-by-side in future MIR work.</p>
<p>The paper is well written, the proposed system is sensible and reasonably novel (parts of the system existed before, but it was not used in this context to my knowledge) and the experiments are conducted rigorously, proving the paper's claims described above to a large extent. The efficiency claims would be better supported if there was an analysis of training and inference cost (in real-world currency, or FLOPS). Also, GPU acceleration is not used, and the benefit of the proposed system depends on how costly GPU usage is compared to the author's CPU setup - as GPU acceleration would substantially reduce training and inference time (and perhaps cost) for the approaches the paper is using for comparison, but likely not for the proposed system.</p>
<p>As a more minor point, the paper also suggests non-DL representations like the proposed one could be more interpretable, but unfortunately does not include any analysis on this.</p>
<p>Minor remarks:
- Some links in the references are broken, sometimes entries are incomplete (e.g. missing authors)
- Figure 1 could be interpreted as factorization being applied for each audio feature in each chunk independently, but in Section 3.2 WMF is applied to the whole dataset, this should be clarified more
- L33 “handful of works” implies citing more than one paper
- L164 - I assume the audio features’ mean and standard deviation, not the audio chunk waveform itself?</p>
<p><strong>Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid “weak accepts” or “weak rejects” if possible.)</strong></p>
<p>Weak accept</p>
<p><strong>Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))</strong></p>
<p>Summary of Reviews and Discussion</p>
<p>This paper proposes an unsupervised music representation learning pipeline that combines classical MIR features with quantization and matrix factorization, offering a computationally efficient alternative to deep learning-based models. Reviewers generally found the approach well-motivated, the writing clear, and the empirical results compelling, particularly given the simplicity of the method.</p>
<p>However, all reviewers noted a lack of methodological detail, particularly in the description of the Quantization-Factorization module. Clarifications on hyperparameters, model architecture, and mathematical formulation are needed, and several figures need to be clarified.</p>
<p>All reviews provided a "weak accept" recommendation.</p>
<p>Final Recommendation</p>
<p>I recommend accepting this paper to ISMIR. Despite the noted lack of clarity, the paper offers a timely and thoughtful contribution to the ISMIR community. Its practical relevance, solid experimental results, and potential to broaden the conversation around music representation learning justify its inclusion. The authors are encouraged to enhance methodological clarity in the camera-ready version.</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review1" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review1Example">
          <span class="font-weight-bold">Review 1:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Weak accept</li>
</ul>
<h4>Main Reviews</h4>
<p>This paper aims to improve music foundation models without the usage of deep neural network, and shows improvements over some of the deep learning based method like CLMR. FM without deep learning is a highly underresearched topic. Even though the results are not STOA, it is still very impressive and provides helpful insights to the community.</p>
<p>The model choice (quantization+factorization) is also reasonable since it retains compact sequential information. Although I do wish the author could explain more the design choice. See comment #5.</p>
<p>My main concern is about the description of the method. The author skips all details on the QF model, leaving only citations, making this paper very hard to understand even with traditional machine learning background. The paper definitely can describe all methods in detail potentially with formula to reduce confusion (currently there is none), and shrink the length of the experiments (Fig. 2 &amp; 3 definitely occupied unnecessarily much space).</p>
<p>Other comments:</p>
<ol>
<li>
<p>Fig. 1: What is a ZScore in Fig. 1? Also in Fig. 1 KMeans should be "KMeans/GMM." Also, the node QF_total should connect to all feature_{1...N} instead of each QF local block.</p>
</li>
<li>
<p>In sec 3.2: there are lots of important hyperparameters in the NGram and WMF model, but are never described in the paper.</p>
</li>
<li>
<p>Line 164: The description of G1: It calculates "...each audio chunk’s mean and standard deviation." I assume the author wants to say the feature of each audio chunk, instead of the raw audio content.</p>
</li>
<li>
<p>How did you train the model? What are the training hyperparameters? I assume that the KMeans/GMM, NGram and WMF all require training and they are trained sequentially using different algorithms. More clarification is needed.</p>
</li>
<li>
<p>It would be better if the author explained the model choices in their introduction: (1) Why quantization? (2) Why use NGram+WMF?</p>
</li>
</ol>
<p>Overall, I think the paper is a clear accept seeing the result, but more refinement is required.</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review2" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review2Example">
          <span class="font-weight-bold">Review 2:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Weak accept</li>
</ul>
<h4>Main Reviews</h4>
<p>This paper presents a method to leverage shallow feature learners (read classical MIR features) to compete with large foundation models. The authors present a Quantization-Factorization module which operates on groups of feature sets to produce "embeddings" for music in an unsupervised manner. The resulting embeddings can be used across several downstream MIR tasks achieving comparable (though slightly worse) metrics when compared to significantly larger and computationally expensive large foundation models for music. </p>
<p>Strengths:
- Well-motivated and clearly written. 
- Shows that there is still room to leverage shallow feature learners in the context of MIR.
- Evaluation and ablation studies are well presented</p>
<p>Weaknesses:
The main drawback of the paper is that the method section needs to be described a little bit better. There are some technical aspects that are not clear to the reader. 
- The temporal resolution of the feature vectors seems inconsistent. It would seem like for every 9 second chunk, each feature set has a resolution of 43 ms (22050 / 512). However, that doesn't seem to be the case for the Patches feature set (which is randomly sampled across the entire 9 second chunk)
- The process of converting the codes to the unigram matrix as well as the WMF step can be explained a bit better. The authors should consider adding some mathematical notations to help the reader better understand the different steps within the QF module along with how the dimensionality of the computed features / final embeddings progresses through the different sub-modules (starting from an audio chunk of say length N). 
- Since WMF is such a core part of the proposed method, the authors should provide some background and description about the method. Simply referring to prior work is not sufficient in this case. 
- If space if a concern, the experimental set-up (downstream datasets and metrics) can be compressed by the moving the details to an Appendix / Supplementary material. </p>
<p>There are also a couple of questions related to the experiments that the authors should clarify:
- It is not clear why the final PCA and QF_total modules were excluded for the ablation experiments in Section 5.3.1.
- One thing that seems to be missing in the ablation study is the influence of the G1 modules. While the authors report the metrics just using the G1 module (as a baseline), it would really interesting to report the results of the QFM models with the G1 modules removed. </p>
<p>Other minor comments:
- Line 127: Typo: <code>a quantization</code> -&gt; either <code>a quantization module</code> or <code>the quantization module</code>
- Line 164: Instead of <code>audio chunk</code> it should be <code>feature sets'</code>
- Line 210: Should be <code>(micro, nano)</code>
- Line 243-244: It is a little weird for the default chunk time to be set based on the default setup of the Tempogram features. Isn't that configurable? 
- Line 295-296: 60 (and 200) vectors are sampled from all the vectors in a chunk. It might be useful to add the total number from which these are sampled
- Line 433: "when computation requirements dictate, a primary embedding". it's not clear what primary embedding means here?
- There seems to be a missing reference to the footnote below Line 474</p>
        </div>
      </div>
    </div> 
  </div>
  
    <div id="review3" class="pp-card m-3 collapse">
      <div class="card-body">
        <div class="card-text">
          <div id="review3Example">
            <span class="font-weight-bold">Review 3:</span>
            <br>
            <ul>
<li><strong>Overall Evaluation</strong>: Weak accept</li>
</ul>
<h4>Main Reviews</h4>
<p>In this paper, the authors compare recent deep-learning foundation models with a feature fusion method of classic music features through quantisation and factorisation. A set of features are first separately quantised using KMeans and then factorised using Weighted Matrix Factorisation. The resulting embeddings are then concatenated with the mean and standard deviation of the original features. Finally, feature fusion is achieved by concatenating the single embeddings and applying PCA. The model is trained on the FMA and Million Song Dataset and evaluated on standard downstream tasks through probing. The authors show that for some downstream tasks, this approach produces results comparable to much more complex deep-learning based foundation models.
The paper is well-written and easy to follow. The proposed method is described in detail and the experimental setup is technically correct. However, I would recommend that the authors improve the readability of Figures 2 and 3, or consider reporting the results in two tables. Strictly speaking, the paper's novelty is somewhat limited, but I believe it could stimulate potential discussions in the community.
Generally, I think the authors missed an opportunity to reflect on why such a simple approach (and sometimes even the considered baseline) can compete with much more complex deep-learning foundation models, especially for some tasks. Such an analysis might highlight potential weaknesses of the popular probing-based evaluation or of the considered datasets, and lead to potential improvements in the evaluation of music foundation models in general.</p>
          </div>
        </div>
      </div> 
    </div>
  
  
</div>


<script src="static/js/poster.js"></script>
<script src="static/js/video_selection.js"></script>

      </div>
    </div>
    
    

    <!-- Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "UA-");
    </script>

    <!-- Footer -->
    <footer class="footer bg-light p-4">
      <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2023 ISMIR Organization Committee</p>
      </div>
    </footer>

    <!-- Code for hash tags -->
    <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });
      });
    </script>
    <script src="static/js/lazy_load.js"></script>
    
  </body>
</html>