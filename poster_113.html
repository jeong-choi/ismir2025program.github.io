


<!DOCTYPE html>
<html lang="en">
  <head>
    


    <!-- Required meta tags -->
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />

    <link rel="icon" type="image/png" sizes="32x32" href="static/images/ismir_tab_icon.png">

    <link rel="stylesheet" href="static/css/main.css" />
    <link rel="stylesheet" href="static/css/custom.css" />
    <link rel="stylesheet" href="static/css/lazy_load.css" />
    <link rel="stylesheet" href="static/css/typeahead.css" />
    <link rel="stylesheet" href="static/css/poster.css" />
    <link rel="stylesheet" href="static/css/music.css" />
    <link rel="stylesheet" href="static/css/piece.css" />


    <!-- <link rel="stylesheet" href="https://gitcdn.xyz/repo/wingkwong/jquery-chameleon/master/src/chameleon.min.css"> -->

    <!-- External Javascript libs  -->
    <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js" integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

    <script
      src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
      integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
      crossorigin="anonymous"
    ></script>


    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js" integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js" integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>
    <!-- <script src="static/js/chameleon.js"></script> -->


    <!-- Library libs -->
    <script src="static/js/typeahead.bundle.js"></script>

    

    <!-- External CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css" integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

    <!-- External Fonts (no google for china) -->
    <link
      href="static/css/Lato.css"
      rel="stylesheet"
    />
    <link href="static/css/Exo.css" rel="stylesheet" />
    <link
      href="static/css/Cuprum.css"
      rel="stylesheet"
    />
    <link
      href="static/css/Ambiant.css"
      rel="stylesheet"
    />
    <!-- <link href="static/css/chameleon.css" rel="stylesheet"/> -->
    <title>ISMIR 2025: Optical Music Recognition of Jazz Lead Sheets</title>
    
<meta name="citation_title" content="Optical Music Recognition of Jazz Lead Sheets" />

<meta name="citation_author" content="Juan Carlos Martinez-Sevilla" />

<meta name="citation_author" content="Francesco Foscarin" />

<meta name="citation_author" content="Patricia Garcia-Iasci" />

<meta name="citation_author" content="David Rizo" />

<meta name="citation_author" content="Jorge Calvo-Zaragoza" />

<meta name="citation_author" content="Gerhard Widmer" />

<meta name="citation_publication_date" content="21-25 September 2025" />
<meta name="citation_conference_title" content="Ismir 2025 Hybrid Conference" />
<meta name="citation_inbook_title" content="Proceedings of the First MiniCon Conference" />
<meta name="citation_abstract" content="In this paper, we address the challenge of Optical Music Recognition (OMR) for handwritten jazz lead sheets, a widely used musical score type that encodes melody and chords. The task is challenging due to the presence of chords, a score component not handled by existing OMR systems, and the high variability and quality issues associated with handwritten images. Our contribution is two-fold. We present a novel dataset consisting of 293 handwritten jazz lead sheets of 163 unique pieces, amounting to 2021 total staves aligned with Humdrum **kern and MusicXML ground truth scores. We also supply synthetic score images generated from the ground truth. The second contribution is the development of an OMR model for jazz lead sheets. We discuss specific tokenisation choices related to our kind of data, and the advantages of using synthetic scores and pretrained models. We publicly release all code, data, and models.&lt;br&gt;&lt;br&gt; &lt;b&gt;&lt;p align=&#34;center&#34;&gt;[Direct link to video]()&lt;/b&gt;" />

<meta name="citation_keywords" content="Music retrieval systems" />

<meta name="citation_keywords" content="Knowledge-driven approaches to MIR" />

<meta name="citation_keywords" content="Evaluation, datasets, and reproducibility" />

<meta name="citation_keywords" content="MIR fundamentals and methodology" />

<meta name="citation_keywords" content="Optical music recognition" />

<meta name="citation_keywords" content="Harmony, chords and tonality" />

<meta name="citation_keywords" content="Symbolic music processing" />

<meta name="citation_keywords" content="Musical features and properties" />

<meta name="citation_keywords" content="Novel datasets and use cases" />

<meta name="citation_keywords" content="Machine learning/artificial intelligence for music" />

<meta name="citation_keywords" content="Applications" />

<meta name="citation_keywords" content="MIR tasks" />

<meta name="citation_pdf_url" content="" />
<meta id="yt-id" data-name= />
<meta id="bb-id" data-name= />


  </head>

  <body>
    <!-- NAV -->
    
    <!--

    ('tutorials.html', 'Tutorials'),
    ('index.html, 'Home'),
    ('special_meetings.html', 'Special Meetings'),
    -->

    <nav
      class="navbar sticky-top navbar-expand-lg navbar-light mr-auto"
      id="main-nav"
    >
      <div class="container">
        <a class="navbar-brand" href="https://ismir2025.ismir.net">
          <img
             class="logo" src="static/images/ismir_tabicon.png"
             height="auto"
             width="300px"
          />
        </a>
        
        <button
          class="navbar-toggler"
          type="button"
          data-toggle="collapse"
          data-target="#navbarNav"
          aria-controls="navbarNav"
          aria-expanded="false"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div
          class="collapse navbar-collapse text-right flex-grow-1"
          id="navbarNav"
        >
          <ul class="navbar-nav ml-auto">
            
            <li class="nav-item ">
              <a class="nav-link" href="calendar.html">Schedule</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="papers.html">Papers</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="music.html">Music</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="lbds.html">LBDs</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="industry.html?session=Platinum">Industry</a>
            </li>
            
            <li class="nav-item ">
              <a class="nav-link" href="jobs.html">Jobs</a>
            </li>
            
          </ul>
        </div>
      </div>
    </nav>
    

    
    <!-- User Overrides -->
     

    <div class="container">
      <!-- Tabs -->
      <div class="tabs">
         
      </div>
      <!-- Content -->
      <div class="content">
        

<!-- Title -->
<div class="pp-card m-3" style="">
  <div class="card-header">
    <h2 class="card-title main-title text-center" style="">
      P6-10: Optical Music Recognition of Jazz Lead Sheets
    </h2>
    <h3 class="card-subtitle mb-2 text-muted text-center">
      
      <a href="papers.html?filter=authors&search=Juan Carlos Martinez-Sevilla" class="text-muted"
        >Juan Carlos Martinez-Sevilla</a
      >,
      
      <a href="papers.html?filter=authors&search=Francesco Foscarin" class="text-muted"
        >Francesco Foscarin</a
      >,
      
      <a href="papers.html?filter=authors&search=Patricia Garcia-Iasci" class="text-muted"
        >Patricia Garcia-Iasci</a
      >,
      
      <a href="papers.html?filter=authors&search=David Rizo" class="text-muted"
        >David Rizo</a
      >,
      
      <a href="papers.html?filter=authors&search=Jorge Calvo-Zaragoza" class="text-muted"
        >Jorge Calvo-Zaragoza</a
      >,
      
      <a href="papers.html?filter=authors&search=Gerhard Widmer" class="text-muted"
        >Gerhard Widmer</a
      >
      
    </h3>
    <p class="card-text text-center">
      <span class="">Subjects:</span>
      
      <a
        href="papers.html?filter=keywords&search=Music retrieval systems"
        class="text-secondary text-decoration-none"
        >Music retrieval systems</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Knowledge-driven approaches to MIR"
        class="text-secondary text-decoration-none"
        >Knowledge-driven approaches to MIR</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Evaluation, datasets, and reproducibility"
        class="text-secondary text-decoration-none"
        >Evaluation, datasets, and reproducibility</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=MIR fundamentals and methodology"
        class="text-secondary text-decoration-none"
        >MIR fundamentals and methodology</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Optical music recognition"
        class="text-secondary text-decoration-none"
        >Optical music recognition</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Harmony, chords and tonality"
        class="text-secondary text-decoration-none"
        >Harmony, chords and tonality</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Symbolic music processing"
        class="text-secondary text-decoration-none"
        >Symbolic music processing</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Musical features and properties"
        class="text-secondary text-decoration-none"
        >Musical features and properties</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Novel datasets and use cases"
        class="text-secondary text-decoration-none"
        >Novel datasets and use cases</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Machine learning/artificial intelligence for music"
        class="text-secondary text-decoration-none"
        >Machine learning/artificial intelligence for music</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=Applications"
        class="text-secondary text-decoration-none"
        >Applications</a
      > ; 
      
      <a
        href="papers.html?filter=keywords&search=MIR tasks"
        class="text-secondary text-decoration-none"
        >MIR tasks</a
      > 
      
    </p>
    <h4 class="text-muted text-center">
      Presented In-person, in Daejeon
    </h4>
    <h4 class="text-muted text-center">
      
        4-minute short-format presentation
      
    </h4>

  </div>
</div>
<div style="display: flex; justify-content: center;" class="btn-toolbar mt-4" role="toolbar">
  <div class="poster-buttons btn-group btn-group-toggle mb-3" data-toggle="buttons">
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#details">
      Abstract
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#paper">
      Paper
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#poster">
      Poster
    </button>
    <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#video">
      Video
    </button>
    
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#meta-review">
        Meta
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review1">
        R1
      </button>
      <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review2">
        R2
      </button>
      
        <button class="card-link btn btn-outline-primary" data-toggle="collapse" type="button" data-target="#review3">
          R3
        </button>
      
    
    <!--  -->
  </div>
  
</div>
<div class="poster-content">
  <div id="details" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="abstractExample">
          <span class="font-weight-bold">Abstract:</span>
          <p>In this paper, we address the challenge of Optical Music Recognition (OMR) for handwritten jazz lead sheets, a widely used musical score type that encodes melody and chords. The task is challenging due to the presence of chords, a score component not handled by existing OMR systems, and the high variability and quality issues associated with handwritten images. Our contribution is two-fold. We present a novel dataset consisting of 293 handwritten jazz lead sheets of 163 unique pieces, amounting to 2021 total staves aligned with Humdrum **kern and MusicXML ground truth scores. We also supply synthetic score images generated from the ground truth. The second contribution is the development of an OMR model for jazz lead sheets. We discuss specific tokenisation choices related to our kind of data, and the advantages of using synthetic scores and pretrained models. We publicly release all code, data, and models.<br><br> <b><p align="center"><a href="">Direct link to video</a></b></p>
        </div>
      </div>
      <p></p>
    </div>
  </div>
  <div id="paper" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "https://drive.google.com/file/d/1v-z1EvHPbtTcJZq2exc-zB1WIfy1PcJD/preview#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="poster" class="collapse">
    
    <button class="fullscreen-button btn btn-primary mb-3">Fullscreen</button>
    <iframe class="fullscreen-iframe" src = "#embedded=true" width='100%' height='500px' allowfullscreen webkitallowfullscreen></iframe>
    
  </div>
  <div id="video" class="collapse">
    
      <div  style="display: flex; justify-content: center;">
      <iframe src="" width="960" height="540" allow="encrypted-media" allowfullscreen></iframe>
      </div>
    
  </div>
  
  <div id="meta-review" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="metaReviewExample">
          <span class="font-weight-bold">Meta Review:</span>
          <br>
          <p><strong>Q2 ( I am an expert on the topic of the paper.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q3 ( The title and abstract reflect the content of the paper.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q4 (The paper discusses, cites and compares with all relevant related work.)</strong></p>
<p>Agree</p>
<p><strong>Q6 (Readability and paper organization: The writing and language are clear and structured in a logical manner.)</strong></p>
<p>Agree</p>
<p><strong>Q7 (The paper adheres to ISMIR 2025 submission guidelines (uses the ISMIR 2025 template, has at most 6 pages of technical content followed by “n” pages of references or ethical considerations, references are well formatted). If you selected “No”, please explain the issue in your comments.)</strong></p>
<p>Yes</p>
<p><strong>Q8 (Relevance of the topic to ISMIR: The topic of the paper is relevant to the ISMIR community. Note that submissions of novel music-related topics, tasks, and applications are highly encouraged. If you think that the paper has merit but does not exactly match the topics of ISMIR, please do not simply reject the paper but instead communicate this to the Program Committee Chairs. Please do not penalize the paper when the proposed method can also be applied to non-music domains if it is shown to be useful in music domains.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q9 (Scholarly/scientific quality: The content is scientifically correct.)</strong></p>
<p>Agree</p>
<p><strong>Q11 (Novelty of the paper: The paper provides novel methods, applications, findings or results. Please do not narrowly view "novelty" as only new methods or theories. Papers proposing novel musical applications of existing methods from other research fields are considered novel at ISMIR conferences.)</strong></p>
<p>Strongly agree</p>
<p><strong>Q12 (The paper provides all the necessary details or material to reproduce the results described in the paper. Keep in mind that ISMIR respects the diversity of academic disciplines, backgrounds, and approaches. Although ISMIR has a tradition of publishing open datasets and open-source projects to enhance the scientific reproducibility, ISMIR accepts submissions using proprietary datasets and implementations that are not sharable. Please do not simply reject the paper when proprietary datasets or implementations are used.)</strong></p>
<p>Agree</p>
<p><strong>Q13 (Pioneering proposals: This paper proposes a novel topic, task or application. Since this is intended to encourage brave new ideas and challenges, papers rated “Strongly Agree” and “Agree” can be highlighted, but please do not penalize papers rated “Disagree” or “Strongly Disagree”. Keep in mind that it is often difficult to provide baseline comparisons for novel topics, tasks, or applications. If you think that the novelty is high but the evaluation is weak, please do not simply reject the paper but carefully assess the value of the paper for the community.)</strong></p>
<p>Agree (Novel topic, task, or application)</p>
<p><strong>Q14 (Reusable insights: The paper provides reusable insights (i.e. the capacity to gain an accurate and deep understanding). Such insights may go beyond the scope of the paper, domain or application, in order to build up consistent knowledge across the MIR community.)</strong></p>
<p>Agree</p>
<p><strong>Q15 (Please explain your assessment of reusable insights in the paper.)</strong></p>
<p>The dataset and model could easily be reused in other music domains requiring chord recognition or handwritten score processing. The tokenization strategies and dataset alignment procedures are generalizable and valuable.</p>
<p><strong>Q16 ( Write ONE line (in your own words) with the main take-home message from the paper.)</strong></p>
<p>This paper introduces a novel dataset and model for OMR of handwritten jazz lead sheets, incorporating chord recognition and showing the benefits of synthetic data, pretraining, and symbolic-aware tokenization.</p>
<p><strong>Q17 (This paper is of award-winning quality.)</strong></p>
<p>No</p>
<p><strong>Q19 (Potential to generate discourse: The paper will generate discourse at the ISMIR conference or have a large influence/impact on the future of the ISMIR community.)</strong></p>
<p>Agree</p>
<p><strong>Q20 (Overall evaluation (to be completed before the discussion phase): Please first evaluate before the discussion phase. Keep in mind that minor flaws can be corrected, and should not be a reason to reject a paper. Please familiarize yourself with the reviewer guidelines at https://ismir.net/reviewer-guidelines.)</strong></p>
<p>Strong accept</p>
<p><strong>Q21 (Main review and comments for the authors (to be completed before the discussion phase). Please summarize strengths and weaknesses of the paper. It is essential that you justify the reason for the overall evaluation score in detail. Keep in mind that belittling or sarcastic comments are not appropriate.)</strong></p>
<p>This paper is an important step forward in the development of OMR systems that can handle handwritten lead sheets, particularly in the jazz domain. The introduction of chord recognition into the OMR pipeline is a novel and valuable contribution, especially given the diversity and inconsistency of chord symbol notations. The dataset of 293 handwritten lead sheets and the aligned symbolic formats provide a much-needed resource for the community. The exploration of different tokenization strategies and the demonstration of how synthetic data and pretraining influence performance are well-executed and informative.</p>
<p>Strengths:
- Novel problem (handwritten jazz lead sheet OMR)
- Carefully constructed dataset with region-level annotations
- Thoughtful tokenization and error metric discussions
- Solid experimental framework with reproducibility in mind</p>
<p>Suggestions for improvement:
- A summary figure/table comparing datasets (e.g., CoCoPops, ChoCo, this dataset) would help emphasize uniqueness.
- Some technical sections (e.g., tokenization and metric rationale) would benefit from additional diagrams or summaries.
- The discussion on chord equivalence is insightful; incorporating this more explicitly into training/evaluation might be a next step.</p>
<p><strong>Q22 (Final recommendation (to be completed after the discussion phase) Please give a final recommendation after the discussion phase. In the final recommendation, please do not simply average the scores of the reviewers. Note that the number of recommendation options for reviewers is different from the number of options here. We encourage you to take a stand, and preferably avoid “weak accepts” or “weak rejects” if possible.)</strong></p>
<p>Strong accept</p>
<p><strong>Q23 (Meta-review and final comments for authors (to be completed after the discussion phase))</strong></p>
<p>This paper presents a compelling and timely contribution to the field of Optical Music Recognition (OMR) by addressing the particularly challenging domain of handwritten jazz lead sheets. It introduces a novel dataset of 293 annotated lead sheets, a thoughtful evaluation pipeline, and an adapted transformer-based model trained with synthetic pretraining and symbolic-aware tokenization.</p>
<p>The reviews converge on the significance of the dataset and the careful attention to tokenization and alignment strategies. While some reviewers expressed reservations due to the model architecture being previously established and the dataset's modest size, these factors do not diminish the impact of the paper’s contributions.</p>
<p>All reviewers acknowledged the relevance of the topic, its novelty in scope and application, and the scientific soundness of the methodology. The primary value of this paper lies not in architectural innovation, but in advancing OMR research by addressing a real-world, underexplored, and musically important use case. The dataset and methods are reusable across music genres, and the experiments are thorough and informative, especially in their treatment of chord symbol challenges.</p>
<p>The reviewers provided useful suggestions for enhancement:</p>
<ul>
<li>Clarifying methodological details (e.g., YOLO fine-tuning, decoding strategies, vocabulary scale).</li>
<li>Quantitatively distinguishing between melody and chord recognition errors.</li>
<li>Providing additional analysis or augmentations for image data.</li>
<li>Addressing minor inconsistencies between images and ground truth representations.</li>
</ul>
<p>These are constructive comments that can be easily addressed in the camera-ready version and do not detract from the overall contribution.</p>
<p>This paper sets a strong precedent for research at the intersection of OMR and jazz studies. Its dataset, methods, and findings will undoubtedly stimulate discourse, inspire follow-up work, and contribute foundational tools and benchmarks to the community.</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review1" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review1Example">
          <span class="font-weight-bold">Review 1:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Weak accept</li>
</ul>
<h4>Main Reviews</h4>
<p>This paper presents an Optical Music Recognition (OMR) system for handwritten jazz lead sheets — a particularly challenging domain due to the diverse handwriting styles and complex musical elements, especially chord symbols. The authors introduce a novel dataset of jazz lead sheets, providing ground truth annotations in both Humdrum **kern and MusicXML formats, and they develop a model tailored for this data.</p>
<p>It is clear that the authors have put significant effort into constructing the dataset. Handwritten jazz lead sheets exhibit a high degree of variation and unique characteristics, such as inconsistent chord notations, as discussed in Section 3.4. The resulting dataset includes 2,021 staff regions from 293 handwritten sheets and 2,208 regions from 326 synthetic scores. The authors employed a region identification system using Ultralytics YOLOv8, combined with manual verification to ensure data quality. For the OMR model, they adopted the previously proposed Sheet Music Transformer by Rios-Fila et al., applying only minimal modifications.</p>
<p>This work appears to be an important first step in addressing the specific challenges of handwritten jazz lead sheets, though there remains significant room for improvement. As the authors acknowledge, the transformer-based model may require more substantial adaptation to handle the peculiarities of this dataset effectively. Additionally, I believe the OMR performance — particularly the recognition of chord symbols — could benefit greatly from integrating techniques from the field of Optical Character Recognition (OCR), especially systems designed for complex or cursive handwriting.</p>
<p>Although the model used is not novel and is only lightly modified, this paper holds significance as the first to focus specifically on handwritten jazz lead sheets. The dataset it provides lays the foundation for future research in this area. For these reasons, I believe the work is worthy of presentation at the conference.</p>
        </div>
      </div>
    </div> 
  </div>
  <div id="review2" class="pp-card m-3 collapse">
    <div class="card-body">
      <div class="card-text">
        <div id="review2Example">
          <span class="font-weight-bold">Review 2:</span>
          <br>
          <ul>
<li><strong>Overall Evaluation</strong>: Weak accept</li>
</ul>
<h4>Main Reviews</h4>
<p>This paper presents the first dedicated OMR dataset of handwritten jazz lead sheets, addressing a relevant and previously underexplored area within the OMR community. Lead sheet handwriting is still common among jazz players and students who train their skills.</p>
<p>In general, the paper is adequatly written and easy to follow. The work is methodologically thorough, the preprocessing and evaluation design are in their way novel, considering existing OMR tasks. The chord symbol alignment and tokenization approaches are also interesting. On the other hand, the used model has already been previously proposed by (possibly different) authors. However, the paper includes two significant contributions (dataset and methodology), which I deem satisfactory, considering the page limit of the conference.
Overall, the paper represents a foundational step toward practical OMR, including currently used hand written scores, and including a new genre (in comparison to traditional OMR tasks).</p>
        </div>
      </div>
    </div> 
  </div>
  
    <div id="review3" class="pp-card m-3 collapse">
      <div class="card-body">
        <div class="card-text">
          <div id="review3Example">
            <span class="font-weight-bold">Review 3:</span>
            <br>
            <ul>
<li><strong>Overall Evaluation</strong>: Strong accept</li>
</ul>
<h4>Main Reviews</h4>
<p>This is a good paper. It is written clearly and explains everything from the background to the details of the method in a way that is both approachable and interesting. While it does not present a novel model architecture, it gives a good example of collecting a dataset for a new OMR variant (handwritten jazz lead sheets) and fine-tuning an existing model on it.</p>
<p>Strengths:
* New open dataset for a challenging OMR variant
* Important details (e.g. standardization of chord spellings) are taken care of in the dataset cleaning process and discussed thoroughly
* Explored alternative Humdrum tokenization schemes
* Comprehensive evaluations, both quantitative and qualitative
* Clear explanations with graphical examples
* Code (including new Humdrum tokenizers) and model weights are released</p>
<p>Weaknesses:
* Dataset is rather small
* The model is not novel</p>
<p>Additional comments:
* Line 185: Unclear, did authors fine tune YOLOv8 for region identification? Is this work published anywhere, or can authors add some detail on this?
* Line 313: This described greedy decoding, is this really the sampling method used (as opposed to beam search)?
* Line 339: Vocab size of 1762 is not “very large” compared to popular transformer models (e.g. BERT has 30,522, GPT2 has 50,257). Anyway, since the music sequences here are very short, sequence length is less of a problem.
* Line 525: Data augmentation can (and should) also be applied to the scanned lead sheets (e.g. adding noise, rotating, cropping).
* Figure 3: Why does the example’s ground truth include clef, time signature and key signature, while the image does not?
* Section 5.3: Considering the qualitative observation that most problems are in chord symbols, I’d be interested in a quantitative evaluation of melody errors vs chord errors.
* Line 493: How can the model know the key of the piece, when it only receives the region in Figure 3, which doesn’t include the key signature? (Or is the image cropped?) This might point at a discrepancy in the dataset between the image and the ground truth.</p>
          </div>
        </div>
      </div> 
    </div>
  
  
</div>


<script src="static/js/poster.js"></script>
<script src="static/js/video_selection.js"></script>

      </div>
    </div>
    
    

    <!-- Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=UA-"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "UA-");
    </script>

    <!-- Footer -->
    <footer class="footer bg-light p-4">
      <div class="container">
        <p class="float-right"><a href="#">Back to Top</a></p>
        <p class="text-center">© 2023 ISMIR Organization Committee</p>
      </div>
    </footer>

    <!-- Code for hash tags -->
    <script type="text/javascript">
      $(document).ready(function () {
        if (location.hash !== "") {
          $('a[href="' + location.hash + '"]').tab("show");
        }

        $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
          var hash = $(e.target).attr("href");
          if (hash.substr(0, 1) == "#") {
            var position = $(window).scrollTop();
            location.replace("#" + hash.substr(1));
            $(window).scrollTop(position);
          }
        });
      });
    </script>
    <script src="static/js/lazy_load.js"></script>
    
  </body>
</html>